{"/about/":{"data":{"":" 哈囉大家好，我叫莊品毅 (CHUANG,PIN-YI)，也可以叫我 Ian，目前是一位 System Architect (SA) 工程師，負責規劃以及測試公司的系統架構，並且協助技術團隊解決系統相關的問題。 協助公司導入災難復原 (GCP \u003e AWS)，並且負責 AWS 服務的架構設計、導入與權限管理，確保公司基礎設施具備高擴展性與靈活性。 以及協助導入 Datadog ，來整合公司內分散的監控系統。\n除此之後，也熟悉使用 Google Cloud Platform (GCP) 雲端相關服務，喜歡使用 Terraform + Terragrunt 來管理雲端大量的 IaC 資源 (GCP Module：12、AWS Module：22)，當然也會使用 Grafana、Prometheus、EFK 等監控 Log 收集工具來確保服務的穩定性以及可追蹤性。並協助 RD 建立 CICD 部署流程。\n在下班空閒時間，我喜歡閱讀技術相關的文件、寫部落格，也會參加一些線下技術社群的活動，曾參加過：Google Next 2025、AWS Summit 2025、DevOpsDay 2022|2023|2024、Cloud Summit 2024|2025，希望能夠透過這些活動來學習更多的知識，並且與更多的技術人員交流。\n歡迎大家使用下方 giscus 留言系統留言交流 d(`･∀･)b。","履歷#履歷":"\n中文版最後上傳日期：2025-08-03 New 英文版最後上傳日期：2025-08-03 New ","工作經驗#工作經驗":" 凡谷興業有限公司 - SA 架構師 (2024/10/01 - 現在)\nAmazon Web Services (AWS) 服務架構設計、導入與權限管理\n設計公司多雲架構，規劃從 GCP 災難備援至 AWS，確保基礎設施具備高擴展性與靈活性。 開發 22 組 Terraform 模組，實現 IaC（基礎設施即程式碼），確保環境一致性並加速團隊上手時間與維運成本。 撰寫 40+ 篇技術文件，涵蓋 AWS IaC 指南、AWS 與 GCP 服務比較、Graviton 遷移效能及優化成本分析等、各項測試報告。 負責多個專案的服務建置、CICD 自動化部署，確保系統能在 AWS 穩定且高效運行。 協助 RD 研究與部署 AWS macOS GitLab Runner 進行 iOS APP 打包，負責資源建立、問題排查與最佳化，確保 CI/CD 流程順暢運行。 與 AWS 代理商合作，導入 AWS Organizations 及 IAM Identity Center，統一帳戶與權限管理，簡化資源治理並確保安全與合規。 雲端服務架構重構與內網識別優化\n重新設計整體服務架構，導入 Internal DNS，實現服務去 IP 化，提升內部 API 識別性與一致性，同時考量 GCP、AWS 跨雲通用性，並為後續導入 Service Mesh 奠定基礎。 優化網路架構，將部分需經地端處理的流量調整為透過雲端服務處理，成功減少地端設備負載與專線依賴，提升系統穩定與維運效率，並大幅降低設備與頻寬相關成本。 將服務動靜態分離，搭配加速 CDN，強化前端靜態資源加載效率與用戶體驗。 Datadog 監控系統導入與實作\n主導技術處導入 Datadog，整合原本分散於各單位的監控系統，提升監控資料集中性與可視化，查詢效率提升達 90%，大幅加速跨單位除錯與協調流程。 與 Datadog 官方技術團隊深入合作，確保公司需求符合最佳實作方案。 撰寫相關技術文件，協助團隊快速導入，確保監控系統長期穩定運維。 技術趨勢追蹤與問題解決\n主動追蹤 GCP GKE 非註冊發布通道的強制升級公告，撰寫應對方案與檢查腳本，協助團隊避免升級造成服務中斷，並防止提前切換至 Extended Channel 而產生約 $3,000 美金開支。\n關注 Docker Image Pull Rate Limit 政策變更，提出解決方案並驗證其可行性，確保 CI/CD 流程穩定運作不受限制影響。\n將最新技術公告與因應措施整理後，於公開會議中向全技術處同仁分享，有效提升團隊對技術變動的掌握與應對效率。\n凡谷興業有限公司 - SRE 工程師 (2022/02/07 - 2024/10/01)\n導入與優化 IaC 工具鏈，建構一致且自動化的基礎設施部署流程\n導入 Helmfile 管理 50+ 微服務的 Helm Chart，實現 Kubernetes 應用部署流程的一致性與可重複性，提升系統可維護性與部署標準化。 重構 Terragrunt 架構，統一 Terraform 模組與目錄結構，強化 IaC 管理可讀性，明顯提升維運效率與團隊協作品質。 開發 12 組 Terraform 模組，推動基礎設施模組化與標準化，取代 UI 操作以避免設定偏差，並導入版本控制機制，大幅降低人為錯誤與維運風險。 建立涵蓋 Helmfile 與 IaC 的 CI/CD 自動化流程，加速服務部署與基礎設施環境重建，縮短交付時間。 落實最小權限原則，系統性清查移除冗餘 IAM 權限，強化基礎設施安全性並符合內部合規要求。 GCP 成本優化實踐\n優化 Prometheus 監控架構，調整 Samples Ingested 設定，針對指標來源進行分析與過濾，在不影響監控品質的前提下，大幅降低儲存與計算資源，每日節省約 $200 美金。 與 RD 協作優化 Log 結構，剔除無效與冗餘訊息，減少不必要的儲存成本，顯著降低日誌佔用空間與長期儲存費用。 熟悉 GCP 生態系統與 SRE / DevOps 實務操作\n熟悉 GCP 各項核心服務，包含 GKE、GCE、GCS、Cloud Load Balancing、Cloud SQL、Memorystore、VPC、IAM 等，具備全善的建置與維運經驗。 熟練使用 GitLab、GitHub、Jenkins、Ansible 等工具，規劃並實作 CI/CD 自動化部署流程，提升開發與交付效率。 建置完整監控與日誌系統，使用 Fluentd、Elasticsearch、Kibana 搭配 Google Managed Prometheus，實現效能監控與問題追蹤。 ","聯絡方式#聯絡方式":" Email：880831ian@gmail.com LinkedIn：https://www.linkedin.com/in/pinyi/ GitHub：https://github.com/880831ian Telegram：https://t.me/pinyichuchu ","證照#證照":"\nGoogle Cloud Professional Cloud Architect證照編號: ed519d0fcb984d428460841eb83419c8 發照日期： Feb 21, 2025 / 到期日：Feb 21, 2027 GCP RED HAT CERTIFIED ENGINEER (RHCE)證照編號: 190-008-011 發照日期： July 5, 2019 / 到期日：July 5, 2021 Red Hat RED HAT CERTIFIED SYSTEM ADMINISTRATOR (RHCSA)證照編號: 190-008-011 發照日期： Jan 11, 2019 / 到期日：Jan 11, 2021 Red Hat "},"title":"關於我"},"/blog/":{"data":{"":"","介紹#介紹":"👋 你好、妳好、大家好，歡迎來到我的秘密花園，這邊主要會記錄我研究一個新的技術或工具、以及處理一些 SA、SRE 遇到的靈異事件問題的小天地。\n會開始寫 Blog 的初衷主要是我的小腦袋瓜，如果不寫下來，過陣子很容易就忘記 (´_ゝ`)，當然也希望可以幫助到有相同問題的人 (可以使用搜尋功能來快速找到相關文件喔)，如果有任何問題或建議，歡迎在下方留言。","聲明#聲明":"由於在學習新的技術或工具時，會參考網路上許多的文件和照片，雖然會加上自己的見解與實作內容改寫而成，且會於文章後附上相關資料來源，如有侵犯到您的權益，請於下方告知，我會立即刪除相關內容，謝謝 (๑•́ ₃ •̀๑)。"},"title":"Blog"},"/blog/agile/":{"data":{"":"此分類包含 敏捷開發 (Agile) 相關的文章。\n串接 Jira 工單自動化通知到 Google Chat發布日期：2024-08-06 "},"title":"敏捷開發 (Agile)"},"/blog/agile/jira-to-google-chat/":{"data":{"":"","介紹#介紹":"我們使用 Jira 的 Ticket 來當作工單，其他單位如果需要請 SRE 協助，都需要填寫此工單，我們的流程是以下：\n待審核 -\u003e SRE 已簽核 -\u003e 進行中 -\u003e (阻塞) -\u003e 完成 工單看板\n我們會由資深工程師來審核，狀態會從 待審核 變成 SRE 已簽核，這時候我們希望能夠收到通知，才由當週值班的工程師來進行處理。","使用-google-chat-card#使用 Google Chat Card":"上面的教學已經完成簡單的通知，但訊息顯示的方式不夠直覺，因此我們可以使用 Google Chat Card 來顯示更多資訊。\nGoogle Chat Cards v2 提供了更多的選項，例如：header、sections、widgets、buttons 等等，可以讓訊息更加豐富。\n官方 Card 範例\n目前 Card 只能使用 v2 版本，它實際上的格式就是一個 JSON，我們可以使用官方提供的 UI Kit Builder 來幫助我們建立 Card。\n可以從左側，選擇想要的元件來自訂 card，右側會預覽顯示結果，最後直接複製 JSON 內容，再貼到 Jira 的自訂資料裡面。\nUI Kit Builder 工具\n最後要注意的是，使用 UI Kit Builder 所產生的 JSON 格式，不是 Google Chat Card 的最終格式，需要自行修改 (很神奇吧 ┐(´д`)┌)。\n需要再多以下的架構，才會正常運作：\n{ \"cardsV2\": [ { \"cardId\": \"unique-card-id\", \"card\": { \u003c\u003c這邊才放入 UI Kit Builder 產生的 JSON 內容\u003e\u003e } } ] } 最終成果如下：\n最終成果\n有太多內容要馬賽克 (´・ω・`)","參考資料#參考資料":"Google Chat Cards v2：https://developers.google.com/workspace/chat/api/reference/rest/v1/cards\nUI Kit Builder：https://addons.gsuite.google.com/uikit/builder?hl=zh-tw\nGoogle Icon：https://fonts.google.com/icons","如何送通知到-google-chat#如何送通知到 Google Chat":"由於公司目前使用 Google Chat 來當作通訊工具，所以我希望能夠在 Jira 工單的狀態變更時，能夠自動送通知到 Google Chat。\n那 Google Chat 有提供幾種方式可以串接，我們這邊用 Webhook 來串接。\n建立聊天室 我們先建立一個聊天室，名稱跟詳細設定就請自行設定\n建立聊天室\n選擇應用程式與整合 進入聊天室，點選聊天室名稱 (例如：jira 串接 google chat 測試)，選擇應用程式與整合\n點選應用程式與整合\n新增 Webhook 選擇 Webhook，並且點擊新增 Webhook，輸入名稱以及自定義的圖片，再點擊儲存\n建立 Webhook\n複製 Webhook 網址 建立完成後，就會在底下出現剛剛建立的 Webhook，點選複製 Webhook 網址\n複製 Webhook 網址\n測試 Webhook 我們可以使用 curl 來測試一下 Webhook 是否正常，可以參考以下指令：\ncurl -X POST \"\u003c\u003c請帶入剛剛所複製的 Google Chat Webhook 連結\u003e\u003e\" \\ -H \"Content-Type: application/json; charset=UTF-8\" \\ -d '{\"text\": \"Hello World\"}' 在剛剛建立的聊天室裡面，有出現 Hello World 就代表成功了\n測試 Webhook 成功","設定-jira#設定 Jira":"接下來我們要設定 Jira，當工單狀態變更時，就會送通知到 Google Chat，我們會使用到 Jira 的工單專案的自動化功能。\n自動化可以自訂觸發條件，也可以寫判斷，針對不同的情境來做不同的動作。\n下面是目前我們的設定，當工單狀態變更時，從 To Do 變成 Approved by SRE，就會觸發此自動化。\n範例自動化設定\n由於我們 SRE 內部有拆組別，所以需要判斷這個工單申請的 RD 單位，是哪個小組負責的，我們這邊會使用 if 條件，來判斷摘要是否包含對應小組的關鍵字。\n最後 Then 如果符合條件，就會使用 『傳送網路要求』 這個動作來送通知到 Google Chat。\n動作選項\n打開傳送網路要求，在 Web 要求 URL，就是貼上剛剛在 Google Chat 建立的 Webhook 連結，HTTP 方法選擇 Post，下面就可以再自訂資料裡面設定想要傳送的內容。\n如果要傳送的內容是變數，例如工單緊急程度、工單號碼等等，就需要使用 {{}} 來包住變數，例如：{{issue.key}}、{{issue.fields.summary}}、{{issue.fields.status.name}} 等等。\n不知道有那些變數，可以點擊圖片箭頭的 {} 圖示，裡面會告訴你哪些變數可以使用：\n變數參考\n如果是使用套件，例如：Template，變數就不會出現在 {} 圖示裡面，可以直接帶入 {{issue.template}}。"},"title":"串接 Jira 工單自動化通知到 Google Chat"},"/blog/aws/":{"data":{"":"此分類包含 Amazon Web Services 相關的文章。\n手動切 AWS Profile 太麻煩？試試 Granted，多帳號管理神器助你效率倍增！發布日期：2025-07-06 AWS SSO Profile 設定介紹發布日期：2025-07-06 如何共用 ECR 給同一個 AWS Organization 的其他 AWS Account EKS 使用發布日期：2025-06-25 AWS EKS Pod 出現 aws-cni failed to assign an IP address to container 錯誤發布日期：2024-12-11 Amazon Virtual Private Cloud (VPC) 介紹發布日期：2024-09-02 如何使用 MFA Token 驗證 AWS CLI發布日期：2024-08-01 Elastic Kubernetes Service (EKS) 介紹發布日期：2024-07-26 Identity and Access Management (IAM) 介紹發布日期：2024-07-26 設定 AWS CLI 以及 AWS CLI 指令說明發布日期：2024-07-26 "},"title":"Amazon Web Services"},"/blog/aws/aws-assuming-roles-tool-introduce/":{"data":{"":"由於 AWS 的服務越來越多，很多時候我們需要在不同的 AWS Account 之間切換來操作，不管是使用單純使用 IAM User 產生 Access Key，或是用 AWS SSO，都會需要頻繁的切換 AWS Profile，如果 AWS SSO 每一個 Account 還有不同的 Permission Set，那就更麻煩了。\n因此如果有一套好用的工具，可以讓我們更方便的切換 AWS Profile，那就太棒了！\n今天就來介紹一個這樣的工具，叫做 Granted。","什麼是-granted#什麼是 Granted？":"Granted 是一個用來管理多個 AWS Account 的工具，它可以讓你更方便的切換 AWS Profile，還支援使用瀏覽器開啟對應的 AWS Account Console 管理介面。\nGranted","參考資料#參考資料":"Granted 官方網站：https://granted.dev/","如何使用-granted#如何使用 Granted":"在使用前，我們需要先做一些初始化，可以先 assume 來設定：\nassume [i] Thanks for using Granted! [i] By default, Granted will open the AWS console with this browser: Chrome [!] Granted works best with Firefox but also supports Chrome, Brave, and Edge (https://docs.commonfate.io/granted/introduction#supported-browsers). You can change this setting later by running 'granted browser set' ? Use Firefox as default Granted browser? (y/N) 這邊可以先設定預設的瀏覽器，建議使用 Firefox，因為 Granted 在 Firefox 上的支援度較好。\n提示\n如果已經設定好預設瀏覽器，之後想要調整，可以使用 granted browser 來更新。\n第二次再重新執行一次 assume，就會出現以下畫面：\nassume ℹ️ To assume roles with Granted, we need to add an alias to your shell profile (https://docs.commonfate.io/granted/internals/shell-alias). ? Install zsh alias at /Users/\u003cusername\u003e/.zshenv Yes Added the Granted alias to /Users/\u003cusername\u003e/.zshenv Shell restart required to apply changes: please open a new terminal window and re-run your command. 來設定 shell 的 alias，這樣就可以在任何地方使用 assume 指令了。","如何安裝-granted#如何安裝 Granted":" 首先，我們使用 Homebrew 來安裝 Granted： brew tap common-fate/granted brew install granted （如果其他作業系統，請參考 Granted 官方網站 的安裝說明）\n安裝完成後，使用以下指令確定 Granted 已經安裝成功： granted --version 如果有顯示版本，就代表安裝成功，例如：\nGranted version: 0.38.0 ","測試切換-aws-profile#測試切換 AWS Profile":"接下來我們可以先測試一下 assume 指令是否可以正常切換 AWS Profile。\n測試 assume\n提示\n上面出現的 Profile 名稱是從 ~/.aws/config 檔案中取得的，如果沒有設定過 AWS Profile，可以上一篇文章 AWS SSO Profile 設定介紹 來設定。\n切換成功後，會顯示如下： 如果沒有驗證，或是需要重新驗證，一樣會跳出瀏覽器來驗證\n切換成功\n可以看到，這個登入的 Profile 有效期限是 12 小時，這樣就可以在這個時間內使用這個 Profile 來操作 AWS 服務了。\n要怎麼知道目前登入的是哪一個 Profile 呢？可以使用以下指令：\naws sts get-caller-identity --no-cli-pager 顯示當前 Profile\n我們也再次切換 Profile，並使用指令查看是否有正確切換到新的 Profile：\n切換成功","開啟-aws-console#開啟 AWS Console":"有時候我們除了使用 CLI 指令來操作或是查看，更多的時候，還是會開啟 AWS Console 來進行操作。\n那以往我們都需要到 AWS SSO 網頁來選擇對應的 AWS Account 以及 Permission Set，才可以進到對應帳號來查看相關資源。\n現在我們可以在 assume 後面加上 --console 或是 -c 參數，這樣就可以直接開啟對應 AWS Account 的 Permission Set 權限的 AWS Console 了：\n開啟 AWS Console\n或是多加上 --serviceor -s 參數，這樣就可以直接開啟對應的 AWS 服務頁面：\n開啟 AWS 服務頁面\n可以快速開啟的資源清單可以參考：https://github.com/fwdcloudsec/granted/blob/main/pkg/console/service_map.go"},"title":"手動切 AWS Profile 太麻煩？試試 Granted，多帳號管理神器助你效率倍增！"},"/blog/aws/aws-cli/":{"data":{"":"","介紹#介紹":"AWS CLI 是用於使用 AWS 服務的命令列工具。它也用於對 IAM 使用者或角色進行身份驗證，以便從本機電腦存取 Amazon EKS 叢集和其他 AWS 資源。若要從命令列在 AWS 中設定資源，您需要取得要在命令列中使用的 AWS 存取金鑰 ID 和金鑰。","參考資料#參考資料":"探索 AWS：從菜鳥到熟練的完全指南(四)IAM 登入方式：https://hackmd.io/@gdw7l5sPTOyNv76kZ_twjA/SkTPl-xP3\n設定 AWS CLI：https://docs.aws.amazon.com/eks/latest/userguide/install-awscli.html\nAWS CLI 指令參考：https://docs.aws.amazon.com/zh_tw/cli/latest/userguide/cli_code_examples_categorized.html","如何安裝-aws-cli#如何安裝 AWS CLI":"Mac 安裝指令：\nbrew install awscli 自動補齊指令：\ncomplete -C '/usr/local/bin/aws_completer' aws ","如何設定-aws-cli#如何設定 AWS CLI":"AWS 跟 GCP 比較不一樣的是 CLI 在 Google (gcloud) 可以使用 Web 登入的方式來驗證 gcloud auto login，但是在 AWS 需要使用 Access Key ID 和 Secret Access Key 來驗證。\n因此我們在開始使用前，需要先透過 UI 產生自己帳號的 Access Key ID 和 Secret Access Key，才能夠有權限下指令。\n建立存取密鑰 需要先確認是否有以下的 IAM 權限：\n\"Action\": [ \"iam:CreateAccessKey\", \"iam:DeleteAccessKey\", \"iam:GetAccessKeyLastUsed\", \"iam:ListAccessKeys\", \"iam:UpdateAccessKey\", \"iam:TagUser\" ] TagUser：是用來幫金鑰加上標籤值，方便管理。 沒有權限會噴錯\n先登入 AWS 的管理控制台。 點擊右上角的 AWS 使用者名稱，並點選「安全憑證」。 在 AWS 的管理控制台，選擇安全憑證\n找到存取金鑰，並點選「建立存取金鑰」，如果有建立過，也會顯示在這邊。 建立存取金鑰\n點選「命令列介面 (CLI)」，下方要勾選一個確認建議的選項，最後按下一步。 選擇命令列介面 (CLI)案例\n可以選擇是否要建立標籤，如果要建立，需要前面提到的 iam:TagUser IAM 權限。 設定描述標籤\n就可以拿到存取金鑰 (Access Key ID) 以及私密存取金鑰 (Secret Access Key)，要記住私密存取金鑰，只有這個畫面可以拿到，如果沒有儲存，就需要重新建立一個新的金鑰。 產生完金鑰\n配置 AWS CLI 接下來我們要在本機把剛剛產生的 Access Key ID 和 Secret Access Key 設定到 AWS CLI，我們可以下這個指令。 aws configure 下完後，就會顯示以下設定畫面，分別輸入 Access Key ID、Secret Access Key、預設區域以及輸出格式。 AWS Access Key ID [None]: AAKIA2HCM5WKG7R647HTV AWS Secret Access Key [None]: ****************************** Default region name [None]: region-code (例如：亞太地區東京 ap-northeast-1) Default output format [None]: json 就代表成功設定完成囉 ლ(́◕◞౪◟◕‵ლ)\n另外，設定的內容會寫到 ~/.aws/credentials 和 ~/.aws/config 這兩個檔案中，如果想要修改設定，也可以直接修改這兩個檔案。\n~/.aws/credentials[default] aws_access_key_id = AAKIA2HCM5WKG7R647HTV aws_secret_access_key = t5CMmR4Y7cuFC/RgxJS1XXXXXXXXXXXXXXXXXXXX ~/.aws/config[default] region = ap-northeast-1 ","常用-的-aws-cli-指令說明#常用 的 AWS CLI 指令說明":" 指令 描述 aws sts get-caller-identity 驗證使用者身份 aws eks list-clusters --region {REGION} 列出區域的 EKS aws iam list-mfa-devices --user-name {USER_NAME} 列出使用者的 MFA 裝置 aws sts get-session-token --duration-seconds 129600 --serial-number ${MFA_DEVICES_NUMBER} --token-code ${MFA_CODE} 取得 MFA Token "},"title":"設定 AWS CLI 以及 AWS CLI 指令說明"},"/blog/aws/aws-cni-failed-to-assign-ip/":{"data":{"":"此文章是我在公司擔任架構師，在負責稽核資源使用時，有發現有同仁的 AWS EKS node_group 資源開太大 (開發環境使用 2xlarge 機器規格)，所以先手動從 UI 去調整 Node 的機器規格 (RD 未提供建立時的 Terraform 版控)，從 m7g.2xlarge 先改成 m7g.large。\n但由於同仁是用 Terraform 來建置整座 EKS + node_group，有 Launch Templates 以及 Auto Scaling Groups 等元件，能調整機器規格的只能從 Launch Templates 以及 Auto Scaling Groups，但 Launch Templates 沒辦法編輯 (只能新增、刪除)，當時為了想要先快速的減少花費，因此先直接調整 Auto Scaling Groups 的 Node 機器規格。\nAuto Scaling Groups 設定","參考資料#參考資料":"Maximum IP addresses per network interface：https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AvailableIpPerENI.html","尋找問題#尋找問題":"排除 Subnet IP 用完的情況 一開始看錯誤訊息 aws-cni failed to assign an IP address to container 以為是 subnet 的 IP 已經被 Pod 用完了，所以先使用以下指令來查看 Subnet 的可用 IP 數量：\naws ec2 describe-subnets \\ --query \"Subnets[*].{SubnetId:SubnetId, AvailableIps:AvailableIpAddressCount, CidrBlock:CidrBlock}\" \\ --output table （輸出範例，非當下 Subnet）\n也可以從 UI 去看每一個 Subnet 的可用 IP 是多少：\nAWS UI 查看 Subnet 可用 IP\nAWS Subnet 計算方式：\n假設是 /24 計算是： 32-24=8，2 的 8 次方 = 256\nAWS 會保留 5 個 IP，如下：\nXX.XX.XX.0：網路位置\nXX.XX.XX.1：VPC 路由器位址\nXX.XX.XX.2：Amazon DNS 位址\nXX.XX.XX.3：保留給未來用途\nXX.XX.XX.255：廣播位址\n所以最多可用的 IP 數量是 256-5=251\n排除 Pod 數量超過 Node 的限制 排除了 Subnet 用完的情況，後來搜尋了一下文件，發現 AWS 的機器規格，會限制一個 Node 上面，最多能長有幾個 Pod，這個跟 Amazon VPC CNI 插件有關，下面附上一些相關的問題連結：\nNon fatal but persistent warning: “Failed to create pod sandbox … failed to assign an IP address to container.” #3066\nAWS VPC CNI PLUGIN - Error: container runtime network not ready due to NetworkPluginNotReady - How to Resolve\n那要怎麼計算什麼機器規格，Pod 的最大限制是多少，可以用這個公式來計算\nENI * (# of IPv4 per ENI - 1) + 2 可以先用這個指令來看 ENI 以及 IPv4 per ENI 各是多少：\naws ec2 describe-instance-types \\ --filters \"Name=instance-type,Values=c5.*\" \\ --query \"InstanceTypes[].{ \\ Type: InstanceType, \\ MaxENI: NetworkInfo.MaximumNetworkInterfaces, \\ IPv4addr: NetworkInfo.Ipv4AddressesPerInterface}\" \\ --output table Maximum IP addresses per network interface\n每個機器規格對應的 IP 以及 MaxENI\n已 c5.4xlarge 來說，就是 8*(30-1)+2 = 234，一個 Node 最多 234 個 Pod，c5.xlarge 來說，就是 4*(15-1)+2 = 58，一個 Node 最多 58 個 Pod。\n但如果每次都要自己計算，會有點麻煩，所以這邊再提供幾個方式：\n官方已經計算好的對應表：https://github.com/awslabs/amazon-eks-ami/blob/main/templates/shared/runtime/eni-max-pods.txt 每個 Amazon EC2 執行個體類型建議的最大 Pod 數腳本：Amazon EKS recommended maximum Pods for each Amazon EC2 instance type (但這個計算最高只會顯示 110，詳細可以看文件) 使用腳本計算不同 CNI version 以及機器的可用 Pod 數量\n但是，如果是超過 Node 的 Pod 最高限制，應該也會是顯示以下的無法調度錯誤，而不是 aws-cni failed to assign an IP address to container 錯誤：\n無法調度的範例錯誤","找到問題#找到問題":"最後發現，因為我們最一開始只調整了 Auto Scaling Groups 的機器規格，調整完後，資源的確是降低了（從 8 vCPUs / 32.0 GiB → 2 vCPUs / 8.0 GiB），但 Node 的 Allocatable pod 卻還是原本的數量限制，原因是 Controller plane 不知道我們有調整 (Auto Scaling Groups 只會調整 node_group)。\n因此，當服務開上去，Node 能長的 Pod 上限還是舊的 m7g.2xlarge 58 個，但對於 aws-node 來說，cni 最多只能 m7g.large 29 個，所以超過 29 的都會出現這個錯誤，把它趕到其他 Node (還沒有滿 29 上限) 就正常。\n可以用這個指令看到目前 Node 的 HOSTNAME、instance-type、allocatable.pods： kubectl get nodes -o custom-columns=\"HOSTNAME:.metadata.name,INSTANCE-TYPE:.metadata.labels.node\\\\.kubernetes\\\\.io/instance-type,ALLOCATABLE-PODS:.status.allocatable.pods\" 有問題的 Node 有問題的 Node Allocatable-Pods\n正常的 Node 正常的 Node Allocatable-Pods","解決問題#解決問題":"所以我們要避免這個問題有兩個解法：\n更換機器規格需要重建 Launch Templates，用先建後拆，不能只更新 Auto Scaling Groups。 用 Terraform 做完整的管理。 ","遇到問題#遇到問題":"調整完後，再使用 Auto Scaling Groups 的 instance refresh 去更新機器規格，這時候會發現，新 Node 上的 Pod 會偶發出現以下錯誤訊息：\nFailed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"XXXXXXXXX\": plugin type=\"aws-cni\" name=\"aws-cni\" failed (add): add cmd: failed to assign an IP address to container "},"title":"AWS EKS Pod 出現 aws-cni failed to assign an IP address to container 錯誤"},"/blog/aws/aws-ecr-permissions/":{"data":{"":"這篇主要是因為公司目前的 AWS 架構，會將各單位的系統依照產品、環境給拆分，但有時候部署，總會有一些共用的 Image。 在這之前，我們需要一個一個將相同的 Image 搬到每個 AWS Account 的 ECR，當時就在想，是否可以直接共用 ECR 給同一個 AWS Organization 的其他 AWS Account 使用 (當然也不要用 Public ECR 來避免安全性問題)。\n範例 AWS Account 說明\n為了方便說明，我會使用兩個 AWS Account 來做範例，632xxxxx1762 (後面簡稱 A 帳號) 以及 722xxxxx9750 (後面簡稱 B 帳號)，這兩個帳號都屬於同一個 AWS Organization。\n那我們就直接來看要如何設定，首先我先在 632xxxxx1762 (後面簡稱 A 帳號) 放了一個 ECR，名稱是：audit/sa/cicd-runner，Tag 是 latest，它會是我之後想要共享給其他 AWS Account 使用的 ECR。\nA 帳號 ECR\n那如果我們使用另一個 AWS Account 722xxxxx9750 (後面簡稱 B 帳號) 直接拉這個 ECR 的 Image，會發生什麼錯誤呢？\n我們先切換到 B 帳號，然後使用 kubectl 來部署一個 Pod，Pod 的 Image 寫 A 帳號的 ECR Image 名稱。\ncicd-runner.yaml apiVersion: apps/v1 kind: Pod metadata: name: cicd-runner spec: containers: - name: cicd-runner image: 632xxxxx1762.dkr.ecr.ap-southeast-1.amazonaws.com/audit/sa/cicd-runner:latest imagePullPolicy: Always 觀察一下 Pod 的狀態，會發現 Pod 會出現 ErrImagePull 的錯誤，這是因為 B 帳號沒有權限去拉取 A 帳號的 ECR Image。\nPod ErrImagePull 錯誤\n那要怎麼解決呢？！ 以及我們要怎麼讓 B 帳號可以拉取 A 帳號的 ECR Image 呢？\n其實很簡單，我們只需要在 A 帳號的 ECR 上設定一個 IAM Policy，讓 B 帳號可以拉取這個 ECR 的 Image。\n如果以 Console 來設定的話，我們可以在 A 帳號的 ECR 上，點選 Private registry \u003e Permissions，然後選擇 Edit policy JSON。 (如果之後使用 Terraform 來設定的話，可以參考 AWS ECR Repository Policy 的相關文件。)\n將以下的 JSON 給貼進去，當然相關的設定可以再自行調整：\n{ \"Version\": \"2008-10-17\", \"Statement\": [ { \"Sid\": \"AllowPullFromOrg\", \"Effect\": \"Allow\", \"Principal\": \"*\", \"Action\": [ \"ecr:BatchCheckLayerAvailability\", \"ecr:BatchGetImage\", \"ecr:GetDownloadUrlForLayer\" ], \"Condition\": { \"StringEquals\": { \"aws:PrincipalOrgID\": \"o-xxxxxxx\" } } } ] } 這裡的 o-xxxxxxx 是你的 AWS Organization ID，可以在 AWS Organizations 的 Console 中找到。\n簡單說明一下這邊的設定：\nPrincipal 設定為 *，表示允許所有人。 Action 設定為 ECR 的相關拉取權限。 Condition 設定為 aws:PrincipalOrgID，這樣就限制了只有同一個 AWS Organization 的帳號可以拉取這個 ECR 的 Image。(當然我們也可以改指定特定的 AWS Account ID，這樣就只有指定的帳號可以拉取。) 設定完畢後，我們可以回到 B 帳號，重新部署剛剛的 Pod。\nkubectl apply -f cicd-runner.yaml 就可以發現 Pod 成功啟動了，並且可以正常拉取 A 帳號的 ECR Image。\nPod 成功啟動\n這樣就完成了如何共用 ECR 給同一個 AWS Organization 的其他 AWS Account 使用的設定。"},"title":"如何共用 ECR 給同一個 AWS Organization 的其他 AWS Account EKS 使用"},"/blog/aws/aws-sso-profile-introduce/":{"data":{"":"之前有介紹過 設定 AWS CLI 以及 AWS CLI 指令說明，大家應該已經知道要怎麼安裝 AWS CLI，以及設定 AWS IAM User 的 Access Key。\n而這次要介紹的是導入 AWS Organizations 以及 AWS IAM Identity Center(SSO) 後，要如何設定 AWS SSO Profile，用來存取 AWS Organizations 中的各個 AWS Account 以及對應的 Permission Set。","aws-configure-sso-session#aws configure sso-session":"或是你也可以使用 aws configure sso-session 指令來設定 SSO Session，基本上流程跟 aws configure sso 類似，但就不會協助設定後面的 Profile。\n設定 aws configure sso-session\n那我們在使用 Profile 的時候，可以直接在指令後面多加上 --profile \u003cyour-profile-name\u003e 來指定要使用的 Profile。 但這個方式很麻煩，所以我們下一篇會介紹「手動切 AWS Profile 太麻煩？試試 Granted，多帳號管理神器助你效率倍增！」這個工具，可以讓你更方便地管理以及切換多個 AWS Profile。","aws-configure-sso-設定#aws configure sso 設定":"步驟也很簡單，首先可以使用 AWS CLI 的 aws configure sso 指令來設定 SSO Profile，這個指令會引導你完成整個設定。\naws configure sso 系統會要求你輸入一些資訊，例如 SSO Session 的名稱、SSO 的起始 URL、SSO 區域等等。\n會像是下方圖片一樣，出現一個連結，也會自動幫你開瀏覽器，要你登入 AWS IAM Identity Center 來進行驗證。\n設定 aws configure sso\n瀏覽器開啟後，如果有登入完畢，會需要授予 Applications and AWS accounts 權限：\n看到這個畫面，接著回到終端機，系統會要求你選擇要存取的 AWS Account 以及對應的 Permission Set：\n會跳出一個選單，讓你選擇要存取的 AWS Account。你可以使用上下鍵來選擇，然後按下 Enter 鍵來確認。\n這邊會協助建立一個新的 Profile，並且會自動填入相關的資訊。\n選擇 AWS Account\n接著，選擇這個 AWS Account 後，如果有不同的 Permission Set，你可以再選擇對應的 Permission Set：\n設定 Permission Set\n最後如果跳出這個畫面，就代表已經設定完成了！\n可以使用以下指令來確認是否有設定成功：\naws sts get-caller-identity --profile \u003cyour-profile-name\u003e 設定 aws configure sso 完成\n檢視設定的 Profile 我們也可以來看一下 ~/.aws/config 檔案，裡面會有剛剛設定的 Profile 資訊：\n[profile AdministratorAccess-72XXXXXXXXX0] sso_session = test sso_account_id = 72XXXXXXXXX0 sso_role_name = AdministratorAccess region = ap-southeast-1 output = json [sso-session test] sso_start_url = https://d-XXXXXXX.awsapps.com/start/# sso_region = ap-southeast-1 sso_registration_scopes = sso:account:access 可以看到會有兩個區塊，分別是 sso-session 以及剛剛設定的 Profile。\nsso-session： 會記錄 SSO 的起始 URL、SSO 區域以及 SSO 的註冊範圍。\nProfile 區塊： 則是記錄使用哪一個 SSO Session、AWS Account ID、Permission Set 以及預設的區域和輸出格式。\n那我們通常只有一個 SSO Session，然後有多個 AWS Account，所以有多個 Profile，如果每個 AWS Account 都有不同的 Permission Set，就會有更多的 Profile。\n那假設你想要手動輸入 SSO Profile 的話，也可以直接在 ~/.aws/config 檔案中新增一個 Profile 區塊，並填入對應的 AWS Account ID、Permission Set 以及 SSO Session 名稱，就可以囉。","參考資料#參考資料":"使用 AWS CLI 設定 IAM Identity Center 驗證：https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-sso.html"},"title":"AWS SSO Profile 設定介紹"},"/blog/aws/cli-mfa/":{"data":{"":"會有這一篇文章也是跟 Identity and Access Management (IAM) 介紹 一樣，在測試 eksctl 建立 EKS 時，發現一直噴錯，說我沒有權限，但我去看我的 IAM Policy 也有加上對應 action 以及 resource，但還是不行，後來發現 cloudformation:* 這個 action 所在的 Policy 有加上 Condition MFA 驗證的條件，所以就來寫一篇如何使用 MFA Token 驗證 AWS CLI 的文章。\n沒有權限噴錯QQ","前情提要#前情提要":"我目前有 3 個 IAM Policy，分別是：\ndefault-policy：放一些個人基本的 IAM 權限 default-policy\npoc-general-policy：放一些我在測試的 PoC 的 IAM 權限，但是仔細看這邊有 Condition MFA 驗證的條件。 poc-general-policy\nReadOnlyAccess：預設的 ReadOnlyAccess 權限。 ReadOnlyAccess\n當你設定好 aws configure 後，你會拿到一個永久性的 IAM 憑證 (也就是 Access Key ID 和 Secret Access Key，設定可以看配置 AWS CLI)，但是如果你的 Policy 有加上 MFA 驗證的條件，那你就需要使用 MFA Token 來驗證，那要怎麼做呢？","參考資料#參考資料":"How do I use an MFA token to authenticate access to my AWS resources through the AWS CLI? ：https://repost.aws/knowledge-center/authenticate-mfa-cli","寫入設定檔#寫入設定檔":"當你拿到 Token 以外，你不可能每次下指令都帶 Token，所以我們要把它寫到 ~/.aws/credentials 檔案中，這樣就可以自動帶入 Token 了。\n我們在 aws configure 的時候，會有一個 profile 的選項，這個選項就是用來設定不同的憑證，預設是 default，我們可以自己設定一個新的憑證，如下：\n[mfa] aws_access_key_id = ASIA2HCXXXXXXXXXXXXX aws_secret_access_key = 25CBN6CjRNLPukXXXXXXXXXXXXX aws_session_token = FwoGZXIvYXdzEXXXXXXXXXXXXX 所以當你要使用 MFA Token 的時候，只要在指令後面加上 --profile mfa 就可以了。\n就可以用 poc-general-policy 的 action 囉～\n當然，如果每次都需要下指令去取得 Token 也是很麻煩的，所以我寫了一個 Shell Script 來幫我們取得 Token 並寫入 ~/.aws/credentials 檔案中，這樣就可以省去每次都要下指令的麻煩了。程式也會同步到 Github，大家可以自行下載使用。\n#! /bin/bash USER_NAME=$1 MFA_CODE=$2 # 顏色設定 RED=\"\\033[1;31m\" GREEN=\"\\033[1;32m\" YELLOW=\"\\033[1;33m\" BLUE=\"\\033[1;34m\" WHITE=\"\\033[0m\" if [ -z $USER_NAME ] || [ -z $MFA_CODE ]; then echo -e \"\\n${YELLOW}提醒說明：請輸入使用者名稱 與 MFA 驗證碼\\n格式如下：./aws_mfa.sh ian_zhuang 235821 \u003c\u003c (請依照手機上的 MFA 號碼)。${WHITE}\\n\" exit 1 fi if [[ ! \"$MFA_CODE\" =~ ^[0-9]{6}$ ]]; then echo -e \"\\n${YELLOW}提醒說明：MFA 驗證碼只支援 6 碼數字。${WHITE}\\n\" exit 1 fi MFA_DEVICES_NUMBER=$(aws iam list-mfa-devices --user-name ${USER_NAME} | jq -r '.MFADevices[0].SerialNumber') if [ -z $MFA_DEVICES_NUMBER ]; then echo -e \"\\n${RED}錯誤說明：MFA 裝置不存在，請確認使用者名稱是否輸入正確${WHITE}\\n\" exit 1 fi MFA_INFO=$(aws sts get-session-token --duration-seconds 129600 --serial-number ${MFA_DEVICES_NUMBER} --token-code ${MFA_CODE}) if [ $? -eq 0 ]; then MFA_ACCESS_KEY_ID=$(echo $MFA_INFO | jq -r '.Credentials.AccessKeyId') MFA_SECRET_ACCESS_KEY=$(echo $MFA_INFO | jq -r '.Credentials.SecretAccessKey') MFA_SESSION_TOKEN=$(echo $MFA_INFO | jq -r '.Credentials.SessionToken') MFA_EXPIRATION=$(echo $MFA_INFO | jq -r '.Credentials.Expiration') # 計算 MFA 到期時間(轉換時區) MFA_0_TIME=$(date -j -f \"%Y-%m-%dT%H:%M:%S\" \"$(echo \"$MFA_EXPIRATION\" | sed 's/+00:00//')\" \"+%Y-%m-%d %H:%M:%S\") MFA_8_TIME=$(date -j -v+8H -f \"%Y-%m-%d %H:%M:%S\" \"$MFA_0_TIME\" \"+%Y-%m-%d %H:%M:%S\") MFA_CREDENTIALS=\"[mfa]\\naws_access_key_id = $MFA_ACCESS_KEY_ID\\naws_secret_access_key = $MFA_SECRET_ACCESS_KEY\\naws_session_token = $MFA_SESSION_TOKEN\" # 檢查是否已經有 [mfa] 區塊 if grep -q \"\\[mfa\\]\" ~/.aws/credentials; then # 更新 [mfa] 區塊 sed -i '' '/\\[mfa\\]/,/^$/d' ~/.aws/credentials echo -e \"$MFA_CREDENTIALS\" \u003e\u003e~/.aws/credentials else # 新增 [mfa] 區塊 echo -e \"\\n\\n$MFA_CREDENTIALS\" \u003e\u003e~/.aws/credentials fi echo -e \"\\n${GREEN}MFA 驗證成功，將 Key 跟 Token 加入or修改到 ~/.aws/credentials 檔案中${WHITE}\" echo -e \"${BLUE}到期時間：${MFA_8_TIME}${WHITE}\\n\" else echo -e \"\\n${RED}MFA 驗證失敗，請確認 MFA 驗證碼是否正確${WHITE}\\n\" exit 1 fi 只需要執行 ./aws_mfa.sh {USER_NAME} {MFA_CODE} 就可以了，程式會自動幫你取得 Token 並寫入 ~/.aws/credentials 檔案中。\n執行腳本","查詢目前有沒有設定-mfa#查詢目前有沒有設定 MFA":"可以先查詢目前有沒有設定 MFA，可以透過以下指令：\naws iam list-mfa-devices --user-name {USER_NAME} 將 {USER_NAME} 換成自己的使用者名稱，如果有設定 MFA，就會顯示出來，如下：\n顯示 mfa 裝置","產生-mfa-token#產生 MFA Token":"我們確定我們有 MFA 設定後，就可以透過已經設定好的 MFA 以及剛剛的 SerialNumber 來產生 MFA Token，指令如下：\naws sts get-session-token --duration-seconds 129600 \\ --serial-number {MFA_DEVICES_NUMBER} \\ --token-code {MFA_CODE} 首先 --duration-seconds 參數是 MFA Token 的有效時間，單位是秒，預設是 12 小時，我們可以調整 900 秒 (15 分鐘) 到 129600 秒 (36 小時)，root 使用者憑證，範圍為 900 秒 (15 分鐘）到 3600 秒 (1 小時)，大家可以自行調整。\n{MFA_DEVICES_NUMBER} 換成剛剛查詢到的 MFA 裝置的 SerialNumber，最後將 {MFA_CODE} 換成你的 MFA 的 Code。\n一起來看一下輸出結果：\n{ \"Credentials\": { \"AccessKeyId\": \"ASIA2HCXXXXXXXXXXXXX\", \"SecretAccessKey\": \"25CBN6CjRNLPukXXXXXXXXXXXXX\", \"SessionToken\": \"FwoGZXIvYXdzEXXXXXXXXXXXXX\", \"Expiration\": \"2024-08-02T19:42:36+00:00\" } } AccessKeyId、SecretAccessKey 就跟 aws configure 拿到的 Access Key ID 和 Secret Access Key 是一樣的類型，只是還多一個 SessionToken，這個 SessionToken 是有經過 MFA 驗證過，所以可以用來執行需要 MFA 驗證條件的 Policy。"},"title":"如何使用 MFA Token 驗證 AWS CLI"},"/blog/aws/eks-introduce/":{"data":{"":"由於公司政策調整，除了原先的 Google Cloud Platform (GCP) 之外，我們也開始使用 AWS 服務，其中包括了 EKS (Elastic Kubernetes Service)。這篇文章將會介紹 EKS 的基本概念以及一些使用上的注意事項。\n後面也會有幾篇 AWS 文章，歡迎大家持續關注。","eks-定價#EKS 定價":"","什麼是-eks#什麼是 EKS？":"那熟悉 Kubernetes 的朋友應該對 EKS 並不陌生，EKS 是 AWS 提供的 Kubernetes 服務，讓使用者可以在 AWS 上快速建立、擴展和管理 Kubernetes 集群。就跟 GCP 的 GKE 一樣。\n流程也跟 GKE 差不多，如圖：\n建立 Cluster：可以透過 eksctl、AWS Console、AWS CLI、Terraform 等等來建立。\neksctl：是一個官方專門給 EKS 的 CLI 工具，可以快速建立、更新和刪除 EKS 集群，詳細可以看：eksctl、Amazon EKS 入門。 Mac 安裝指令：\nbrew tap weaveworks/tap brew install weaveworks/tap/eksctl AWS CLI：是官方的 CLI 工具，除了建立 EKS 集群，還可以管理其他 AWS 服務，詳細可以看：我的另一篇筆記 設定 AWS CLI 以及 AWS CLI 指令說明。 選擇運算資源的方式：可以分成 Fargate、Karpenter、託管節點群組和自管理節點。那我們使用主要是使用託管節點群組。其他的方式會在後面的文章介紹。\n設定：設定必要的控制器、驅動程式或是服務等等。\n部署工作負載：自訂 Kubernetes 物件，例如 Pod、Service、Deployment 等等。\n管理：監督工作負載，整合 AWS 服務以簡化維運並提高工作負載效能。\n在雲端中執行 Amazon EKS 的基本流程\n其他比較詳細的架構以及部署選項可以參考：Amazon EKS 架構、部署選項，這邊就不再贅述。","參考資料#參考資料":"What is Amazon EKS?：https://docs.aws.amazon.com/eks/latest/userguide/what-is-eks.html\nAmazon EKS 入門：eksctl：https://docs.aws.amazon.com/zh_tw/eks/latest/userguide/getting-started-eksctl.html","建立第一個-ekseksctl#建立第一個 EKS：eksctl":"詳細請參考：Amazon EKS 入門：eksctl\n在建立 EKS 前，我們要先確認我們要建立的節點是要使用 Fargate 還是託管節點群組，我們這邊拿 GCP 來說明，託管節點群組就是我們一般建立的 Node_Pool，而 Fargate 就是 Serverless 的概念，不需要自己管理節點，只需要管理 Pod，詳細我後面也會寫一篇文章來介紹，可以點我查看。\n那我們這邊就先以託管節點群組來建立 EKS，建立使用以下指令來建立：\neksctl create cluster --name {my-cluster} --region {region-code} eksctl create cluster --name ian-test --region us-east-1 將 {my-cluster} 替換成自己要的名稱，只能英文數字字元(區分大小寫)和連字號。必須字母數字字元開頭，且長度不可以超過 100 個字元，名稱在該區域中必須是唯一的。\n{region-code} 則是替換成自己要的區域，例如：us-east-1。可以從 Amazon EKS endpoints and quotas 這邊查看 region-code。 如果是 Fargate 的話，只需要在指令後面加上 --fargate\n提醒：使用這個指令建置，會需要有 CloudFormation 的相關權限，如果缺少權限，會顯示以下錯誤：\n( CloudFormation 是將基礎設施視為程式碼的服務，可以對 AWS 和第三方資源進行建模、佈建和管理，詳細一樣請參考後續文章 AWS CloudFormation 介紹。) 使用 eksctl 建立 EKS 缺少 CloudFormation 權限"},"title":"Elastic Kubernetes Service (EKS) 介紹"},"/blog/aws/iam-introduce/":{"data":{"":"先來說一下為什麼會寫這篇文章，主要是在寫跟測試 Elastic Kubernetes Service (EKS) 介紹 文章時，想要用 eksctl 來建立 EKS，它實際上是透過 CloudFormation 來建立的，詳細可以去看 EKS 文章，總之當時遇到權限的問題，找了很久，也看不懂 AWS 的 ARN 是什麼，所以就先來寫一篇 IAM 的文章 (´≖◞౪◟≖)。","iam-policy-範例#IAM Policy 範例":"那了解了 IAM Policy 的每個結構以及功能，我下面就舉例兩個 IAM Policy 來讓大家複習看看。\n範例一 { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"ec2:CreateVolume\", \"Resource\": \"*\" }, { \"Effect\": \"Allow\", \"Action\": \"ec2:CreateTags\", \"Resource\": \"arn:aws:ec2:::volume/*\", \"Condition\": { \"StringLike\": { \"aws:RequestTag/project\": \"*\" } } }, { \"Effect\": \"Deny\", \"Action\": \"ec2:CreateTags\", \"Resource\": \"arn:aws:ec2:::volume/*\", \"Condition\": { \"StringEquals\": { \"aws:ResourceTag/environment\": \"production\" } } } ] } 這個 Policy 有 3 個 Sub-Statement，分別是：\n允許對所有的 EC2 instance 創建 Volume。 允許有 aws:RequestTag/project 的 Tag 對所有的 EC2 Volume 創建 Tag。 拒絕有 aws:ResourceTag/environment 是 production 的 Tag 對所有的 EC2 Volume 創建 Tag。 範例二 { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"ecs:RunTask\", \"ecs:StartTask\" ], \"Resource\": [ \"*\" ], \"Condition\": { \"StringEquals\": { \"aws:RequestTag/environment\": [ \"production\", \"prod-backup\" ] }, \"ArnEquals\": { \"ecs:cluster\": [ \"arn:aws:ecs:us-east-1:111122223333:cluster/default1\", \"arn:aws:ecs:us-east-1:111122223333:cluster/default2\" ] } } } ] } 這個 Policy 有 1 個 Sub-Statement：\n允許在 arn:aws:ecs:us-east-1:111122223333:cluster/default1 和 arn:aws:ecs:us-east-1:111122223333:cluster/default2 這兩個 ECS 集群上，對 aws:RequestTag/environment 標籤為 production 或 prod-backup 的任務執行 RunTask 和 StartTask 操作。\n範例參考：單值內容索引鍵政策範例","policy-type#Policy Type":"Policy Type 有三種：\nAWS 受管\n由 AWS 提供的 Policy，這些 Policy 會包含一些常見的操作，例如：S3 的讀寫、EC2 的啟動、停止等等。如果 AWS 有新增服務，也會同時更新這些 policy。詳細可以看：AWS 受管理政策 AWS 受管 – 職務職能\n這些 Policy 會針對特定的工作角色，例如：系統管理員 (AdministratorAccess)、網路管理員任務角色 (NetworkAdministrator)、資料庫管理員任務角色 (DatabaseAdministrator) 等等，這些 Policy 會包含這些角色常見的操作。詳細可以看：AWS 受管理的工作職能政策\n客戶受管\n使用者自己定義的 Policy，這些 Policy 會針對自己的需求來定義，例如：只能讀取某個 S3 bucket、只能啟動某個 EC2 instance 等等。詳細可以看：客戶受管政策","policy-結構#Policy 結構":"IAM Policy 會是一個 JSON 格式的文件，可以定義了一個或多個 Action，這個 Policy 會告訴 AWS 這個身份可以對哪些資源 (Resource) 進行哪些操作 (Action)。而這個 Policy 會被附加到一個身份上，這個身份可以是一個 IAM User、IAM Group 或 IAM Role。然後一個 User、Group 或 Role 可以有多個 Policy。\n我目前公司帳號的 Policy\n上面的圖片就是一個典型的 IAM 的 Policy 結構，可以看到這個 Policy 會有 Version、Statement、Effect、Action、Resource、Condition (上面沒有 xD) 等等主要元素。\n下面就針對 Version、Statement、Effect、Action、Resource、Condition 這些元素來做一些介紹。\n重點提醒：\n所有的元素都是有區分大小寫的，寫錯大小寫會導致 Policy 無法正確解析。 Version Version：用來告知 AWS 這個 Policy 是使用哪個版本的 IAM Policy 語法，目前最新的版本是 2012-10-17，舊版本是 2008-10-17。詳細請參考：IAM JSON 政策元素：Version\nStatement Statement：是 Policy 的主要元素。此元素為必填。Statement 元素可包含單一陳述式，或是個別陳述式的陣列。每個個別的陳述式區塊都必須用大括號 { } 括起。針對多個陳述式，陣列必須用方括號 [ ] 括起。詳細請參考：IAM JSON 政策元素：Statement\n例如：\n\"Statement\": [{...},{...},{...}] Effect Effect：元素是必填的，指定陳述式是否允許或拒絕。\nEffect 的有效值為 Allow 和 Deny。Effect 值會區分大小寫。 AWS 預設是 Deny，所以如果沒有設定 Effect，則預設是 Deny，需要明確指定 Allow。 Policy 每個 Statement 都必須有一個 Effect 元素。 如果同時有 Allow 和 Deny 套用同一個資源，則 Deny 會優先於 Allow。 詳細請參考：IAM JSON 政策元素：Effect\nAction Action：描述哪一個服務可以做哪些動作。\n每個 AWS 服務都有自己的一組動作，描述您可以使用該服務執行的哪些任務。 使用服務命名做為動作開頭 (iam、ec2、sqs、sns、s3 等) 來指定值，後面則加上對應的動作名稱。 Action 服務和動作名稱不區分大小寫。例如，iam:ListAccessKeys 與 IAM:listaccesskeys 相同 下面舉例幾個 Action：\nAmazon S3 取得物件： \"Action\": \"s3:GetObject\" IAM 修改密碼： \"Action\": \"iam:ChangePassword\" EC2 啟動實例： \"Action\": \"ec2:StartInstances\" 可以從上面得知 s3、iam、ec2 是 AWS 提供的服務，而 GetObject、ChangePassword、StartInstances 是這些服務提供的動作。\n當然，服務很多，再加上每個服務的動作也不一樣，那我們可以依照 武功秘籍 提到的 Actions, resources, and condition keys for AWS services 去查詢對應的動作列表。\n如何方便的撰寫 Action？ 如果，我們需要新增服務很多的 Action，我們只能一個一個輸入嗎？像下面這張圖片，我們可以看到 Elastic Kubernetes Service (EKS) 的 Action 部分清單，假如我們要這些全部的動作，需要把每個都寫出來嗎？\nElastic Kubernetes Service Action 部分清單 Amazon Elastic Kubernetes Service 定義的動作\n當然不用！我們可以使用 * 來代表所有的動作，例如：\n\"Action\": \"eks:*\" 這樣就可以代表所有的 EKS 的動作。\n或是只想要 eks 的 Create 相關動作，可以這樣寫：\n\"Action\": \"eks:Create*\" 這樣就包含了 CreateAccessEntry、CreateAddon、CreateCluster、CreateFargateProfile、CreateNodegroup 等等動作。\n或是我們只想要有 eks 的 cluster 相關動作(不要 Addon、Nodegroup 等等)，可以這樣寫：\n\"Action\": \"eks:*Cluster*\" 這樣就只有 CreateCluster、DeleteCluster、DescribeCluster、ListClusters 等等的動作，沒有 CreateAddon、CreateNodegroup 動作。\n詳細請參考：IAM JSON 政策元素：Action\nResource Resource：描述哪一個服務的哪一個資源。\n這邊以 Amazon S3 為例，我們建立了一個 bucket，它就是一個 Resource，我們上傳了一個檔案到這個 bucket，這個檔案也是一個 Resource。\n但是，我們要如何去定義我們可以對哪些的 AWS Resource 做哪些的操作呢？這時候就可以使用 ARN (Amazon Resource Name) 來定義。\nARN 它的命名會將不同服務、不同 region、不同使用者帳號都考慮進去，產生一個唯一識別 AWS 資源的字串，下面是三種 ARN 格式：\narn:\u003cpartition\u003e:\u003cservice\u003e:\u003cregion\u003e:\u003caccount-id\u003e:\u003cresource-id\u003e arn:\u003cpartition\u003e:\u003cservice\u003e:\u003cregion\u003e:\u003caccount-id\u003e:\u003cresource-type\u003e/\u003cresource-id\u003e arn:\u003cpartition\u003e:\u003cservice\u003e:\u003cregion\u003e:\u003caccount-id\u003e:\u003cresource-type\u003e:\u003cresource-id\u003e 詳細可以參考：ARN 格式\n那我們一樣，將 partition、service、region、account-id、resource-type、resource-id 都個別簡單介紹一下：\npartition (分區) 它是 AWS 的分區，目前有 aws 和 aws-cn 以及 aws-us-gov，分別代表 AWS 區域和中國區域以及 AWS GovCloud (US) 區域，所以基本上我們都會使用 aws。\nservice (服務) 它是 AWS 的服務名稱字首，例如：s3、iam、ec2、eks 等等。不清楚每個服務的字首可以參考 AWS 服務的動作、資源和條件索引鍵，假設我們點進去看到 Amazon S3 的服務名稱字首是 s3。\nAmazon S3 的動作、資源和條件索引鍵\nregion (區域) 它是 AWS 的區域，例如：us-east-1、us-west-2、ap-northeast-1 等等。如果需區域代碼清單，可以查看區域端點\naccount-id (帳號識別碼) 它是擁有資源的 AWS 帳號識別碼 (不含連字號)，例如：123456789012。\nresource-type (資源類型) 它是資源的類型，例如：vpc (虛擬私有雲)，bucket (S3 bucket)，instance (EC2 instance) 等等。\nresource-id (資源識別碼) 它是資源名稱、資源 ID 或資源路徑。某些資源識別碼包括父項資源 (sub-resource-type/父資源/子資源) 或限定元，\n例如：\nIAM User arn:aws:iam::123456789012:user/ian_zhuang 這個在最上面的 Policy 結構中的圖片可以看到我的 User ARN。\nVPC arn:aws:ec2:us-east-1:123456789012:vpc/vpc-0e9801d129EXAMPLE S3 Bucket arn:aws:s3:::* arn:aws:s3:::demo-bucket-a/* arn:aws:s3:::demo-bucket-b/downloads/* 這邊 S3 Bucket 的 ARN 有三個：\n第一個代表的是所有的 S3 bucket \u0026 object。\n第二個代表的 demo-bucket-a 對於此 bucket 的所有 object。\n第三個代表的 demo-bucket-b 的 downloads 目錄下的所有 object。\n另外，還會發現，因為 S3 服務它沒有 \u003cregion\u003e、\u003caccount-id\u003e 所以中間兩個就留空即可。\n詳細可以參考：IAM JSON 政策元素：Resource\nCondition 在上面介紹的 Effect 我們可以設定 Allow 或 Deny 來限制 User、Group、Role 對資源操作的權限，但是有時候我們可能會需要更多的條件來限制這個 Policy，這時候就可以使用 Condition 來設定。\nCondition：它可用於 Policy 生效時指定條件，Condition 元素是選填的。\n這個功能等於是加了一個 if 的判斷，以及 Statement 可以有很多個 Sub-Statement，每個 Sub-Statement 都可以設定 Condition，就可以設定很多種可能。\n它的格式是：\n\"Condition\": { \"\u003ccondition-operator\u003e\": { \"\u003ccondition-key\u003e\": \"\u003ccondition-value\u003e\" } } 會有三個元素組成：分別是 condition-operator、condition-key、condition-value。(condition-value 就是一般要判斷的資料，所以這邊就不多做介紹)\ncondition-operator condition-operator：是一個比較運算子，例如：\n字串的有： 運算子 說明 StringEquals 檢查 condition-key 跟 condition-value 字串是否相等 StringNotEquals 檢查 condition-key 跟 condition-value 字串是否不相等 StringEqualsIgnoreCase 檢查 condition-key 跟 condition-value 字串是否相等，不區分大小寫 StringNotEqualsIgnoreCase 檢查 condition-key 跟 condition-value 字串是否不相等，不區分大小寫 由於有些 condition-key 沒有區分大小寫，所以運算子會有 IgnoreCase 的不區分大小寫的運算子，在設計的時候要注意。\n數值的有： 運算子 說明 NumericEquals 檢查 condition-key 跟 condition-value 數值是否相等 NumericNotEquals 檢查 condition-key 跟 condition-value 數值是否不相等 NumericLessThan 檢查 condition-key 是否小於 condition-value 數值 NumericLessThanEquals 檢查 condition-key 是否小於等於 condition-value 數值 NumericGreaterThan 檢查 condition-key 是否大於 condition-value 數值 還有像是日期條件運算子、布林值條件運算子、二進位條件運算子、IP 地址條件運算子等等，由於運算子蠻多的，可以參考：IAM JSON 原則元素:條件運算子。\ncondition-key 那 condition-key 是什麼呢？它是一個 key，用來指定要檢查的條件，例如：aws:username、aws:MultiFactorAuthAge、s3:authType、eks:namespaces 等等。\n在 condition-key 又分為兩種類型：分別是：\nGlobal Condition Key\n這些是 AWS 提供的全域條件，可以在所有的服務或是 Action 中使用，例如：aws:username、aws:userid 等等。但是有些特定狀況下才可以使用，例如：aws:MultiFactorAuthAge，詳細可以參考：AWS global condition context keys\nService Condition Key\n這些是服務提供的條件，不同服務的 Service 也會有所不同，例如：s3:authType、eks:namespaces 等等。\n想要知道哪些服務有什麼 Condition Key，可以再次開啟 武功秘籍 提到的 Actions, resources, and condition keys for AWS services，我們假設選擇 Amazon Elastic Kubernetes Service (EKS) 服務：\n因為每個 Action 都有對應的 Condition Key，不是任何一個 Action 都與任一個 Condition Key 有搭配，如下圖：\nActions defined by Amazon Elastic Kubernetes Service\n所以需要查看文件裡面的清單，例如像是 AssociateAccessPolicy Action 就會有 eks:policyArn、eks:namespaces、eks:accessScope 的 Condition Key。\n另外，想要知道 Condition Key 對應到哪個資料類型，也可以參考 Condition keys for Amazon Elastic Kubernetes Service，後面會告訴你哪個 Condition Key 的資料類型，方便找到對應的 Condition Operator。\nCondition keys for Amazon Elastic Kubernetes Service\nCondition 規範 Condition 可以在 (condition-operator) 有多個條件 (condition-key)，每個條件都是 and 來相互關聯，例如： { (省略其他元素) \"Condition\": { \"StringEquals\": { \"aws:PrincipalTag/department\": \"finance\", \"aws:PrincipalTag/role\": \"audit\" } } } 代表的是 aws:PrincipalTag/department 跟 aws:PrincipalTag/role 都要符合才會生效。\n如果同一個條件有多個值，每個值都是 or 來相互關聯，例如： { (省略其他元素) \"Condition\": { \"StringEquals\": { \"aws:PrincipalTag/department\": [ \"finance\", \"hr\", \"legal\" ], \"aws:PrincipalTag/role\": [ \"audit\", \"security\" ] } } } 官方的圖片解釋的更清楚：\nCondition 的 condition-operator 不能重複，例如： { (省略其他元素) \"Condition\": { \"StringEquals\": { \"aws:PrincipalTag/department\": \"finance\" }, \"StringEquals\": { \"aws:PrincipalTag/role\": \"audit\" } } } 這樣是錯誤的寫法，因為 StringEquals 出現兩次，要將兩個條件合併成一個條件，例如：\n{ (省略其他元素) \"Condition\": { \"StringEquals\": { \"aws:PrincipalTag/department\": \"finance\", \"aws:PrincipalTag/role\": \"audit\" } } } 詳細可以參考：IAMJSON 政策元素：Condition","什麼是-iam#什麼是 IAM？":"IAM 的全名是 Identity and Access Management，從字面上來看就可以得知，它是用來管理 AWS 資源的身份和存取權限的服務。透過 IAM，您可以控制對 AWS 資源的存取權限，以及對這些資源的操作權限。也就是 Who (身份) 可以 Do (操作) What (資源) 的一個功能。","參考資料#參考資料":"[AWS IAM] 學習重點節錄(2) - IAM Policy：https://godleon.github.io/blog/AWS/learn-AWS-IAM-2-policy/\nActions, resources, and condition keys for AWS services：https://docs.aws.amazon.com/service-authorization/latest/reference/reference_policies_actions-resources-contextkeys.html\n其餘對應的參考資料都直接附在文章中。","武功秘籍#武功秘籍":"文章主要參考：[AWS IAM] 學習重點節錄(2) - IAM Policy，這篇文章寫得很詳細，當然，我會加上一些自己的理解跟測試範例，如果有興趣的話，可以去看看原文。\n下方文章主要都圍繞官方的 Actions, resources, and condition keys for AWS services 文件上，如果後面沒有附連結，基本上就在講這份，請大家可以打開網頁一起看。"},"title":"Identity and Access Management (IAM) 介紹"},"/blog/aws/vpc-introduce/":{"data":{"":"此篇單純是我自己學習 AWS VPC 的筆記，主要是為了熟悉 AWS 的 VPC，了解 VPC 包含哪些元件，以及如何建立 VPC、設定 VPC 等等。","default-vpc#Default VPC":"每個 AWS 帳號都會有一個 Default VPC，這個 VPC 是 AWS 預設給你的，裡面已經設定好一些設定，方便你可以快速啟動資源。\n在 Default VPC 中，會在每個 Availability Zone (AZ) 建立好 Subnet。\n每個 Subnet 都會有一組可以連到 internet 的 Route Table，所以每個 Subnet 都預設具備連網的功能\n每個 Subnet 都會有一個 Internet Gateway 存在以及連接，因此 Default VPC 所有 Subnet 都屬於 Public Subnet。","vpc-ip-範圍#VPC IP 範圍":"目前 AWS 支援的 IP 範圍是根據 RFC 1918 標準。以下是可用的私有 IP 範圍：\n10.0.0.0 - 10.255.255.255（10.0.0.0/8，總共 16,777,216 個 IP 地址） 172.16.0.0 - 172.31.255.255（172.16.0.0/12，總共 1,048,576 個 IP 地址） 192.168.0.0 - 192.168.255.255（192.168.0.0/16，總共 65,536 個 IP 地址） 一個 VPC 最多可以使用 5 個 Ipv4 CIDR 設定，但一般只會設定一個","vpc-peering#VPC Peering":"當不同的 VPC 想要連接時，可以透過 VPC Peering 來連接，VPC Peering 是一種私有的連接，可以在兩個 VPC 之間傳輸流量，使用 Private IP 將兩個 VPC 連接起來。\n不同的 VPC 的 CIDR 不能重疊，否則無法建立 VPC Peering。\nVPC Peering 不限制同一個帳號，可以跟其他帳號的 VPC 連接。\n也可以跨 Region，這種稱為 Inter-Region VPC Peering\nPeering 屬於一對一的連接，如果要連接多個 VPC，則需要建立多個 Peering。例如：A 與 B Peering，B 與 C Peering，A 與 C 之間是無法直接溝通。\n可以設定整個 VPC 進行 Peering，也可以設定特定 Subnet 進行 Peering。\n需要額外設定 Route Table，才可以兩個 VPC 之間可以溝通。\n詳細可以看：What is VPC peering? 什麼是 VPC 對等互連？","vpc-元件#VPC 元件":"從上面可以簡單知道 VPC 架構，那我們來詳細看一下每個組成的元件是什麼，先看細圖：\nVPC Data Flow Virtual Private Cloud\n我們從最大的元件開始看：\nRegion \u0026 VPC 上方淺藍色外框就代表 Region，每個 Region 可以有多個 VPC，範例的 Region 是 eu-west-2。\n深藍色外框就代表 VPC，VPC 是在 Region 層級底下。\nInternet Gateway (IGW) \u0026 Virtual Private Gateway (VGW) 要存取 VPC 的方式有兩種，一種是透過 IGW 可以從 Internet 存取 VPC，另一種是透過 VGW 可以從地端網路存取 VPC。\n不管從 IGW 或 VGW 存取 VPC，都會先到 Route，Route 會根據 traffic 的來源跟目的地，決定要套用哪個 Route Table Rule。\n接著 Route Table 會根據 Rule 決定 traffic 要往哪個方向走，並導向到不同的 Network ACL 來進行流量的控制。\nInternet Gateway (IGW) (比較會用到，另外拉出來說)\nInternet Gateway (IGW) 有自動水平擴展，以及 HA (High Availability) 的特性，會由 AWS 負責管理，我們不需要特別設定。\n沒有對外的頻寬限制。\n每個 default VPC 都會有一個 Internet Gateway (IGW)。\nRoute Table Route Table 是用來指引網路流量要怎麼走，以及去哪裡。Route Table 會有目的地 IP 以及下一站要去哪。\n預設情況下，VPC 內部的 traffic 可以在不同的 subnet 之間自由流動，依靠的就是 local route。\nlocal route 是預設存在，且無法修改的。\nVPC 中可以設定多組的 Route Table，但每個 Subnet 只能指定一組 Route Table，且已經與 Subnet 關聯的 Route Table 不能刪除。\nNetwork ACL (NACL) Network ACL 是網路進到 VPC 的第一道防禦，可以把它想成是一個子網級別的防火牆，用於控制進入和出去子網的流量。\nNetwork ACL 是 stateless，這意味著進入和出去的流量都必須明確允許。如果允許進入流量 (inbound traffic)，出去的流量 (outbound traffic) 必須單獨設定規則允許。\nSecurity Group (SG) 是網路進入的第二道防禦，是虛擬防火牆，用於控制 EC2 實例的進入和出去流量。\n但是比較不一樣的是 Security Group 是 stateful，這意味著如果允許進入流量 (inbound traffic)，相應的出去回應流量 (outbound response traffic) 也自動被允許，反之亦然。\n就算移除 Allow Outbound Traffic 規則，一樣還是會通\nSubnet (Public \u0026 Private) Subnet 是 VPC 的子網路，可以在不同的 Availability Zone (AZ) 建立 Subnet。\nSubnet 又分成 Public Subnet 跟 Private Subnet，Public Subnet 可以連到 Internet，Private Subnet 則不能。\n但如果 Private Subnet 需要連到 Internet (例如 EC2 更新套件版本等)，則需要透過 NAT Gateway 來達成 (只有單向，沒辦法讓外部網路連進來)。\nSubnet 跟 Subnet 之間要溝通，則需要透過 Route Table 來指引路線。","vpc-懶人包#VPC 懶人包":"如果對於上面比較詳細的說明有點複雜，這邊我總結一下 VPC 的重點：\n以網路的角度出發 下面都先用 EC2 來當作 Subnet 底下的後端服務：\nPrivate Subnet 沒辦法對外連網，外面也連不進來。 同一個 Subnet 裡面的 EC2 可以互相溝通，不需要透過其他的元件。 不同的 Subnet 要溝通時，則需要透過 Route Table 元件。 Route Table 功用就是去指引網路流量要怎麼走，以及去哪裡。會有目的地 IP 以及下一站要去哪。會經過 Local 中繼站。 Public Subnet 讓裡面的 EC2 可以連到 Internet，一樣需要 Route Table 去指引路線，目的地是 Internet，下一站則需要到 Internet Gateway (IGW)。 Internet Gateway (IGW) 會放在 VPC 層級上面，是 Public Subnet 對外連線的重要元件。 如果要讓 Private Subnet 能夠連 Internet，Route Table 下一站則需要到 NAT Gateway。 NAT Gateway 是設定在 Public Subnet 上面，透過它才能讓 Private Subnet 連到 Internet。整個流程如下：Private Subnet \u003e Route Table \u003e NAT Gateway \u003e Internet Gateway (IGW) \u003e Internet，這個流程是單向的，外面的網路還是連不進來 Private Subnet。 以安全性的角度出發 外部連線想要進來 Subnet 時，會需要先經過 Network Access Control List (NACL)。 Network Access Control List (NACL) 是隸屬於 Subnet 層級，NACL 會規範，什麼請求可以進來，什麼請求可以出去 Subnet。 進入到 Subnet 後，要在到 EC2 時，會在經過一層保護名為 Security groups (SG)。 Security groups (SG) 是給 EC2 instance 用的，類似防火牆規範。 Network Access Control List (NACL) 跟 Security groups (SG) 差異\n差異 NACL SG 說明 是一個子網級別的防火牆，用於控制進入和出去子網的流量。 是虛擬防火牆，用於控制 EC2 實例的進入和出去流量。 狀態 stateless，這意味著進入和出去的流量都必須明確允許。如果允許進入流量，出去的流量必須單獨設定規則允許。 stateful，這意味著如果允許進入流量（inbound traffic），相應的出去回應流量（outbound response traffic）也自動被允許，反之亦然。 最後如果懶得看文字，建議大家可以去看這部影片：AWS 專家教你：打造高效 VPC 網絡（包含 Subnet、IGW、NAT 等） ，6 分鐘的影片，可以快速了解 VPC 的重點。","什麼是-vpc#什麼是 VPC？":"VPC 的全名是 Virtual Private Cloud，可以在 AWS 雲端內配置邏輯上隔離的虛擬網路。透過建立自己的 VPC，可以完全控制網路環境，包括定義 IP 位址範圍、子網路、路由表和連接選項的能力。\n每個 AWS 帳號包含每個 AWS 區域的預設 VPC。預設 VPC 有先設置好一些設定，方便我們可以快速啟動資源的便捷選項。但是預設 VPC 沒辦法符合長期的網路需求，這時候就需要自己建立 VPC。\n建立額外的 VPC 還有很多優勢，例如：可以按照部門或業務隔離工作負載，可以設定更多的安全性控制 (Network ACL \u0026 Security Group)，可以設定更多詳細的網路設定。\n還可以幫 VPC 建立 VPN 連線，將地端網路與 AWS VPC 連接起來，變成一個 Hybrid Cloud 架構。\nVPC 架構 什麼是 Amazon VPC\n從圖片來看，可以知道：\nVPC 會在 Region 層級底下，每個 Region 可以有多個 VPC。 VPC 是跨 Availability Zone (AZ) 的，可以在不同的 AZ 建立 Subnet。 VPC 會透過 Internet Gateway (IGW) 連接到 Internet。 ","參考資料#參考資料":"什麼是 Amazon VPC：https://docs.aws.amazon.com/zh_tw/vpc/latest/userguide/what-is-amazon-vpc.html\nAWS CSA Associate 學習筆記 - VPC(Virtual Private Cloud) Part 1：https://godleon.github.io/blog/AWS/AWS-CSA-associate-VPC-part1/\nVirtual Private Cloud：https://salmankhalid85.wordpress.com/aws/virtual-private-cloud/\n【技術分享】Internet Gateway 和 NAT Gateway 的區別在哪? 功能分別是什麼?：https://medium.com/@awseducate.cloudambassador/%E6%8A%80%E8%A1%93%E5%88%86%E4%BA%AB-internet-gateway-%E5%92%8C-nat-gateway-%E7%9A%84%E5%8D%80%E5%88%A5%E5%9C%A8%E5%93%AA-%E5%8A%9F%E8%83%BD%E5%88%86%E5%88%A5%E6%98%AF%E4%BB%80%E9%BA%BC-b676f62b1d31","武功秘笈#武功秘笈":"文章主要參考：AWS CSA Associate 學習筆記 - VPC(Virtual Private Cloud) Part 1，這篇文章寫得很詳細，當然，我會加上一些自己的理解跟測試範例，如果有興趣的話，可以去看看原文。"},"title":"Amazon Virtual Private Cloud (VPC) 介紹"},"/blog/conference-or-lecture/":{"data":{"":"此分類包含參加研討會後的心得分享，以及擔任不同講師的講座內容，歡迎參考\n朝陽科技大學 - 安全前瞻：網站防護與 DevOps 技術講座 (2024/05/29)發布日期：2024-05-29 "},"title":"研討會 / 講座"},"/blog/conference-or-lecture/20240529-devops-introduce/":{"data":{"":"","docker-和-kubernetes-概念介紹#Docker 和 Kubernetes 概念介紹":"\nDocker 介紹 Docker 是一種軟體平台，它可以快速建立、測試和部署應用程式。為什麼可以快速建立呢？因為 Docker 會將軟體封裝到名為『容器』的標準單位。其中會包含程式庫、系統工具、程式碼、執行軟體所需的所有項目。 剛剛有提到容器 (Container)，是一種虛擬化技術，它高效率虛擬化及易於遷移和擴展的特性，非常適合現代雲端的開發及佈署。\n那 Container 與傳統的虛擬機 (VM) 有什麼差別呢？我們來看看下面這張圖\nContainer 與 VM 的差異 圖片來源\n可以看到 Container 是以應用程式為單位，而 VM 則是以作業系統為單位。雖然本質來說兩者都是運行在有實體的硬體機器上，但 Container 是一個封裝了相依性資源與應用程式的執行環境。VM 則是一個需要配置好 CPU、RAM 與 Storage 的作業環境，為了更好的做區別，我把 Container、VM 兩個差別用表格來說明：\n比較 容器 (Container) 虛擬機(VM) 單位 應用程式 作業系統 適用服務 多使用於微服務 使用較大型的服務 硬體資源 是以程式為單位，需要的硬體資源很少 VM 會先佔用 CPU、RAM 等等硬體資源，不管有沒有使用都會先佔用 造成衝突 Container 間是彼此隔離的，因此在同一台機器可以執行不同版本的服務 會因為版本不同造成環境衝突 系統支援數量 單機支援上千個容器 一般最多幾十個 優點 Image 較小，通常都幾 MB\n啟動速度快，通常幾秒就可以生成一個 Container\n更新較為容易，只需要利用新的 Image 重新啟動就會更新完了 因為硬體層以上都虛擬化，因此安全性相對較高\n系統選擇較多，在 VM 可以選擇不同的 OS\n不需要降低應用程式內服務的耦合性，不需要將程式內的服務個別拆開來部署 缺點 安全性較 VM 差，因為環境與硬體都與本機共用\n在同一台機器中，每一個 Container 的 OS 都是相同的，無法一個為 Windows、一個為 Linux，還是依賴 Host OS\nContainer 通常會切成微服務的方式作部署，在各元件中的網路連結會比較複雜 Image 的大小通常 GB 以上，比 Container 大很多\n啟動速度通常要花幾分鐘，因此服務重啟速度較慢\n資源使用較多，因為不只程式本身，還要將一部分資源分給 VM 的作業系統 Docker 小總結 更快速的交付和部署：對於開發和維運人員來說，最希望就是一次建立或設定，可以再任意地方正常運行。開發者可以使用一個標準的映像檔來建立一套開發容器，開發完成之後，維運人員可以直接使用這個容器來部署程式。Docker 容器很輕很快！容器的啟動時間都是幾秒中的事情，大量地節約開發、測試、部署的時間。\n更有效率的虛擬化：Docker 容器的執行不需要額外的虛擬化支援，它是核心層級的虛擬化，因此可以實作更高的效能和效率。\n更輕鬆的遷移和擴展：Docker 容器幾乎可以在任意的平台上執行，包括實體機器、虛擬機、公有雲、私有雲、個人電腦、伺服器等。 這種兼容性可以讓使用者把一個服務從一個平台直接遷移到另外一個。\n更簡單的管理：使用 Docker，只需要小小的修改，就可以替代以往大量的更新工作。所有的修改都以增量的方式去更新，從而實作自動化並且有效率的管理。\nDocker 四大元素 Docker 是由這四個東西所組成，了解後就可以知道整個 Docker 的生命週期。\nDockerfile 映像檔 (Image) 容器 (Container) 倉庫 (Repository) Dockerfile 開發人員在使用 Docker 時發現，大多現成的 Docker 映像檔無法滿足他們的需求，因此需要一種能夠生成映像檔的工具。Dockerfile 是一種簡易的文件檔，裡面包含了建立新映像檔所需的指令。\nDockerfile 語法主要由 Command（命令）和 Argument （參數選擇）兩大元素組成。以下是一個簡易的 Dockerfile 示意圖：\n命令式語法＋選擇參數（Command + Argument）\nDockerfile 圖片來源\n映像檔 (Image) Docker 映像檔是一個唯獨的模板。\n例如：一個映像檔可以用一個 Ubuntu Linux 作業系統當作基底，裡面只安裝 Nginx 或使用者想用的套件等。\n我們會使用映像檔來建立 Docker 的容器 (Container) ，Docker 也提供很簡單的機制來建立映像檔或是更新現有的映像檔，也可以去下載別人已經做好的映像檔。\nImage 圖片來源\n容器 (Container) Docker 是利用容器來執行應用程式。\n每一個容器都是由映像檔所建立的的執行程式。它可以被啟動、開始、停止、刪除。且每一個容器都是相互隔離的，不會相互影響。\nContainer 圖片來源\n倉庫 (Repository) 倉庫是集中放置映像檔的所在地，倉庫分為公開倉庫 (Public) 和 私有倉庫 (Private) 兩種形式。最大的倉庫註冊伺服器當然是 Docker hub，存放數量龐大的映像檔供使用者下載，使用者也可以在本地網路內建立一個私有倉庫。可以將倉庫的概念理解成跟 Git 相似。\nRepository 圖片來源\n這樣講還是有點抽象，我們來看一個實際的例子：\n首先我們先看一下 Docker 的 Logo，你們覺得這是什麼動物呢？\nDocekr Logo 圖片來源\n沒錯就是鯨魚，但你們有沒有注意圖片有三個大元素呢？\n海洋 鯨魚 貨櫃 海洋代表的是 Docker 的執行環境，不論是在哪個海洋 (計算環境)，都可以執行，鯨魚代表的是 Docker 這個平台 (或可以指 Image)，而貨櫃代表的是 Docker 的容器，也就是說我們可以有很多個 Container 在同一個 Image 上執行，且互不影響，能夠快速的部署和遷移，並在不同的環境中執行。\n由於時間關係，其他 Docker 詳細介紹以及資料可以參考我之前寫的文章：Docker 介紹 (如何使用 Docker-compose 建置 PHP+MySQl+Nginx 環境)\nDocker 小試身手 為了大家能夠更了解 Docker 的使用，我們來實際操作一下：\n首先請大家先到 GitHub 下載範例程式碼：https://github.com/880831ian/20240529-devops-introduce 接著先執行 docker build -t 0529:latest . 這個指令，這個指令是用來建立一個 Image，這個 Image 是用來執行 Nginx 的程式碼。 接著執行 docker run -d -p 8080:80 0529:latest 這個指令，這個指令是用來執行一個 Container，並且將 Container 的 80 port 對應到本機的 8080 port。 打開瀏覽器，輸入 http://localhost:8080，就可以看到 Nginx 的首頁了。 我們先將關閉 Container，自行修改 index.html 的內容，然後再重新執行 docker build -t 0529:v0.0.1 . 和 docker run -d -p 8080:80 0529:v0.0.1，就可以看到修改後的內容了。 我們下 docker tag 0529:v0.0.1 880831ian/0529:v0.0.1 這個指令，將 Image 改成 Docker Hub 帳號跟 Image 名稱，例如：880831ian/0529:v0.0.1。 最後我們下 docker push 880831ian/0529:v0.0.1 這個指令，這樣就可以將 Image 上傳到 Docker Hub 上，這樣其他人就可以使用你的 Image 來部署服務了。 Docker 運作流程 步驟一、撰寫 Dockerfile\n步驟二、將 Dockerfile 建立為 Image\n步驟三、將 Image 運行為容器。透過這三個簡單的步驟，就能創建自己的 Docker 容器囉！\nDocekr 運作流程 圖片來源\nKubernetes 介紹 Kubernetes 也可以叫 K8s，這個名稱來源希臘語，意思是舵手或是飛行員，所以我們可以看到它的 logo 是一個船舵的標誌，之所以叫 K8s 是因為 Kubernetes 的 k 到 s 中間有 8 的英文字母，為了方便，大家常以這個名稱來稱呼它！\nK8s Logo\nK8s 是一種開源的容器資源調度平台，能夠將部署流程自動化、擴展並管理不同容器間的工作負載。它的構想理念是「Automated container deployment, scaling, and management」，意即透過自動化功能提升應用程式的可靠性和減輕維運負擔，讓開發人員專注於軟體開發任務。\n另外 K8s 也是 Google 開發的，並且是 CNCF (Cloud Native Computing Foundation) 的一個專案，所以 K8s 也是一個非常受歡迎的容器管理系統。\n我們之前在 Docker 介紹 文章中，已經有介紹以往傳統虛擬機以及容器化的 Docker 差異以及優點，那當我們在管理容器時，其中一個容器出現故障，則需要啟動另一個容器，如果要用手動，會十分麻煩，所以這時就是 Kubernetes 的厲害的地方了，Kubernetes 提供：\nKubernetes 優點 輕量級：K8s 的輕量化特性讓應用程式能被輕易地部署至不同環境，例如：地端資料中心、公有雲或其他雲端混合環境。K8s 容器化的本質讓封裝在內的應用程式與其相依的資源能夠緊密結合，從而解決不同平台的兼容問題，並降低在不同基礎架構上部署的難度。 服務、系統部屬更方便：由於容器可在任何容器平台運行，因此無論是同時將多個 Container 部屬到一台機器，或是多個 Container 部屬至多台機器都不是問題。 自動化管理，重啟、擴張皆可行：且 K8s 可自動偵測、管理各 Container 的狀態，若有需要，可對 Container 執行自動擴展。而若偵測到有 Container 發生故障，也可自動重啟以確保服務正確且持續地運行。 彈性化運用：K8s 中每個服務、系統皆可獨立部屬，因此不會因為其中一個系統出現錯誤而影響整個運作，甚至各 Container 也可依各自需求來修改，運用上擁有高度彈性化。 Kubernetes 是如何幫我們管理以及部署 Container ? 要了解 Kubernetes 如何運作，就要先了解它的元件以及架構：\nKubernetes 元件介紹 那我們由小的往大的來做介紹：依序是 Pod、Worker Node、Master Node、Cluster\nPod Kubernetes 運作中最小的單位，一個 Pod 會對應到一個應用服務 (Application)，舉例來說一個 Pod 可能會對應到一個 Nginx Server。\n每個 Pod 都有一個定義文件，也就是屬於這個 Pod 的 yaml 檔。 一個 Pod 裡面可以有一個或多個 Container，但一般情況一個 Pod 最好只有一個 Container。 同一個 Pod 中的 Containers 共享相同的資源以及網路，彼此透過 local port number 溝通。 Worker Node Kubernetes 運作的最小硬體單位，一個 Worker Node (簡稱 Node) 對應到一台機器，可以是實體例如你的筆電、或是虛擬機，例如：GCP 上的一台 Computer Engine。\nMaster Node (Control Plane) 負責各個 Worker Node 的管理，可稱作是 K8S 的發號施令的中樞。\n其他更詳細介紹，可以參考我之前寫的文章：Kubernetes (K8s) 介紹 - 基本\nCluster Cluster 也叫叢集，可以管理眾多機器的存在，在一般的系統架設中我們不會只有一台機器而已，通常都是多個機器一起運行，在沒有 Kubernetes 的時候就必須要土法煉鋼的一台一台機器去更新，但有了 Kubernetes 我們就可以透過 Cluster 進行控管，只要更新 Master 旗下的機器，也會一併將更新的內容放上去，十分方便。\nK8s 元件 圖片來源\nKubernetes 小試身手 為了大家能夠更了解 Kubernetes 的使用，我們來實際操作一下： (但由於 K8s 建立以及部署需要一些時間，所以這邊會直接拿我擁有的環境做測試，其他更詳細的操作可以參考我之前寫的文章：Kubernetes (K8s) 介紹 - 基本、Kubernetes (K8s) 介紹 - 進階 (Service、Ingress、StatefulSet、Deployment、ReplicaSet、ConfigMap))\n首先，我們接續上面的 Docker 小試身手 的範例程式碼：\n先執行 docker buildx build --platform=linux/amd64 -t 880831ian/0529-arm64:latest .，將 Image 建立起來 (這邊會多 buildx 跟 platform 是因為我的電腦是 Mac M 系列處理器，系統架構是 arm64，所以要放到 GKE 上面跑，需要多指令平台)。\n接著執行 docker push 880831ian/0529-arm64:latest，將 Image 上傳到 Docker Hub 上。\n進入 k8s 資料夾，裡面有幾個檔案，分別是：namespace.yaml、deployment.yaml、service.yaml、ingress.yaml。\n先執行 kubectl apply -f namespace.yaml，這個指令是用來建立一個 namespace，這樣我們就可以將我們的服務放到這個 namespace 下。\n接著執行 kubectl apply -f .，將其他服務也建立到 GKE 上。\n最後我們打開瀏覽器，輸入 https://myapp.pin-yi.me，就可以看到我們寫的首頁了。 (此為範例，講座結束後會關閉服務)\n看完 Kubernetes 部署服務的方式，是不是覺得有點麻煩呢？還需要手動去部署，這時候就需要 CI/CD 來幫助我們自動化部署服務了！","什麼是-cicd#什麼是 CI/CD":"一樣先來了解一下為什麼要 CI/CD？\n目前大多數的企業公司都是採用敏捷開發的方式，所以在追求快速開發、快速部署，以及大量的測試時會消耗很多開發人員的時間和精力，所以這時候就需要 CI/CD 來幫助我們自動化部署服務。\n接著再介紹 CI/CD 之前還有一個名詞要跟大家介紹，那就是 DevOps。\n什麼是 DevOps DevOps 是一種結合軟體開發人員 (Development) 和 IT 運維技術人員 (Operations) 的文化，目的是縮短開發和運營之間的距離，並且提高開發和運營的效率。而 CI/CD 工具就是為了此概念產生的自動化工具，透過持續整合 (CI) 和持續部署 (CD) 的方式，在開發階段能自動協助開發人員偵測程式碼問題，並部署到 Server 上。\nCI/CD cycle\nCI (Continuous Integration) 持續整合 持續整合，顧名思義，就是當開發人員完成一個階段性的程式碼後就經由自動化工具測試、驗證，協助偵測程式碼問題，並建置出即將部署的版本（Build）。\nCD (Continuous Deployment) 持續部署 持續部署可以說是 CI 的下一階段，經過 CI 測試後所構建的程式碼可以透過 CD 工具部署至伺服器，減少人工部署的時間。\nCI/CD 常用的工具 GitHub GitHub 算是目前最受歡迎的程式碼管理平台，其 CI/CD 服務稱為 GitHub Action，提供了多項控制 API，能夠幫助開發者編排、掌握工作流程，在提交程式碼後自動編譯、測試並部署至伺服器，也提供了很多現成的 Action 可以使用。\nGitLab GitLab 算是公司企業內部比較主流的程式碼管理平台，其 CI/CD 服務稱為 GitLab CI/CD，其 CI/CD Pipeline 功能簡單又實用，使用者只需要設定於專案根目錄下的「.gitlab-ci.yml」檔，便可以開始驅動各種 Pipeline 協助您完成自動化測試及部署\nCI/CD 小試身手 我們今天會使用 GitGub Action 來進行 CI/CD 範例\n這邊也跟上面 Kubernetes 小試身手 一樣，由於 K8s 建立以及部署需要一些時間，所以這邊會直接拿我擁有的環境做測試。\n首先，我們接續上面的 Docker 小試身手 的範例程式碼：\n打開 .github/workflows/google.yml 這個檔案，裡面是我已經寫好的 GitHub Action 的流程。 這邊說明一下這個檔案的內容 (下面拆開說明)：\nname: Build and Deploy to GKE on: push: branches: [\"main\"] env: PROJECT_ID: pin-yi-project GAR_LOCATION: asia-east1 GKE_CLUSTER: cluster GKE_ZONE: asia-east1-b DEPLOYMENT_NAME: myapp REPOSITORY: pin-yi-image IMAGE: myapp NAMESPACE: myapp jobs: setup-build-publish-deploy: name: Setup, Build, Publish, and Deploy runs-on: ubuntu-latest environment: production 這邊主要是說明這個 Action 的名稱，以及當 push 到 main branch 時，就會觸發這個 Action，還有對應的 env，env 包含了 GCP 專案 ID、Image 儲存的地點、GKE 的 Cluster 名稱、GKE 的 Zone、部署的名稱、Image 的名稱、Namespace 的名稱。\n接著是 job 的設定，設定名稱、要用什麼 image 當做基底，以及環境等等。job 可以把它理解成每個要做的小事情，例如 Build Image、Push Image、Deploy Image 等等。\nsteps: - name: Checkout uses: actions/checkout@v3 - id: \"auth\" uses: \"google-github-actions/auth@v2\" with: credentials_json: \"${{ secrets.GCP_CREDENTIALS }}\" - name: Docker configuration run: |- echo '${{ secrets.GCP_CREDENTIALS }}' | docker login -u _json_key --password-stdin https://$GAR_LOCATION-docker.pkg.dev - name: Set up GKE credentials uses: google-github-actions/get-gke-credentials@v2 with: cluster_name: ${{ env.GKE_CLUSTER }} location: ${{ env.GKE_ZONE }} - name: Build run: |- docker build \\ --tag \"$GAR_LOCATION-docker.pkg.dev/$PROJECT_ID/$REPOSITORY/$IMAGE:$(echo $GITHUB_SHA | head -c7)\" \\ --build-arg GITHUB_SHA=\"$GITHUB_SHA\" \\ --build-arg GITHUB_REF=\"$GITHUB_REF\" . - name: Add Image Tag run: docker tag $GAR_LOCATION-docker.pkg.dev/$PROJECT_ID/$REPOSITORY/$IMAGE:$(echo $GITHUB_SHA | head -c7) $GAR_LOCATION-docker.pkg.dev/$PROJECT_ID/$REPOSITORY/$IMAGE:latest - name: Publish run: |- docker push \"$GAR_LOCATION-docker.pkg.dev/$PROJECT_ID/$REPOSITORY/$IMAGE:$(echo $GITHUB_SHA | head -c7)\" \u0026\u0026 \\ docker push \"$GAR_LOCATION-docker.pkg.dev/$PROJECT_ID/$REPOSITORY/$IMAGE:latest\" - name: Set Image run: |- kubectl set image deployment/$DEPLOYMENT_NAME \\ myapp=\"$GAR_LOCATION-docker.pkg.dev/$PROJECT_ID/$REPOSITORY/$IMAGE:$(echo $GITHUB_SHA | head -c7)\" -n $NAMESPACE - name: Deploy run: |- kubectl rollout status deployment/$DEPLOYMENT_NAME -n $NAMESPACE kubectl get services -o wide 接著就是各種的 job，有先 checkout 程式碼、登入 GCP、登入 Docker、設定 GKE 的 credentials、Build Image、Push Image、Deploy Image 等等。\n接著我們嘗試調整 index.html 的內容，然後 push 到 GitHub 上，就可以看到 GitHub Action 會自動幫我們 Build Image、Push Image、Deploy Image 到 GKE 上。\n最後我們打開瀏覽器，輸入 https://myapp.pin-yi.me，就可以看到我們調整的首頁了。(此為範例，講座結束後會關閉服務)","參考資料#參考資料":"地端是什麼？可以吃嗎？：https://wanchunghuang.com/what-is-on-premises/\n雲端 VS 地端，數位轉型怎麼選？：https://blog.hackmd.io/zh/blog/2022/02/22/cloud-vs-on-premises\n什麼是雲端？ | 雲端定義： https://www.cloudflare.com/zh-tw/learning/cloud/what-is-the-cloud/\n企業如何挑選合適的雲端服務？公有雲、私有雲差異比較總整理：https://aws.amazon.com/tw/events/taiwan/techblogs/public-cloud-private-cloud/\n給資料科學家的 Docker 指南：3 種活用 Docker 的方式（上）：https://leemeng.tw/3-ways-you-can-leverage-the-power-of-docker-in-data-science-part-1-learn-the-basic.html\nDocker 是什麼？Docker 基本觀念介紹與容器和虛擬機的比較：https://www.omniwaresoft.com.tw/product-news/docker-news/docker-introduction/\nCI/CD 是什麼？一篇認識 CI/CD 工具及優勢，將日常瑣事自動化：https://www.wingwill.com.tw/zh-tw/%E9%83%A8%E8%90%BD%E6%A0%BC/%E9%9B%B2%E5%9C%B0%E6%B7%B7%E5%90%88%E6%87%89%E7%94%A8/cicd%E5%B7%A5%E5%85%B7/\n[DevOps] CI/CD 介紹 - 基礎概念與導入準備：https://enzochang.com/cicd-introduction/\nKubernetes（K8s）是什麼？基礎介紹+3 大優點解析：https://www.metaage.com.tw/news/technology/293\n我之前寫過的所有文章：https://pin-yi.me/blog","如何最快恢復網站#如何最快恢復網站":"如何最快恢復網站，這是一個非常重要的議題，因為網站的停擺會造成業務的損失，也會讓使用者對網站的信任度下降，所以我們需要有一個很好的應急計畫，來應對這種狀況。\n網站停擺的原因 那在討論如何恢復前，我們要先了解會導致網站停擺的原因有哪些？\n伺服器供應商故障 這邊的伺服器供應商，如果以雲端的供應商來說，也就是 GCP、AWS、Azure 等等，這些供應商可能會因為硬體故障、軟體錯誤、網路問題等原因而停擺。 當然地端的機器也可能會因為主機的供應商導致網站中斷，例如：Windows 自動更新 xD。\n網站伺服器故障 網站的伺服器當然也可能會因為硬體故障、軟體錯誤、網路問題等原因而停擺。這也是最常見的原因之一。 這個的伺服器也可以是地端的伺服器，也可以是雲端的伺服器。\n我們以雲端的 K8s 來舉例，如果 K8s 配置錯誤，會是資源不足，或是 Pod 的數量不夠，都會導致網站停擺。\n網站程式碼錯誤 這邊的程式碼，大家可以把他理解成網站運行的 Code，例如：PHP、Python、JavaScript 等等，有可能程式碼寫了一個無窮迴圈，或是什麼特殊的 Bug，導致資源衝高，也會導致網站停擺。\n網站被駭客攻擊 當然網站會依照使用需求，開放給不同的使用者，如果是一般開在公網上的網站，就有可能會被駭客攻擊，例如：DDos 攻擊、SQL Injection 等等，這些攻擊都有可能導致網站停擺，還有可能導致資料外洩。\n網站被判定成非法網站 這邊的非法網站，是指網站內容違反了法律規定，例如：色情、賭博、毒品等等，這些內容都有可能導致網站被封鎖，或是被檢舉，進而導致網站停擺。\n網站解析被擋 圖片來源\n網站恢復(避免)的方法 伺服器供應商故障 雲端供應商故障的發生機率在這幾個裡面來說，算是最少的，因為雲端供應商有很多的備援機制，當然為了避免意外，在有能力之餘，也可以考慮多個雲端供應商來當作備援，這樣就可以避免單一供應商故障。\n我們以雲端的 GCP 來舉例，如果 GCP 的服務有問題，可以到 Google Cloud Status Dashboard 來查看目前服務的狀況。\n分享一下最近發生的案例：https://status.cloud.google.com/incidents/xVSEV3kVaJBmS7SZbnre\n另外，GCP 也有提供 SLA，當服務有問題並超出一開始 Goolge 所承諾的服務水準時，可以向 GCP 提出申請，來獲得賠償。\nGoogle Cloud Platform Service Level Agreements\n網站伺服器故障 會出現錯誤的原因有很多，例如：硬體故障、軟體錯誤、網路問題等等，這邊我們可以透過對服務系統進行不同的監控來提早發現問題，並且提早解決。當然也建議多做備份，當網站出現問題時，可以快速的恢復。平常也要演練災難復原計畫。\n網站程式碼錯誤 程式碼的錯誤，要再上線前多做測試，例如：單元測試、整合測試、壓力測試等等，也要先做好程式碼的規範，例如：程式碼的風格、程式碼的註解等等，避免出現一些不必要的問題。\n網站被駭客攻擊 網路安全是很重要的，所以從入口的防火牆設計、黑白名單，再者到程式碼的安全性，例如：SQL Injection、XSS、CSRF 等等，都要做好防護，並且要定期的去更新程式碼裡面使用的套件，才可以避免被駭客攻擊。\n另外在雲端的使用上有一點要特別注意，就是權限的設定劃分要明確，確保最小權限原則，避免帳號金鑰外流導致駭客取得權限。\n網站被判定成非法網站 這邊當然不是討論去架設相關的網站，導致封鎖要怎麼辦 (◕ܫ◕)，我們這邊討論的是，不一定真的是架設非法的網站，才會被封鎖，我們來看一下這個新聞：Google 被刑事局認定「涉及詐騙」網傻眼！台灣大曝真相 這邊建議使用的網址要是可辨識，不要使用一些奇怪的網址，才可以避免被誤判，以及如果使用雲端供應商提供的 IP 以前，也先可以先去掃描一下 IP 是否有被組織或是單位確認是非法 IP。如果真的被封鎖，可以向相關單位申訴，來恢復網站。","網站維運日常#網站維運日常":"這邊我就以目前我在公司的經驗來跟大家分享一下，網站維運日常都會做哪些事情：\n主要分為以下幾件事情：\n監控 我們需要做各種的監控，例如：網站的流量、CPU 使用率、記憶體使用率等等，這些都是我們需要監控的項目。除此以外也需要監控 RD 使用的程式語言有對應的指標，例如：PHP 的 FPM 的進程等等指標。\n監控的目的是，讓我們服務在出現問題前，就可以提早透過告警知道有可能潛在的問題，並且提早解決。避免被使用者發現問題，進而影響使用者的體驗。\n做各種的監控\n查 Log 我們每個服務都需要去紀錄 Log，不管是使用者的操作、系統、還是程式的錯誤，都需要去紀錄下來，這樣當網站出現問題時，我們可以透過 Log 來查找問題，並且解決問題。\n當然 Log 也不是一個簡單的議題，Log 數量很多，在不影響效能以及費用的情況下，要怎麼只存精簡且重要的 Log，也是一個需要思考的問題。\n查 Log\n協助 RD 調整設定以及排查問題 由於雲端使用 K8s 微服務的架構，所以會有很多 K8s 上的設定，例如：服務入口的 Nginx 倒轉、timeout 設定的問題、IP 是來源哪一個服務等等，有些時候我們還需要配合 RD 來進行排查程式碼，才能真正解決問題。\n所以要成為一個優秀的 SRE 也是需要具備一定的程式能力，才能夠更好的協助 RD 來解決問題。\n不要變成 Site Restart Engineer 工程師 (雖然有時候真的 restart 就好了 ( ͡° ͜ʖ ͡°))\nSite Restart Engineer XDDD\n開發能夠在維運上更方便的工具 除了上面說的已經可以讓我們身心俱疲的幾件日常，我們也會開發一些讓我們維護上可以更方便的一些工具，盡量都變成自動化的方式，去減少我們的工作量。\n例如：\n簡單的壓測網站 HTTP 狀態碼工具：chece_url_http_code 自動修復 elasticsearch 錯誤工具：auto_repair_es 計算哪時候可以下班的工具：can_get_off_work XD ","要走軟體工程師需要具備哪些技能#要走軟體工程師需要具備哪些技能？":"下面是我準備的小 bonus，是出自我自己的經驗，以及我偷偷觀察我合作的 RD 夥伴們，得出的小小心得，當然這只是我自己的看法，不一定是對的，希望能夠幫助大家。\n當然，想成為一個軟體工程師，你一定要會程式語言，例如 PHP、Python、JavaScript 等等，假如你對於未來的發展方向還不確定，或是不知道哪個程式語言是未來的趨勢，可以看看 stackoverflow 每年的調查報告，來了解目前最流行的程式語言是哪些： https://survey.stackoverflow.co/2023/#most-popular-technologies-language\n但除了這個基本的寫 Code 技能外，我還建議能夠提早學習到以下幾項技能，會對你未來的發展有很大的幫助：\nGit 版本控制 Git 可以說是現代軟體工程師必備的技能之一，可以幫助你管理程式碼的版本，當你的程式碼出現問題時，可以很快的回復到之前的版本，也是你跟其他工程師協作的最好工具。\n因為目前，你可能都是自己一個人寫 Code，所以不會碰到同時間有多個人在調整程式時，導致程式碼衝突的問題，當你沒有使用 Git 時，你會不知道這是誰改的，這個是什麼時候改的，這個改動是為了什麼，這樣會讓你的程式碼變得非常雜亂，也會讓你的程式碼變得非常難以維護。\n寫 Blog 培養寫 Blog 的習慣，可以幫助你整理自己的知識，也可以幫助你記錄自己的學習過程，像我當初會寫 Blog 還有一個原因，是因為我記憶不好，如果我不寫下來，就會常常忘記一樣的問題 (๑•́ ₃ •̀๑)。\n當然在找工作面試時，它也可以當作自己的作品集，讓面試官能夠了解你、並知道你的學習能力。\n溝通能力 在職場上，溝通能力是非常重要的一環，因為你不可能一個人在公司裡面工作，你一定會跟其他人合作，所以你要學會如何跟其他人溝通，如何表達自己的想法，如何聆聽別人的意見，這些都是非常重要的技能。\n每個行業都是，但我認為軟體工程師更需要這個技能，因為軟體工程師的工作是非常複雜的，你需要跟 PM、RD、QA、UI/UX、SRE 等等不同的部門合作，所以你要學會如何跟不同的人合作，這樣才能夠讓你的工作更順利。","講座資訊#講座資訊":" 時間：2024/05/29 19:00 - 21:00 地點：朝陽科技大學 \u0026 Google Meet 線上連結 Google Developer Student Clubs 相關連結：https://gdsc.community.dev/events/details/developer-student-clubs-chaoyang-university-of-technology-taichung-taiwan-presents-wang-zhan-fang-hu-yu-devopsji-shu-jiang-zuo-ft-zhuang-pin-yi-gong-cheng-shi/ 簡報：https://docs.google.com/presentation/d/10yH_zQikWeYz5pyv4st6ICMs6F2F9F_Vks1ngDaNROY/edit?usp=sharing 講座錄影：https://drive.google.com/file/d/1-TRJ3mAYehmPSr8Sg4p09dA7t4mor4CY/view?usp=drive_link ","雲端和地端的差別#雲端和地端的差別":"首先，想問一下大家，你們知道雲端和地端的差別嗎？\n地端 我們先來説説「地端」是什麼？地端的英文是 On-Premises，簡稱 On-Prem，代表所有安裝、運行都在個人電腦或公司自己內部的伺服器上所執行的運行方式。\n最好的例子就是你電腦裡面裝的 Windows 作業系統、Office 文書軟體、或是公司內部的 ERP 系統等等。這些軟體都是安裝在你的電腦或公司伺服器上，並且由你自己或公司的 IT 團隊負責管理，所以這也是目前最普遍的ㄧ種軟體運行方式。\n地端的優點是什麼？ 控制權：你可以完全控制你的伺服器和軟體，不用擔心第三方服務商的服務品質，想把機器擺在哪裡就放哪裡、用什麼型號都可以自己決定。 安全性：企業或是個人可以自行控制資料的存取權限，不用擔心第三方服務商的資料外洩問題。 成本：一次性購買伺服器和軟體，不需要每個月支付雲端服務費用。 地端的缺點是什麼？ 成本：一次性購買伺服器和軟體，需要花費大量的資金，且不確定是否符合需求使用。 維護：需要自行管理伺服器和軟體，包括硬體維護、軟體更新、資料備份等等。 擴充性：當使用者或是需求增加時，需要自行擴充伺服器和軟體，且需要花費大量的時間和金錢。 那相信大家已經大致了解地端的定義了，接下來我們來看看「雲端」是什麼？\n雲端 「雲端」的英文是 Cloud，不需要安裝在個人電腦或公司伺服器上，而是透過網際網路存取的伺服器，以及在這些伺服器上執行的軟體或是資料庫。常見的例子有 Google 雲端硬碟、Dropbox、或是 Google Cloud、AWS、Azure 等雲端服務商。\n如何使用雲端 圖片來源\n雲端得以實現，是因為一種稱為虛擬化的技術。虛擬化允許在一台實體的電腦上建立許多模擬「虛擬」電腦，其行為如同具有自己硬體的實體電腦，這類電腦的術語稱為虛擬機器。虛擬主機簡單介紹到這邊，後面的 Docker 介紹 會在提到。\n雲端會有很多資料中心，一個資料中心又有很多台的伺服器，一個伺服器可以執行許多虛擬「伺服器」，能夠為許多組織或是客戶提供不同的服務，即使個別伺服器故障，雲端供應商會在多個機器及多個區域備份服務。Google Data Center 影片連結\nGoogle Data Center 圖片來源\n雲端又可以分為以下幾種：\n公有雲（Public Cloud） 故名思義，公有雲由第三方雲端服務商提供，如 Google Cloud、AWS、Azure 等。這些服務商提供的平台允許用戶租用伺服器、資料庫、儲存空間等資源，用戶只需一組帳號和密碼即可訪問。公有雲的特點包括：\n彈性和可擴展性：用戶可以根據需求動態調整資源的規模。 成本效益：按需付費模式，企業可以根據實際使用情況支付費用。Google Cloud Pricing Calculator 全球覆蓋：可選擇部署的地區範圍廣泛。Google Cloud 據點 除了彈性和成本效益，最重要的還有服務商安全管理和維護及 SLA 的優勢。Google Cloud Platform Service Level Agreements\nTop 10 Cloud Service Providers Globally in 2024 圖片來源\n私有雲（Private Cloud） 私有雲是由企業自建或由第三方供應商為特定企業提供的雲端基礎設施，僅供企業內部使用。其特點包括：\n專屬使用：企業獨享全部資源，不與其他用戶共享。 高可控和安全性：企業可以自行管理和控制數據，確保數據安全性和合規性。 高成本：需要自行購買和維護硬體和基礎設施，初期投資和運營成本較高。 What Is Private Cloud? 圖片來源\n聽完公有雲和私有雲的介紹，有沒有可以結合兩種雲端的優點，來達到更好的效果呢？\n混合雲（Hybrid Cloud） 混合雲是一種結合了公有雲和私有雲優勢的雲端架構，允許企業在公有雲和私有雲之間靈活地配置和管理資源。這意味著企業可以將一些工作負載和應用程式部署在公有雲上，以利用其彈性和成本效益，同時將敏感資料和關鍵業務應用部署在私有雲或內部資料中心，以確保安全性和控制。\n所以以下幾個就是混合雲的優點：\n彈性和可擴展性：企業可以根據需求動態調整公有雲和私有雲的使用比例。例如，當工作負載增加時，可以暫時使用公有雲資源來擴展運算能力，而不必立即擴展內部基礎設施。\n成本效益：通過利用公有雲的按需付費模式，企業可以降低運營成本，避免在私有雲上過度投資。而私有雲則可以用來運行穩定且持久的工作負載。\n安全性和合規性：敏感數據和應用可以保存在私有雲或內部資料中心，以滿足企業的安全和合規要求。同時，非敏感的工作負載可以部署在公有雲上，以利用其便利性和成本優勢。\n高可用性和災難恢復：混合雲架構允許企業在公有雲和私有雲之間設置備份方案，增強系統的高可用性和災難恢復能力。\n所以混合雲也是目前最多企業採用的雲端架構，可以兼顧公有雲和私有雲的優勢，並根據實際需求靈活調整資源的使用。\n雲端還是地端好，要如何選擇？ 沒有一個明確的答案，要看你的需求和預算，以及公司的規模和發展方向。\n假設你是一個小型公司，可能沒有太多資金去購買伺服器和軟體，也沒有太多人力去管理、或是者你的公司業務是跨國的，需要在全球提供服務以及需求高可用性，或是長時間有大量的流量，例如：遊戲公司，那麼雲端就是一個不錯的選擇。\n如果公司資料敏感性高，需要自行控制資料存取權限，或是公司有自己的 IT 團隊，有能力管理伺服器和軟體，那麼地端就是一個不錯的選擇。\n我們了解了雲端和地端的差別，接下來了解一下有什麼技術可以幫助我們更好的部署我們的服務呢？"},"title":"朝陽科技大學 - 安全前瞻：網站防護與 DevOps 技術講座"},"/blog/docker/":{"data":{"":"此分類包含 Docker 相關的文章。\nDocker Image Pull Rate Limit 說明 + 掃描腳本發布日期：2025-02-24 使用 Prometheus 和 Grafana 打造監控預警系統 (Docker 篇)發布日期：2022-05-18 Docker 介紹 (如何使用 Docker-compose 建置 PHP+MySQl+Nginx 環境發布日期：2022-03-14 "},"title":"Docker 相關"},"/blog/docker/docker/":{"data":{"":"","docker-基本概念#Docker 基本概念":" Dockerfile 映像檔 (Image) 容器 (Container) 倉庫 (Repository) 這四個是 Docker 最基本的組成，了解後就可以知道整個 Docker 的生命週期。\nDockerfile 開發人員在使用 Docker 時發現，大多現成的 Docker 映像檔無法滿足他們的需求，因此需要一種能夠生成映像檔的工具。Dockerfile 是一種簡易的文件檔，裡面包含了建立新映像檔所需的指令。\nDockerfile 語法主要由 Command（命令）和 Argument （參數選擇）兩大元素組成。以下是一個簡易的 Dockerfile 示意圖：\n命令式語法＋選擇參數（Command + Argument）\n映像檔 (Image) Docker 映像檔是一個唯獨的模板。\n例如：一個映像檔可在包含一個完整的 Linux 作業系統環境，裡面可以只安裝 Nginx 或使用者會使用到的其他應用程式。\n我們會使用映像檔來建立 Docker 的容器 (Container) ，Docker 也提供很簡單的機制來建立映像檔或是更新現有的映像檔，也可以去下載別人已經做好的映像檔。\n容器 (Container) Docker 是利用容器來執行應用程式。\n每一個容器都是由映像檔所建立的的執行程式。它可以被啟動、開始、停止、刪除。且每一個容器都是相互隔離的，不會相互影響。","docker-實作#Docker 實作":"本章節會介紹 Docker 三大組件映像檔 (image)、容器 (Container)、倉庫 (Repository) 要如何實際操作，以及他們的關聯性是什麼～ 映像檔 (Image) 本小節會介紹有關映像檔的內容，包括：\n如何從倉庫取得映像檔 如何管理本地主機上的映像檔 映像檔實作 (Dockerfile) 如何從倉庫取得映像檔 我們可以先到 Docker hub 上面看看有什麼服務或程式想要下載來做使用 [ 詳細介紹會放到倉庫 (Repository) 章節]，找到想要的服務，我們可以下 docker pull {要下載的服務、程式名稱} ，我們這邊就先下載 Mysql 這個映像檔。\nmysql $ docker pull mysql Using default tag: latest latest: Pulling from library/mysql 15115158dd02: Pull complete .... 省略 .... Digest: sha256:b17a66b49277a68066559416cf44a185cfee538d0e16b5624781019bc716c122 Status: Downloaded newer image for mysql:latest docker.io/library/mysql:latest 由於我們下載的沒有加任何的 tag ，也就是版本，所以我們都是下載最新版 latest ，如果想要下載特定版本，可以在服務名稱後面加上 :{版本} ，就可以下載對應的版本囉！\n如果有標記 Official Image 就代表是官方釋出的映像檔 ~ 在穩定性以及安全上更有保障，所以大家可以優先下載歐！\nDocker hub 下載 Image\n管理本地主機上的映像檔 查看映像檔 (images) 當我們下載好映像檔後，可以使用 docker images 來列出本機已下載的映像檔。\n$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE php latest d6229b88aa29 3 days ago 484MB mysql latest 826efd84393b 6 days ago 521MB nginx latest c919045c4c2b 13 days ago 142MB 我們來看看都列出哪些訊息吧！\nREPOSITORY：來自於哪一個倉庫，像是 php、mysql、nginx。 TAG：映像檔的標籤，因為我們都下載最新版本所以是 latest。 IMAGE ID：它的唯一 ID 號碼。 CREATED：建立時間。 VIRTUAL SIZE：映像檔大小。 儲存映像檔 (save) 想要儲存映像檔，可以使用 docker save {映像檔名稱} \u003e {檔案名稱}.tar，來做儲存。\n$ docker save mysql \u003e mysql.tar $ ls | grep 'mysql' mysql.tar 刪除映像檔 (rmi) 想要刪除映像檔，可以使用 docker rmi {映像檔名稱} 來做刪除。\n$ docker rmi demo-image Untagged:demo-image:latest Deleted: sha256:1f56acbcbe9ec613a37e26934a84d98bed73879059f424dc69754520086baa37 想要同時刪除映像檔時，可以先用 docker images -aq 列出全部映像檔的 IMAGE ID，再一起刪除。\ndocker rmi -f $(docker images -aq) 注意！刪除映像檔(Image)時，要先用 docker rm {容器} 去刪除所有依賴這個映像檔的容器。 接著要怎麼建立自己的映像檔呢？我們要使用 Dockerfile 來建立映像檔。\n映像檔實作 撰寫 Dockerfile 映像檔 Dockerfile 是一種文字格式的設定檔，可以透過 Dockerfile 快速建立自訂的映像檔，換句話說，Dockerfile 就像是建置 Docker Image 的腳本。\n舉個例子：可以把自己想像成一位設計師，設計好房子的格局、擺飾等，畫好設計圖 (Dockerfile) 後，最後請師傅 (Docker) 依你的構思完成就可以了。\n我們就來開始實作一個 Dockerfile 吧 ~\n我們先創建一個來放 Dockerfile 的資料夾，可以直接在路徑下建立出映像檔 mkdir demo-dockerfile cd demo-dockerfile/ Dockerfile 結構，大致可以分為四個部分\n基礎映像檔資訊 維護者資訊 映像檔操作指令 容器啟動時需執行的指令 我們有說過 Dockerfile 是一個文字格式的設定檔，所以我們用 vim 來編寫 Dockerfile。 vim Dockerfile # 基礎映像檔資訊 FROM nginx:latest # 維護者資訊 LABEL maintainer=\"880831ian@gmail.com\" # 映像檔操作指令 RUN apt-get update -y\\ \u0026\u0026 apt-get install nginx -y # 運行時容器提供服務的通道 EXPOSE 80 # 容器啟動時需執行的指令 CMD [\"nginx\",\"-g\",\"daemon off;\"] FROM nginx:latest\n第一行為必要的指定基礎映像檔，這邊使用 nginx 作為基礎映像檔，我們用最新版本，所以是 latest。\nLABEL maintainer=\"880831ian@gmail.com\"\n維護者資訊想不也是不可以少的，這邊也可以輸入 Email 資訊，只是要注意的是此資訊會寫入產出映像檔的 Author 名稱屬性中。\nRUN apt-get update -y\\ \u0026\u0026 apt-get install nginx -y\n這邊是最重要的部分，想要在映像檔案上設定或安裝都需要將命令寫在這，格式必須依 RUN ，RUN 指令後面放 Linux 指令，如果指令太長可以使用\\來換行。 -y 是在安裝 Nginx，會同意所有進行中所出現的問題。\nEXPOSE 80\n設定運行時容器提供服務的通道。\nCMD [\"nginx\",\"-g\",\"daemon off;\"]\n最後就是啟動指定容器時預設執行的指令，格式是 CMD [“executable”,“param1”,“param2”]。\nDocker 運行 Nginx 時為什麼要使用 daemon off;\n因為 Docker 容器啟動時，默認會把容器內部第一個進程，作爲 docker 容器是否正常運行的依據，如果 docker 容器 pid = 1 到進程就掛了，docker 就會退出！\nDocker 未執行自定義的 CMD 之前， Nginx 的 pid 是 1，執行到 CMD 之後，Nginx 就在後台運行，bash 或是 sh 的腳本就會變成 pid =1 。\n所以一但執行完 CMD，Nginx 容器就會退出了，所以才需要加上 -g daemon off;。\n在 Nginx 官方的 Docker Repository 也有說明，在 Complex configuration 內。\n順便說一下使用 Dockerfile 的優點：1 . 可以進行 Git 版控，讓你管理或分享更方便 2 . 佔用容量小，因為只是純文字檔而已。\n使用 Dockerfile 建立映像檔 我們已經撰寫完 Dockerfile 檔案了，接下來要執行來產生映像檔，我們要使用 docker build 來建立，我們一起來看看吧\ndocker build -t demo-image . 因為我們在 dockerfile 的目錄下，所以直接使用 “.” 來做建立動作，也可以使用 -f 來指定 dockerfile 的路徑位置。使用 -t 來設定映像檔的名稱，我們這邊取名叫 demo-image。\n[+] Building 2.7s (7/7) FINISHED =\u003e [internal] load build definition from Dockerfile 0.0s =\u003e =\u003e transferring dockerfile: 44B 0.0s =\u003e [internal] load .dockerignore 0.0s =\u003e =\u003e transferring context: 2B 0.0s =\u003e [internal] load metadata for docker.io/library/nginx:latest 2.6s =\u003e [auth] library/ubuntu:pull token for registry-1.docker.io 0.0s =\u003e [1/2] FROM docker.io/library/nginx:latest@sha256:8ae9bafbb64f63a50caab98fd3a5e37b3eb837a3e0780b78e5218e63193961f 0.0s =\u003e CACHED [2/2] RUN apt-get update -y \u0026\u0026 apt-get install nginx -y 0.0s =\u003e exporting to image 0.0s =\u003e =\u003e exporting layers 0.0s =\u003e =\u003e writing image sha256:1f56acbcbe9ec613a37e26934a84d98bed73879059f424dc69754520086baa37 0.0s =\u003e =\u003e naming to docker.io/demo-image 完成後我們使用 docker images 來看看是否建立成功。\n$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE demo-image latest 1f56acbcbe9e 2 hours ago 166MB php latest d6229b88aa29 4 days ago 484MB mysql latest 826efd84393b 6 days ago 521MB nginx latest c919045c4c2b 13 days ago 142MB 執行容器的部分，我們放到容器 (Container)的章節在介紹 ~\n容器 (Container) 我們在介紹指令前，先來了解一下 Dockerfile、Docker Image、Docker Container 這三個的關係，可以先參考以下圖片\n容器 Container 組成\n我們在啟動 Container 時，會有這三個部分組成，最底層是映像檔 (Image)，這一層主要是透過撰寫 Dockerfile 之後 build 出來的 Docker Image，就像我們前面說的它是一個唯獨的檔案。執行啟動了 Docker Container，就會加上第二層，就是需要先 Init Container 的設定，例如是 hostname、環境變數、網路連接等系統設定，最後最上層再加上一層讓使用者可以在此層去讀寫資料。\n有關於容器 (Container) 的指令非常多，光是簡單的 run 就有很多參數，我們先列出比較常用且基本的 Container 指令～\nContainer 執行時的操作 執行容器 (run) 我們想要創建一個新的容器並運行，就可以使用 docker run，我們來看看他可以使用哪些參數吧！\ndocker run [OPTIONS] IMAGE [COMMAND] [ARG...] OPTIONS 說明： 參數 描述 -d 後台運行容器 -i 命令互動模式，通常與 -t 同時使用 -t 為容器重新分配一個假裝的輸入終端，通常與 -i 同時使用 -p 指定容器與本機的 Port ，格式是 主機 Port : 容器 Port –name=\"{名稱}\" 為容器設定名稱 –net=\"{網路類型}\" 指定容器的網路連接類型，支援 bridge/host/none/container 四種模式 –link=\"{其他容器}\" 添加連接到另一個容器 –volume,-v 將容器檔案路徑映射到本地端，格式是 本機路徑：容器路徑 我們啟動我們下載好的 Nginx 來試試看吧！\n$ docker run -d -p 7777:80 --name=\"demo-nginx\" -v /Users/ian_zhuang/Desktop/data:/var/www/html nginx 31a4a4a56e3ef2fb75d538c4c9eea4914ac506a84a6ff97e1fbbb6c3213cc6b7 我們將 Nginx 容器在背景執行，且將預設的 80 Port 與本機的 7777 Port 綁在一起，讓我們在本機瀏覽 7777 Port 會直接導向容器的 80Port，設定容器的名字叫做 “demo-nginx\" ，我們 Nginx 容器的檔案路徑映射到本地端的桌面 data 資料夾，我們就可以在本機新增檔案同步到容器中。\n顯示容器 (ps) 使用 docker ps 來檢查一下是否啟動成功 (ps 可以顯示映像檔的基本資訊，如果沒有加 -a 只會顯示執行中的容器)\n$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 31a4a4a56e3e demo-image \"nginx -g 'daemon of…\" About a minute ago Up About a minute 0.0.0.0:7777-\u003e80/tcp demo-nginx 接著我們測試是否有把桌面 data 資料夾掛到容器的路徑，我們先在 data 新增一個 hello.html ，裡面隨意輸入，瀏覽一下 http://127.0.0.1:7777/hello.html ，看看是否成功。\n容器 Container -volume 測試\n顯示容器紀錄 (logs) 想要看到我們執行 Container 的紀錄，可以使用 logs 指令來顯示。\n$ docker logs demo-nginx /docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration /docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/ /docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh ... 省略 ... 2022/03/15 09:27:51 [notice] 1#1: start worker process 33 2022/03/15 09:27:51 [notice] 1#1: start worker process 34 2022/03/15 09:27:51 [notice] 1#1: start worker process 35 刪除容器 (rm -f) 想要刪除不需要的 Container，可以使用 rm 指令來做刪除，-f 是強制刪除容器。\n$ docker rm -f demo-nginx demo-nginx 進入容器 (exec) 想要進入 container 來查看資料或是修改檔案，可以使用 exec 來進入容器中。\n$ docker exec -it demo-nginx /bin/bash root@31a4a4a56e3e:/# ls bin boot dev\tdocker-entrypoint.d docker-entrypoint.sh etc\thome lib lib64 media mnt opt proc root run sbin srv sys tmp usr var root@31a4a4a56e3e:/# cd usr/bin/ root@31a4a4a56e3e:/usr/bin# pwd /usr/bin 匯出檔案 (export) 我們前面有提到說，如果刪除了容器，以前寫入的資料也會不見，如果想要輸出資料，可以使用 export 將可讀可寫的那一層匯成檔案。\n$ docker export demo-nginx \u003e demo-nginx.tar $ ls | grep 'demo' demo-nginx.tar save 跟 export 的區別：\n還記得我們在儲存映像檔的時候有介紹到 save 嗎，那他跟 export 的區別是什麼呢？我們可以理解成\nsave 是把 Docker Image 原始檔做儲存，export 是把修改 Docker Image 的內容都一併儲存。\n匯入檔案 (import) 有匯出檔案，當然也有匯入檔案拉，可以使用 import 將我們匯出的檔案匯入 Docker Image 裡面。\n$ cat ~/Desktop/demo-nginx.tar| docker import - import-nginx sha256:7106935f0bfbdbb84f9eb20edb8cdb2c53207f5e0963f6a4e89d8267e9d98c56 $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE import-nginx latest 7106935f0bfb 19 seconds ago 140MB Container 的狀態 檢查容器狀態 (inspect) 想要查看容器的狀態數據，可以使用 inspect 來顯示。\n[ { \"Id\": \"sha256:1f56acbcbe9ec613a37e26934a84d98bed73879059f424dc69754520086baa37\", \"RepoTags\": [ \"demo-image:latest\" ], \"RepoDigests\": [], \"Parent\": \"\", \"Comment\": \"buildkit.dockerfile.v0\", \"Created\": \"2022-03-15T03:02:26.8321887Z\", \"Container\": \"\", \"ContainerConfig\": { \"Hostname\": \"\", \"Domainname\": \"\", \"User\": \"\", .... 省略 .... 查看容器的 CPU、記憶體及網路使用 (stats) 想要查看容器的 CPU、記憶體及網路使用，可以使用 stats 來顯示。\nCONTAINER ID NAME CPU % MEM USAGE / LIMIT MEM % NET I/O BLOCK I/O PIDS 31a4a4a56e3e demo-nginx 0.00% 5.797MiB / 1.939GiB 0.29% 6.29kB / 2.83kB 1.46MB / 20.5kB 5 倉庫 (Repository) 我們在映像檔的章節有使用 docker pull 來下載別人的映像檔來使用，那我們要如何把我們做好的上傳上去 Docker hub 呢！(由於 Docker hub 是公開平台，所以大家都可以自由的下載映像檔，所以是公司機密的映像檔，就要避免上傳歐！)\n$ docker login Authenticating with existing credentials... Login Succeeded 由於我有下載桌面版的 Docker ，所以登入時不需要再另外設定！\n我們在上傳到 Docker hub 之前，需要先修改 Image 的 tag ，格式 docker tag {Image Name} {DockerHub帳號}/{想要取的 Image Name}\n$ docker images | grep 'demo-image' demo-image latest 1f56acbcbe9e 24 hours ago 166MB $ docker tag demo-image 880831ian/demo-image $ docker images | grep 'demo-image' 880831ian/demo-image latest 1f56acbcbe9e 24 hours ago 166MB demo-image latest 1f56acbcbe9e 24 hours ago 166MB 接下來就使用 docker push，將映像檔上傳到 Docker hub 上！\n$ docker push 880831ian/demo-image Using default tag: latest The push refers to repository [docker.io/880831ian/demo-image] 7722c88c8d69: Pushed 68a85fa9d77e: Mounted from library/ubuntu latest: digest: sha256:814caacaf3dad3eccb43dc9bcad635d0473bd5946295d40ca1ec23d13a5f6d0f size: 741 我們也登入 Docker hub 看一下，是不是真的上傳成功了～\nrepository","docker-進階#Docker 進階":"本章節會分成三個常用功能來說明\nVolumes 介紹 Network 模式介紹和比較 Docker-compose 介紹與實作 Volumes 介紹 我們在先前介紹 Container 時，也有說到，Container 會分成 Image 層、Init 層以及使用者可讀可寫層的這三層。當我們將 Container 刪除後，存放在 Docker Container 上的資料也會不見，雖然可以用 export 來儲存，但我們應該在根本上解決問題。\n所以我們可以使用兩種方式來解決！\n在執行 docker run 指令時加入 -v 參數，將 Container 的檔案路徑映射到本地端的檔案路徑。 在撰寫 Dockerfile 時，加入 VOLUME 指令，可以將資料存放在實體主機上。使用這個方法還要搭配我們介紹 Container 的狀態 \u003e 檢查容器狀態 (inspect) ，來查詢本地端檔案的存放路徑在哪。 使用 -v 指令將容器映射到本地端 在使用 docker run 指令時，使用 -v 將容器檔案路徑映射到本地的檔案路徑。\n$ docker run -it -v /Users/ian_zhuang/Desktop/data:/storage centos /bin/bash latest: Pulling from library/centos Digest: sha256:a27fd8080b517143cbbbab9dfb7c8571c40d67d534bbdee55bd6c473f432b177 Status: Downloaded newer image for centos:latest [root@a99b41fba3ca /]# 我們在著面的 data 資料夾隨意新增檔案或是資料，再進入 Docker Container 內的 /storage (該檔案是因為使用 -v 而新增的資料夾) ，看看有沒有同步新增。\n[root@a99b41fba3ca /]# cd storage [root@a99b41fba3ca storage]# pwd /storage [root@a99b41fba3ca storage]# ls 1.html\t2.html\t3.html Dockerfile VOLUME 使用 我們先打開上次的 Dockerfile 檔案，在基礎映像檔資訊的底下使用 VOLUME 指令，加入 storage 資料夾，再將 Image Build 起來，啟動 Container，在 /storage 裡面隨便新增資料，最後我們在用 docker inspect 指令來找映射在本地端的路徑。\n在基礎映像檔資訊的底下使用 VOLUME 指令，加入 storage 資料夾 # 基礎映像檔資訊 FROM ubuntu:latest VOLUME [\"/storage\"] .... 省略 .... 建立映像檔 $ docker build -t demo-image:v2 . [+] Building 1.3s (6/6) FINISHED =\u003e [internal] load build definition from Dockerfile 0.0s =\u003e =\u003e transferring dockerfile: 44B 0.0s =\u003e [internal] load .dockerignore 0.0s =\u003e =\u003e transferring context: 2B 0.0s =\u003e [internal] load metadata for docker.io/library/ubuntu:latest 1.2s =\u003e [1/2] FROM docker.io/library/ubuntu:latest@sha256:8ae9bafbb64f63a50caab98fd3a5e37b3eb837a3e0780b78e5218e63193961f9 0.0s =\u003e CACHED [2/2] RUN apt-get update -y \u0026\u0026 apt-get install nginx -y 0.0s =\u003e exporting to image 0.0s =\u003e =\u003e exporting layers 0.0s =\u003e =\u003e writing image sha256:0618bb2685ecfe200d9df4a91380d482031352d0e00cbfdf70fcd063aa8654fa 0.0s =\u003e =\u003e naming to docker.io/library/demo-image:v2 啟動 Container，並在 /storage 內新增隨意資料 $ docker run -it demo-image:v2 /bin/bash root@4f8712562dfe:/# echo \"Hello ian\" \u003e /storage/helloworld.txt root@4f8712562dfe:/# ll /storage total 12 drwxr-xr-x 2 root root 4096 Mar 16 05:39 ./ drwxr-xr-x 1 root root 4096 Mar 16 05:38 ../ -rw-r--r-- 1 root root 10 Mar 16 05:39 helloworld.txt 使用 inspect 指令，來找到 Volume 在本地端映射的資料夾路徑，看看裡面有沒有我們在 Container 裡面新增的資料吧 $ docker inspect -f '{{.Mounts}}' 4f8712562dfe [{volume 4fe10ca3f...省略 /var/lib/docker/volumes/4fe10ca3f234633164d9b3c541893c68db1b4f98806525076a2edd5c1c7863c4/_data /storage local true }] $ docker run -it --privileged --pid=host debian nsenter -t 1 -m -u -n -i sh / # cd /var/lib/docker/volumes/4fe10ca3f234633164d9b3c541893c68db1b4f98806525076a2edd5c1c7863c4/_data /var/lib/docker/volumes/4fe10ca3f234633164d9b3c541893c68db1b4f98806525076a2edd5c1c7863c4/_data # ls helloworld.txt mac OS 找不到 /var/lib/docker/volumes： 由於 macOS 下的 docker 實際是在 vm 裡又多加一層，所以沒辦法直接訪問 /var/lib/docker/volumes，必須先透過以下指令進入 VM 中。\ndocker run -it --privileged --pid=host debian nsenter -t 1 -m -u -n -i sh 詳細內容可以參考 Where is /var/lib/docker on Mac/OS X。\n容器與容器之間資料共享 如何啟用容器與容器之間的資料共享，可以用以下方式\n先啟動第一個容器指令如下 docker run -it -v /data --name=container1 centos /bin/bash [root@ a0307ce757ca /]# 另二個容器指令如下 docker run -it --volumes-from container1 --name=container2 centos /bin/bash [root@720d57983cd4 /]# --volumes-from 參數指定 container1 的資料會與 container2 做共享。 我們在第一個容器，進入 /data 資料夾，隨機輸入資料 [root@a0307ce757ca /]# cd /data/ [root@a0307ce757ca data]# echo \"ian~\" \u003e hello.txt 再來看一下第二個容器 /data 資料夾，是否有我們在容器(a0307ce757ca)產生的資料 [root@720d57983cd4 /]# cat /data/hello.txt ian~ Network 模式和比較 在執行 docker run 其中一個參數是 --net ，他可以設定 Container 要使用哪一種的網路模式，以下分別說明這些網路模式\nnone：在執行 Container 時，網路功能是關閉的，所以無法與此 Container 連線。 container：使用相同的 Network Namespace ，假設 Container 1 的 IP 是 172.17.0.2，那 Container 2 的 IP 也是 172.17.0.2。 host：Container 的網路設定和實體主機使用相同的網路設定，所以 Container 裡面也可以修改實體機器的網路設定，因此使用此模式需要考慮網路安全性上的問題。 bridge：Docker 預設就是此網路模式，這個網路模式就像是 NAT 的網路模式，例如實體主機的 IP 是 192.168.1.10 它會對應到 Container 裡面的 172.17.0.2，在啟動 Docker 的服務時會有一個 docker0 的網路卡來做此網路的橋接。 overlay：Container 之間可以在不同的實體機器上做連線，例如 Ｈ ost 1 有一個 Container 1 ，然後 Host 2 有一個 Container 2，Container 1 可以直接使用 overlay 的網路模式和 Container 2 做網路連線。 macvlan：可以直接分配實體網卡的 MAC address 給特定的 Container，讓 Container 透過實體的網卡使用網路。 那我們就來實作每一個網路模式吧！\nnone 我們使用 docker run指令，在後面加入參數 --net=none ，我們建立 jonlabelle/network-tools (裡面有很多網路測試工具)。\n$ docker run -it --net=none jonlabelle/network-tools [docker@network-tools]$ ifconfig lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 .... 省略 .... 可以看到我們使用 ifconfig 查詢，只有本地端的 127.0.0.1 IP，我們在使用 ping 來測試 google 網站吧\n[docker@network-tools]$ ping www.google.com ping: www.google.com: Try again container 我們先啟動一個名為 container1 的容器\ndocker run --name container1 -it jonlabelle/network-tools 再開啟另一個 Terminal 來啟動 container2 的容器，並設定相同的 Network Namespace\ndocker run --name container2 --net=container:container1 -it jonlabelle/network-tools 一樣使用 ifconfig 查詢，可以發現兩個容器的 IP 都是相同的\neth0 Link encap:Ethernet HWaddr 02:42:AC:11:00:02 inet addr:172.17.0.2 Bcast:172.17.255.255 Mask:255.255.0.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 .... 省略 .... host 在執行 docker run 指令時，在後面加入參數 --net=host ，來測試 host 模式\ndocker run --net=host -it jonlabelle/network-tools 可以看到 Container 的網路資訊和實體主機的網路資訊是相同的結果\n[docker@network-tools]$ ifconfig br-36a27cab1817 Link encap:Ethernet HWaddr 02:42:20:DF:DB:FD inet addr:172.20.0.1 Bcast:172.20.255.255 Mask:255.255.0.0 UP BROADCAST MULTICAST MTU:1500 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 .... 省略 .... host 網路模式\nbridge 在執行 docker run 指令時，在後面加入參數 --net=bridge ，來測試 bridge 模式\ndocker run --net=bridge -it jonlabelle/network-tools docker 會新增一個虛擬網卡作為容器網路對外的出口，預設名稱為 docker0，docker0 會跟本機的對外網卡(圖中的 eth0 )相連，藉此取得對外連線的能力，也因為每一個容器都會使用一個 veth device 與 docker0 相連，所以也具備對外連線的能力。\nbridge 網路模式\noverlay 下圖是說明 Host1 實體主機裡面有 Container1，然後 Host2 實體主機裡面有 Container2，可以透過 Docker overlay 模式將 Container1 和 Container2 連接做溝通。另外還需要一個 Consol 來存連線的資料庫，在使用 overlay 時要先在 Docker 做設定，這樣才能存放 overlay 網路模式的連線資訊。\noverlay 網路模式\nmacvlan macvlan 的原理就是在本機的網卡上虛擬出很多個子網卡，通過不同的 MAC 位置在數據鏈路層進行網路資料的轉發。\nmacvlan 網路模式\nDocker-compose 我們在執行多個容器時，需要重複的下 run 指令來執行，以及容器與容器之間要做關聯也要記得每一個之間要怎麼連結，會變得很麻煩且不易管理，所以有了 docker-compose 可以將多個容器組合起來，變成一個強大的功能。\n只要寫一個 docker-compose.yml 把所有會使用到的 Docker image 以及每一個容器之間的關聯或是網路的設定都寫上去，最後再使用 docker-compose up 指令，就可以把所有的容器都執行起來囉！\n我們就直接來實作我們這次的標題，要使用 docker-compose 來整合 PHP MySQL Nginx 環境。\n我們先開啟一個資料夾，取名叫 docker-compose ，來放置我們的 docker-compose 檔案 接著新增 docker-compose.yml 檔案，要準備來撰寫我們的設定檔囉！ 由於內容有點長，所以我分段說明，(這邊有放已經寫好的檔案歐) 檔案目錄\nDocker-compose.yml docker-volume html index.php info.php nginx Dockerfile default.conf php Dockerfile version: \"3.8\" services: ... 省略 .... 可以看到一開頭，會先寫版本，這邊代表的是會使用 3.8 版本的設定檔，詳細版本對照可以參考 Compose file versions and upgrading\nservices 可以設定用來啟動多的容器，裡面我們總共放了三個容器，分別是 nginx、php、mysql 。\n那我們來看看 nginx 裡面放了什麼吧！我會依照程式碼往下說明，有不清楚的可以底下留言！\nnginx nginx: build: ./nginx/ container_name: nginx ports: - 7777:80 volumes: - ./docker-volume/log/:/var/log/nginx/ nginx 的 build 就是要執行這個 nginx 容器的映像檔，還記得我們也可以使用 Dockerfile 來撰寫映像檔案嗎！?\n由於我們還要設定其他內容，所以特別另外拉一個 nginx 資料夾來放置，裡面放了兩個檔案，分別是 Dockerfile、default.conf。\nDockerfile 檔案裡面會使用 nginx 版本 1.20 ，並將 default.conf 複製到容器的 /etc/nginx/conf.d/default.conf 來取代設定。\n以及我們使用 ports 將容器 80 Port 指向本機 7777 Port ，格式是 本機 Port : 容器 Port，\n再使用 volumes 來設定我們 nginx 容器 log 資料夾映射到本機的 ./docker-volume/log/ 資料夾。\nphp php: build: ./php/ container_name: php expose: - 9000 volumes: - ./docker-volume/html/:/var/www/html/ php 的 build 是要執行這個 php 容器的映像檔，由於我們還要設定其他內容，所以特別另外拉一個 php 資料夾來放置 Dokcerfile。\nDockerfile 檔案裡面會使用 php 版本 7.4-fpm，並且在容器執行 docker-php-ext-install、mysqli。\n並將 Port 9000 發佈於本機，再使用 volumes 來設定 /var/www/html 網站根目錄映射到本機的 ./docker-volume/html/ 資料夾。\nmysql mysql: image: mysql:8.0.28 container_name: mysql volumes: - ./docker-volume/mysql/:/var/lib/mysql environment: MYSQL_ROOT_PASSWORD: secret MYSQL_DATABASE: mydb MYSQL_USER: myuser MYSQL_PASSWORD: password mysql 使用的映像檔是 mysql 版本是 8.0.28，我們為了要保留資料庫的資料，所以將容器的 /var/lib/mysql 映射到本地端 ./docker-volume/mysql 資料夾。\n最後的環境變數，設定 root 帳號的登入密碼，以及要使用的資料庫、使用者的帳號、使用者的密碼。\n最後在上面的 (這邊有放已經寫好的檔案歐) 裡面還有多一個 docker-volume/html 的資料夾，就是我們剛剛映射到本地端的資料夾，資料夾內已經放有連線測試的檔案，輸入網址 http://127.0.0.1:7777/index.php，如果開啟後有顯示下方畫面，就代表我們成功用 docker-compose 將 PHP MySQL Nginx 整合再一起囉！\n測試是否成功用 docker-compose 整合 PHP MySQL Nginx","什麼是-docker-#什麼是 Docker ?":"Docker 是一種軟體平台，它可以快速建立、測試和部署應用程式。為什麼可以快速建立呢？因為 Docker 會將軟體封裝到名為『容器』的標準單位。其中會包含程式庫、系統工具、程式碼、執行軟體所需的所有項目。 剛剛有提到容器 (Container)，是一種虛擬化技術，它高效率虛擬化及易於遷移和擴展的特性，非常適合現代雲端的開發及佈署。那 Container 與傳統的虛擬機有什麼差別呢？我們來看看下面這張圖\nContainer 與 VM 的差異\n可以看到 Container 是以應用程式為單位，而 VM 則是以作業系統為單位。Container 是一個封裝了相依性資源與應用程式的執行環境 ; VM 則是一個配置好 CPU、RAM 與 Storage 的作業系統，為了更好的做區別，我把 Container、VM 兩個差別用表格來說明～\n區別比較 Container VM 單位 應用程式 作業系統 適用服務 多使用於微服務 使用較大型的服務 硬體資源 是以程式為單位，需要的硬體資源很少 VM 會先佔用 CPU、RAM 等等硬體資源，不管有沒有使用都會先佔用 造成衝突 Container 間是彼此隔離的，因此在同一台機器可以執行不同版本的服務 會因為版本不同造成環境衝突 系統支援數量 單機支援上千個容器 一般最多幾十個 優點 1 . Image 較小，通常都幾 MB\n2 . 啟動速度快，通常幾秒就可以生成一個 Container\n3 . 更新較為容易，只需要利用新的 Image 重新啟動就會更新了 1 . 因為硬體層以上都虛擬化，因此安全性相對較高\n2 . 系統選擇較多，在 VM 可以選擇不同的 OS\n3 . 不需要降低應用程式內服務的耦合性，不需要將程式內的服務個別拆開來部署 缺點 1 . 安全性較 VM 差，因為環境與硬體都與本機共用\n2 . 在同一台機器中，每一個 Container 的 OS 都是相同的，無法一個為 Windows、一個為 Linux，還是依賴 Host OS\n3 . Container 通常會切成微服務的方式作部署，在各元件中的網路連結會比較複雜 1 . Image 的大小通常 GB 以上，比 Container 大很多\n2 . 啟動速度通常要花幾分鐘，因此服務重啟速度較慢\n3 . 資源使用較多，因為不只程式本身，還要將一部分資源分給 VM 的作業系統 總結\n更快速的交付和部署：對於開發和維運人員來說，最希望就是一次建立或設定，可以再任意地方正常運行。開發者可以使用一個標準的映像檔來建立一套開發容器，開發完成之後，維運人員可以直接使用這個容器來部署程式。Docker 容器很輕很快！容器的啟動時間都是幾秒中的事情，大量地節約開發、測試、部署的時間。\n更有效率的虛擬化：Docker 容器的執行不需要額外的虛擬化支援，它是核心層級的虛擬化，因此可以實作更高的效能和效率。\n更輕鬆的遷移和擴展：Docker 容器幾乎可以在任意的平台上執行，包括實體機器、虛擬機、公有雲、私有雲、個人電腦、伺服器等。 這種兼容性可以讓使用者把一個服務從一個平台直接遷移到另外一個。\n更簡單的管理：使用 Docker，只需要小小的修改，就可以替代以往大量的更新工作。所有的修改都以增量的方式被分發和更新，從而實作自動化並且有效率的管理。","參考資料#參考資料":"Docker 官網：https://docs.docker.com/get-started/\n什麼是 Docker？：https://aws.amazon.com/tw/docker/\nDocker－－從入門到實踐：https://philipzheng.gitbook.io/docker_practice/#yuan-chu-chu-ji-can-kao-zi-liao\nDockerfile 建立自訂映像檔 — 架起網站快速又簡單（一）：https://medium.com/@jackercleaninglab/dockerfile-%E5%BB%BA%E7%AB%8B%E8%87%AA%E8%A8%82%E6%98%A0%E5%83%8F%E6%AA%94-%E6%9E%B6%E8%B5%B7%E7%B6%B2%E7%AB%99%E5%BF%AB%E9%80%9F%E5%8F%88%E7%B0%A1%E5%96%AE-%E4%B8%80-22b2743f97b9\n用 30 天來介紹和使用 Docker：https://ithelp.ithome.com.tw/users/20103456/ironman/1320\nDocker 網路簡介：https://godleon.github.io/blog/Docker/docker-network-overview/\nDocker-compose Giving static IP in network mode : bridge：https://stackoverflow.com/questions/61949319/docker-compose-giving-static-ip-in-network-mode-bridge"},"title":"Docker 介紹 (如何使用 Docker-compose 建置 PHP+MySQl+Nginx 環境)"},"/blog/docker/dockerhub-pull-ratelimit/":{"data":{"":"","#":"背景說明 因 Docker 官方最近宣布從 2025 年 04 月 01 日開始，未經過驗證的帳戶 (一般來說，GKE 拉 Docker Hub 的 Image 都屬於這類)，會有對應的 Rate Limit 限制，需要盤點及建議不要直接使用 Docker Hub 的 Image，避免 Node 擴縮 (因為沒有 Image 暫存)、或是 ImagePullPolicy 是 IfNotPresent 情況，讓 Container 需要重新 Pull 大量的 Image 出現 429 錯誤，導致 Pod 啟動不了，造成線上問題。\n另外 GKE Node 有設定 Mirror Registry，可以減少直接從 Docker Hub 拉取 Image 的次數，但還是建議大家把 Image 搬到 GAR (Artifact Registry) 或是其他的 Registry 上。\nGKE Node Mirror Registry\n提醒\n因近期官方有再次調整時間，原本是公告 03/01，有往後延長，如後續又有延後，會在同步更新在此文件中。\n03/01 延長到 04/01 可以參考：https://github.com/docker/docs/commit/d9eb2332f9581b0b7eac1c90606528707380bf85#diff-7c1db5bfcc9a88aeecd94d07fa2138f5709a9ba977b795f67516106ce3a977ba\n官方限制\n目前限制是 Unauthenticated users 未經驗證的用戶，每個 IPv4 or IPv6 Subnet 一小時只能 Pull 10 次，如果是免費帳號(需要登入)，則是一小時 100 次。\n這些請求包含：網頁、API、Image Pull 等，如果超出請求限制，會出現 429 Too Many Requests 的錯誤訊息。\n建議處理及掃描腳本 為了避免因為 Pull 頂到限制，進而造成線上問題，需要確認目前線上使用的 Image，是否還有直接 Pull 使用 Docker Hub 上面的 Image，請大家將 Image 搬到 GAR (Artifact Registry) 或是其他的 Registry 上。\n有簡單寫一個掃描腳本，可以列出每個 Deployment、Statefulset、Damonset、CronJob 使用的 image，有初步排除 asia-docker.pkg.dev、asia-east1-docker.pkg.dev、gcr.io、quay.io、registry.k8s.io、ghcr.io (可再自行新增)\n#!/bin/bash # 列出 deployments 的 images kubectl get deployments -A -o custom-columns=\"NAMESPACE:.metadata.namespace,NAME:.metadata.name,IMAGE:.spec.template.spec.containers[*].image\" | awk ' BEGIN { format=\"| %-18s | %-40s | %-70s |\\n\" line=\"+--------------------+------------------------------------------+------------------------------------------------------------------------+\" print line printf format, \"Namespace\", \"Deployment\", \"Image Name\" print line } NR\u003e1 { split($3, images, \",\") for (i in images) { if (images[i] !~ /asia-docker.pkg.dev|asia-east1-docker.pkg.dev|gcr.io|quay.io|registry.k8s.io|ghcr.io/) { printf format, $1, $2, images[i] } } } END { print line }' echo \"\\n\" # 列出 statefulsets 的 images kubectl get statefulsets -A -o custom-columns=\"NAMESPACE:.metadata.namespace,NAME:.metadata.name,IMAGE:.spec.template.spec.containers[*].image\" | awk ' BEGIN { format=\"| %-18s | %-40s | %-70s |\\n\" line=\"+--------------------+------------------------------------------+------------------------------------------------------------------------+\" print line printf format, \"Namespace\", \"Statefulset\", \"Image Name\" print line } NR\u003e1 { split($3, images, \",\") for (i in images) { if (images[i] !~ /asia-docker.pkg.dev|asia-east1-docker.pkg.dev|gcr.io|quay.io|registry.k8s.io|ghcr.io/) { printf format, $1, $2, images[i] } } } END { print line }' echo \"\\n\" # 列出 daemonsets 的 images kubectl get daemonsets -A -o custom-columns=\"NAMESPACE:.metadata.namespace,NAME:.metadata.name,IMAGE:.spec.template.spec.containers[*].image\" | awk ' BEGIN { format=\"| %-18s | %-40s | %-70s |\\n\" line=\"+--------------------+------------------------------------------+------------------------------------------------------------------------+\" print line printf format, \"Namespace\", \"Damonset\", \"Image Name\" print line } NR\u003e1 { split($3, images, \",\") for (i in images) { if (images[i] !~ /asia-docker.pkg.dev|asia-east1-docker.pkg.dev|gcr.io|quay.io|registry.k8s.io|ghcr.io/) { printf format, $1, $2, images[i] } } } END { print line }' echo \"\\n\" # 列出 cronjobs 的 images kubectl get cronjobs -A -o custom-columns=\"NAMESPACE:.metadata.namespace,NAME:.metadata.name,IMAGE:.spec.jobTemplate.spec.template.spec.containers[*].image\" | awk ' BEGIN { format=\"| %-18s | %-40s | %-70s |\\n\" line=\"+--------------------+------------------------------------------+------------------------------------------------------------------------+\" print line printf format, \"Namespace\", \"CronJob\", \"Image Name\" print line } NR\u003e1 { split($3, images, \",\") for (i in images) { if (images[i] !~ /asia-docker.pkg.dev|asia-east1-docker.pkg.dev|gcr.io|quay.io|registry.k8s.io|ghcr.io/) { printf format, $1, $2, images[i] } } } END { print line }' 成果 掃描結果範例\n掃描結果範例\n掃描結果範例\n參考文件 Docker Hub usage and limits"},"title":"Docker Image Pull Rate Limit 說明 + 掃描腳本"},"/blog/docker/prometheus-grafana-docker/":{"data":{"":"還記得我們上次架設 EFK 來獲得容器的日誌嗎!? 身為一個 SRE 除了收集日誌外，還需要監控每個系統或是服務的運行狀況，並在警急情況即時通知相關人員作為應對處理。所以透過好的 Monitoring/Alert System 了解目前 Server 硬體系統狀況和整個 Service 的網路狀況是一件非常重要的一件事情。\n在眾多的 Monitor 工具中，Prometheus 是一個很方便且完善的監控預警框架 TSDB (Time Series Database) 時間序列資料庫，可以快速且容易的建立不同維度的指標 (Metrics) 和整合不同的 Alert Tool 以及資訊視覺化圖表的監控工具並提供自帶的 PromQL 進行 query 查詢。\nPrometheus Logo\n我們先來看看 Prometheus 的架構圖，可以更了解 Prometheus 整體的定位：\nPrometheus 架構圖 (圖片來源：使用 Prometheus 和 Grafana 打造 Flask Web App 監控預警系統)\n有一個 Prometheus server 主體，會去 Prometheus Client Pull 相關的指標 (Metrics)，若是短期的 Job 例如 CronJob 在還來不及 Pull 資料回來可能就已經完成任務了、清洗掉資料。所以會有一個 pushgateway 接收 Job Push 過來的相關資訊，Prometheus Server 再從其中拉取資料。 (圖片左半部)\nService Discovery 可以更好的蒐集 Kubernetes 相關的資訊。 (圖片上半部)\nPrometheus Server 主體會將資料儲存在 Local On-Disk Time Series Database 或是可以串接 Remote Storage Systems。(圖片下半部)\nPrometheus Server 資料拉回來後可以使用本身自帶的 Web UI 或是 Grafana 等其他的 Client 來呈現。(圖片右下半部)\n當抓取資料的值超過 Alert Rule 所設定的閥值 (threshold) 時，Alertmanager 就會將訊息送出，可以透過 Email、Slack 等訊息通知，提醒相關人員進行處理。(圖片右上半部)\nPrometheus 可能在儲存擴展上比不上其他 Time Series Database，但在整合各種第三方的 Data Source 上十分方便，且在支援雲端服務和 Container 容器相關工具也十分友好。但在圖片的表現上就相較於單薄，所以會搭配我們接下來要介紹的 Grafanac 精美儀表板工具來進行資訊視覺化和圖表的呈現。\nGrafana Logo\nGrafana 是由 Grafana Lab 經營的一個非常精美的儀表板系統，可以整合各種不同的 Data Source，例如：Prometheus、Elasticsearch、MySQL、PostgreSQL 等。透過不同種指標 (Metrics) 呈現在 Dashboard 上。如果還是不太清楚，可以把 Prometheus Grafana 分別想成 Prometheus 是 EFK 的 Elasticsearch，Grafana 想成是 EFK 的 Kibana。\n今天我們要透過 Docker-Compose 搭配 Nginx 實作一個簡單的 Web Service 範例，並整合 Prometheus 和 Grafana 來建立一個 Web Service 監控預警系統。\n此文章程式碼也會同步到 Github ，需要的也可以去查看歐！要記得先確定一下自己的版本 Github 程式碼連結 😆","nginx-指標-metrics-描述#Nginx 指標 (Metrics) 描述":"我們在 http://localhost:9113/metrics 中可以看到許多指標 (Metrics) 那他們各代表什麼意思呢？我把它整理成表格讓大家可以選擇要使用的指標 (Metrics)：\n指標 描述 nginx_connections_accepted 接受用戶端的連接總數量 nginx_connections_active 當前用戶端連接數量 nginx_connections_handled Handled 狀態的連接數量 nginx_connections_reading 正在讀取的用戶端連接數量 nginx_connections_waiting 正在等待中的用戶端連接數量 nginx_connections_writing 正在寫入的用戶端連接數量 nginx_http_requests_total 客戶端總請求數量 nginx_up Nginx Exporter 是否正常運行 nginxexporter_build_info Nginx Exporter 的構建資訊 ","參考資料#參考資料":"使用 Prometheus 和 Grafana 打造 Flask Web App 監控預警系統：https://blog.techbridge.cc/2019/08/26/how-to-use-prometheus-grafana-in-flask-app/\nNginx Exporter 接入：https://cloud.tencent.com/document/product/1416/56039\n通過 nginx-prometheus-exporter 監控 nginx 指標：https://maxidea.gitbook.io/k8s-testing/prometheus-he-grafana-de-dan-ji-bian-pai/tong-guo-nginxprometheusexporter-jian-kong-nginx\n使用 nginx-prometheus-exporter 監控 nginx：https://www.cnblogs.com/rongfengliang/p/13580534.html\n使用阿里雲 Prometheus 監控 Nginx（新版）：https://help.aliyun.com/document_detail/171819.html?spm=5176.22414175.sslink.29.6c9e1df9DdpLPP\nGrafana Image Renderer：https://grafana.com/grafana/plugins/grafana-image-renderer/\ngrafana 的 image render 设置：https://blog.csdn.net/dandanfengyun/article/details/115346594","實作#實作":"接下來會依照執行的流程來跟大家說明歐！那要開始囉 😁\n我們要建立一個 Nginx 來模擬受監控的服務，我們要透過 nginx-prometheus-exporter 來讓 Prometheus 抓到資料最後傳給 Grafana，所以我們在 Docker-compose 裡面會有 nginx、nginx-prometheus-exporter、prometheus、grafana、grafana-image-renderer 幾個容器，我們先看一下程式碼，再來說明程式碼設定了哪些東西吧！\nDocker-compose.yamlversion: \"3.8\" services: nginx: build: ./nginx/ container_name: nginx ports: - 8080:8080 nginx-prometheus-exporter: image: nginx/nginx-prometheus-exporter:0.10 container_name: nginx-prometheus-exporter command: -nginx.scrape-uri http://nginx:8080/stub_status ports: - 9113:9113 depends_on: - nginx prometheus: image: prom/prometheus:v2.35.0 container_name: prometheus volumes: - ./prometheus.yaml:/etc/prometheus/prometheus.yaml - ./prometheus_data:/prometheus command: - \"--config.file=/etc/prometheus/prometheus.yaml\" ports: - \"9090:9090\" renderer: image: grafana/grafana-image-renderer:3.4.2 environment: BROWSER_TZ: Asia/Taipei ports: - \"8081:8081\" grafana: image: grafana/grafana:8.2.5 container_name: grafana volumes: - ./grafana_data:/var/lib/grafana environment: GF_SECURITY_ADMIN_PASSWORD: pass GF_RENDERING_SERVER_URL: http://renderer:8081/render GF_RENDERING_CALLBACK_URL: http://grafana:3000/ GF_LOG_FILTERS: rendering:debug depends_on: - prometheus - renderer ports: - \"3000:3000\" nginx：因為 Nginx 會通過 stub_status 頁面來開放對外的監控指標。所以我們要另外寫一個 Dockerfile 設定檔，先將 conf 放入 Nginx 中。 nginx-prometheus-exporter：這裡要注意的是需要使用 command 來設定 nginx.scrapt-url，我們設定 http://nginx:8080/stub_status，他的預設 Port 是 9113，並設定依賴 depends_no，要 nginx 先啟動後才會執行 nginx-prometheus-exporter。 prometheus：將 prometheus.yaml 設定檔放入 /etc/prometheus/prometheus.yaml，以及掛載一個 /prometheus_data 來永久保存 prometheus 的資料，最後 command 加入 --config.file 設定。 renderer：這是 grafana 顯示圖片的套件，我們使用 3.4.2 版本，記得要設定環境變數，照片顯示的時間才會正確，並開啟 8081 Port 讓 grafana 訪問。 grafana：一樣我們先掛載一個 /grafana_data 來永久保存 grafana 的設定，在環境變數中設定預設帳號 admin 的密碼是 pass，設定 renderer 套件的服務位置是 http://renderer:8081/render 以及回傳到 http://grafana:3000/，並設定依賴 depends_on prometheus 跟 renderer，最後設定 grafana 要呈現的畫面 3000 Port。 nginx/DockerfileFROM nginx:1.21.6 COPY ./status.conf /etc/nginx/conf.d/status.conf 選擇我們要使用的 nginx image 版本，並將我們的設定檔，複製到容器內。\nnginx/status.confserver { listen 8080; server_name localhost; location /stub_status { stub_status on; access_log off; } } 這邊最重要的就是要設定 /stub_status 路徑，並開啟 stub_status ，這樣才可以讓 nginx-prometheus-exporter 抓到資料！(要怎麼知道 Nginx 是否開啟 stub_status，可以使用 nginx -V 2\u003e\u00261 | grep -o with-http_stub_status_module 指令檢查，我們這次裝的 Image 已經有幫我們啟動)\nprometheus.yamlglobal: scrape_interval: 5s # Server 抓取頻率 external_labels: monitor: \"my-monitor\" scrape_configs: - job_name: \"prometheus\" static_configs: - targets: [\"localhost:9090\"] - job_name: \"nginx_exporter\" static_configs: - targets: [\"nginx-prometheus-exporter:9113\"] 這邊是 prometheus 的設定檔，例如有 scrape_interval 代表 Server 每次抓取資料的頻率，或是設定 monitor 的 labels，下面的 configs，分別設定了 prometheus 它的 targets 是 [\"localhost:9090\"] 以及 nginx_exporter 它的 targets 是 [\"nginx-prometheus-exporter:9113\"]。\ntest.sh#!/bin/bash docker=\"docker exec nginx\" for i in {1..10} do $docker curl http://nginx:8080/stub_status -s done 這個是我自己另外寫的測試程式，在本機執行後他會訪問 nginx 容器內部，並模擬 nginx 流量，讓我們在 Grafana 可以清楚看到資料。\n執行/測試 當我們都寫好設定檔後，在專案目錄下，也就是有 Docker-compose 路徑下，使用 docker-compose up -d 來啟動容器：\n啟動容器\n接下來我們依序檢查容器是否都有正常運作，開啟瀏覽器瀏覽 http://localhost:9113/metrics 查看是否有出現跟下面圖片差不多的內容：\n檢查 Nginx 以及 nginx-prometheus-exporter 的設定\n如果有出現，恭喜你完成了 Nginx 以及 nginx-prometheus-exporter 的設定，我們將 Nginx 的 stub_status 服務，透過 http://nginx:8080/stub_status 讓 nginx-prometheus-exporter 可以抓到圖片中的這些指標 (Metrics)。\nPrometheus 接著我們瀏覽 http://localhost:9090/targets，看看我們的 Prometheus 有沒有設定正確，抓到我們設定好的 targets：\n檢查 Prometheus targets\n如果兩個出現的都是 綠色的 UP 就代表正常有抓到資料囉！\n那要怎麼測試才知道有抓到資料呢？我們可以先用 Prometheus 內建的圖形化介面來檢查，在瀏覽器瀏覽 http://localhost:9090/graph 就可以看到下面的畫面：\nPrometheus 內建的圖形化介面\n我們選擇 Graph，並在上面的搜尋欄，打上 nginx_connections_accepted 按下右邊的 Execute 就會產生一張圖表，圖表裡面只有一條綠色的線，那這個線是什麼呢？它就是我們剛剛在 http://localhost:9113/metrics 其中一個指標 (Metrics)，它代表 Nginx 接受用戶端連接總數量：\nPrometheus 內建的圖形化介面\n這個功能就是把我們所收到的 Nginx 指標 (Metrics)，轉換成圖表讓我們可以知道他的變化。\n為了更明顯的看出變化，這時候就要使用我所寫好的 test.sh 腳本，使用 sh test.sh 來執行，再回來觀察圖型是否變化：\n經過測試顯示的 nginx_connections_accepted 圖形\n可以發現剛剛原本只有 1 個的連接數因為我們模擬總共跑了 10 次，所以連接數變成 11 了！\nGrafana Prometheus 的圖形化比較單調，所以我們使用 Grafana 來美化我們的儀表板，瀏覽器瀏覽 http://localhost:3000/ ，可以看到一個登入頁面：帳號是 admin，密碼是我們在環境變數中所設定的 pass：\nGrafana 登入頁面\n登入後我們看到首頁，選擇 Add your first data source 來新增資料來源：\nGrafana 新增資料來源\n選擇第一個 Prometheus，我們到 HTTP 的 URL 設定 http://prometheus:9090 其他設定在我們測試環境中，不需要去調整，滑到最下面按下 Save \u0026 test：\nGrafana 新增資料來源\n接著我們要來設計我們的儀表板，在 Grafana 除了自己設計以外，還可以 Import 別人做好的儀表板。\n我們點選左側欄位的 ＋ 符號 \u003e 裡面的 Import，可以在這邊 Import 別人做好的儀表板，使用方式也很簡單，只需要先去 Grafana Labs dashboard 裡面找到自己要使用的儀表板，右側會有一個 ID，把 ID 貼上我們的 Grafana 就 Import 成功囉！很神奇吧 XD\n我們要使用的儀表板是別人已經做好的 NGINX exporter，它的 ID 是 12708，把 ID 貼入後，按下 Load，就會有 NGINX exporter 的基本資訊，我們在最下面的 Prometheus 選擇我們要使用的 data source，就是我們剛剛先設定好的 Prometheus，最後按下 Import，就完成拉。\nGrafana 載入別人做好的儀表板\n如果設定都沒有錯誤的話，應該可以看到下面這個畫面，最上面是監測 Nginx 服務的狀態，以及下方有不同的指標在顯示：\nGrafana 儀表板\n接下來我們一次用 test.sh 來測試一下是否有成功抓到資料：\n測試 Grafana 是否成功抓到資料\n可以看到在我們使用完測試腳本後，在該時段的資料有明顯的不一樣，代表我們有成功抓到資料 😄\n此外也可以將 Nginx 服務暫停，看看儀表板上方的 NGINX Status 狀態是否改變：\n測試暫停 Nginx 查看 Grafana 儀表板 NGINX Status\nAlerting 警報 當然除了監控以外，我們還需要有警報系統，因為我們不可能每天都一直盯著儀表板看哪裡有錯誤，所以我們要設定警報的規則，以及警報要發送到哪裡，接著我們一起看下去吧：\n我們先點左側的 Alerting 🔔 \u003e 點選 Notification channels 來新增我們要發送到哪裡。這次我們一樣使用 Telegram，我們在 type 下拉式選單選擇 Telegram，輸入我們的 BOT API Token 以及 Chat ID，儲存之前可以點選 test 來測試！\n怎麼使用 Telegram Bot：請參考這一篇 Ansible 介紹與實作 (Inventory、Playbooks、Module、Template、Handlers) 來取得 BOT API Token 以及 Chat ID。 Alerting 設定\nAlerting 測試結果\n接著我們來設計一個屬於我們的控制板 (Panel)，順便幫他加上 Alerting，稍後也用 test.sh，看看他會不會自動發出提醒到 Telegram Bot 😬\n首先點選左側欄位的 ＋ 符號 \u003e 裡面的 Create，在選擇 Add an empty panel：\nCreate Panel\n再 Query 的 A Metrics browser 輸入 nginx_connections_accepted 一樣來取得 Nginx 接受用戶端連接總數量的圖表，到右上角選擇 Last 5 minutes，旁邊的圖型我們選擇 Graph (old)，下面的 Title 可以修改一下這個圖表的名稱，最後按下 Save，就可以看到我們建好一個控制板囉 🥳\n設定 Panel\n接著我們來設定 Alert，可以看到剛剛在 Query 旁邊有一個 Alert，點進去後按 Create Alert，我們先修改 Evaluate every 後面的 For 改為 1m (代表當數值超過我們所設定的閥值後，狀態會從 OK 變成 Pending，這時候還不會發送警報，會等待我們現在設定的 1m 分鐘後，情況還是沒有好轉，才會發送通知)，再 Conditions 後面欄位加入 10 (我們所設定的閥值，代表 nginx_connections_accepted 超過 10 就會進入 Pending 狀態)，往下滑 Notifications 的 Send to 選擇我們上面所建立的 channels 名稱，按下 Save。\n設定好 Alert 的控制板\n接著執行 test.sh 兩次，讓 nginx_connections_accepted 超過我們所設定的閥值，可以看到控制板超過 10 以上變成紅色：\n超過閥值，控制板變成紅色\n接著等待幾分鐘後，狀態會從 OK 綠色變成黃色的 Pending，最後轉成紅色的 Alert，這時候 Telegram 就會收到通知囉 ❌\n自動發送通知到 Telegram Bot，並附上控制板圖片","檔案結構#檔案結構":" docker-compose.yaml nginx Dockerfile status.conf prometheus.yaml test.sh 這是主要的結構，簡單說明一下：\ndocker-compose.yaml：會放置要產生的 nginx、nginx-prometheus-exporter、prometheus、grafana、grafana-image-renderer 容器設定檔。 nginx/Dockerfile：因為在 nginx 要使用 stub_status 需要多安裝一些設定，所以用 Dockerfile 另外寫 nginx 的映像檔。 nginx/status.conf：nginx 的設定檔。 prometheus.yaml：prometheus 的設定檔。 test.sh：測試用檔案(後續會教大家如何使用)。 ","版本資訊#版本資訊":" macOS：11.6 Docker：Docker version 20.10.14, build a224086 Nginx：1.21.6 Prometheus：v.2.35.0 nginx-prometheus-exporter：0.10 Grafana：8.2.5 (最新版本是 8.5.2，但選擇 8.2.5，是因為 8.3.0 後 Alerting 沒有辦法附上圖片，詳細原因可以參考 Add “include image” option into Grafana Alerting ) grafana/grafana-image-renderer：3.4.2 "},"title":"使用 Prometheus 和 Grafana 打造監控預警系統 (Docker 篇)"},"/blog/gcp/":{"data":{"":"此分類包含 Google Cloud Platform 相關的文章。\nGKE DNS 相關注意事項及結論發布日期：2025-08-12 GKE DNS 使用 Cloud DNS + NodeLocal DNSCache 運作測試發布日期：2025-08-08 GKE DNS 使用 Cloud DNS 運作測試發布日期：2025-08-06 GKE DNS 使用 KubeDNS 運作測試發布日期：2025-08-04 GKE DNS 使用 KubeDNS + NodeLocal DNSCache 運作測試發布日期：2025-08-01 GCP Load Balancer 介紹發布日期：2025-05-26 GCS Bucket CORS 錯誤解決方法發布日期：2025-03-18 Google Kubernetes Engine Local ephemeral storage 計算方式發布日期：2024-09-02 GCP Prometheus Samples Ingested 計算方式及如何減少費用發布日期：2024-07-10 GCP Memorystore HA 高可用性 failover 測試發布日期：2024-07-02 如何過濾 GCP LOG，減少 Cloud Logging API 的花費發布日期：2023-07-30 Google Kubernetes Engine CronJob 會有短暫時間沒有執行 Job發布日期：2023-02-10 Google Cloud Platform (GCP) - Cloud Build發布日期：2022-07-06 Google Cloud Platform (GCP) - Cloud Source Repositories發布日期：2022-07-03 Google Cloud Platform (GCP) - Container Registry發布日期：2022-07-01 Google Cloud Platform (GCP) - Compute Engine發布日期：2022-06-29 Google Cloud Platform (GCP) - IAM 與管理發布日期：2022-06-29 "},"title":"Google Cloud Platform"},"/blog/gcp/cloud-build/":{"data":{"":"跟大家介紹一下今天的主題 Cloud Build，Cloud Build 可以幫我們做持續建構、測試和部署，我們可以把它想成簡易版的 Jenkins，從整個映像檔案打包到部署，也就幾分鐘的事情，且內建許多指令。\n我們今天文章，需要使用前幾天提到的 Cloud Source Repositories 、Compute Engine、Container Registry，我們需要先透過 GitLab 將程式鏡像到 Cloud Source Repositories，再透過 Cloud Build 觸發將 GitLab 上面的 Dockerfile 建置到 Container Registry 中，再部署到 Compute Engine VM 上。大家可以參考流程圖，會更清楚今天的流程！那我們就開始囉 🥸\n流程圖","cloud-source-repositories-測試#Cloud Source Repositories 測試":"前面 GitLab 鏡像設定，請先參考上上篇 Google Cloud Platform (GCP) - Cloud Source Repositories，上上篇會帶大家從 GitLab 鏡像到 Cloud Source Repositories，所以我們就接續之前的內容，繼續往下開始學習吧～\n開啟 GCP，選擇左側的 menu \u003e 點擊 Cloud Build \u003e 選擇 觸發條件，點擊 建立觸發條件 按鈕。 輸入觸發條件的名稱，事件可以設定我們要怎麼進行觸發，我們這邊先選擇 推送至分支的版本 來觸發，在來源選擇上上篇建立好的 Cloud Source Repositories 存放區，分支版本我們先使用預設的 master，也就是推程式到 master 他會就觸發 Cloud Build： 建立觸發條件 1\n設定類型我們選擇 Cloud Build 設定檔，他也是 Cloud Build 專用的設定檔，後面會帶大家寫一份 Cloud Build，位置當然是使用我們 Cloud Source Repositories 存放區，以及可以依照專案來修改 cloudbuild.yaml 放在專案的哪裡，最後都沒問題，就按下建立： 建立觸發條件 2\n撰寫 cloudbuild.yaml 設定檔 在開始建立檔案前，先來跟大家說說檔案內有哪些設定吧：\n首先 Cloud Build 建構器是裝有常用的程式語言和工具的容器映像。我們可以配置 Cloud Build，讓建構器中運行特定命令，我舉個例子讓大家了解：\n以下程式碼是來自 Docker Hub 的 ubnutu 映像檔中所執行的命令：\nsteps: - name: \"ubuntu\" args: [\"echo\", \"hello world\"] 可以看到我們配置文件中 steps 參數是指我們要建構的步驟， name 字段指定 Docker 映像檔的位置，以及 args 字段中是指定映像檔運行的命令。\n我們的 Cloud Build 一樣會需要用 name 來指定建構容器的映像檔，以及使用 args 來執行我們映像檔所要運行的命令。\n我們的 Cloud Build 設定檔中的 name 常用的建構器映像檔如下：\nBuilder 名稱 bazel gcr.io/cloud-builders/bazel docker gcr.io/cloud-builders/docker git gcr.io/cloud-builders/git gcloud gcr.io/cloud-builders/gcloud gke-deploy gcr.io/cloud-builders/gke-deploy 接著我們來試著寫一個 cloudbuild.yaml 來建構我們的 nginx 服務，並部署到 Compute Engine 上。\n我們先回到 Gitlab 該專案下的目錄，新增 cloudbuild.yaml 檔案，將複製以下內容：\nsteps: # Docker Build - name: \"gcr.io/cloud-builders/docker\" args: [\"build\", \"-t\", \"gcr.io/$PROJECT_ID/ian-test:ian-nginx-test\", \".\"] # Docker Push - name: \"gcr.io/cloud-builders/docker\" args: [\"push\", \"gcr.io/$PROJECT_ID/ian-test:ian-nginx-test\"] # Build VM - name: \"gcr.io/google.com/cloudsdktool/cloud-sdk\" entrypoint: \"gcloud\" args: [ \"compute\", \"instances\", \"create-with-container\", \"ian-test-vm\", \"--container-image\", \"gcr.io/$PROJECT_ID/ian-test:ian-nginx-test\", ] env: - \"CLOUDSDK_COMPUTE_REGION=asia-east1\" - \"CLOUDSDK_COMPUTE_ZONE=asia-east1-b\" 我們一個一個來說說的這個 cloudbuild.yaml 裡面的設定吧！(我以前面的註解來區分)\nDocker Build：這邊的 name 我們用 gcr.io/cloud-builders/docker，代表我們將使用 docker 建構器，args 這邊下的意思是要把與 cloudbuild.yaml 放在一起的 Dokcerfile 給 build 起來，並改名為 gcr.io/$PROJECT_ID/ian-test:ian-nginx-test。 Docker Push：這邊一樣使用 gcr.io/cloud-builders/docker，args 指令部分變成我們要把他 push 到 gcr.io/$PROJECT_ID/ian-test 這個 Cloud Source Repositories，其中這個映像檔案的 tag 為 ian-nginx-test。 Build VM：這邊我們使用 gcr.io/google.com/cloudsdktool/cloud-sdk，可以透過它來建立 VM，並且執行 tag 名為 ian-nginx-test 的映像檔，後面環境變數是來設定 VM 的區域等等。 ian-test 是 Container Registry 資料夾名稱，ian-nginx-test 是 Container Registry 映像檔的 tag，ian-test-vm 是我們建立 VM 的名字，所以要記得改成自己的命名歐！ 撰寫 Dockerfile 接下來剛剛有提到 Docker Build 會將我們放在一起的 Dockerfile 給 build 起來，所以我們也要先寫好要用的 Dockerfile：\nFROM nginx:latest COPY ./index.html /usr/share/nginx/html/ 我們的 Dockerfile 很簡單，簡單寫了要使用的映像檔，以及將我們等等要測試的 index 複製到裡面\n撰寫測試 index.html \u003c!DOCTYPE html\u003e \u003chtml lang=\"en\"\u003e \u003chead\u003e \u003cmeta charset=\"UTF-8\" /\u003e \u003cmeta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\" /\u003e \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" /\u003e \u003ctitle\u003e測試測試測試\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e 我是測試檔案 \u003c/body\u003e \u003c/html\u003e 這個測試檔案，會蓋過 nginx 的預設畫面，當我們成功將 VM 建立後，瀏覽 80 Port 時，應該會跳出這個測試的網頁。\nCommit 到 GitLab 當我們都新增好檔案後，我們就將程式推到我們前幾篇的 Cloud Source Repositories 已經鏡像的 GitLab 中，接著就是等待見證奇蹟的時候了ＸＤ\n當我們推送後，我們先檢查 Cloud Source Repositories 是否有成功從 GitLab 鏡像過來： Cloud Source Repositories\n檢查看看 Container Registry 是否多了名為 ian-test 的資料夾，且裡面有一個 tag 為 ian-nginx-test 的映像檔： Container Registry\n檢查一下 Cloud Build 的觸發條件是不是在運作，最後成功可以看到類似下方圖片內容： Cloud Build 的觸發條件\n檢查 Compute Engine 的 VM 是否有成功被建立： Compute Engine\n最後就是測試這個映像檔案，是不是我們所 Build 的，測試方法很簡單，我們剛剛在 Dockerfile 有複製我們自己寫的 index.html 檔案，去蓋掉原本 nginx 的預設檔，所以我們可以瀏覽上面圖片的外部 IP，就可以看到我們所改的頁面囉！ 測試用 index.html","參考資料#參考資料":"Cloud Builder：https://cloud.google.com/build/docs/cloud-builders\nDay27 - 用 Cloud Build 實作 CI 部分：https://ithelp.ithome.com.tw/articles/10224727"},"title":"Google Cloud Platform (GCP) - Cloud Build"},"/blog/gcp/cloud-source-repositories/":{"data":{"":"跟大家介紹一下今天的主題 Cloud Source Repositories，聽到 Source Repositories 是不是感覺跟什麼東西很像呀，沒錯，就跟我們的 GitHub or GitLab 一樣，可以用來存放我們的程式碼的儲存庫，我們來看看官方怎麼介紹他吧：\n官方介紹 Cloud Source Repositories\n很好歐，非常簡單明瞭 🤣，沒錯，Cloud Source Repositories 就是託管在 Google Cloud 上功能齊全(？)的私有 Git 儲存庫。為什麼會打一個問號呢？是因為他其實沒有那麼好用，所以我們通常的做法，還是會依靠 GitHab 或是 GitLab 來存放程式碼，再透過鏡像 (mirror) 的方式到 Google Cloud Source Repositories。 那我們就開始囉～","cloud-source-repositories-測試#Cloud Source Repositories 測試":"建立 GitLab Project 首先，我們用 GitLab 來當示範，如何透過鏡像 (mirror) 到 Cloud Source Repositories 上面，我們先在 GitLab 上建立一個 Project：\n建立 GitLab Project\n使用 gcloud 指令建立 Source Repo 首先，一定要先裝 gcloud 指令到本機，這個步驟，前面文章也有說過，這邊就不在說明，我們先使用一下指令來查看目前所在的 GCP 專案： gcloud config get-value project 正常來說，如果有先用 config 設定好，會直接跳出你目前的專案 ID，如果沒有跳出來，請使用下面指令來設定：\ngcloud config set project \u003cproject id\u003e 接著我們要啟動該專案的 Cloud Source Repositories API： gcloud services enable sourcerepo.googleapis.com 創建 Cloud Source Repositories gcloud source repos create \u003crepo name\u003e 完成後，開啟 GCP 檢查一下是否有建立成功～點擊左側 menu \u003e Source Repositories， 開啟 Source Repositories\n成功建立 Source Repositories\n將程式碼新增至存放區中 我們要在這一步來設定鏡像 (mirror)，首先我們看剛剛上面建立好的 Source Repositories，其中有一個手動產生的憑證，點選 產生及儲存 Git 憑證 產生及儲存 Git 憑證\n點完後會需要先登入你的 GCP 帳號，登入完後會出現以下內容： Configure Git\n接著把藍色框框內的輸入到終端機內 Configure Git\n接著請複製以下指令貼到終端機內，會生成憑證密碼： grep 'source.developers.google.com' ~/.gitcookies | tail -1 | cut -d= -f2 生成憑證密碼\n接著請複製以下指令貼到終端機內，將用戶名存儲在 CSR_USER 環境變量中： CSR_USER=$(grep 'source.developers.google.com' ~/.gitcookies | \\ tail -1 | cut -d$'\\t' -f7 | cut -d= -f1) 用戶名存儲在 CSR_USER 環境變量中\n接著請複製以下指令貼到終端機內，將 GCP 存儲庫的 URL 存儲在 CSR_REPO 環境變量中 (repo name 要改成你在 gcp 上面的 repo)： CSR_REPO=$(gcloud source repos describe \u003crepo name\u003e --format=\"value(url)\") 將 GCP 存儲庫的 URL 存儲在 CSR_REPO 環境變量中\n接著請複製以下指令貼到終端機內，將存儲庫的 URL（包括用戶名）印到終端機上： echo $CSR_REPO | sed \"s/:\\/\\//:\\/\\/${CSR_USER}@/\" 存儲庫的 URL（包括用戶名）印到終端機上\n經過上面操作，我們可以在第 4 步驟拿到密碼，以及在第 7 步驟拿到完整的 GCP URL，接著我們要到 GItLab Mirror 來設定鏡像。\n到 GitLab Mirror 設定鏡像 先從右側 muen \u003e 選擇 Settings \u003e 點選 Repository，找到 Mirroring repositories GitLab Mirror 設定鏡像\n將剛剛拿到的 URL 以及密碼各別輸入 Git repository URL 以及 Password，記得要選擇 Mirror direction，因為我們是要將 gitlab 的鏡像到 GCP 的 Cloud Source Repositories，所以我們要選擇 PUSH，最後按下 Mirror repository： GitLab Mirror 設定鏡像\n如果沒有跳出錯誤，基本上是沒有問題了！\nGitLab Mirror 檢查\n就可以試著在 gitlab 上面推程式，看看有沒有跑到 Cloud Source Repositories 上面囉！ GitLab 推程式測試","參考資料#參考資料":"Cloud Source Repositories documentation\nMirroring GitLab repositories to Cloud Source Repositories"},"title":"Google Cloud Platform (GCP) - Cloud Source Repositories"},"/blog/gcp/compute-engine/":{"data":{"":"跟大家介紹一下今天的主題 Google Compute Engine(GCE)，GCE 是 Google Cloud 上的基礎架構服務 (IaaS)，該平台可以提供大規模的虛擬機器以及相關的基礎建設 (包含硬碟、網路、附載平衡器… 等等)來建置及運作您的服務，那我們可以將 GCE 服務的主要功能劃分成以下幾點：","google-compute-engine-測試#Google Compute Engine 測試":"首先我們使用 cloudskillsboost 提供的 Creating a Virtual Machine 來做練習，打開後，請先登入自己的 Google 帳號，接著點選左上角的 Start Lab，會跳出與下面圖片類似的內容：\n測試用的帳號密碼\n新增新的 VM 實例 點選 Open Google Console 按鈕來開啟 GCP 主控台 登入帳號就使用上面圖片所提供的帳號密碼來進行登入 登入成功會進入 GCP 主控台，點選左側的 menu \u003e Compute Engine \u003e VM Instances，可以參考下方圖片 新增 VM 實例\n點選 CREATE INSTANCE，請依照下方表格來進行設定： 標題 設定值 說明 Name gcelab 虛擬機實例的名稱 Region us-central1（愛荷華州） 有關區域的更多信息，請參閱 Compute Engine 指南 Regions and zones Zone us-central1-f Series N1 Machine type n1-standard-2 這是一個 2 vCPU、7.5 GB RAM 實例 Boot disk New balanced persistent disk/10 GB/Debian GNU/Linux 10 Firewall Allow HTTP taffic VM 實例 (Name、Region、Zone、Series)\nVM 實例 (Machine type、Boot disk、Firewall)\n新增完後，大約需要等待一分鐘，新的虛擬機就會列在 VM Instances 頁面上。 VM 實例\n安裝 NGINX Web 服務器 點擊 VM instances 實例最後的 SSH，會開啟 SSH 用戶端 在 SSH 終端，要先獲得 root 訪問權限，才更方便的進行後續的動作，請先使用一下指令： sudo su - 利用 root 用戶，來更新操作系統： apt-get update 更新操作系統\n安裝 NGINX： apt-get nginx -y 安裝 NGINX\n最後確認 NGINX 是否運行： ps auwx | grep nginx 確認 NGINX 是否運行\n可以打開瀏覽器瀏覽 http://外部 IP/ 或是使用 curl 外部IP，外部 IP 會在跟剛剛 VM 實例的 External IP 欄位呦～ curl 外部 IP\n完成後，記得可以點選 Check my progress 來檢查進度吧！ Check my progress\n使用 gcloud 創建一個新實例 我們剛剛是使用網頁版來新增，當然也可以使用 gcloud 指令來新增，這個工具有預先裝在 Google Cloud Shell 中。Cloud Shell 是一個基於 Debian 的虛擬機，包含了常用的開發工具 (gcloud、git 等工具)，另外你也可以將 gcloud 下載至本機上來做使用，請閱讀 gcloud 命令行工具指南\n在 Cloud Shell 中，使用 gcloud 來新增新的虛擬機實例： gcloud compute instances create gcelab2 --machine-type n1-standard-2 --zone us-central1-f 就會跳出以下圖片的內容，過一陣子去查看 VM Instances 也可以看到我們所新增的虛擬機實例歐～\nCheck my progress\n到這邊就完成了我們 Google Compute Engine 測試囉～我們知道可以新增 GCE 將實體主機的內容，移植到雲端上囉！希望大家會喜歡今天的文章 🥰","google-compute-engine-特色#Google Compute Engine 特色":"穩健的網路功能 提供使用者擁有穩健的網路功能，以運行各項應用程式及服務。\n自訂網路與預設網路 GCE 包含內部與外部的網路連線能力，讓使用者可以透過自訂規劃來建置屬於自己服務適用的網路，而在 GCE 服務開通當下，也提供預設的 default network，內建常用的路由與防火牆設定 (例如：SSH、RDP、ICMP… 等)，供入門使用者直接使用。\n防火牆規則 除了預設防火牆規則外，使用者也可以透過自建防火牆來開放可以連入的 IP。\n各區域的 HTTP(S) 的負載平衡 為 Layer 7 的負載平衡設備，透過設定可以串連多台主機或是主機群組，讓服務不再只是依賴於單點存在的伺服器。Layer 7 的負載平衡更可以識別路由規劃，進一步可以提供不同路由的重導規則設定，也可以提供 CDN 的 Cache 功能。\n網路的負載平衡 為 Layer4 的負載平衡設備，可以透過 Protocol 與 Port 的方式來重導外部流量到 GCE 主機或主機群，並可以透過 Health Check 的方式，讓流量僅通過健康的主機，避免服務中斷。\n子網路 透過 CIDR 的方式設定 GCE Network 中的各子網路範圍，並可以透過路由的方式串接各子網路中間的通訊，讓 GCE 網路的設計規劃可以更有彈性，也可以讓子網路的規劃來實作更安全的雲端網路架構。","參考資料#參考資料":"Compute Engine 基本介紹：https://gdgcloud-taipei.gitbook.io/google-cloud-platform-in-practice/google-cloud-shang-de-yun-suan-fu-wu/compute-engine/compute-engine-ji-ben-jie-shao\nCreating a Virtual Machine：https://www.cloudskillsboost.google/focuses/3563?locale=zh_TW\u0026parent=catalog"},"title":"Google Cloud Platform (GCP) - Compute Engine"},"/blog/gcp/container-registry/":{"data":{"":"跟大家介紹一下今天的主題 Container Registry，Container Registry 是儲存、管理和保護 Docker 容器映像檔的存放區，可以讓團隊透過同一項服務服務集中管理 Docker 映像檔、也可以執行安全漏洞分析，還能透過精密的存取權管理機制，來決定誰可以存取哪些內容。簡單來說他就是一個讓我們存放 Docker 映像檔的地方，他有以下幾個特點：","container-registry-特點#Container Registry 特點":" 安全的私人 Docker 註冊資料庫：只需要幾分鐘的時間，即可開始在 Google Cloud Platform 中使用安全的私人 Docker 映像檔儲存空間，控管能夠存取、檢視或下載映像檔的人員，並在受到 Google 安全防護機制保護的基礎架構上穩定執行。 自動建立及部署映像檔：在您修訂 Cloud Source Repositories 中的程式碼時，系統會自動建立映像檔並推送至私人註冊資料庫。您可以輕鬆設定持續整合/持續推送軟體更新管道，並整合至 Cloud Build，或是直接將管道部署至 Google Kubernetes Engine、App Engine、Cloud Functions 或 Firebase。(這個就是我們在下一篇文章會使用到的功能) 深度安全漏洞掃描：在軟體部署週期的早期階段找出安全漏洞，藉此確保您可以安全地部署容器映像檔。資料庫會持續重新整理，讓您的安全漏洞掃描作業取得最新型惡意軟體的相關資訊。 鎖定有風險的映像檔：運用二進位授權的原生整合功能來定義政策，避免部署與所設定政策相衝突的映像檔。您可以觸發容器映像檔的自動鎖定功能，禁止將有風險的映像檔部署至 Google Kubernetes Engine。 原生支援 Docker：您不僅可以視需求定義多個註冊資料庫，也能使用標準的 Docker 指令列介面，將 Docker 映像檔推送和提取到您的私人 Container Registry。Docker 提供依據名稱和標記搜尋映像檔的功能，讓您可以順暢使用。 快速的高可用性存取能力：您可使用我們遍布全球的區域性私人存放區，於全世界皆能享有最快速的回應時間。您可以就近將映像檔儲存在位於歐洲、亞洲或美國的運算執行個體中，並透過 Google 的高效能全球網路快速完成部署作業。 上面有提到 Container Registry 其中一個特點就是掃描映像檔，會檢查有沒有奇怪的內容或是錯誤，讓我們在部署前可以先做檢查，底下是整個的流程圖：\nContainer Registry 掃描流程圖\n我們可以延續上一篇文章 Cloud Source Repositories 來說明上面圖片，一開始先 commit 到 GCP 上，也可以把它當成 Cloud Source Repositories，就著透過會透過下一篇要講的 Cloud Build 來建置，並且掃描，如果沒有問題就會 Pubish 到 Container Registry 存放囉～\n我們就簡單說明要怎麼查看我們的 Container Registry，點選則左側的 menu \u003e 選擇 Container Registry \u003e 點擊 映像檔：\nContainer Registry\n一開始可能沒有任何的資料夾，之後當我們 Build 時，可以新增特定的資料夾來放我們的映像檔，可以參考下方圖片：\nContainer Registry 資料夾\n進入該資料夾後，就可以看到裡面的映像檔案：\nContainer Registry 映像檔"},"title":"Google Cloud Platform (GCP) - Container Registry"},"/blog/gcp/gcp-lb-introduce/":{"data":{"":"","#":"最近公司在導入 Multi-Zone，有發現大量的跨域費用產生，主要是不同 Cluster 或是 Cluster 跟 VM 之間的跨域流量，與 Google TAM 討論，他們有提出可以嘗試用 Service load balancing policy 的 Waterfall by zone，或是之後會推出的 Zone affinity，這些都會需要使用到 Load Balancer，簡單整理一下發現 GCP 的 Load Balancer 其實有很多種，因此，這篇文章就來介紹一下 GCP 的 Load Balancer。 (如果 Service load balancing policy 測試有結果，也會再寫一篇文章來介紹)\n想知道自己用的 Load Balancer 是哪一種嗎？\n由於以下會有多種的 Load Balancer 種類，且 gcloud CLI 沒辦法直接過濾相關 LB 名稱 (UI 可以，但需要每個都進去查看)，因此有特別寫了一隻腳本，可以來掃描查詢，請參考： gcp-load-balancer-type\nGCP 的 Load Balancer 主要分為三種，分別是：\nApplication Load Balancer (ALB) – 應用程式負載平衡器 層級：第 7 層（L7） 支援協定：HTTP、HTTPS 功能： 基於內容的路由（如 URL 路徑、主機名稱） SSL/TLS 終止 整合 Cloud CDN、Cloud Armor 支援全球負載平衡（Premium Tier） 適用場景：需要進行應用層路由、SSL 終止，以及全球流量分配的 Web 應用程式。 Application Load Balancer 又可以再另外分為五種 (包含內、外網以及不同的 Region)：\nGlobal external Application Load Balancer 將此負載平衡器用於具有全球分散用戶或多個區域的後端服務的外部 HTTP(S) 工作負載。 (官方建議使用)\n資訊 中文：全球外部應用程式負載平衡器 縮寫：GLB 內部/外部：Public facing (external) 區域：Global Load balancing scheme：EXTERNAL_MANAGED 是否支援 Cloud CDN：✅ 是否支援 Cloud Armor：✅ 是否支援 Service load balancing policy：✅ 是否支援 SSL：✅ 是否支援 PROXY protocol：❌ 特點 與 GKE 相容，使用閘道或獨立 NEG 支援先進的流量管理 只能使用 Premium 等級的 Network Service 可以跨多個專案以及區域來存取後端 Global external Application Load Balancer\nClassic Application Load Balancer 此負載平衡器在高階層中是全球性的。在 Premium 網路服務層，此負載平衡器提供多區域負載平衡，嘗試將流量引導至具有容量的最近的健康後端，並儘可能靠近使用者終止 HTTP(S) 流量。\n在 Standard 網路服務層中，此負載平衡器只能將流量分配到單一區域內的後端。(建議不要再使用該 LB)\n資訊 中文：經典應用程式負載平衡器 縮寫：CLB 內部/外部：Public facing (external) 區域：Global Load balancing scheme：EXTERNAL 是否支援 Cloud CDN：✅ 是否支援 Cloud Armor：✅ 是否支援 Service load balancing policy：❌ 是否支援 SSL：✅ 是否支援 PROXY protocol：❌ 特點 與 GKE 相容，使用閘道、Ingress 或獨立 NEG 可以選擇 Standard 或是 Premium 等級的 Network Service 可以跨多個區域來存取後端 (選擇 Standrad 不能用) 有支援 Cloud CDN，但選擇 Standrad 則不能用 Classic Application Load Balancer\nRegional external Application Load Balancer 此負載平衡器包含現有的經典應用程式負載平衡器， 以及先進的流量管理能力。 如果您只想從一個地理位置提供內容，請使用此負載平衡器。\n資訊 中文：區域外部應用程式負載平衡器 縮寫：ALB 內部/外部：Public facing (external) 區域：Regional Load balancing scheme：EXTERNAL_MANAGED 是否支援 Cloud CDN：❌ 是否支援 Cloud Armor：✅ 是否支援 Service load balancing policy：❌ 是否支援 SSL：✅ 是否支援 PROXY protocol：❌ 特點 與 GKE 相容，使用獨立 NEG 支援先進的流量管理 可以選擇 Standard 或是 Premium 等級的 Network Service 可以跨多個專案，但只能選擇該區域的資源 (且沒辦法接 Bucket) Regional external Application Load Balancer\nCross-Region internal Application Load Balancer 這是一個多區域負載平衡器，它基於開源 Envoy 代理實現為託管服務。跨區域模式使您能夠將流量負載平衡到全球分佈的後端服務，包括確保流量定向到最近的後端的流量管理。此負載平衡器還具有高可用性。將後端放置在多個區域有助於避免單一區域故障。如果一個區域的後端發生故障，流量可以轉移到另一個區域。\n資訊 中文：跨區域內部應用程式負載平衡器 縮寫：Cross-Region internal ALB 內部/外部：Internal 區域：Global Load balancing scheme：INTERNAL_MANAGED 是否支援 Cloud CDN：❌ 是否支援 Cloud Armor：❌ 是否支援 Service load balancing policy：✅ 是否支援 SSL：✅ 是否支援 PROXY protocol：❌ 特點 始終可在全球範圍內訪問。 VPC 中任何 Google Cloud 區域的用戶端都可以將流量傳送到負載平衡器 負載平衡器可以將流量傳送到任何區域的後端 (可以跨多個專案) 自動故障轉移到同一或不同區域的健康後端 Cross-Region internal Application Load Balancer\nRegional internal Application Load Balancer 這是一個區域負載平衡器，它基於開源 Envoy 代理程式作為託管服務實作。區域模式可確保所有用戶端和後端都來自指定區域，這在您需要區域合規性時很有幫助。此負載平衡器具備基於 HTTP(S)參數的豐富流量控制功能。負載平衡器配置完成後，它會自動指派 Envoy 代理程式來滿足您的流量需求。\n資訊 中文：區域內部應用程式負載平衡器 縮寫：ILB 內部/外部：Internal 區域：Regional Load balancing scheme：INTERNAL_MANAGED 是否支援 Cloud CDN：❌ 是否支援 Cloud Armor：✅ 是否支援 Service load balancing policy：❌ 是否支援 SSL：✅ 是否支援 PROXY protocol：❌ 特點 預設無法由不同 Region 進行訪問，需要額外開啟全球訪問設定 負載平衡器只能將流量傳送到與負載平衡器的代理程式位於相同區域的後端 (可以跨多個專案) 自動故障轉移到同一區域內的健康後端 Regional internal Application Load Balancer\nProxy Network Load Balancer (PNLB) – 代理網路負載平衡器 層級：第 4 層（L4） 支援協定：TCP、SSL 功能： 作為反向代理，終止 TCP 或 SSL 連線 支援 SSL/TLS 終止（僅限 SSL Proxy 模式） 可選擇全球（Premium Tier）或區域性（Standard Tier）部署 適用場景：需要處理加密的 TCP 流量，並在負載平衡器層級終止 SSL 的應用程式。 Proxy Network Load Balancer 又可以再另外分為五種 (包含內、外網以及不同的 Region)：\nGlobal external Proxy Network Load Balancer 此負載平衡器適用於需要全球可用性和高效能的 TCP/SSL 應用程式。它支援使用 Zonal NEGs（包括 VM 和 GKE Pod）作為後端，並整合 Google Cloud Armor 進行安全防護。\n資訊 中文：全球外部代理網路負載平衡器 縮寫：Global Proxy NLB 內部/外部：Public facing (external) 區域：Global Load balancing scheme：EXTERNAL_MANAGED 是否支援 Cloud CDN：❌ 是否支援 Cloud Armor：✅ 是否支援 Service load balancing policy：✅ 是否支援 SSL：✅ 是否支援 PROXY protocol：✅ 特點 只能使用 Premium 等級的 Network Service 支援 TCP 和 SSL 協定 支援 SSL/TLS 卸載 支援使用 Zonal NEGs（包括 VM 和 GKE Pod）作為後端 整合 Google Cloud Armor 進行 DDoS 防護 Global external Proxy Network Load Balancer\nClassic Proxy Network Load Balancer 此負載平衡器適用於現有使用傳統實例群組（Instance Groups）的應用程式。在 Premium 網路服務層中，它可以作為全球性的負載平衡器；在 Standard 網路服務層中，僅限於區域性部署。它支援 TCP 和 SSL 協定，並支援 SSL/TLS 卸載。\n資訊 中文：經典代理網路負載平衡器 縮寫：Classic Proxy NLB 內部/外部：Public facing (external) 區域：Global Load balancing scheme：EXTERNAL 是否支援 Cloud CDN：❌ 是否支援 Cloud Armor：✅ 是否支援 Service load balancing policy：❌ 是否支援 SSL：✅ 是否支援 PROXY protocol：✅ 特點 可以選擇 Standard 或是 Premium 等級的 Network Service 支援 TCP 和 SSL 協定 支援 SSL/TLS 卸載 支援使用傳統實例群組作為後端 在 Premium Tier 中可作為全球性負載平衡器，在 Standard Tier 中僅限於區域性部署 Classic Proxy Network Load Balancer\nRegional external Proxy Network Load Balancer 此負載平衡器適用於需要在單一區域內處理 TCP 流量的應用程式。它在該區域內提供外部 IP，並將進來的 TCP 流量轉發至後端服務。此負載平衡器支援 TCP 協定，並可選擇使用 Premium 或 Standard 網路服務層級。\n資訊 中文：區域外部代理網路負載平衡器 縮寫：Regional Proxy NLB 內部/外部：Public facing (external) 區域：Regional Load balancing scheme：EXTERNAL_MANAGED 是否支援 Cloud CDN：❌ 是否支援 Cloud Armor：❌ 是否支援 Service load balancing policy：❌ 是否支援 SSL：❌ 是否支援 PROXY protocol：✅ 特點 可以選擇 Standard 或是 Premium 等級的 Network Service 支援 TCP 協定 支援使用 Compute Engine 虛擬機（VM）作為後端服務 相較於全球性負載平衡器，區域性負載平衡器的成本較低，適合預算有限的應用程式 Regional external Proxy Network Load Balancer\nCross-Region internal Proxy Network Load Balancer 這是一個多區域負載平衡器，它基於開源 Envoy 代理實現為託管服務。跨區域模式可讓您將流量負載平衡到全球分佈的後端服務，包括確保流量定向到最近的後端的流量管理。此負載平衡器還具有高可用性。將後端放置在多個區域有助於避免單一區域故障。如果一個區域的後端發生故障，流量可以轉移到另一個區域。\n資訊 中文：跨區域內部代理網路負載平衡器 縮寫：Cross-Region internal Proxy NLB 內部/外部：Internal 區域：Global Load balancing scheme：INTERNAL_MANAGED 是否支援 Cloud CDN：❌ 是否支援 Cloud Armor：❌ 是否支援 Service load balancing policy：✅ 是否支援 SSL：❌ 是否支援 PROXY protocol：✅ 特點 始終可在全球範圍內訪問。 VPC 中任何 Google Cloud 區域的用戶端都可以將流量傳送到負載平衡器 負載平衡器可以將流量傳送到任何區域的後端 自動故障轉移到同一或不同區域的健康後端 Cross-Region internal Proxy Network Load Balancer\nRegional internal Proxy Network Load Balancer 這是一個區域負載平衡器，它基於開源 Envoy 代理程式作為託管服務實作。區域模式可確保所有用戶端和後端都來自指定區域，這在您需要區域合規性時很有幫助。\n資訊 中文：區域內部代理網路負載平衡器 縮寫：Regional internal Proxy NLB 內部/外部：Internal 區域：Regional Load balancing scheme：INTERNAL_MANAGED 是否支援 Cloud CDN：❌ 是否支援 Cloud Armor：❌ 是否支援 Service load balancing policy：❌ 是否支援 SSL：❌ 是否支援 PROXY protocol：✅ 特點 預設無法由不同 Region 進行訪問，需要額外開啟全球訪問設定 負載平衡器只能將流量傳送到與負載平衡器的代理程式位於相同區域的後端 自動故障轉移到同一區域內的健康後端 Regional internal Proxy Network Load Balancer\nPassthrough Network Load Balancer (NLB) – 直通網路負載平衡器 層級：第 4 層（L4） 支援協定：TCP、UDP、ESP、GRE、ICMP、ICMPv6 等 功能： 不終止連線，將流量直接傳遞給後端 保留原始封包資訊（來源 IP、目的地 IP 等） 僅支援區域性部署 適用場景：需要低延遲、高效能，並保留原始封包資訊的內部服務，如資料庫、內部微服務通訊等。 Passthrough Network Load Balancer 主要分為兩種 (內、外網)：\nExternal passthrough Network Load Balancer 這是一種區域性的第 4 層（L4）負載平衡器，將來自網際網路的流量分配至同一區域內的後端服務。它不進行代理或 SSL/TLS 卸載，並保留原始的客戶端 IP 位址。\n資訊 中文：外部直通網路負載平衡器 縮寫：External NLB 內部/外部：Public facing (external) 區域：Regional Load balancing scheme：EXTERNAL 是否支援 Cloud CDN：❌ 是否支援 Cloud Armor：✅ 是否支援 Service load balancing policy：✅ 是否支援 SSL：❌ 是否支援 PROXY protocol：❌ 特點 支援 TCP、UDP、ESP、GRE、ICMP 和 ICMPv6 等協定 保留原始客戶端 IP 位址，適用於需要此資訊的應用程式 支援使用區域性的後端服務或目標集（Target Pool）作為後端 可與 Google Cloud Armor 整合，提供進階的 DDoS 防護 支援 IPv4 和 IPv6 流量 適用於需要低延遲和高效能的應用場景 External passthrough Network Load Balancer\nInternal passthrough Network Load Balancer 這是一種區域性的第 4 層（L4）負載平衡器，將流量分配至同一 VPC 網路內的後端服務。它僅在內部網路中運作，適用於內部服務之間的通訊。\n資訊 中文：內部直通網路負載平衡器 縮寫：Internal NLB 內部/外部：Internal 區域：Regional Load balancing scheme：INTERNAL 是否支援 Cloud CDN：❌ 是否支援 Cloud Armor：❌ 是否支援 Service load balancing policy：❌ 是否支援 SSL：❌ 是否支援 PROXY protocol：❌ 特點 支援 TCP、UDP、ICMP、ICMPv6、SCTP、ESP、AH 和 GRE 等協定 保留原始客戶端 IP 位址，適用於需要此資訊的內部應用程式 支援使用區域性的後端服務作為後端 可作為靜態路由的下一跳，實現更靈活的流量控制 支援與 Service Directory 整合，方便服務發現與管理 適用於需要高效能和低延遲的內部服務通訊 Internal passthrough Network Load Balancer","參考資料#參考資料":"Network Service Tiers：https://cloud.google.com/network-tiers?hl=zh-tw\nExternal Application Load Balancer：https://cloud.google.com/load-balancing/docs/https\nInternal Application Load Balancer：https://cloud.google.com/load-balancing/docs/l7-internal\nProxy Network Load Balancer：https://cloud.google.com/load-balancing/docs/proxy-network-load-balancer\nInternal proxy Network Load Balancer：https://cloud.google.com/load-balancing/docs/tcp/internal-proxy\nPassthrough Network Load Balancer：https://cloud.google.com/load-balancing/docs/passthrough-network-load-balancer\nInternal passthrough Network Load Balancer：https://cloud.google.com/load-balancing/docs/internal","總結#總結":" 負載平衡器類型 層級 協定 功能 適用場景 Application Load Balancer (ALB) L7 HTTP, HTTPS 基於內容的路由、SSL/TLS 終止、整合 Cloud CDN 和 Cloud Armor Web 應用程式需要應用層路由和 SSL 終止 Proxy Network Load Balancer (PNLB) L4 TCP, SSL 反向代理、SSL/TLS 終止 處理加密的 TCP 流量，並在負載平衡器層級終止 SSL 的應用程式 Passthrough Network Load Balancer (NLB) L4 TCP, UDP, ESP, GRE, ICMP 等 不終止連線，保留原始封包資訊 低延遲、高效能的內部服務，如資料庫、內部微服務通訊等 "},"title":"GCP Load Balancer 介紹"},"/blog/gcp/gcp-log-reduce-cloud-logging-api/":{"data":{"":"當我們使用 GCP 的 Cloud Logging 服務來查看 Log 時，有時候會有一些我們不需要顯示出來的，或是從來都不會去查詢的 Log，再者是 GCP 本身的錯誤導致大量噴錯的 Log ，這些 Log 都會導致 Cloud Logging 的費用增加。","介紹-cloud-logging#介紹 Cloud Logging":"先來簡單說一下 Cloud Logging 這項服務的基本架構，請看圖：\nCloud Logging 基本架構\n可以看到 Logs Data 會透過 API 再經過 _Default log sink (router) 存到相應命名的 log bucket (預設配置)，圖中 _Required 以及 _Default 的 log sink 都是 GCP 自動創建的接收器，下面簡述一下它們的區別：\n_Required 日誌儲存桶 Cloud Logging 會將以下類型的 Log 存到 _Required 儲存桶\n管理員活動審核 Log\n系統事件審核 Log\nAccess Transparency Log\nCloud Logging 會將 _Required 儲存桶 Log 保留 400 天，無法調整該期限，且無法修改或刪除 _Required 儲存桶，也沒辦法停用 _Required log sink 接受器路由到 _Required 儲存桶的設定。\n_Default 日誌儲存桶 只要不是存在 _Required 日誌儲存桶的 Log 就會透過 _Default log sink 接受器路由到 _Default 儲存桶。\n除非有另外配置自定義設定， 否則 _Default 日誌儲存桶 Log 只會保留 30 天，也一樣無法刪除 _Default 日誌儲存桶。此外 Cloud Logging 的費用是以存在 _Default 日誌儲存桶來計算。\n功能 價格 每月免費額度 Logging 提取 提取的 Log $0.5/GiB 每個項目前 50 GiB Logging 儲存 保留超過 30 天的 Log，每月每 GiB $0.01 在默認保留期限的 Log 不會有額外儲存費用 查看該專案使用的 Cloud Logging API 費用：https://console.cloud.google.com/apis/api/logging.googleapis.com/cost","參考資料#參考資料":"Routing and storage overview https://cloud.google.com/logging/docs/routing/overview","過濾-log#過濾 Log":"那現在知道 Cloud Logging 的架構，那當我們遇到需要過濾 Log 時，我們可以使用以下步驟來過濾以節省 Cloud Logging API 費用：\n範例說明 這次範例是 google 在 2023/07/06 所發佈的 Service Health，當 GKE 版本大於 1.24 以上，就會噴\nFailed to get record: decoder: failed to decode payload: EOF\ncannot parse ‘0609 05:31:59.491654’ after %L\ninvalid time format %m%d %H:%M:%S.%L%z for ‘0609 05:31:59.490647’\n這三種類型的錯誤 Log，在等待官方修復前，官方的建議是先將他給過濾掉，避免一直刷噴錢 ┐(´д`)┌\n附上當時的 Service Health 連結：https://status.cloud.google.com/incidents/y5XvpyBXFhsphSt4DfiE\n我們在上面架構圖有說到，Log 會透過 log sink 路由到 bucket，所以我們要將過濾條件加在 log router 上：\n選擇 Log Router 選擇 Log Router\n選擇 Log Router Sinks 選擇 _Default 的 Log Router Sinks，點選右邊按鈕的 Edit Sink\n選擇 _Default 的 Log Router Sinks\n設定 Sink details 第一個是 details，可以輸入說明，這邊輸入：google 有 bug 會噴大量的意外 LOG，怕費用飆高，先用官方建議的來過濾 LOG，詳細可以看： https://status.cloud.google.com/incidents/y5XvpyBXFhsphSt4DfiE\n輸入 Sink details 說明\n選擇 Sink Service 跟 Bucket 接著 sink 服務選擇 Logging bucket，以及對應儲存的 log bucket (這邊基本上都是預設)\n選擇 Logging bucket\n設定 include Log 選擇那些可以被 include 到 sink 接收器的 LOG 格式 (這邊基本上都是預設)\n預設的 LOG 格式\n設定 filter Log 這邊就是我們要輸入過濾的地方，先點擊 ADD EXCLUSION，輸入過濾的名稱，以及過濾的內容格式，我們輸入 google 在 Service Health 所提供的格式，最後按下 UPDATE SINK\n新增要過濾的 LOG 格式\n設定完成 等待更新完成，就可以看到我們已經在接收器上設定好過濾條件囉～\n查看詳細接收器設定\n檢查 Log 是否過濾成功 最後再檢查一下 Log 是不是沒有收到該錯誤的 Log 內容\n檢查 LOG 是否不會再出現"},"title":"如何過濾 GCP LOG，減少 Cloud Logging API 的花費"},"/blog/gcp/gcp-memorystore-failover/":{"data":{"":"此文章主要針對 GCP Memorystore failover 來做測試，測試 Memorystore 高可用性 (HA) 在標準 Standard Tier failover 故障轉移需要多久，以及不同 replica 條件下轉移是否會有差異等等。","參考資料#參考資料":"Memorystore for Redis FAQ：https://cloud.google.com/memorystore/docs/redis/faq\nHigh availability for Memorystore for Redis：https://cloud.google.com/memorystore/docs/redis/high-availability-for-memorystore-for-redis#when_a_failover_is_triggered\nAbout manual failover：https://cloud.google.com/memorystore/docs/redis/about-manual-failover#stackdriver_verification\nInitiate a manual failover：https://cloud.google.com/memorystore/docs/redis/initiate-manual-failover\nGeneral best practices：https://cloud.google.com/memorystore/docs/redis/general-best-practices\nExponential backoff：https://cloud.google.com/memorystore/docs/redis/exponential-backoff","實作測試-1m1s#實作測試 (1m1s)":"可以只用之前 IaC 文章來建立，或是用 UI 來建立 Memorystore，這邊用 IaC 來示範：\nterraform { source = \"${get_path_to_repo_root()}/modules/memorystore\" } include { path = find_in_parent_folders() } inputs = { name = \"ian-1m1s\" memory_size_gb = 5 region = \"asia-east1\" network = \"bbin-XXX\" replica_count = 1 redis_version = \"REDIS_6_X\" redis_configs = { \"maxmemory-policy\" = \"allkeys-lru\" } } 這邊 memorystore module 預設是 Standard Tier，另外小提醒，如果要使用 Replica memory_size_gb 最小必須是 5G。\n設定監控 建立完成後，我們也先把監控給拉出來，設定可以參考 About manual failover，主要是選擇 Cloud Memorystore Redis \u003e replication \u003e Node role\n監控圖表\n可以看到 node-0 跟 node-1，數字 1 代表 Primary (node-0)、數字 0 代表 Replica (node-1)\n查看 describe 我們也可以下指令查看：\ngcloud redis instances describe ian-1m1s --region=asia-east1 --project={project_id} describe 畫面\n可以看到 currentLocationId 代表現在 Primary (node-0) asia-east1-a，locationId 代表最初配置 Primary 區域，alternativeLocationId 則是最初配置 Replica 的區域，因此我們也可以在故障轉移後，來查看區域是否變化。詳細可以參考：gcloud verification\n接著我們可以參考 Initiate a manual failover 文章，手動來觸發 failover，這邊提醒一下 手動 failover 有分兩種資料保護模式：limited-data-loss、force-data-loss，詳細請看 Optional data protection mode，這邊測試就不討論兩種保護模式，都使用預設的 limited-data-loss 來測試。\n寫 shell 測試 redis 連線 另外，我們在寫一個 shell 到 GKE 裡面 (用同 VPC 打) 打 Memorystore Redis 的 Primary Endpoint 以及 Read Endpoint，看看當 failover 是不是會有連不上的問題 ～\napiVersion: v1 kind: Pod metadata: name: redis-test namespace: default spec: initContainers: - name: copy-scripts image: busybox command: [ \"sh\", \"-c\", \"cp /config/redis.sh /scripts/ \u0026\u0026 chmod +x /scripts/redis.sh\", ] volumeMounts: - name: config-volume mountPath: /config - name: scripts mountPath: /scripts containers: - name: redis-cli image: redis:alpine command: [\"sh\", \"-c\", \"while true; do /scripts/redis.sh; sleep 1; done\"] env: - name: READ_PRIMARY_HOST value: \"172.18.0.69\" - name: READ_REPLICA_HOST value: \"172.18.0.68\" - name: REDIS_PASSWORD value: \"XXXX\" volumeMounts: - name: scripts mountPath: /scripts workingDir: /scripts volumes: - name: config-volume configMap: name: redis-scripts - name: scripts emptyDir: {} --- apiVersion: v1 kind: ConfigMap metadata: name: redis-scripts namespace: default data: redis.sh: | #!/bin/sh ## 顏色設定 GREEN=\"\\033[1;32m\" RED=\"\\033[1;31m\" NC=\"\\033[0m\" READ_PRIMARY_HOST=${READ_PRIMARY_HOST} READ_REPLICA_HOST=${READ_REPLICA_HOST} READ_REPLICA_PORT=6379 REDIS_PASSWORD=${REDIS_PASSWORD} while true; do TIMESTAMP=$(TZ=\"Asia/Taipei\" date +\"%Y-%m-%d %H:%M:%S\") PRIMARY_OUTPUT=$(timeout 1 redis-cli -h $READ_PRIMARY_HOST -p $READ_REPLICA_PORT -a $REDIS_PASSWORD --no-auth-warning PING) 1\u003e/dev/null if [ $? -eq 0 ]; then echo -e \"[$TIMESTAMP] PRIMARY PING ${GREEN}SUCCESS${NC}：\" \"$PRIMARY_OUTPUT\" else echo -e \"[$TIMESTAMP] PRIMARY PING ${RED}FAILED${NC}\" fi REPLICA_OUTPUT=$(timeout 1 redis-cli -h $READ_REPLICA_HOST -p $READ_REPLICA_PORT -a $REDIS_PASSWORD --no-auth-warning PING) 1\u003e/dev/null if [ $? -eq 0 ]; then echo -e \"[$TIMESTAMP] REPLICA PING ${GREEN}SUCCESS${NC}：\" \"$REPLICA_OUTPUT\" else echo -e \"[$TIMESTAMP] REPLICA PING ${RED}FAILED${NC}\" fi sleep 1 done 開始手動觸發 failover 我們準備好監控以及也先看好 describe 後，就可以下指令來手動觸發 failover：\ngcloud redis instances failover ian-1m1s --data-protection-mode=limited-data-loss --region=asia-east1 --project={project_id} 可以看到目前 ian-1m1s 就開始 Failing over 了～\nmemorystore UI 畫面","實作測試-1m2s#實作測試 (1m2s)":"由於設定都差不多，所以上述 1m1s 設定這邊就不重複說明，記得把 IaC 的 replica_count 改成 2，以及 shell 的 IP 跟 Password 要記得換！\n直接看監控部分：可以看到現在 node-2 是 1，代表 node-2 是 Primary，其他 node-1、node-0 都是 Replica。\n監控畫面\n接著看 describe：currentLocationId 是 asia-east1-c，也就是 node-2 (Primary)。\ndescribe 畫面\n一樣下指令來跑 shell，並觀察監控、describe 來有 shell： 變成 node-0 變成 Primary，node-2 變成 Replica，node-1 一樣是 Replica。\n監控畫面\ncurrentLocationId 變成是 asia-east1-a，Primary 從 node-2 變成 node-0。\ndescribe 畫面\n觀察 shell 發現，有出現 FAILED 都是 Primary，Replica 都正常。\nshell 畫面","後續觀察#後續觀察":"查看監控 首先我們先看監控部分，可以看到原本的 node-1 是 Primary 後來掉下來的同時，由 node-2 變成 Primary 最後完成故障轉移。\n監控畫面\n再來看 describe 的部分，可以發現原本的 currentLocationId 從 Primary (node-0) asia-east1-a 變成(node-1) asia-east1-b，完成故障轉移，因此更換區域。\ndescribe 畫面\n查看腳本 最後，查看一下腳本的執行紀錄可以發現：在 18:12:58， 出現第一筆 Primary PING 不到的錯誤，到 18:13:18 才恢復，代表故障轉移時間約 20 左右 (最後面會測試 5 次來取平均)，其流程是 Primary 會先連不到，接著變成 Primary 跟 Replica 都連不到，最後剩下 Replica，到都正常。\nshell 畫面","整理故障轉移時間#整理故障轉移時間":" 測測次數 1m1s (秒) 1m2s (秒) #1 16 13 #2 19 10 #3 22 6 #4 20 10 #5 18 9 可以發現，多一個 Replica，在故障轉移的時間可以更快速。","文件說明#文件說明":"首先我們可以先閱讀 Google Memorystore for Redis FAQ 文章，可以得知 Standard Tier 在 failover 轉移，大約會花 30 秒左右。\nGoogle Memorystore for Redis FAQ\n我們也可以從 High availability for Memorystore for Redis 文章中知道，當 Primary 資料庫發生故障時，就會發生故障轉移。在轉移期間，Primary Endpoint 和 Read Endpoint 會自動重新導向新的 Primary 資料庫 和 Replica。這時候 Primary Endpoint 和 Read Endpoint 連線都會被刪除。\nwhen a failover is triggered\n因次會導致有幾秒鐘連不到 Redis。重新連接時，將使用原本的 IP 位址即可自動重新定向到新的服務上。故障轉移後，不需要調整連線設定。\n接下來我們來實際測試看看，我們分成兩個測試組，分別為：1m1s、1m2s 的方式，來看看故障轉移時，需要花多久，還有不同 Replica 條件下會不會有差異。","最佳實踐#最佳實踐":"所以我們知道，每當發生 Failover 時，一定會出現斷線的問題，官方在 Memorystore for Redis 的 General best practices 中有提到需要重新連線的操作和場景，以下都會導致與 Redis instance 網路連線中斷：\nVersion upgrade\nScaling up/down\nImporting\nManual failover\nSystem maintenance\nCertificate Authority rotation for Redis instances that have in-transit encryption enabled\nEmergency failover\n所以我們需要再設計應用程式時，考量到這一點，可以參考 Exponential backoff ，要加上重試邏輯，讓應用程式自動重新連線並持續正常運作。"},"title":"GCP Memorystore HA 高可用性 failover 測試"},"/blog/gcp/gcp-prometheus-sample-ingested-calculate/":{"data":{"":"最近在優化公司帳單費用，發現公司有幾個 project 裡面的 Prometheus Samples Ingested 費用很高，此文章會針對費用如何計算以及如何去減少費用來做說明。\nPrometheus Samples Ingested 就是 Prometheus 攝取的樣本數，因為上述幾個專案，我們都是使用 Google Managed Prometheus (GMP) 的方式來接 Metrics，所以可以先推測是 Metrics 導致此費用增加。\nGoogle 帳單 (project_1)\nGoogle 帳單 (project_2)","如何減少花費#如何減少花費":"我們可以從上面的表中知道，是 phpfpm_process_state/gauge 這個 metric，總共收了 30.71 B 也就是大約 1842.60 美元的花費是裡面最高的。所以我們就先這對如何減少這個 metric 來做說明：\n要先知道 phpfpm_process_state/gauge 這個 metric 是怎麼來的呢？\n首先我們可以從 metric 名稱知道它是 phpfpm 的 process 狀態。我們有開啟 Cluster 的 Managed Service for Prometheus，來使用 Google 管理的 Prometheus (GMP)， 並在服務上面設定 phpfpm-exporter，並使用 PodMonitoring 來將 Pod 上的 metrics 接到 GMP 上。\n所以我們可以先到有 phpfpm-exporter 的 Pod 隨意的 Container 去看看這個 metrics，下 curl -s 127.0.0.1:9253/metrics 指令來查看 (請依照設定 port 去查看，這邊不詳細列出)，可以看到會有很多 phpfpm-exporter 的 metrics。\nphpfpm-exporter metrics\n其中我們也可以找到 phpfpm_process_state 這個 metrics，這邊我們發現一個比較特別的事情，一般的 metrics 都只有一筆，但 phpfpm_process_state 它有 6 個狀態。\nphpfpm-exporter metrics\n所以代表送到 GMP 的 metrics 也會比其他 phpfpm 相關的 metrics 還多 6 倍，這也可以解釋為什麼他是所有 metrics 裡面費用最高的，以及 phpfpm_process_state 費用是其他 phpfpm 像是：phpfpm_process_request_duration 的 6 倍了。\nMetrics Management phpfpm metrics\n因此我們可以來計算一下費用，phpfpm_process_XXX 的 metrics 它會因為 process 的數量而改變，以 phpfpm_process_state 這個 metrics 來計算，計算公式就是：\nprocess 數量 * 6 (上面說的6個狀態) * Pod 數量 * PodMonitoring interval 的秒數\n(PodMonitoring interval 的秒數這個我們後續再說)\n可以先看下圖，我們這個的 api 有 30 的 process，乘 6 個狀態，可以看到每次的搜集就會收集 180 筆的 phpfpm_process_state。\nphpfpm status\n我們也可以透過 GCP 的 Metrics explorer 查詢到對應的數值。\nphpfpm status\n剛剛提到的 PodMonitoring interval 的秒數，在 project_1 的 PodMonitoring 設定間隔是 60s，所以上面才不需要另外乘，但像是 project_1 的秒數都是 5s，所以我們還需要再多乘上 12 (60/5) 才是我們在 GCP 的 Metrics explorer 看到每分鐘的數量。\n因此，我們可以得知費用會跟 process (範例的 metrics)、裡面的 metrics 有幾個、Pod 數量、 PodMonitoring 間隔時間有關，跟 Google 的文件說明也是一樣的\nhttps://cloud.google.com/stackdriver/pricing?hl=zh-tw#pricing_examples_samples\n所以要減少費用，可以調整上述影響 metrics 數量的變因，另一種方式就是，針對需要收集哪些 metrics 來做過濾，可以參考此文件：Get started with managed collection，過濾不要的 metrics ，避免送到 GMP 上，而有多餘的費用，我們可以在 PodMonitoring 上去來過濾要送出 metrics，如下：\napiVersion: monitoring.googleapis.com/v1 kind: PodMonitoring metadata: name: {{ $.Values.deployment.name }} namespace: {{ $.Release.Namespace }} spec: selector: matchLabels: app: {{ $.Values.deployment.name }} endpoints: - port: metrics interval: {{ .endpoints.metrics_interval_sec }} metricRelabeling: - action: drop regex: phpfpm_process_state sourceLabels: [__name__] 透過 metricRelabeling 使用 drop 方式，搭配正規表示法來過濾 name 是 phpfpm_process_state 的 metrics，另外也可以使用 keep 的方式，決定要送什麼到 GMP 上。\n過濾後，指標不見惹\n上圖就是在 PodMonitoring 上新增 drop 來過濾 phpfpm_process_state，也可以看 Samples billable volume 變成 0 Sample，就代表我們把這個 metrics 給過濾掉拉～～～ 也就可以省錢囉xDD\nMetrics Management phpfpm metrics","費用計算說明#費用計算說明":"首先我們先來計算一下費用是不是正確，Prometheus Samples Ingested 的 SKU 是 A4E4-DF03-CDB6，可以透過這個頁面來查詢：\nGoogle Cloud Platform SKUs\n我們可以看到如果是 0 ~ 50,000,000,000 的 Samples，會以每 1,000,000/0.06 USD 來計算，其他以此類推，為了更方便的計算，我有簡單寫一個腳本來計算費用：\ncalc_metrics.sh #! /bin/bash if [[ $# -ne 1 ]]; then echo \"請使用： $0 \u003csamples\u003e 來計算費用\" exit 1 fi if [[ $1 =~ [a-zA-Z] ]]; then number=$(echo \"$1\" | sed -E 's/([0-9]+(\\.[0-9]+)?)\\s*[a-zA-Z]+/\\1/') unit=$(echo \"$1\" | sed -E 's/[0-9]+(\\.[0-9]+)?\\s*([a-zA-Z]+)/\\2/') case $unit in K) factor=1000 ;; M) factor=1000000 ;; B) factor=1000000000 ;; *) echo \"未知單位: $unit\" exit 1 ;; esac result=$(printf \"%'d\" $(printf \"%.0f\" $(echo \"$number * $factor\" | bc))) else result=$1 fi echo \"Samples：${result}\" result=$(echo \"${result}\" | tr -d ',') if [[ ${result} -le 50000000000 ]]; then cost=$(echo \"scale=2; ${result} * 0.06 / 1000000\" | bc) elif [[ ${result} -le 250000000000 ]]; then cost=$(echo \"scale=2; ${result} * 0.048 / 1000000\" | bc) elif [[ ${result} -le 500000000000 ]]; then cost=$(echo \"scale=2; ${result} * 0.036 / 1000000\" | bc) else cost=$(echo \"scale=2; ${result} * 0.024 / 1000000\" | bc) fi echo \"費用：$cost USD\" 我們分別帶入 project_1 (106,676,274,756) 以及 project_2 (66,631,967,760) 的 Samples 來計算看看金額是否正確。\n用腳本檢查金額是否與帳單一樣\n計算完與實際的帳單費用差不多，接著我們可以打開 GCP 的 Metrics Management 來查看一下，我們用了哪些 Metrics 導致費用這麼高。\n以下以 project_1 為例：\n打開 Metrics Management 後，將時間選擇前 30 天(30d)，與上面帳單選擇一樣，接著可以看到 Billable samples ingested 這邊，這裡就是指我們 30 天總共收了多少筆的 samples，也可以把他理解收了多少筆的 Metrics。也可以看到底下表格的 Samples billable volume 可以透過排序知道是誰使用最多。這邊的 B 代表 10 億，也就是 1000000000，所以我們上個月收了 1066 多億筆的 samples。\n查看 Metrics Management\n可以使用剛剛的腳本來計算 (沒錯，它也支援數字單位的轉換 xD)，計算如下：\n查看 Metrics Management\n所以可以得知，帳單 Prometheus Samples Ingested 這個 SKU 會這麼高就是這邊的 Billable samples ingested 所導致，那我們現在可以依照下面的表格 Metrics 知道是哪個 Metrics 花錢最兇，在針對這些 Metrics 來進行調整，以達到減少費用的目的。"},"title":"GCP Prometheus Samples Ingested 計算方式及如何減少費用"},"/blog/gcp/gcs-cors/":{"data":{"":"最近公司有需求需要透過前端去打 GCS Bucket 的檔案，但會遇到 CORS 錯誤，所以寫一篇來記錄此問題的解決方法。\n有先簡單寫一個前端頁面，可以透過 Axios 去打後端，詳細程式可以點我查看，我們也另外建立一個公開的 GCS Bucket，並放一個測試用的 JSON 檔案 (都有附在此專案中)。\n公開的 GCS Bucket URL 及內容 (bucket 會用 ian-test-demo 來示範，請自行修改成自己的 bucket 名稱) curl 測試\n使用測試程式需要先做以下步驟：\n執行 cd code; docker build -t gcs-cors-test .\n執行 docker run -d -p 8080:80 --name gcs-cors-test gcs-cors-test\n開啟瀏覽器 127.0.0.1:8080\n開啟後，我們輸入剛剛公開 GCS Bucket URL 到輸入欄位，開啟 F12 Network，並按下測試\n使用程式來測試\n就會發現，出現 CORS error 錯誤，我們可以查看下方的錯誤說明\n發現 CORS 錯誤\n因為我們從 http://localhost:8080 要打到 https://storage.googleapis.com/ian-test-demo/hello.json ，觸發了瀏覽器的 CORS 限制，所以導致噴錯，CORS 的說明詳細可以直接參考：https://www.explainthis.io/zh-hant/swe/what-is-cors\n那要怎麼解決呢，根據 Google 文件，我們需要為 Bucket 配置 cors 設定\n我們可以先下此指令來查看該 bucket 是否有設定 cors： gcloud storage buckets describe gs://ian-test-demo --format=\"default(cors_config)\"\ngcloud 指令查看 cors 設定\n如果還沒設定就會顯示 null\n那我們先來寫一下 cors 的設定檔案\n[ { \"origin\": [\"*\"], \"method\": [\"GET\", \"POST\", \"PUT\", \"DELETE\"], \"responseHeader\": [\"Content-Type\", \"Authorization\"], \"maxAgeSeconds\": 30 } ] (可以再根據自己需求去調整，先以這樣來 demo)\n寫完後，我們需要設定到 Bucket 上面，可以用此指令將 cors.json 設定到指定的 Bucket 上：gcloud storage buckets update gs://ian-test-demo --cors-file=cors.json\n下完指令後，我們可以在用 describe 來確認是否設定完成，正常會如下顯示：\n設定及檢查\n接著我們就可以來測試看看是否還會碰到 CORS error 的問題了\n再次使用前端程式測試\n另外，如果不需要設定 CORS 時，可以用以下指令移除該 Bucket 的 CORS 設定：gcloud storage buckets update gs://ian-test-demo --clear-cors\n移除 CORS 設定","參考#參考":"Set up and view CORS configurations\nCORS configuration examples"},"title":"GCS Bucket CORS 錯誤解決方法"},"/blog/gcp/gke-cloud-dns-nodelocaldnscache/":{"data":{"":"最近在評估要將公司內的 EndPoint 都改成 Cloud DNS 的 Private Zone (打造內部的 internal dns 服務機制)，到時候 DNS 解析的請求會比以往還要多，所以需要先測試評估 GKE 內的 DNS 解析方案，避免再次發生 Pod 出現 cURL error 6: Could not resolve host，此篇文章測試的是： Cloud DNS + NodeLocal DNSCache 的運作。\n首先，先建立一個 dns-test pod 程式連結 以及 nginx 的 pod + svc 程式連結，會分別測試\n叢集內部 cluster.local (nginx-svc.default.svc.cluster.local)\ninternal-dns 使用 cloud dns private (aaa.test-audit.com)\n外部 dns (ifconfig.me)\n並使用 nslookup 腳本進行確認回傳 DNS 解析，每一次測試都會重新建立 KubeDNS、NodeLocal DNSCache Pod\n相關程式以及 Prometheus、Grafana 的設定可以參考：https://github.com/880831ian/gke-dns\n在建立完 Cluster 後，可以先觀察在 Cloud DNS 是否新增的 Private Zone\n由於裡面有很多的 Record，我們先搜尋下面會用到的 nginx-svc.default.svc.cluster.local 來當作範例","internal-dns-cloud-dns-private#Internal DNS (Cloud DNS Private)":"先建立一個 cloud dns private，以下範例是 aaa.test-audit.com \u003e 10.1.1.4，並將此 use VPC 與 GKE 的 VPC 打通\n設定 cloud dns private\nNodeLocal DNSCache Prometheus 監控設定參數：zones=\".\"\n相關 Prometheus 監控指標： coredns_cache_requests_total{job=\"cloud-dns-nodelocaldns\", zones=\".\"} coredns_cache_entries{job=\"cloud-dns-nodelocaldns\", zones=\".\"} coredns_cache_hits_total{job=\"cloud-dns-nodelocaldns\", zones=\".\"} coredns_cache_misses_total{job=\"cloud-dns-nodelocaldns\", zones=\".\"} kubedns_dnsmasq_hits{job=\"kubedns-dns\"} kubedns_dnsmasq_misses{job=\"kubedns-dns\"} 測試腳本：\n#!/bin/bash get_taiwan_time() { # 獲取當前 UTC 時間的 Unix 時間戳 UTC_TIMESTAMP=$(date -u +%s) # 加上 8 小時 (台灣時間是 UTC+8) TAIPEI_TIMESTAMP=$((UTC_TIMESTAMP + 28800)) # 將時間戳轉換為格式化後的日期時間 date -d \"@$TAIPEI_TIMESTAMP\" \"+%Y-%m-%d %H:%M:%S\" } DOMAIN=\"aaa.test-audit.com\" EXPECTED_IP=\"10.1.1.4\" START_TIME=$(get_taiwan_time) COUNT=10000 SUCCESS_COUNT=0 FAIL_COUNT=0 echo \"== NSLOOKUP TEST START: $START_TIME ==\" | tee -a nslookup_full.log for i in $(seq 1 \"$COUNT\"); do OUTPUT=$(nslookup \"$DOMAIN\" 2\u003e\u00261) ADDR_LINE=$(echo \"$OUTPUT\" | grep -E '^Address:') if [[ \"$ADDR_LINE\" == *\"$EXPECTED_IP\"* ]]; then SUCCESS_COUNT=$((SUCCESS_COUNT + 1)) else FAIL_COUNT=$((FAIL_COUNT + 1)) fi echo \"[$i] === $(get_taiwan_time) 成功: $SUCCESS_COUNT 失敗: $FAIL_COUNT\" done END_TIME=$(get_taiwan_time) echo \"== NSLOOKUP TEST END: $END_TIME ==\" | tee -a nslookup_full.log echo \"成功次數: $SUCCESS_COUNT\" | tee -a nslookup_full.log echo \"失敗次數: $FAIL_COUNT\" | tee -a nslookup_full.log 10.1.1.4 是隨機亂取的 IP，只是為了確認 domain 是否能夠正常解析\n測試腳本 測試結果：\n測試結果\nPrometheus 監控指標\n結論\n可以觀察 NodeLocal DNSCache hit 跟 request 都有持續上升，但與 KubeDNS + NodeLocal DNSCache 模式 metrics 比較不一樣的地方是：Server 從 KubeDNS IP 變成 169.254.20.10，可以回去看：GKE KubeDNS + NodeLocal DNSCache 運作測試 #Internal DNS (Cloud DNS Private)\n模擬 KubeDNS Pod 異常 接下來會在測試中，將 KubeDNS 調整到 0 顆，再開回去，觀察此模式對於 KubeDNS 的依賴\n測試結果：\n測試結果\nPrometheus 監控指標\n結論\n大約在 2000 筆請求時左右將 KubeDNS 關成 0 顆，可以看到 NodeLocal DNSCache 有往上漲，也因為我們已經改用 Cloud DNS 來解析 internal-dns DNS，所以就算關閉 KubeDNS 也不會有影響，因為 NodeLocal DNSCache 會去問 Cloud DNS，並 Cache 在 NodeLocal DNSCache\n模擬 NodeLocal DNSCache Pod 異常 接下來我們測試最後一個情境，故意用壞 NodeLocal DNSCache 服務，觀察此模式對於 NodeLocal DNSCache 的依賴\n測試情境：\n先調整 COUNT 參數變成 50000\n先跑 15000 筆有 NodeLocal DNSCache，然後使用以下指令讓 NodeLocal DNSCache 無法使用\nkubectl patch daemonset node-local-dns -n kube-system --type='strategic' -p '{\"spec\":{\"template\":{\"spec\":{\"nodeSelector\":{\"this-label-does-not-exist-on-any-node\":\"true\"}}}}}' 等待約 30000 筆時，使用以下指令恢復 NodeLocal DNSCache\nkubectl patch daemonset node-local-dns -n kube-system --type='strategic' -p '{\"spec\":{\"template\":{\"spec\":{\"nodeSelector\":null}}}}' 測試結果：\n測試結果\n只要 NodeLocal DNSCache 故障解析就會卡住，等到恢復，就能繼續正常運作\n模擬 NodeLocal DNSCache 故障\nPrometheus 監控指標\n結論\n發現當 NodeLocal DNSCache 掛了後，會直接卡住 (時間每五秒是因為打不到 timeout)，也不會切換到 KubeDNS 來做解析，所以最後測試沒有等到 30000 筆才下指令恢復 NodeLocal DNSCache，先提早下指令讓 NodeLocal DNSCache 正常，等到恢復，解析就正常了","k6-測試#k6 測試":"額外在另一個 cluster 建立 nginx deployment 開 5 個 pod 以及 svc 改成 lb (L4)，然後在 cloud dns 的 test-audit-com 設定 nginx-lb-internal.test-audit.com 解析到內網的 svc (10.156.16.19)\n使用 k6 測試 KubeDNS + NodeLocal DNSCache 模式下 IP 跟 DNS 的差異\n相關程式可以參考：https://github.com/880831ian/gke-dns\n這邊測試的 Node 是用 e2-medium 而非 c3d-standard-4\n第一次測試 IP (avg=144.26ms / 3895 RPS)、DNS (avg=145.16ms / 3887 RPS)\nIP 第一次測試\nDNS 第一次測試\n第二次測試 IP (avg=141.37ms / 3960 RPS)、DNS (avg=144.9ms / 3806 RPS)\nIP 第二次測試\nDNS 第二次測試\n第三次測試 IP (avg=172.8ms / 2959 RPS)、DNS (avg=150.34ms / 3715 RPS)\nIP 第三次測試\nDNS 第三次測試\n第四次測試 IP (avg=168.01ms / 3071 RPS)、DNS (avg=168.14ms / 3018 RPS)\nIP 第四次測試\nDNS 第四次測試\n結論\n理論上 ip 應該會比 dns 還要快，但測試 4 次發現其實不一定","參考資料#參考資料":"搭配 Cloud DNS 使用 NodeLocal DNSCache：https://cloud.google.com/kubernetes-engine/docs/how-to/nodelocal-dns-cache?hl=zh-tw#cloud_dns_dataplane","叢集內部-clusterlocal#叢集內部 cluster.local":"一樣會觀察 KubeDNS Pod Metrics，因為就算開啟 Cloud DNS 後，KubeDNS 還是會留著\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/cloud-dns?hl=zh-tw#architecture\nNodeLocal DNSCache Prometheus 監控設定參數：zones=\"cluster.local.\"\n相關 Prometheus 監控指標： coredns_cache_requests_total{job=\"cloud-dns-nodelocaldns\", zones=\"cluster.local.\"} coredns_cache_entries{job=\"cloud-dns-nodelocaldns\", zones=\"cluster.local.\"} coredns_cache_hits_total{job=\"cloud-dns-nodelocaldns\", zones=\"cluster.local.\"} coredns_cache_misses_total{job=\"cloud-dns-nodelocaldns\", zones=\"cluster.local.\"} kubedns_dnsmasq_hits{job=\"kubedns-dns\"} kubedns_dnsmasq_misses{job=\"kubedns-dns\"} 測試腳本：\n#!/bin/bash get_taiwan_time() { # 獲取當前 UTC 時間的 Unix 時間戳 UTC_TIMESTAMP=$(date -u +%s) # 加上 8 小時 (台灣時間是 UTC+8) TAIPEI_TIMESTAMP=$((UTC_TIMESTAMP + 28800)) # 將時間戳轉換為格式化後的日期時間 date -d \"@$TAIPEI_TIMESTAMP\" \"+%Y-%m-%d %H:%M:%S\" } DOMAIN=\"nginx-svc.default.svc.cluster.local\" EXPECTED_IP=\"10.36.16.172\" START_TIME=$(get_taiwan_time) COUNT=10000 SUCCESS_COUNT=0 FAIL_COUNT=0 echo \"== NSLOOKUP TEST START: $START_TIME ==\" | tee -a nslookup_full.log for i in $(seq 1 \"$COUNT\"); do OUTPUT=$(nslookup \"$DOMAIN\" 2\u003e\u00261) ADDR_LINE=$(echo \"$OUTPUT\" | grep -E '^Address:') if [[ \"$ADDR_LINE\" == *\"$EXPECTED_IP\"* ]]; then SUCCESS_COUNT=$((SUCCESS_COUNT + 1)) else FAIL_COUNT=$((FAIL_COUNT + 1)) fi echo \"[$i] === $(get_taiwan_time) 成功: $SUCCESS_COUNT 失敗: $FAIL_COUNT\" done END_TIME=$(get_taiwan_time) echo \"== NSLOOKUP TEST END: $END_TIME ==\" | tee -a nslookup_full.log echo \"成功次數: $SUCCESS_COUNT\" | tee -a nslookup_full.log echo \"失敗次數: $FAIL_COUNT\" | tee -a nslookup_full.log 10.36.16.172 是 nginx-svc Cluster IP\n需要先確認 nginx-svc 的 IP 是多少，然後修改腳本中的 EXPECTED_IP 變數。\n測試腳本 測試結果：\n測試結果\nPrometheus 監控指標\n結論\n可以觀察 NodeLocal DNSCache hit 跟 request 都有持續上升，但與 KubeDNS + NodeLocal DNSCache 模式 metrics 比較不一樣的地方是：Server 從 KubeDNS IP 變成 169.254.20.10，可以回去看：GKE KubeDNS + NodeLocal DNSCache 運作測試 #叢集內部 cluster.local\n模擬 KubeDNS Pod 異常 接下來會在測試中，將 KubeDNS 調整到 0 顆，再開回去，觀察此模式對於 KubeDNS 的依賴\n測試結果：\n測試結果\nPrometheus 監控指標\n結論\n大約在 2000 筆請求時左右將 KubeDNS 關成 0 顆，可以看到 NodeLocal DNSCache 有往上漲，也因為我們已經改用 Cloud DNS 來解析叢集內部 DNS，所以就算關閉 KubeDNS 也不會有影響，因為 NodeLocal DNSCache 會去問 Cloud DNS，並 Cache 在 NodeLocal DNSCache\n模擬 NodeLocal DNSCache Pod 異常 接下來我們測試最後一個情境，故意用壞 NodeLocal DNSCache 服務，觀察此模式對於 NodeLocal DNSCache 的依賴\n測試情境：\n先調整 COUNT 參數變成 50000\n先跑 15000 筆有 NodeLocal DNSCache，然後使用以下指令讓 NodeLocal DNSCache 無法使用\nkubectl patch daemonset node-local-dns -n kube-system --type='strategic' -p '{\"spec\":{\"template\":{\"spec\":{\"nodeSelector\":{\"this-label-does-not-exist-on-any-node\":\"true\"}}}}}' 等待約 30000 筆時，使用以下指令恢復 NodeLocal DNSCache\nkubectl patch daemonset node-local-dns -n kube-system --type='strategic' -p '{\"spec\":{\"template\":{\"spec\":{\"nodeSelector\":null}}}}' 測試結果：\n測試結果\n發現會卡住，直到手動下指令恢復 NodeLocal DNSCache\nPrometheus 監控指標\n結論\n發現當 NodeLocal DNSCache 掛了後，會直接卡住 (時間每五秒是因為打不到 timeout)，也不會切換到 KubeDNS 來做解析，所以最後測試沒有等到 30000 筆才下指令恢復 NodeLocal DNSCache，先提早下指令讓 NodeLocal DNSCache 正常，等到恢復，解析就正常了","外部-dns-ifconfigme#外部 DNS (ifconfig.me)":"NodeLocal DNSCache Prometheus 監控設定參數：zones=\".\"\n相關 Prometheus 監控指標： coredns_cache_requests_total{job=\"cloud-dns-nodelocaldns\", zones=\".\"} coredns_cache_entries{job=\"cloud-dns-nodelocaldns\", zones=\".\"} coredns_cache_hits_total{job=\"cloud-dns-nodelocaldns\", zones=\".\"} coredns_cache_misses_total{job=\"cloud-dns-nodelocaldns\", zones=\".\"} kubedns_dnsmasq_hits{job=\"kubedns-dns\"} kubedns_dnsmasq_misses{job=\"kubedns-dns\"} 測試腳本：\n#!/bin/bash get_taiwan_time() { # 獲取當前 UTC 時間的 Unix 時間戳 UTC_TIMESTAMP=$(date -u +%s) # 加上 8 小時 (台灣時間是 UTC+8) TAIPEI_TIMESTAMP=$((UTC_TIMESTAMP + 28800)) # 將時間戳轉換為格式化後的日期時間 date -d \"@$TAIPEI_TIMESTAMP\" \"+%Y-%m-%d %H:%M:%S\" } DOMAIN=\"ifconfig.me\" EXPECTED_IP=\"34.160.111.145\" START_TIME=$(get_taiwan_time) COUNT=10000 SUCCESS_COUNT=0 FAIL_COUNT=0 echo \"== NSLOOKUP TEST START: $START_TIME ==\" | tee -a nslookup_full.log for i in $(seq 1 \"$COUNT\"); do OUTPUT=$(nslookup \"$DOMAIN\" 2\u003e\u00261) ADDR_LINE=$(echo \"$OUTPUT\" | grep -E '^Address:') if [[ \"$ADDR_LINE\" == *\"$EXPECTED_IP\"* ]]; then SUCCESS_COUNT=$((SUCCESS_COUNT + 1)) else FAIL_COUNT=$((FAIL_COUNT + 1)) fi echo \"[$i] === $(get_taiwan_time) 成功: $SUCCESS_COUNT 失敗: $FAIL_COUNT\" done END_TIME=$(get_taiwan_time) echo \"== NSLOOKUP TEST END: $END_TIME ==\" | tee -a nslookup_full.log echo \"成功次數: $SUCCESS_COUNT\" | tee -a nslookup_full.log echo \"失敗次數: $FAIL_COUNT\" | tee -a nslookup_full.log 34.160.111.145 是 ifconfig.me 的 IP，只是為了確認 domain 是否能夠正常解析\n測試腳本 測試結果：\n測試結果\nPrometheus 監控指標\n結論\n可以觀察 NodeLocal DNSCache hit 跟 request 都有持續上升，但與 KubeDNS + NodeLocal DNSCache 模式 metrics 比較不一樣的地方是：Server 從 KubeDNS IP 變成 169.254.20.10，可以回去看： GKE KubeDNS + NodeLocal DNSCache 運作測試 #外部 DNS (ifconfig.me)\n模擬 KubeDNS Pod 異常 接下來會在測試中，將 KubeDNS 調整到 0 顆，再開回去，觀察此模式對於 KubeDNS 的依賴\n測試結果：\n測試結果\nPrometheus 監控指標\n結論\n大約在 2000 筆請求時左右將 KubeDNS 關成 0 顆，可以看到 NodeLocal DNSCache 有往上漲，也因為我們已經改用 Cloud DNS 來解析 internal-dns DNS，所以就算關閉 KubeDNS 也不會有影響，因為 NodeLocal DNSCache 會去問 Cloud DNS，並 Cache 在 NodeLocal DNSCache\n模擬 NodeLocal DNSCache Pod 異常 接下來我們測試最後一個情境，故意用壞 NodeLocal DNSCache 服務，觀察此模式對於 NodeLocal DNSCache 的依賴\n測試情境：\n先調整 COUNT 參數變成 50000\n先跑 15000 筆有 NodeLocal DNSCache，然後使用以下指令讓 NodeLocal DNSCache 無法使用\nkubectl patch daemonset node-local-dns -n kube-system --type='strategic' -p '{\"spec\":{\"template\":{\"spec\":{\"nodeSelector\":{\"this-label-does-not-exist-on-any-node\":\"true\"}}}}}' 等待約 30000 筆時，使用以下指令恢復 NodeLocal DNSCache\nkubectl patch daemonset node-local-dns -n kube-system --type='strategic' -p '{\"spec\":{\"template\":{\"spec\":{\"nodeSelector\":null}}}}' 測試結果：\n測試結果\n只要 NodeLocal DNSCache 故障解析就會卡住，等到恢復，就能繼續正常運作\n模擬 NodeLocal DNSCache 故障\nPrometheus 監控指標\n結論\n發現當 NodeLocal DNSCache 掛了後，會直接卡住 (時間每五秒是因為打不到 timeout)，也不會切換到 KubeDNS 來做解析，所以最後測試沒有等到 30000 筆才下指令恢復 NodeLocal DNSCache，先提早下指令讓 NodeLocal DNSCache 正常，等到恢復，解析就正常了","結論#結論":"可以發現，使用 Cloud DNS + NodeLocal DNSCache 的模式下，不管是哪一個解析，不會使用到 KubeDNS，所以可以把 KubeDNS 關成 0 顆，避免浪費資源。\n但是也可以發現當 NodeLocal DNSCache 只要發生問題，所有的解析都會有異常，所以不太建議使用 Cloud DNS + NodeLocal DNSCache 的模式\nCloud DNS + NodeLocal DNSCache 流程圖\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/nodelocal-dns-cache?hl=zh-tw#cloud_dns_dataplane\n由於官方流程圖有些細節沒有揭露的完成，我們有額外詢問 Google TAM，並畫出以下流程圖，Google TAM 也確認流程正確\n自己重新畫的 Cloud DNS + NodeLocal DNSCache 流程圖\n另外也可以從官方文件中得知 KubeDNS + NodeLocal DNSCache 的 /etc/resolv.conf 設定值，可以知道 Pod 最開始使用的 DNS 解析 Server 是誰。\n/etc/resolv.conf 設定值 服務探索和 DNS /etc/resolv.conf\n實際查看 /etc/resolv.conf"},"title":"GKE DNS 使用 Cloud DNS + NodeLocal DNSCache 運作測試"},"/blog/gcp/gke-cloud-dns/":{"data":{"":"最近在評估要將公司內的 EndPoint 都改成 Cloud DNS 的 Private Zone (打造內部的 internal dns 服務機制)，到時候 DNS 解析的請求會比以往還要多，所以需要先測試評估 GKE 內的 DNS 解析方案，避免再次發生 Pod 出現 cURL error 6: Could not resolve host，此篇文章測試的是： Cloud DNS 的運作。\n首先，先建立一個 dns-test pod 程式連結 以及 nginx 的 pod + svc 程式連結，會分別測試\n叢集內部 cluster.local (nginx-svc.default.svc.cluster.local)\ninternal-dns 使用 cloud dns private (aaa.test-audit.com)\n外部 dns (ifconfig.me)\n並使用 nslookup 腳本進行確認回傳 DNS 解析，每一次測試都會重新建立 KubeDNS Pod\n相關程式以及 Prometheus、Grafana 的設定可以參考：https://github.com/880831ian/gke-dns\n在建立完 Cluster 後，可以先觀察在 Cloud DNS 是否新增的 Private Zone\n由於裡面有很多的 Record，我們先搜尋下面會用到的 nginx-svc.default.svc.cluster.local 來當作範例","internal-dns-cloud-dns-private#Internal DNS (Cloud DNS Private)":"先建立一個 cloud dns private，以下範例是 aaa.test-audit.com \u003e 10.1.1.4，並將此 use VPC 與 GKE 的 VPC 打通\n設定 cloud dns private\n相關 Prometheus 監控指標： kubedns_dnsmasq_hits{job=\"kubedns-dns\"} kubedns_dnsmasq_misses{job=\"kubedns-dns\"} 測試腳本：\n#!/bin/bash get_taiwan_time() { # 獲取當前 UTC 時間的 Unix 時間戳 UTC_TIMESTAMP=$(date -u +%s) # 加上 8 小時 (台灣時間是 UTC+8) TAIPEI_TIMESTAMP=$((UTC_TIMESTAMP + 28800)) # 將時間戳轉換為格式化後的日期時間 date -d \"@$TAIPEI_TIMESTAMP\" \"+%Y-%m-%d %H:%M:%S\" } DOMAIN=\"aaa.test-audit.com\" EXPECTED_IP=\"10.1.1.4\" START_TIME=$(get_taiwan_time) COUNT=10000 SUCCESS_COUNT=0 FAIL_COUNT=0 echo \"== NSLOOKUP TEST START: $START_TIME ==\" | tee -a nslookup_full.log for i in $(seq 1 \"$COUNT\"); do OUTPUT=$(nslookup \"$DOMAIN\" 2\u003e\u00261) ADDR_LINE=$(echo \"$OUTPUT\" | grep -E '^Address:') if [[ \"$ADDR_LINE\" == *\"$EXPECTED_IP\"* ]]; then SUCCESS_COUNT=$((SUCCESS_COUNT + 1)) else FAIL_COUNT=$((FAIL_COUNT + 1)) fi echo \"[$i] === $(get_taiwan_time) 成功: $SUCCESS_COUNT 失敗: $FAIL_COUNT\" done END_TIME=$(get_taiwan_time) echo \"== NSLOOKUP TEST END: $END_TIME ==\" | tee -a nslookup_full.log echo \"成功次數: $SUCCESS_COUNT\" | tee -a nslookup_full.log echo \"失敗次數: $FAIL_COUNT\" | tee -a nslookup_full.log 10.1.1.4 是隨機亂取的 IP，只是為了確認 domain 是否能夠正常解析\n測試腳本 測試結果：\n測試結果\nPrometheus 監控指標\n結論\n因為改用 Cloud DNS，所以 KubeDNS 的 hit 沒有往上衝高\n模擬 KubeDNS Pod 異常 接下來會在測試中，將 KubeDNS 調整到 0 顆，再開回去，觀察此模式對於 KubeDNS 的依賴\n測試結果：\n測試結果\nPrometheus 監控指標\n結論\n大約在 2000 筆請求時左右將 KubeDNS 關成 0 顆，可以發現解析還是正常，代表使用 Cloud DNS 後，KubeDNS 對於 internal-dns 解析不會有影響","k6-測試#k6 測試":"額外在另一個 cluster 建立 nginx deployment 開 5 個 pod 以及 svc 改成 lb (L4)，然後在 cloud dns 的 test-audit-com 設定 nginx-lb-internal.test-audit.com 解析到內網的 svc (10.156.16.16)\n使用 k6 測試 kube dns 模式下 IP 跟 DNS 的差異\n相關程式可以參考：https://github.com/880831ian/gke-dns\n這邊測試的 Node 是用 e2-medium 而非 c3d-standard-4\n第一次測試 IP (avg=179.06ms / 3479 RPS)、DNS (avg=191.56ms / 3348 RPS)\nIP 第一次測試\nDNS 第一次測試\n第二次測試 IP (avg=193.67ms / 3322 RPS)、DNS (avg=302.73ms / 2430 RPS)\nIP 第二次測試\nDNS 第二次測試\n第三次測試 IP (avg=174.94ms / 3529 RPS)、DNS (avg=253.71ms / 2761 RPS)\nIP 第三次測試\nDNS 第三次測試\n第四次測試 IP (avg=227.56ms / 2971 RPS)、DNS (avg=316.07ms / 2361 RPS)\nIP 第四次測試\nDNS 第四次測試\n結論\n有發現之前在 KubeDNS 跟 KubeDNS + NodeLocal DNSCache 會有 DNS RPS 大於 IP 的情況，但使用 Cloud DNS 後則沒有發生，推測是因為。Cloud DNS 在 Cluster 外部，所以會比較慢","參考資料#參考資料":"使用 GKE 適用的 Cloud DNS：https://cloud.google.com/kubernetes-engine/docs/how-to/cloud-dns?hl=zh-tw","叢集內部-clusterlocal#叢集內部 cluster.local":"一樣會觀察 KubeDNS Pod Metrics，因為就算開啟 Cloud DNS 後，KubeDNS 還是會留著\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/cloud-dns?hl=zh-tw#architecture\n相關 Prometheus 監控指標： kubedns_dnsmasq_hits{job=\"kubedns-dns\"} kubedns_dnsmasq_misses{job=\"kubedns-dns\"} 測試腳本：\n#!/bin/bash get_taiwan_time() { # 獲取當前 UTC 時間的 Unix 時間戳 UTC_TIMESTAMP=$(date -u +%s) # 加上 8 小時 (台灣時間是 UTC+8) TAIPEI_TIMESTAMP=$((UTC_TIMESTAMP + 28800)) # 將時間戳轉換為格式化後的日期時間 date -d \"@$TAIPEI_TIMESTAMP\" \"+%Y-%m-%d %H:%M:%S\" } DOMAIN=\"nginx-svc.default.svc.cluster.local\" EXPECTED_IP=\"10.36.16.243\" START_TIME=$(get_taiwan_time) COUNT=10000 SUCCESS_COUNT=0 FAIL_COUNT=0 echo \"== NSLOOKUP TEST START: $START_TIME ==\" | tee -a nslookup_full.log for i in $(seq 1 \"$COUNT\"); do OUTPUT=$(nslookup \"$DOMAIN\" 2\u003e\u00261) ADDR_LINE=$(echo \"$OUTPUT\" | grep -E '^Address:') if [[ \"$ADDR_LINE\" == *\"$EXPECTED_IP\"* ]]; then SUCCESS_COUNT=$((SUCCESS_COUNT + 1)) else FAIL_COUNT=$((FAIL_COUNT + 1)) fi echo \"[$i] === $(get_taiwan_time) 成功: $SUCCESS_COUNT 失敗: $FAIL_COUNT\" done END_TIME=$(get_taiwan_time) echo \"== NSLOOKUP TEST END: $END_TIME ==\" | tee -a nslookup_full.log echo \"成功次數: $SUCCESS_COUNT\" | tee -a nslookup_full.log echo \"失敗次數: $FAIL_COUNT\" | tee -a nslookup_full.log 10.36.16.243 是 nginx-svc Cluster IP\n需要先確認 nginx-svc 的 IP 是多少，然後修改腳本中的 EXPECTED_IP 變數。\n測試腳本 測試結果：\n測試結果\nPrometheus 監控指標\n結論\n因為改用 Cloud DNS，所以 KubeDNS 的 hit 沒有往上衝高\n模擬 KubeDNS Pod 異常 接下來會在測試中，將 KubeDNS 調整到 0 顆，再開回去，觀察此模式對於 KubeDNS 的依賴\n測試結果：\n測試結果\nPrometheus 監控指標\n結論\n大約在 2000 筆請求時左右將 KubeDNS 關成 0 顆，可以發現解析還是正常，代表使用 Cloud DNS 後，KubeDNS 對於叢集內部解析不會有影響","外部-dns-ifconfigme#外部 DNS (ifconfig.me)":"\n相關 Prometheus 監控指標： kubedns_dnsmasq_hits{job=\"kubedns-dns\"} kubedns_dnsmasq_misses{job=\"kubedns-dns\"} 測試腳本：\n#!/bin/bash get_taiwan_time() { # 獲取當前 UTC 時間的 Unix 時間戳 UTC_TIMESTAMP=$(date -u +%s) # 加上 8 小時 (台灣時間是 UTC+8) TAIPEI_TIMESTAMP=$((UTC_TIMESTAMP + 28800)) # 將時間戳轉換為格式化後的日期時間 date -d \"@$TAIPEI_TIMESTAMP\" \"+%Y-%m-%d %H:%M:%S\" } DOMAIN=\"ifconfig.me\" EXPECTED_IP=\"34.160.111.145\" START_TIME=$(get_taiwan_time) COUNT=10000 SUCCESS_COUNT=0 FAIL_COUNT=0 echo \"== NSLOOKUP TEST START: $START_TIME ==\" | tee -a nslookup_full.log for i in $(seq 1 \"$COUNT\"); do OUTPUT=$(nslookup \"$DOMAIN\" 2\u003e\u00261) ADDR_LINE=$(echo \"$OUTPUT\" | grep -E '^Address:') if [[ \"$ADDR_LINE\" == *\"$EXPECTED_IP\"* ]]; then SUCCESS_COUNT=$((SUCCESS_COUNT + 1)) else FAIL_COUNT=$((FAIL_COUNT + 1)) fi echo \"[$i] === $(get_taiwan_time) 成功: $SUCCESS_COUNT 失敗: $FAIL_COUNT\" done END_TIME=$(get_taiwan_time) echo \"== NSLOOKUP TEST END: $END_TIME ==\" | tee -a nslookup_full.log echo \"成功次數: $SUCCESS_COUNT\" | tee -a nslookup_full.log echo \"失敗次數: $FAIL_COUNT\" | tee -a nslookup_full.log 34.160.111.145 是 ifconfig.me 的 IP，只是為了確認 domain 是否能夠正常解析\n測試腳本 測試結果：\n測試結果\nPrometheus 監控指標\n結論\n因為改用 Cloud DNS，所以 KubeDNS 的 hit 沒有往上衝高\n模擬 KubeDNS Pod 異常 接下來會在測試中，將 KubeDNS 調整到 0 顆，再開回去，觀察此模式對於 KubeDNS 的依賴\n測試結果：\n測試結果\nPrometheus 監控指標\n結論\n大約在 2000 筆請求時左右將 KubeDNS 關成 0 顆，可以發現解析還是正常，代表使用 Cloud DNS 後，KubeDNS 對於外部 DNS解析不會有影響","結論#結論":"可以發現，使用 Cloud DNS 的模式下，所有的 DNS 解析都會依靠 Cloud DNS，不會使用到 KubeDNS，所以可以把 KubeDNS 關成 0 顆，避免浪費資源。\nGKE Cloud DNS\n由於官方流程圖有些細節沒有揭露的完成，我們有額外詢問 Google TAM，並畫出以下流程圖，Google TAM 也確認流程正確\n自己重新畫的 GKE Cloud DNS\n以下是我們與 Google 討論的內容：\n我們：\n圖中粉紅框所選取的兩個 Cloud DNS\u003cCloud DNS \u0026 Cloud DNS (Data Plane)\u003e 在實際的架構中，是兩個獨立的物件嗎？或者其實是同一個物件?\n承(1)，以下項目所述的理解是否正確？若有差異的部分請協助說明：目前我們的認知是，在 Internal DNS 上會有三個 Cloud DNS：\nGKE 的設定改用 Cloud DNS 的 provider，此時系統會協助我們建立一個 Cloud DNS (Data Plane)。 我們會自己建立一個 Cloud DNS(Private) For 我們內部自己要用的 Internal DNS。 進行外部 DNS 解析時，目前的理解是會從 metadata server 對 Cloud DNS 轉送解析需求，是否屬實？ 完成解析後，Cache 的部分是否在 metadata server 裡面進行？\nGoogle：\n處理 private zone 和公有網域名稱的是不同物件，但都在 GCE DNS 服務裡\n主要是建立 cluster-scoped private zone，進到 metadata server 後的行為與原本相同、2 跟 3 正確 是，除了metadata server 外，同時也會在 GCE DNS 服務裡快取\n另外也可以從官方文件中得知 KubeDNS 的 /etc/resolv.conf 設定值，可以知道 Pod 最開始使用的 DNS 解析 Server 是誰。\n/etc/resolv.conf 設定值 服務探索和 DNS /etc/resolv.conf\n實際查看 /etc/resolv.conf"},"title":"GKE DNS 使用 Cloud DNS 運作測試"},"/blog/gcp/gke-cronjob-not-working/":{"data":{"":"前陣子公司建立在 Google Kubernetes Engine 叢集上的 CronJob 服務會有短暫時間沒有執行 Job。先前情提要一下，此 CronJob 的設定是每分鐘都會執行 (圖一)，所以理當來說 Log 應該要可以看到每分鐘都有此 CronJob 的紀錄，但有時候會發生 CronJob 短暫時間都沒有執行的狀況，找了一陣子都沒有找到原因，最後開支援單請 Google 那邊協助查看，終於找到原因拉 😍。那就跟我一起看一下發生的過程，以及 Google 幫我們找到的原因，以及要如何解決等等～\n(圖一) CronJob schedule 時間為每分鐘執行","參考資料#參考資料":"[1] Standard cluster upgrades：https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-upgrades\n[2] maintenance-windows-and-exclusions：https://cloud.google.com/kubernetes-engine/docs/concepts/maintenance-windows-and-exclusions","問題發生以及問題原因#問題發生以及問題原因":"我們使用 Google Cloud Platform 裡面的記錄功能，可以看到 (圖二)，在每分鐘執行的 Log 中，有短暫時間沒有執行 Job，但這個時間除了 CronJob 以外，其他的服務都是好的。\n(圖二) Google Cloud Platform 記錄有短暫沒有執行\n找了一陣子都沒有找到原因，於是我們開支援單請 Google 那邊協助查看，Google 那邊找了一陣子後終於找到原因拉！！！我們一起來看看吧 (圖三)\n(圖三) Google 支援單回覆\n就如同 Google 所說，使用提供的指令參數來查詢，叢集在該時段有發生 master control plane 的升級，如 (圖四)，再加上我們建的這個 cluster 是使用 zonal cluster (圖五)，所以叢集只有一個 control plane，當 control plane 在更新時，會無法部署新的 workload，導致該 CronJob 沒有執行 Job。參考資料 [1]\n(圖四) 發生 master control plane 的升級\n(圖五) 有問題的叢集位置類型\nGoogle 的建議是可以考慮使用另一個 regional cluster，讓 master node 在更新時不會只在單一地區，或是一樣使用舊的 zonal cluster，透過設定 Maintenance window 或者 Maintenance exclusions 來降低服務受到 workload 的影響。參考資料 [2]\n就算把 node_pool 裡面的自動升級給停掉，也沒有辦法解決此問題！因為此 master control plane (也就是 master node) 的升級，不是 worker node 的 node pool 升級，是由 GKE 負責維護的，所以他們會定期升級 control plane，也沒辦法停止此類的升級。\n若已經建立好 zonal cluster 後，想要改成 regional cluster ，是沒有辦法使用修改的方式，一定只能重建 cluster，所以大家在建立時要注意～","解決問題#解決問題":"最後我們選擇將叢集給整個重建，來確保 CronJob 不會有沒有執行到的狀況發生，重建叢集跟搬服務的過程很辛苦的 😰 希望大家不要發生 QQ，最後我們來看一下重建完後，在 master control plane 更新的時候，還會不會有 CronJob 沒有執行的情況發生。\n新叢集使用 regional cluster 來建立，在 2/1 也有 master control plane 的升級。 (圖六)\n(圖六) 發生 master control plane 的升級\n查看 CronJob 執行的紀錄可以發現並沒有 Job 沒有執行的情況發生。 (圖七)\n(圖七) 叢集位置類型\n(圖八) 新叢集位置類型"},"title":"Google Kubernetes Engine CronJob 會有短暫時間沒有執行 Job"},"/blog/gcp/gke-dns/":{"data":{"":"一樣先前情提要：\n最近在評估要將公司內的 EndPoint 都改成 Cloud DNS 的 Private Zone (打造內部的 internal dns 服務機制)，到時候 DNS 解析的請求會比以往還要多，所以需要先測試評估 GKE 內的 DNS 解析方案，避免再次發生 Pod 出現 cURL error 6: Could not resolve host，此篇文章再來回顧以及整理，可以依照情境測試後的結論以及注意事項，來選擇適合 GKE DNS 組合：\nGKE DNS 使用 KubeDNS 運作測試\nGKE DNS 使用 KubeDNS + NodeLocal DNSCache 運作測試\nGKE DNS 使用 Cloud DNS 運作測試\nGKE DNS 使用 Cloud DNS + NodeLocal DNSCache 運作測試","注意事項#注意事項":" 測試 KubeDNS、KubeDNS + NodeLocal DNSCache、Cloud DNS、Cloud DNS + NodeLocal DNSCache 使用 K6 簡單進行壓測，會發現除了 Cloud DNS 以外，其他的 DNS 解析 RPS 不一定會慢於直接打 IP 如果有需求需要管理 KubeDNS Deployment 服務，可以參考：自訂 kube-dns 部署作業，但會有相對的維運成本 (因為官方預設的沒辦法調整 Deployment，會因為 addonmanager.kubernetes.io/mode=Reconcile 被 GKE 還原) 所有只要經過 Cloud DNS 以及 Metadata Server (在 Node 上) 都會有 Cache 機制 將 GKE 叢集的 DNS 服務從 KubeDNS + NodeLocal DNSCache 換成 Cloud DNS + NodeLocal DNSCache，且此過程會導致舊節點無法解析網域，造成服務中斷，因此建議在維護期間執行，可以參考：從 kube-dns 遷移至 Cloud DNS 後，啟用 NodeLocal DNSCache 時 DNS 解析失敗 在升級 GKE Cluster 時，因為會更新 KubeDNS，所以有可能會發生 DNS 解析失敗的情況，不管是否有開啟 NodeLocal DNSCache，都也可能發生 (通常只有一小部分節點會遇到 10%)，可以參考：kube-dns 的效能限制 將 GKE 叢集的 DNS 服務從 KubeDNS 換成 Cloud DNS VPC 範圍，必須在重建節點後才會生效，且此過程會導致舊節點無法解析網域，造成服務中斷，因此建議在維護期間執行，可以參考：在現有叢集啟用 VPC 範圍 DNS 如果已經建立 GKE Cloud DNS VPC 範圍的 Cluster，是沒辦法切換回 KubeDNS，只能重建 Cluster，可以參考：停用 Cloud DNS for GKE ","測試對於情境的依賴性-不管-liveness-或是有-cache以結果來看#測試對於情境的依賴性 (不管 Liveness 或是有 Cache，以結果來看)":"✅：代表服務異常不會對解析造成影響\n❌：代表服務異常會對解析造成影響\nGKE DNS 情境 KubeDNS KubeDNS + NodeLocal DNSCache Cloud DNS Cloud DNS + NodeLocal DNSCache 叢集內部 cluster.local\nKubeDNS Pod 異常 ❌ ❌ ✅ ✅ 叢集內部 cluster.local\nNodeLocal DNSCache Pod 異常 - ✅ - ❌ Internal DNS (Cloud DNS Private)\nKubeDNS Pod 異常 ❌ ✅ ✅ ✅ Internal DNS (Cloud DNS Private)\nNodeLocal DNSCache Pod 異常 - ✅ - ❌ 外部 DNS (ifconfig.me)\nKubeDNS Pod 異常 ❌ ✅ ✅ ✅ 外部 DNS (ifconfig.me)\nNodeLocal DNSCache Pod 異常 - ✅ - ❌ ","測試情境的依賴性#測試情境的依賴性":" GKE DNS 情境 KubeDNS KubeDNS + NodeLocal DNSCache Cloud DNS Cloud DNS + NodeLocal DNSCache 叢集內部 cluster.local\nKubeDNS Pod 異常 KubeDNS 切成 0 顆後，不會馬上無法解析，因為 KubeDNS 有 Liveness，不會馬上關掉，等到 KubeDNS 關成 0 後，就無法解析 KubeDNS 切成 0 顆後，不會馬上無法解析，因為有 NodeLocal DNSCache，透過 Cache 還可以回覆 DNS 請求，但等到 TTL 時間到，就會出現無法解析 KubeDNS 切成 0 顆後，解析依舊正常，因為 provider 已經改用 Cloud DNS，對解析不會有影響 KubeDNS 切成 0 顆後，解析依舊正常，因為 provider 已經改用 Cloud DNS，對解析不會有影響 叢集內部 cluster.local\nNodeLocal DNSCache Pod 異常 - 當 NodeLocal DNSCache 切成 0 顆後，會短暫卡住，但他會自動切換到 KubeDNS 上繼續解析 - 當 NodeLocal DNSCache 切成 0 顆後，會直接卡住，會出現無法解析，也不會自動切換成 KubeDNS Internal DNS (Cloud DNS Private)\nKubeDNS Pod 異常 KubeDNS 切成 0 顆後，不會馬上無法解析，因為 KubeDNS 有 Liveness，不會馬上關掉，等到 KubeDNS 關成 0 後，就無法解析 KubeDNS 切成 0 顆後，解析依然正常，因為 Cloud DNS Private 他不是 cluster.local，所以不會透過 KubeDNS 來做解析 KubeDNS 切成 0 顆後，解析依舊正常，因為 provider 已經改用 Cloud DNS，對解析不會有影響 KubeDNS 切成 0 顆後，解析依舊正常，因為 provider 已經改用 Cloud DNS，對解析不會有影響 Internal DNS (Cloud DNS Private)\nNodeLocal DNSCache Pod 異常 - 當 NodeLocal DNSCache 切成 0 顆後，會短暫卡住，但他會自動切換到 KubeDNS 上繼續解析 - 當 NodeLocal DNSCache 切成 0 顆後，會直接卡住，會出現無法解析，也不會自動切換成 KubeDNS 外部 DNS (ifconfig.me)\nKubeDNS Pod 異常 KubeDNS 切成 0 顆後，不會馬上無法解析，因為 KubeDNS 有 Liveness，不會馬上關掉，等到 KubeDNS 關成 0 後，就無法解析 KubeDNS 切成 0 顆後，解析依然正常，因為外部 DNS 他不是 cluster.local，所以不會透過 KubeDNS 來做解析 KubeDNS 切成 0 顆後，解析依舊正常，因為 provider 已經改用 Cloud DNS，對解析不會有影響 KubeDNS 切成 0 顆後，解析依舊正常，因為 provider 已經改用 Cloud DNS，對解析不會有影響 外部 DNS (ifconfig.me)\nNodeLocal DNSCache Pod 異常 - 當 NodeLocal DNSCache 切成 0 顆後，會短暫卡住，但他會自動切換到 KubeDNS 上繼續解析 - 當 NodeLocal DNSCache 切成 0 顆後，會直接卡住，會出現無法解析，也不會自動切換成 KubeDNS ","結論#結論":"我們上面進行測試的內容，是為了選出 4 種在 GKE DNS 比較好的一個選型，最終考慮：\n服務的可控性 (能夠自己管理 KubeDNS 以及建立 NodeLocal DNSCache)\n服務的穩定性 (服務若異常可以快速恢復，且不需要依靠 GCP 協助)\n服務的監控性 (能夠監控 KubeDNS 以及 NodeLocal DNSCache)\n額外產生的費用 \u0026 人力成本等 (使用 Cloud DNS 會有額外成本)\n我們公司最終選型為：GKE 內使用 KubeDNS + NodeLocal DNSCache\n補充：\n在測試過程中發現，我們使用的是 nslookup 並搭配 busybox image 基底測試，中間有換成 alpine 並安裝 bind-tools 發現在 KubeDNS + NodeLocal DNSCache 將 NodeLocal DNSCache 用壞時，會無法正常解析，或是解析速度會下降很多，我們推測是跟底層的 C 語言 lib 有關。\n下面提供幾個最終可能會導致 DNS 解析有差異的因素，會需要再去考慮以及測試：\n系統底層 C 語言 lib 選用：glibc / msul\n程式 lib 選用 (例如 Golang)：net.http / goresty\n連線方式：短 / 長連線\n晶片架構：amd / arm\nGolang 是否 Build 開啟 CGO_ENABLED 等等\n我們上述的選型主要考慮的不是 KubeDNS 或是 NodeLocal DNSCache 故障等異常情形 (這個異常是可以透過監控以及好的維運設定改善的)，而是上面那四點綜合評估下來的結論","結論與優缺點#結論與優缺點":" 模式 結論 優點 缺點 /etc/resolv.conf KubeDNS 對 KubeDNS 的依賴性極高。只要壞掉，所有解析都會失效 GKE 預設設定，能快速使用 容易出現單點故障，影響所有 DNS 解析。\nGKE KubeDNS 可以將 kube-dns-autoscaler configmap 納管進版控來調整 Pod 數量 (但沒辦法動態調整)\nDeployment 沒辦法調整，會被還原，因為有：addonmanager.kubernetes.io/mode=Reconcile 設定，由 GKE 維運 (也可以自建 DNS) KubeDNS 的 IP KubeDNS + NodeLocal DNSCache 大幅降低對 KubeDNS 的依賴。快取可以應對 KubeDNS 短暫異常 提高 DNS 解析的效能和可靠性 需要維護 KubeDNS 的 kube-dns-autoscaler configmap 來調整 Pod 數量 KubeDNS 的 IP Cloud DNS 所有的 DNS 解析都依賴 Cloud DNS。可以將 KubeDNS Pod 縮減為 0 高可用性。DNS 責任交由託管服務處理，節省維護成本 會有額外的費用，以及若 GCP DNS 出現異常會導致 GKE 服務無法解析問題 169.254.169.254 Cloud DNS + NodeLocal DNSCache 當 NodeLocal DNSCache 只要發生問題，所有的解析都會有異常。\n所以不太建議使用 Cloud DNS + NodeLocal DNSCache 的模式 提供更快的 DNS 解析速度，減少延遲 當 NodeLocal DNSCache 出現問題時，所有解析都會失效 169.254.20.10 "},"title":"GKE DNS 相關注意事項及結論"},"/blog/gcp/gke-kubedns-nodelocaldnscache/":{"data":{"":"最近在評估要將公司內的 EndPoint 都改成 Cloud DNS 的 Private Zone (打造內部的 internal dns 服務機制)，到時候 DNS 解析的請求會比以往還要多，所以需要先測試評估 GKE 內的 DNS 解析方案，避免再次發生 Pod 出現 cURL error 6: Could not resolve host，此篇文章測試的是： KubeDNS + NodeLocal DNSCache 的運作。\n首先，先建立一個 dns-test pod 程式連結 以及 nginx 的 pod + svc 程式連結，會分別測試\n叢集內部 cluster.local (nginx-svc.default.svc.cluster.local)\ninternal-dns 使用 cloud dns private (aaa.test-audit.com)\n外部 dns (ifconfig.me)\n並使用 nslookup 腳本進行確認回傳 DNS 解析，每一次測試都會重新建立 KubeDNS、NodeLocal DNSCache Pod\n相關程式以及 Prometheus、Grafana 的設定可以參考：https://github.com/880831ian/gke-dns","internal-dns-cloud-dns-private#Internal DNS (Cloud DNS Private)":"先建立一個 cloud dns private，以下範例是 aaa.test-audit.com \u003e 10.1.1.4，並將此 use VPC 與 GKE 的 VPC 打通\n設定 cloud dns private\nNodeLocal DNSCache Prometheus 監控設定參數：zones=\".\"\n相關 Prometheus 監控指標： coredns_cache_requests_total{job=\"kubedns-nodelocaldns\", zones=\".\"} coredns_cache_entries{job=\"kubedns-nodelocaldns\", zones=\".\"} coredns_cache_hits_total{job=\"kubedns-nodelocaldns\", zones=\".\"} coredns_cache_misses_total{job=\"kubedns-nodelocaldns\", zones=\".\"} kubedns_dnsmasq_hits{job=\"kubedns-dns\"} kubedns_dnsmasq_misses{job=\"kubedns-dns\"} 測試腳本：\n#!/bin/bash get_taiwan_time() { # 獲取當前 UTC 時間的 Unix 時間戳 UTC_TIMESTAMP=$(date -u +%s) # 加上 8 小時 (台灣時間是 UTC+8) TAIPEI_TIMESTAMP=$((UTC_TIMESTAMP + 28800)) # 將時間戳轉換為格式化後的日期時間 date -d \"@$TAIPEI_TIMESTAMP\" \"+%Y-%m-%d %H:%M:%S\" } DOMAIN=\"aaa.test-audit.com\" EXPECTED_IP=\"10.1.1.4\" START_TIME=$(get_taiwan_time) COUNT=10000 SUCCESS_COUNT=0 FAIL_COUNT=0 echo \"== NSLOOKUP TEST START: $START_TIME ==\" | tee -a nslookup_full.log for i in $(seq 1 \"$COUNT\"); do OUTPUT=$(nslookup \"$DOMAIN\" 2\u003e\u00261) ADDR_LINE=$(echo \"$OUTPUT\" | grep -E '^Address:') if [[ \"$ADDR_LINE\" == *\"$EXPECTED_IP\"* ]]; then SUCCESS_COUNT=$((SUCCESS_COUNT + 1)) else FAIL_COUNT=$((FAIL_COUNT + 1)) fi echo \"[$i] === $(get_taiwan_time) 成功: $SUCCESS_COUNT 失敗: $FAIL_COUNT\" done END_TIME=$(get_taiwan_time) echo \"== NSLOOKUP TEST END: $END_TIME ==\" | tee -a nslookup_full.log echo \"成功次數: $SUCCESS_COUNT\" | tee -a nslookup_full.log echo \"失敗次數: $FAIL_COUNT\" | tee -a nslookup_full.log 10.1.1.4 是隨機亂取的 IP，只是為了確認 domain 是否能夠正常解析\n測試腳本 測試結果：\n測試結果\nPrometheus 監控指標\n結論\n可以觀察 NodeLocal DNSCache 內的指標 hit 跟 request 都有持續上升\n模擬 KubeDNS Pod 異常 接下來會在測試中，將 KubeDNS 調整到 0 顆，再開回去，觀察此模式對於 KubeDNS 的依賴\n測試結果：\n測試結果\n因為 KubeDNS 關成 0 顆，不會馬上關掉\nPrometheus 監控指標\n結論\n總共打了兩輪的 10000 筆，在第一輪大約在 2000 筆請求時左右將 KubeDNS 關成 0 顆，因為前面 KubeDNS 切成 0，Pod 不會馬上關掉，避免有測試誤差，所以在打第二輪 10000 筆，但從結果發現，所有的 DNS 請求都是走 NodeLocal DNSCache，因為 cloud dns private 不是 .cluster.local，所以就算沒有 KubeDNS 也能正常運作\n模擬 NodeLocal DNSCache Pod 異常 接下來我們測試最後一個情境，故意用壞 NodeLocal DNSCache 服務，觀察此模式對於 NodeLocal DNSCache 的依賴\n測試情境：\n先調整 COUNT 參數變成 50000\n先跑 15000 筆有 NodeLocal DNSCache，然後使用以下指令讓 NodeLocal DNSCache 無法使用\nkubectl patch daemonset node-local-dns -n kube-system --type='strategic' -p '{\"spec\":{\"template\":{\"spec\":{\"nodeSelector\":{\"this-label-does-not-exist-on-any-node\":\"true\"}}}}}' 等待約 30000 筆時，使用以下指令恢復 NodeLocal DNSCache\nkubectl patch daemonset node-local-dns -n kube-system --type='strategic' -p '{\"spec\":{\"template\":{\"spec\":{\"nodeSelector\":null}}}}' 測試結果：\n測試結果\n分別在 18:26:11 跟 18:28:03 下指令調整\n模擬 NodeLocal DNSCache 故障\nPrometheus 監控指標\n結論\n發現當 NodeLocal DNSCache 掛了後，會短暫卡住，但會直接切換到 KubeDNS 上繼續進行解析，因此以結果論，如果 NodeLocal DNSCache 有短暫異常，不會出現無法解析的問題 從 Prometheus 可以發現，前面 NodeLocal DNSCache 正常運作，當我們在 18:26:11 調整後，變成 KubeDNS 起來開始處理解析，在 18:28:03 切換讓 NodeLocal DNSCache 恢復，後面又會變成由 NodeLocal DNSCache 來處理解析 紫色是 NodeLocal DNSCache，黃色是 KubeDNS","k6-測試#k6 測試":"額外在另一個 cluster 建立 nginx deployment 開 5 個 pod 以及 svc 改成 lb (L4)，然後在 cloud dns 的 test-audit-com 設定 nginx-lb-internal.test-audit.com 解析到內網的 svc (10.156.17.230)\n使用 k6 測試 KubeDNS + NodeLocal DNSCache 模式下 IP 跟 DNS 的差異\n相關程式可以參考：https://github.com/880831ian/gke-dns\n這邊測試的 Node 是用 e2-medium 而非 c3d-standard-4\n第一次測試 IP (avg=181.7ms / 3511 RPS)、DNS (avg=335.43ms / 2278 RPS)\nIP 第一次測試\nDNS 第一次測試\n第二次測試 IP (avg=336.98ms / 2272 RPS)、DNS (avg=503.79ms / 1640 RPS)\nIP 第二次測試\nDNS 第二次測試\n第三次測試 IP (avg=128.9ms / 4310 RPS)、DNS (avg=129.23ms / 4310 RPS)\nIP 第三次測試\nDNS 第三次測試\n第四次測試 IP (avg=149.5ms / 3965 RPS)、DNS (avg=133.73ms / 4230 RPS)\nIP 第四次測試\nDNS 第四次測試\n結論\n理論上 ip 應該會比 dns 還要快，但測試 4 次發現其實不一定","參考資料#參考資料":"設定 NodeLocal DNSCache：https://cloud.google.com/kubernetes-engine/docs/how-to/nodelocal-dns-cache?hl=zh-tw","叢集內部-clusterlocal#叢集內部 cluster.local":"NodeLocal DNSCache Prometheus 監控設定參數：zones=\"cluster.local.\"\n相關 Prometheus 監控指標： coredns_cache_requests_total{job=\"kubedns-nodelocaldns\", zones=\"cluster.local.\"} coredns_cache_entries{job=\"kubedns-nodelocaldns\", zones=\"cluster.local.\"} coredns_cache_hits_total{job=\"kubedns-nodelocaldns\", zones=\"cluster.local.\"} coredns_cache_misses_total{job=\"kubedns-nodelocaldns\", zones=\"cluster.local.\"} kubedns_dnsmasq_hits{job=\"kubedns-dns\"} kubedns_dnsmasq_misses{job=\"kubedns-dns\"} 測試腳本：\n#!/bin/bash get_taiwan_time() { # 獲取當前 UTC 時間的 Unix 時間戳 UTC_TIMESTAMP=$(date -u +%s) # 加上 8 小時 (台灣時間是 UTC+8) TAIPEI_TIMESTAMP=$((UTC_TIMESTAMP + 28800)) # 將時間戳轉換為格式化後的日期時間 date -d \"@$TAIPEI_TIMESTAMP\" \"+%Y-%m-%d %H:%M:%S\" } DOMAIN=\"nginx-svc.default.svc.cluster.local\" EXPECTED_IP=\"10.36.16.35\" START_TIME=$(get_taiwan_time) COUNT=10000 SUCCESS_COUNT=0 FAIL_COUNT=0 echo \"== NSLOOKUP TEST START: $START_TIME ==\" | tee -a nslookup_full.log for i in $(seq 1 \"$COUNT\"); do OUTPUT=$(nslookup \"$DOMAIN\" 2\u003e\u00261) ADDR_LINE=$(echo \"$OUTPUT\" | grep -E '^Address:') if [[ \"$ADDR_LINE\" == *\"$EXPECTED_IP\"* ]]; then SUCCESS_COUNT=$((SUCCESS_COUNT + 1)) else FAIL_COUNT=$((FAIL_COUNT + 1)) fi echo \"[$i] === $(get_taiwan_time) 成功: $SUCCESS_COUNT 失敗: $FAIL_COUNT\" done END_TIME=$(get_taiwan_time) echo \"== NSLOOKUP TEST END: $END_TIME ==\" | tee -a nslookup_full.log echo \"成功次數: $SUCCESS_COUNT\" | tee -a nslookup_full.log echo \"失敗次數: $FAIL_COUNT\" | tee -a nslookup_full.log 10.36.16.35 是 nginx-svc Cluster IP\n需要先確認 nginx-svc 的 IP 是多少，然後修改腳本中的 EXPECTED_IP 變數。\n測試腳本 測試結果：\n測試結果\nPrometheus 監控指標\n結論\n可以觀察 NodeLocal DNSCache 內的指標 hit 跟 request 都有持續上升\n模擬 KubeDNS Pod 異常 接下來會在測試中，將 KubeDNS 調整到 0 顆，再開回去，觀察此模式對於 KubeDNS 的依賴\n測試結果：\n測試結果\n第二輪才出現錯誤\n因為 KubeDNS 關成 0 顆，不會馬上關掉\n將 KubeDNS 調整回來，就正常解析\nPrometheus 監控指標\n結論\n總共打了兩輪的 10000 筆，在第一輪大約在 2000 筆請求時左右將 KubeDNS 關成 0 顆，但到了第二輪的 2838 筆的時候才開始出現解析失敗，因為前面 KubeDNS 切成 0，Pod 不會馬上關掉，所以還能夠解析 DNS，中間又因爲有 NodeLocal DNSCache 做 Cache，所以 DNS 解析還有相關紀錄可以回覆，但後面當 Cache TTL 到期後，需要先訪問 KubeDNS 時，此時 KubeDNS 也已經關閉，最後才會出現解析錯誤 從 Prometheus 可以發現，前面 NodeLocal DNSCache request 跟 hit 差不多，但當 15:33 線圖開始 request 大於 hit，且出現 miss，這代表因為後面的 KubeDNS 異常，導致 NodeLocal DNSCache 沒辦法做 Cache hit\n模擬 NodeLocal DNSCache Pod 異常 接下來我們測試最後一個情境，故意用壞 NodeLocal DNSCache 服務，觀察此模式對於 NodeLocal DNSCache 的依賴\n測試情境：\n先調整 COUNT 參數變成 50000\n先跑 15000 筆有 NodeLocal DNSCache，然後使用以下指令讓 NodeLocal DNSCache 無法使用\nkubectl patch daemonset node-local-dns -n kube-system --type='strategic' -p '{\"spec\":{\"template\":{\"spec\":{\"nodeSelector\":{\"this-label-does-not-exist-on-any-node\":\"true\"}}}}}' 等待約 30000 筆時，使用以下指令恢復 NodeLocal DNSCache\nkubectl patch daemonset node-local-dns -n kube-system --type='strategic' -p '{\"spec\":{\"template\":{\"spec\":{\"nodeSelector\":null}}}}' 測試結果：\n測試結果\n分別在 15:55:53 跟 15:57:04 下指令調整\n模擬 NodeLocal DNSCache 故障\n觀察調整讓 NodeLocal DNSCache 故障 Log\n觀察修正 NodeLocal DNSCache Log\nPrometheus 監控指標\n結論\n發現當 NodeLocal DNSCache 掛了後，會短暫卡住，但會直接切換到 KubeDNS 上繼續進行解析，因此以結果論，如果 NodeLocal DNSCache 有短暫異常，不會出現無法解析的問題 從 Prometheus 可以發現，前面 NodeLocal DNSCache 正常運作，當我們在 15:55:53 調整後，變成 KubeDNS 起來開始處理解析，在 15:57:04 切換讓 NodeLocal DNSCache 恢復，後面又會變成由 NodeLocal DNSCache 來處理解析 紫色是 NodeLocal DNSCache，黃色是 KubeDNS","外部-dns-ifconfigme#外部 DNS (ifconfig.me)":"NodeLocal DNSCache Prometheus 監控設定參數：zones=\".\"\n相關 Prometheus 監控指標： coredns_cache_requests_total{job=\"kubedns-nodelocaldns\", zones=\".\"} coredns_cache_entries{job=\"kubedns-nodelocaldns\", zones=\".\"} coredns_cache_hits_total{job=\"kubedns-nodelocaldns\", zones=\".\"} coredns_cache_misses_total{job=\"kubedns-nodelocaldns\", zones=\".\"} kubedns_dnsmasq_hits{job=\"kubedns-dns\"} kubedns_dnsmasq_misses{job=\"kubedns-dns\"} 測試腳本：\n#!/bin/bash get_taiwan_time() { # 獲取當前 UTC 時間的 Unix 時間戳 UTC_TIMESTAMP=$(date -u +%s) # 加上 8 小時 (台灣時間是 UTC+8) TAIPEI_TIMESTAMP=$((UTC_TIMESTAMP + 28800)) # 將時間戳轉換為格式化後的日期時間 date -d \"@$TAIPEI_TIMESTAMP\" \"+%Y-%m-%d %H:%M:%S\" } DOMAIN=\"ifconfig.me\" EXPECTED_IP=\"34.160.111.145\" START_TIME=$(get_taiwan_time) COUNT=10000 SUCCESS_COUNT=0 FAIL_COUNT=0 echo \"== NSLOOKUP TEST START: $START_TIME ==\" | tee -a nslookup_full.log for i in $(seq 1 \"$COUNT\"); do OUTPUT=$(nslookup \"$DOMAIN\" 2\u003e\u00261) ADDR_LINE=$(echo \"$OUTPUT\" | grep -E '^Address:') if [[ \"$ADDR_LINE\" == *\"$EXPECTED_IP\"* ]]; then SUCCESS_COUNT=$((SUCCESS_COUNT + 1)) else FAIL_COUNT=$((FAIL_COUNT + 1)) fi echo \"[$i] === $(get_taiwan_time) 成功: $SUCCESS_COUNT 失敗: $FAIL_COUNT\" done END_TIME=$(get_taiwan_time) echo \"== NSLOOKUP TEST END: $END_TIME ==\" | tee -a nslookup_full.log echo \"成功次數: $SUCCESS_COUNT\" | tee -a nslookup_full.log echo \"失敗次數: $FAIL_COUNT\" | tee -a nslookup_full.log 34.160.111.145 是 ifconfig.me 的 IP，只是為了確認 domain 是否能夠正常解析\n測試腳本 測試結果：\n測試結果\nPrometheus 監控指標\n結論\n可以觀察 NodeLocal DNSCache 內的指標 hit 跟 request 都有持續上升\n模擬 KubeDNS Pod 異常 接下來會在測試中，將 KubeDNS 調整到 0 顆，再開回去，觀察此模式對於 KubeDNS 的依賴\n測試結果：\n測試結果\n因為 KubeDNS 關成 0 顆，不會馬上關掉\nPrometheus 監控指標\n結論\n總共打了兩輪的 10000 筆，在第一輪大約在 2000 筆請求時左右將 KubeDNS 關成 0 顆，因為前面 KubeDNS 切成 0，Pod 不會馬上關掉，避免有測試誤差，所以在打第二輪 10000 筆，但從結果發現，所有的 DNS 請求都是走 NodeLocal DNSCache，因為外部 dns 不是 .cluster.local，所以就算沒有 KubeDNS 也能正常運作\n模擬 NodeLocal DNSCache Pod 異常 接下來我們測試最後一個情境，故意用壞 NodeLocal DNSCache 服務，觀察此模式對於 NodeLocal DNSCache 的依賴\n測試情境：\n先調整 COUNT 參數變成 50000\n先跑 15000 筆有 NodeLocal DNSCache，然後使用以下指令讓 NodeLocal DNSCache 無法使用\nkubectl patch daemonset node-local-dns -n kube-system --type='strategic' -p '{\"spec\":{\"template\":{\"spec\":{\"nodeSelector\":{\"this-label-does-not-exist-on-any-node\":\"true\"}}}}}' 等待約 30000 筆時，使用以下指令恢復 NodeLocal DNSCache\nkubectl patch daemonset node-local-dns -n kube-system --type='strategic' -p '{\"spec\":{\"template\":{\"spec\":{\"nodeSelector\":null}}}}' 測試結果：\n測試結果\n分別在 19:52:28 跟 19:53:16 下指令調整\n模擬 NodeLocal DNSCache 故障\nPrometheus 監控指標\n結論\n發現當 NodeLocal DNSCache 掛了後，會短暫卡住，但會直接切換到 KubeDNS 上繼續進行解析，因此以結果論，如果 NodeLocal DNSCache 有短暫異常，不會出現無法解析的問題 從 Prometheus 可以發現，前面 NodeLocal DNSCache 正常運作，當我們在 19:52:28 調整後，變成 KubeDNS 起來開始處理解析，在 19:53:16 切換讓 NodeLocal DNSCache 恢復，後面又會變成由 NodeLocal DNSCache 來處理解析 紫色是 NodeLocal DNSCache，黃色是 KubeDNS","結論#結論":"可以發現，使用 KubeDNS + NodeLocal DNSCache 的模式下，對於 KubeDNS 的依賴性降低了很多，因為 NodeLocal DNSCache 會先做 cache hit，這樣就算 KubeDNS 異常（只有 cluster 內的，且 cache 失效才會影響），否則不會影響到 pod 的 DNS 解析。\nKubeDNS + NodeLocal DNSCache 流程圖\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/nodelocal-dns-cache?hl=zh-tw#architecture\n由於官方流程圖有些細節沒有揭露的完成，我們有額外詢問 Google TAM，並畫出以下流程圖，Google TAM 也確認流程正確\n自己重新畫的 GKE KubeDNS + NodeLocal DNSCache 流程圖\n以下是我們與 Google 討論的內容：\n我們：\nMetaData Server 是否會快取外部DNS的解析結果？若會的話，是否 DNS Provider 改為 Cloud DNS才會有這個行爲發生？\n公用的 Cloud DNS 是否有什麼統一的稱呼 (例：Common Cloud DNS…之類的)？或者就真的只叫 “Cloud DNS”？\nGoogle：\n會快取並遵循 TTL，無論是否設定為 Cloud DNS，內外部的 DNS 解析都需要透過 Cloud DNS 查詢，只有是否先經過 kube-dns 的差異，因此快取無論開啟 Cloud DNS 都會有\n就叫 Cloud DNS，或內部稱 GCE DNS\n這是 K8s 官方的 NodeLocalDNS 流程圖：\nUsing NodeLocal DNSCache in Kubernetes Clusters\nhttps://kubernetes.io/docs/tasks/administer-cluster/nodelocaldns/#architecture-diagram\n另外也可以從官方文件中得知 KubeDNS + NodeLocal DNSCache 的 /etc/resolv.conf 設定值，可以知道 Pod 最開始使用的 DNS 解析 Server 是誰。\n/etc/resolv.conf 設定值 服務探索和 DNS /etc/resolv.conf\n實際查看 /etc/resolv.conf"},"title":"GKE DNS 使用 KubeDNS + NodeLocal DNSCache 運作測試"},"/blog/gcp/gke-kubedns/":{"data":{"":"最近在評估要將公司內的 EndPoint 都改成 Cloud DNS 的 Private Zone (打造內部的 internal dns 服務機制)，到時候 DNS 解析的請求會比以往還要多，所以需要先測試評估 GKE 內的 DNS 解析方案，避免再次發生 Pod 出現 cURL error 6: Could not resolve host，此篇文章測試的是： KubeDNS 的運作。\n首先，先建立一個 dns-test pod 程式連結 以及 nginx 的 pod + svc 程式連結，會分別測試\n叢集內部 cluster.local (nginx-svc.default.svc.cluster.local)\ninternal-dns 使用 cloud dns private (aaa.test-audit.com)\n外部 dns (ifconfig.me)\n並使用 nslookup 腳本進行確認回傳 DNS 解析，每一次測試都會重新建立 KubeDNS Pod\n相關程式以及 Prometheus、Grafana 的設定可以參考：https://github.com/880831ian/gke-dns","internal-dns-cloud-dns-private#Internal DNS (Cloud DNS Private)":"先建立一個 cloud dns private，以下範例是 aaa.test-audit.com \u003e 10.1.1.4，並將此 use VPC 與 GKE 的 VPC 打通\n設定 cloud dns private\n相關 Prometheus 監控指標： kubedns_dnsmasq_hits{job=\"kubedns-dns\"} kubedns_dnsmasq_misses{job=\"kubedns-dns\"} 測試腳本：\n#!/bin/bash get_taiwan_time() { # 獲取當前 UTC 時間的 Unix 時間戳 UTC_TIMESTAMP=$(date -u +%s) # 加上 8 小時 (台灣時間是 UTC+8) TAIPEI_TIMESTAMP=$((UTC_TIMESTAMP + 28800)) # 將時間戳轉換為格式化後的日期時間 date -d \"@$TAIPEI_TIMESTAMP\" \"+%Y-%m-%d %H:%M:%S\" } DOMAIN=\"aaa.test-audit.com\" EXPECTED_IP=\"10.1.1.4\" START_TIME=$(get_taiwan_time) COUNT=10000 SUCCESS_COUNT=0 FAIL_COUNT=0 echo \"== NSLOOKUP TEST START: $START_TIME ==\" | tee -a nslookup_full.log for i in $(seq 1 \"$COUNT\"); do OUTPUT=$(nslookup \"$DOMAIN\" 2\u003e\u00261) ADDR_LINE=$(echo \"$OUTPUT\" | grep -E '^Address:') if [[ \"$ADDR_LINE\" == *\"$EXPECTED_IP\"* ]]; then SUCCESS_COUNT=$((SUCCESS_COUNT + 1)) else FAIL_COUNT=$((FAIL_COUNT + 1)) fi echo \"[$i] === $(get_taiwan_time) 成功: $SUCCESS_COUNT 失敗: $FAIL_COUNT\" done END_TIME=$(get_taiwan_time) echo \"== NSLOOKUP TEST END: $END_TIME ==\" | tee -a nslookup_full.log echo \"成功次數: $SUCCESS_COUNT\" | tee -a nslookup_full.log echo \"失敗次數: $FAIL_COUNT\" | tee -a nslookup_full.log 10.1.1.4 是隨機亂取的 IP，只是為了確認 domain 是否能夠正常解析\n測試腳本 測試結果：\n測試結果\nPrometheus 監控指標\n結論\n可以觀察 KubeDNS 內的指標 kubedns_dnsmasq_hits hit 有持續上升\n模擬 KubeDNS Pod 異常 接下來會在測試中，將 KubeDNS 調整到 0 顆，再開回去，觀察此模式對於 KubeDNS 的依賴\n測試結果：\n測試結果\nKubeDNS 關成 0 顆，過一陣子後開始噴錯\n因為 KubeDNS Pod 不會馬上關掉，所以還能夠解析 DNS\nPrometheus 監控指標\n結論\n大約在 2000 筆請求時左右將 KubeDNS 關成 0 顆，但到了 7130 筆的時候才開始出現解析失敗，觀察後發現，因為 KubeDNS 切成 0，Pod 不會馬上關掉，所以還能夠解析 DNS，最後再將 KubeDNS 切成 1 顆後，就正常可以解析了","k6-測試#k6 測試":"額外在另一個 cluster 建立 nginx deployment 開 5 個 pod 以及 svc 改成 lb (L4)，然後在 cloud dns 的 test-audit-com 設定 nginx-lb-internal.test-audit.com 解析到內網的 svc (10.156.16.5)\n使用 k6 測試 kube dns 模式下 IP 跟 DNS 的差異\n相關程式可以參考：https://github.com/880831ian/gke-dns\n這邊測試的 Node 是用 e2-medium 而非 c3d-standard-4\n第一次測試 IP (avg=137.2ms / 4080 RPS)、DNS (avg=140.4ms / 4024 RPS)\nIP 第一次測試\nDNS 第一次測試\n第二次測試 IP (avg=151.05ms / 3878 RPS)、DNS (avg=260.19ms / 2715 RPS)\nIP 第二次測試\nDNS 第二次測試\n第三次測試 IP (avg=150.84ms / 3858 RPS)、DNS (avg=155.8ms / 3806 RPS)\nIP 第三次測試\nDNS 第三次測試\n第四次測試 IP (avg=321.47ms / 2337 RPS)、DNS (avg=254.11ms / 2775 RPS)\nIP 第四次測試\nDNS 第四次測試\n結論\n理論上 ip 應該會比 dns 還要快，但測試 4 次發現其實不一定","參考資料#參考資料":"使用 KubeDNS：https://cloud.google.com/kubernetes-engine/docs/how-to/kube-dns?hl=zh-tw","叢集內部-clusterlocal#叢集內部 cluster.local":"\n相關 Prometheus 監控指標： kubedns_dnsmasq_hits{job=\"kubedns-dns\"} kubedns_dnsmasq_misses{job=\"kubedns-dns\"} 測試腳本：\n#!/bin/bash get_taiwan_time() { # 獲取當前 UTC 時間的 Unix 時間戳 UTC_TIMESTAMP=$(date -u +%s) # 加上 8 小時 (台灣時間是 UTC+8) TAIPEI_TIMESTAMP=$((UTC_TIMESTAMP + 28800)) # 將時間戳轉換為格式化後的日期時間 date -d \"@$TAIPEI_TIMESTAMP\" \"+%Y-%m-%d %H:%M:%S\" } DOMAIN=\"nginx-svc.default.svc.cluster.local\" EXPECTED_IP=\"10.36.16.255\" START_TIME=$(get_taiwan_time) COUNT=10000 SUCCESS_COUNT=0 FAIL_COUNT=0 echo \"== NSLOOKUP TEST START: $START_TIME ==\" | tee -a nslookup_full.log for i in $(seq 1 \"$COUNT\"); do OUTPUT=$(nslookup \"$DOMAIN\" 2\u003e\u00261) ADDR_LINE=$(echo \"$OUTPUT\" | grep -E '^Address:') if [[ \"$ADDR_LINE\" == *\"$EXPECTED_IP\"* ]]; then SUCCESS_COUNT=$((SUCCESS_COUNT + 1)) else FAIL_COUNT=$((FAIL_COUNT + 1)) fi echo \"[$i] === $(get_taiwan_time) 成功: $SUCCESS_COUNT 失敗: $FAIL_COUNT\" done END_TIME=$(get_taiwan_time) echo \"== NSLOOKUP TEST END: $END_TIME ==\" | tee -a nslookup_full.log echo \"成功次數: $SUCCESS_COUNT\" | tee -a nslookup_full.log echo \"失敗次數: $FAIL_COUNT\" | tee -a nslookup_full.log 10.36.16.255 是 nginx-svc 的 Cluster IP\n需要先確認 nginx-svc 的 IP 是多少，然後修改腳本中的 EXPECTED_IP 變數。\n測試腳本 測試結果：\n測試結果\nPrometheus 監控指標\n結論\n可以觀察 kubedns_dnsmasq_hits hit 有持續上升\n模擬 KubeDNS Pod 異常 接下來會在測試中，將 KubeDNS 調整到 0 顆，再開回去，觀察此模式對於 KubeDNS 的依賴\n測試結果：\n測試結果\nKubeDNS 關成 0 顆，過一陣子後開始噴錯\n因為 KubeDNS Pod 不會馬上關掉，所以還能夠解析 DNS\nPrometheus 監控指標\n結論\n大約在 2000 筆請求時左右將 KubeDNS 關成 0 顆，但到了 8733 筆的時候才開始出現解析失敗，觀察後發現，因為 KubeDNS 切成 0，Pod 不會馬上關掉，所以還能夠解析 DNS，最後再將 KubeDNS 切成 1 顆後，就正常可以解析了","外部-dns-ifconfigme#外部 DNS (ifconfig.me)":"\n相關 Prometheus 監控指標： kubedns_dnsmasq_hits{job=\"kubedns-dns\"} kubedns_dnsmasq_misses{job=\"kubedns-dns\"} 測試腳本：\n#!/bin/bash get_taiwan_time() { # 獲取當前 UTC 時間的 Unix 時間戳 UTC_TIMESTAMP=$(date -u +%s) # 加上 8 小時 (台灣時間是 UTC+8) TAIPEI_TIMESTAMP=$((UTC_TIMESTAMP + 28800)) # 將時間戳轉換為格式化後的日期時間 date -d \"@$TAIPEI_TIMESTAMP\" \"+%Y-%m-%d %H:%M:%S\" } DOMAIN=\"ifconfig.me\" EXPECTED_IP=\"34.160.111.145\" START_TIME=$(get_taiwan_time) COUNT=10000 SUCCESS_COUNT=0 FAIL_COUNT=0 echo \"== NSLOOKUP TEST START: $START_TIME ==\" | tee -a nslookup_full.log for i in $(seq 1 \"$COUNT\"); do OUTPUT=$(nslookup \"$DOMAIN\" 2\u003e\u00261) ADDR_LINE=$(echo \"$OUTPUT\" | grep -E '^Address:') if [[ \"$ADDR_LINE\" == *\"$EXPECTED_IP\"* ]]; then SUCCESS_COUNT=$((SUCCESS_COUNT + 1)) else FAIL_COUNT=$((FAIL_COUNT + 1)) fi echo \"[$i] === $(get_taiwan_time) 成功: $SUCCESS_COUNT 失敗: $FAIL_COUNT\" done END_TIME=$(get_taiwan_time) echo \"== NSLOOKUP TEST END: $END_TIME ==\" | tee -a nslookup_full.log echo \"成功次數: $SUCCESS_COUNT\" | tee -a nslookup_full.log echo \"失敗次數: $FAIL_COUNT\" | tee -a nslookup_full.log 34.160.111.145 是 ifconfig.me 的 IP，只是為了確認 domain 是否能夠正常解析\n測試腳本 測試結果：\n測試結果\nPrometheus 監控指標\n結論\n可以觀察 KubeDNS 內的指標 kubedns_dnsmasq_hits hit 有持續上升，且有比較特別的現象是 叢集內部 cluster.local 跟 internal-dns kubedns_dnsmasq_misses 會跟 kubedns_dnsmasq_hits 差不多，但外部 DNS 卻不會 cluster 內 / cloud DNS private zone 流程會是 Pod → dnsmasq (miss) → kubedns 回答 → dnsmasq cache → 下次 hit 所以會是 N hit + N miss 外部 domain 流程會是 Pod → kubedns → upstream → dnsmasq 只 cache 回應，不算 miss 加上 dnsmasq 本身會為不同類型的查詢（例如 A/AAAA 或 CNAME chain）各記一次 hit 所以會是 2N hit，0 miss\n模擬 KubeDNS Pod 異常 接下來會在測試中，將 KubeDNS 調整到 0 顆，再開回去，觀察此模式對於 KubeDNS 的依賴\n測試結果：\n測試結果\nKubeDNS 關成 0 顆，過一陣子後開始噴錯\n因為 KubeDNS Pod 不會馬上關掉，所以還能夠解析 DNS\nPrometheus 監控指標\n結論\n大約在 2000 筆請求時左右將 KubeDNS 關成 0 顆，但到了 8934 筆的時候才開始出現解析失敗，觀察後發現，因為 KubeDNS 切成 0，Pod 不會馬上關掉，所以還能夠解析 DNS，最後再將 KubeDNS 切成 1 顆後，就正常可以解析了","結論#結論":"可以發現，使用 KubeDNS 的模式下，對於 KubeDNS 的依賴性極高，只要壞掉，不管任何的解析(叢集內、internal-dns、外部 dns)都會失效。\nGKE KubeDNS 流程圖\n由於官方流程圖有些細節沒有揭露的完成，我們有額外詢問 Google TAM，並畫出以下流程圖，Google TAM 也確認流程正確\n自己重新畫的 GKE KubeDNS 流程圖\n以下是我們與 Google 討論的內容：\n我們：\n圖中的 metadata server 是畫在 GKE Cluster 層內，與 Kube DNS 上游伺服器 (kube-dns svc) 以及 Kube DNS 在同一層。而 Kube DNS + NodeLocal DNSCache 版本架構圖中的 metadata server 則是被標示在Worker Node 裡面，這兩者實質上歸屬與位置皆不同是正確的嗎？為何會這樣？\nGoogle：\nmetadata server 是存在每一台 VM 上面\n如果是 Kube DNS，請求流向是 application pod -\u003e kube-dns svc -\u003e 隨機 kube-dns pod，如果 kube-dns 沒答案，跳轉該 kube-dns pod 上面的 VM/worker node 的 metadata server\n(情境2)如果是 node-local-dns，請求流向會是 application pod -\u003e node-local-dns pod\n如果是 cluster.local 就會轉發到 kube-dns 如果有設定 stubDomain 就轉發到設定的 DNS 如果是其他的則轉發這台 VM/pod 的 metadata server –\n我們補充詢問：\n附上的圖是 KubeDNS 的請求解析圖，我們知道 Metadata Server 是在每一個 VM (Worker Node 上)，只是圖上範例是將 Metadata Server 放到 GKE Cluster 的 Scope 內，而不是在一個 Worker Node，因為情境2. 的 KubeDNS + NodeLocal DNSCache 是將 Metadata Server 放到 Worker Node 裡面，而不是 GKE Cluster，想確認是不是 KubeDNS 請求解析圖在這一塊比較沒有畫清楚呢？\nGoogle 回覆： KubeDNS Only：所有請求都會先到 kube-dns pod，如果是 cluster 之外的會轉發到 kube-dns pod 上面的 VM/worker node 的 metadata server，不是每一台 worker。\nKubeDNS + node-local-dns 如果不是 cluster.svc.local 就會直接轉發到該台 worker 的 metadata 了\n因此用到的 Metadata server 的 worker 不太一樣，一個是指會用到 kube-dns pod 上面的 VM，一個是全部 VM/worker 都用到\n同意您說的 “圖上範例是將 Metadata Server 放到 GKE Cluster 的 Scope 內” 可能會有點誤導\n您畫的圖示意是正確的\n–\n另外也可以從官方文件中得知 KubeDNS 的 /etc/resolv.conf 設定值，可以知道 Pod 最開始使用的 DNS 解析 Server 是誰。\n/etc/resolv.conf 設定值 服務探索和 DNS /etc/resolv.conf\n實際查看 /etc/resolv.conf"},"title":"GKE DNS 使用 KubeDNS 運作測試"},"/blog/gcp/gke-local-ephemeral-storage/":{"data":{"":"此文章主要針對 Google Kubernetes Engine Local ephemeral storage 的計算方式來做介紹。","local-ephemeral-storage-計算方式#Local ephemeral storage 計算方式":"根據官方文件 Local ephemeral storage reservation，我們可以知道 GKE 會為每個 node 提供本地的臨時儲存空間，當這個儲存空間在 node 故障刪除時，資料也會被一起遺失。\nGKE 會依照下列的方式計算本機預留的暫存空間，也就是被偷走的空間 /ᐠ .ᆺ. ᐟ\\ﾉ：\nEVICTION_THRESHOLD + SYSTEM_RESERVATION EVICTION_THRESHOLD 是驅逐閾值 ; SYSTEM_RESERVATION 是系統保留空間。\nEVICTION_THRESHOLD 驅逐閥值計算 在預設情況下，臨時儲存空間是由開機磁碟的大小來決定的，驅逐閾值是開機磁碟大小的 10%。\nEVICTION_THRESHOLD = 10% * BOOT_DISK_CAPACITY SYSTEM_RESERVATION 系統保留空間計算 系統保留空間也跟開機磁碟大小有關：\nSYSTEM_RESERVATION = Min(50% * BOOT_DISK_CAPACITY, 6GiB + 35% * BOOT_DISK_CAPACITY, 100 GiB) 會計算 50% * BOOT_DISK_CAPACITY、6GiB + 35% * BOOT_DISK_CAPACITY、 100 GiB 三者中的最小值。奇怪的計算方式 (*´･д･)?。\n計算可用空間 最後將開機磁碟大小減去 EVICTION_THRESHOLD 與 SYSTEM_RESERVATION 就可以得到可用空間：\nBOOT_DISK_CAPACITY - (EVICTION_THRESHOLD + SYSTEM_RESERVATION) 計算範例 根據上面的公式我們可以知道 被偷走的空間有多少，下面舉例兩個不同的開機磁碟大小來計算：\n開機磁碟大小 100GB 先計算 EVICTION_THRESHOLD：\nEVICTION_THRESHOLD = 10% * 100GB = 10GB 再計算 SYSTEM_RESERVATION：\nSYSTEM_RESERVATION = Min(50% * 100GB, 6GiB + 35% * 100GB, 100 GiB) = Min(50GB, 6GiB + 35GB, 100GB) = Min(50GB, 41GB, 100GB) = 41GB 接著將 EVICTION_THRESHOLD 與 SYSTEM_RESERVATION 相加：\nEVICTION_THRESHOLD + SYSTEM_RESERVATION = 10GB + 41GB = 51GB 就可以得出被偷走的空間為 51GB，代表我們建立 100GB 的開機磁碟時，實際上只有 49GB 空間可以使用。\n開機磁碟大小 300GB 先計算 EVICTION_THRESHOLD：\nEVICTION_THRESHOLD = 10% * 300GB = 30GB 再計算 SYSTEM_RESERVATION：\nSYSTEM_RESERVATION = Min(50% * 300GB, 6GiB + 35% * 300GB, 100 GiB) = Min(150GB, 6GiB + 105GB, 100GB) = Min(150GB, 111GB, 100GB) = 100GB 接著將 EVICTION_THRESHOLD 與 SYSTEM_RESERVATION 相加：\nEVICTION_THRESHOLD + SYSTEM_RESERVATION = 30GB + 100GB = 130GB 就可以得出被偷走的空間為 130GB，代表我們建立 300GB 的開機磁碟時，實際上只有 170GB 可以使用。\n了解計算方式後，那為什麼會出現 Local ephemeral storage 不足的問題呢？\n後來發現，因為我的多數服務都會使用 gcsfuse 掛載 GCS bucket，而 gcsfuse 預設 ephemeral storage request 會使用 5GB，所以以 100 GB 的開機磁碟，我一個 node 只要超過 9 個 Pod，就會導致 Local ephemeral storage 不足的問題。詳細就請參考：Configure resources for the sidecar container","前情提要#前情提要":"為什麼會突然在看這個主題呢？\n主要是這幾天在優化自己 GKE 的 Node Pool 規格時，原先是開 e2-standard-2 / 100GB 的 node，數量是 8 ~ 12 顆 node (透過自動擴展)，改成 n2-standard-4 / 100GB 的 node，數量是 6 ~ 10 顆 node (透過自動擴展)。\n發現明明 CPU/MEM 資源都夠且更多，但 node 數量卻還是跟原始規格一樣長到 9 顆，使用 Cloud logging 來查看 unschedulable，發現與 Local ephemeral storage 有關，因此就來了解一下 Local ephemeral storage 是如何計算的。\n我們在 Cloud Logging 用以下指令來查看為何 node 會需要長新的，而不是進到還有資源的 node 上：\nunschedulable severity=WARNING \"ephemeral-storage\" Cloud Logging 查詢結果\n可以看到是因為 Local ephemeral storage 不足，導致無法排程，才會長新的 node。\n但這個很奇怪，我們明明建立了 100GB 的 node，為什麼會不足呢？而且我們還是開 spot instance，理論上 node 會被收回，node disk 也會被清空，所以不應該會出現這個問題才對。\n因此我們就先來了解 Local ephemeral storage 是如何計算的。","參考資料#參考資料":"Local ephemeral storage reservation：https://cloud.google.com/kubernetes-engine/docs/concepts/plan-node-sizes#local_ephemeral_storage_reservation\nConfigure resources for the sidecar container：https://cloud.google.com/kubernetes-engine/docs/how-to/persistent-volumes/cloud-storage-fuse-csi-driver#sidecar-container-resources","哪裡可以查看-local-ephemeral-storage-使用量#哪裡可以查看 Local ephemeral storage 使用量？":" 可以從 GKE 的 Node Pool UI 介面查看： GKE Node Pool 頁面\n可以看到實際上 101.2 GB 的 node，可用空間只有 47.06 GB，與我們上面的計算方式差不多。\n可以透過 kubectl describe node 來查看： kubectl describe node 頁面\n可以看到 Capacity ephemeral-storage: 98831908Ki，換算成 GB 大約是 101.19 GB。\nAllocatable ephemeral-storage: 47060071478，換算成 GB 大約是 47.06 GB。\nkubectl describe node 頁面\n也可以從 describe 看到目前 request ephemeral-storage 設定的大小，來避免 Local ephemeral storage 出現不足的問題。"},"title":"Google Kubernetes Engine Local ephemeral storage 計算方式"},"/blog/gcp/iam/":{"data":{"":"IAM 的全名是 Identity and Access Management，當我們藉由 IAM，可以授與特定 Google Cloud 資源的 精細 訪問權限，並防止對其他資源的訪問。疑！？為什麼是精細？我們接著看下去，我們可以採用最小權限安全原則，該原則要求任何人都不應擁有超出實際所需的權限。","iam-測試#IAM 測試":"接下來，我們來測試看看 IAM 實際設定以及用途吧！ 我們會使用 GCP 所以提供的 [Qwiklabs] Cloud IAM：Qwik Start 來進行測試，之後的步驟會跟 Cloud IAM：Qwik Start 內容一樣，所以大家可以邊操作邊參考呦！那我們開始囉 🙃\n進入網頁後，請先登入自己的 Google 帳號，接著點選左上角的 Start Lab，會跳出與下面圖片類似的內容：\n測試用的帳號密碼\n這邊會提供兩組的帳號及密碼，分別是 Username1 以及 Username2 (後面會以 Username 1 跟 2 來說明)，密碼會用共用，且會在同一個專案下。\n用第一個 Username1 登入 GCP 點選 Open Google Console 按鈕來開啟 GCP 主控台 登入的帳號就輸入第一個 Username1 ，密碼輸入共用密碼 ，最後登入 登入成功會進入 GCP 主控台，會跳出下方圖片內容，國家選擇台灣，點選同意 Terms of Service，最後按 AGREE AND CONTINUE 登入 Username 1 GCP 主控台\n用第二個 Username2 登入 GCP 步驟與第一個相同，這邊就不在重複，但建議使用無痕，避免 Username1 跟 Username2 搶來搶去，以及登入帳號要選 Username2，應該不會選錯吧 🤣 Username1 IAM 主控台 到 Username1 的 GCP 主控台首頁 點選左側的 menu \u003e IAM 與管理 點擊頁面上方的 +ADD 按鈕，可以從下拉選單去查看各式各項的專案相關角色，可以看到超級多的角色設定，所以我們一開始才會說他是可以設定 ** 精細** 的訪問權限 我們選擇 Base，右側有 4 種角色，分別是瀏覽者 (Browser)、編輯者 (Editor)、所有者 (Owner)、檢視者 (Viewer)，詳細區別請看下面 👇👇 ADD IAM\n此表是取 Google Cloud IAM 文章基本角色中的定義，簡單說明 4 種角色的差別：\n角色名稱 權限 role/viewer (檢視者) 不影響狀態的只讀操作權限，例如：查看 (但不修改) 現有資源或是資料 role/editor (編輯者) 所有查看者權限，以及修改狀態的操作權限，例如：更改現有資源 role/owner (擁有者) 以下操作的所有編輯權限： 1. 管理項目和項目內的所有資源角色及權限 2. 為項目設置帳單 role/browser (瀏覽者) 讀取權限以及瀏覽項目的層次結構，包含資料夾、組織和 IAM 政策。但此角色不包含查看項目中資源的權限 我們的 Username1 為 owner，Username2 為 viewer\nGoogle 建議：Base 角色包含所有 Google Cloud 服務的數千個權限。除非別無選擇，否則不要授與用戶 Base 角色，請設定最有限的自訂義角色或是可以滿足該需求的角色即可 切換到 Username2 IAM 主控台 我們可以在表格裡面收尋 Username1 跟 Username2 的帳號，可以看一下他們授與的角色是否與上面說的一致，大概可以參考以下圖片：\nUsername1 跟 Username2 權限\n在 Username2 因為是 Viewer 權限，所以點擊上面的 + ADD，不會反應，會跳出以下照片內容：\nUsername2 權限不足\n再切回 Username1 Cloud Storage 接下來我們要建立一個 GCS 儲存空間，點選 menu \u003e Cloud Storage \u003e Browser 點選 Create a bucket 給予他一個獨特的名稱，以及在 Choose where to store your data 選擇 Multi-Region 最後點選 CREATE Create a bucket\n上傳範例檔案 進入新建立的 Cloud Storage，點選 Upload fiiles 按鈕 上傳一個 txt 檔案，可以先取名為 sample.txt，或是上傳後使用最後三個小點圖案內的 Rename 來修改名稱 Upload sample.txt fiiles\n驗證專案檢視者存取權限 我們再切換回 Username2 的主控台，選擇 menu \u003e Cloud Storage \u003e Browser 就可以看到跟上面一樣的儲存區 Username2 被授與 “檢視者 Viewer” 角色，這個角色有不影響狀態的只讀權限。這個範例中說明這個功能，他僅能檢視，沒有辦法上傳\n移除專案存取權限 我們再次切回 Username 主控台，選擇 menu \u003e IAM 與管理 ，找到 Username2 旁邊的鉛筆圖案 修改 Username2 權限\n點擊角色名稱的垃圾桶來移除 Username2 的檢視者權限，點擊 Save 移除 Username2 權限\n這個動作要完成生效到所有服務上，所以會需要一點時間，詳細可以參考點我 檢查 Username2 是否有存取權限 切換到 Username2 主控台，選擇 menu \u003e Cloud Storage \u003e Browser 會發現出現以下的錯誤訊息，代表我們移除權限成功 Username2 沒有權限\n新增儲存角色 我們再次切換到 Username1 主控版，選擇 menu \u003e \u003e IAM 與管理，點選上方 + ADD，在 New principals 上貼上 Username2 的帳號，Role 選擇 Storage Object Viewer 新增 Username2 角色\n查看 Username2 權限\n檢查 Username2 是否有存取權限 切換到 Username2 主控台，因為 Username2 沒有專案檢視者的角色，所以看不到專案以及任何的資源，但這個使用者對我們剛剛設定的 Cloud Storage 有特別的存取權 打開右上角的 Activate Cloud Shell 命令列工具 ，如下圖： 開啟 Activate Cloud Shell 命令列工具\n輸入以下指令 gsutil ls gs://\u003c剛剛建的 Cloud Storage 名稱\u003e 如果出現跟下方圖片一樣，就代表我們設定成功囉！\nUsername2 Storage Object Viewer 權限\n最後的最後，如果有跟我們一步一步來的朋友，在 [Qwiklabs] Cloud IAM：Qwik Start 頁面中，應該會看到一個叫 Check my progress 的按鈕，做完每一步驟，都可以點一下，他會自動去判斷你是否有完成這項動作歐！\nCheck my progress\n到這邊就完成了我們在 IAM 的測試囉～我們知道 IAM 可以設定很多的角色，以及測試了查看、新增、修改、刪除角色的功能，希望大家會喜歡今天的文章 🥰","iam-的工作原理#IAM 的工作原理":"首先我們先來了解一下 IAM 的工作原理，藉由 IAM，我們可以定義誰 (哪一個身份) 對哪些資源有哪種的訪問權限 (角色) 來管理訪問權限控制。什麼是資源？例如， Compute Engine 虛擬機 (GCE)、Google Kubernetes Engine (GKE) 集群和 Cloud Storage 存儲分區都是 Google Cloud 資源，我們用於整理資源的組織或資料夾、項目等也都是資源\nGCP IAM Logo\n我們可以把它理解成\n什麼 『 人 』，可以對什麼『 資源 』，做什麼『 事情 』 IAM 不會直接向用戶授與資源的訪問權限，而是將權限分成多個角色，然後將這些角色授與經過身份驗證的主帳號。(以前 IAM 會將主帳號稱為成員，目前部分 API 仍然使用此術語。)\nIAM 中的權限管理\n可以看到這張圖片，訪問權限管理主要包含三個部分：\n主帳號 (Principal)：主帳號可以是 Google 帳號 (針對用戶)、服務帳號 (針對應用和計算工作負載)、Google 群組或 Workspace 帳號或可以訪問資源的 Cloud Identity 網域等等 角色 (Role)：一個角色對應一組權限，權限決定了可以對資源執行的操作。向主帳號授與某個腳色， 代表授與該角色包含的所有權限給主帳號 政策 (Policy)：允許政策 (Allow Policy) 是將一個或多個主體綁定在各個角色，當想要定義誰 (主體) 對資源擁有何種類型的訪問 (角色) 時，可以創建允許政策並將其附加到資源。 ","參考資料#參考資料":"IAM 概覽：https://cloud.google.com/iam/docs/overview\n什麼是 Cloud IAM？GCP 權限管理服務介紹：https://blog.cloud-ace.tw/identity-security/what-is-cloud-iam/"},"title":"Google Cloud Platform (GCP) - IAM 與管理"},"/blog/git-or-cicd/":{"data":{"":"此分類包含 Git 或 CICD 相關的文章。\n如何啟用 GitLab 的 Package Registry 以及將儲存位置從伺服器改到 GCS 上發布日期：2022-12-29 如何合併多個 commit，且推到遠端呢？發布日期：2022-06-21 部署 Laravel 於 Heroku 搭配 GitLab CI/CD發布日期：2022-05-26 如何從頭打造專屬的 GitLab CI/CD發布日期：2022-05-26 Ansible 介紹與實作 (Inventory、Playbooks、Module、Template、Handlers)發布日期：2022-05-16 使用 Jenkins 設定 GitHub 觸發程序並通知 Telegram Bot發布日期：2022-05-16 Jenkins 及 Ansible IT 自動化 CI/CD 介紹發布日期：2022-05-11 Git 介紹發布日期：2022-04-15 "},"title":"Git 或 CICD 相關"},"/blog/git-or-cicd/ansible/":{"data":{"":"本篇文章是接續前面兩篇 Jenkins 及 Ansible IT 自動化 CI/CD 介紹 跟 使用 Jenkins 設定 GitHub 觸發程序並通知 Telegram Bot 文章，歡迎大家先去觀看前面兩篇文章 🤪\n本篇所使用到的程式碼都會整理於 GitHub 連結，大家有興趣可以去瀏覽看看歐！","ansible-安裝與實作#Ansible 安裝與實作":"安裝之前先讓大家看一下版本吧！大家要記得檢查自己的版本與教學是否相同，如果不同，記得要先查看官網是否有修改內容。\n版本 macOS：11.6 Docker：Docker version 20.10.14, build a224086 Aansible：ansible [core 2.12.5] 如何安裝 Ansible 在控制主機 由於 Ansible 是一套開源的軟體，所以在目前大部分主流作業系統上都可以透過對應的套件管理 (package manager) 進行安裝。\n本人使用 macOS ，所以這邊僅列出 masOS 安裝方式，其他的可以參考官方的安裝指南。\nmacOS 安裝可以使用兩種方式，官方較推薦使用 pip 來做安裝：\nPip Install Packages (pip 官方較推薦) $ sudo pip install ansible Homebrew (brew) $ sudo brew install ansible 安裝完後，可以使用 --version 指令來檢查是否安裝完成：\n$ ansible --version ansible [core 2.12.5] config file = None configured module search path = ['/Users/ian_zhuang/.ansible/plugins/modules', '/usr/share/ansible/plugins/modules'] ansible python module location = /usr/local/Cellar/ansible/5.7.1/libexec/lib/python3.10/site-packages/ansible ansible collection location = /Users/ian_zhuang/.ansible/collections:/usr/share/ansible/collections executable location = /usr/local/bin/ansible python version = 3.10.4 (main, Apr 26 2022, 19:43:24) [Clang 13.0.0 (clang-1300.0.29.30)] jinja version = 3.1.2 libyaml = True 如何安裝 Ansible 在被控節點 不需要！！！ 透過 Ansible 進行管理的被控節點完全不需要安裝 Ansible。我們只需要確保這個節點可以透過 SSH 與控制主機做溝通，並安裝 Python 2.6 以上版本就可以透過控制主機來進行部署及管理了。\n那我們為了要模擬，所以我們使用 Docker 來模擬 Managed Node，首先老樣子，一樣先寫一個 Dockerfile 來建立我們的映像檔，此映像檔是微調 chusiang/ansible-managed-node.dockerfile 的內容，修改 ubuntu 版本以及內容作調整，我會把程式碼放在 GitHub 連結 ，以及 DockerHub 連結，歡迎大家前去下載使用。\nFROM ubuntu:22.10 LABEL maintainer=\"880831ian@gmail.com\" # Update the index of available packages. RUN apt-get update # Install the requires package. RUN apt-get install -y openssh-server sudo curl wget bash-completion openssl \u0026\u0026 apt-get clean # Setting the sshd. RUN mkdir /var/run/sshd RUN echo 'root:root' | chpasswd RUN sed -i 's/PermitRootLogin without-password/PermitRootLogin yes/' /etc/ssh/sshd_config # SSH login fix. Otherwise user is kicked off after login RUN sed 's@session\\s*required\\s*pam_loginuid.so@session optional pam_loginuid.so@g' -i /etc/pam.d/sshd ENV NOTVISIBLE \"in users profile\" RUN echo \"export VISIBLE=now\" \u003e\u003e /etc/profile # Create a new user. # # - username: docker # - password: docker RUN useradd --create-home --shell /bin/bash \\ --password $(openssl passwd -1 docker) docker # Add sudo permission. RUN echo 'docker ALL=(ALL) NOPASSWD: ALL' \u003e\u003e /etc/sudoers # Setting ssh public key. RUN wget https://raw.githubusercontent.com/chusiang/ansible-jupyter.dockerfile/master/files/ssh/id_rsa.pub \\ -O /tmp/authorized_keys \u0026\u0026 \\ mkdir /home/docker/.ssh \u0026\u0026 \\ mv /tmp/authorized_keys /home/docker/.ssh/ \u0026\u0026 \\ chown -R docker:docker /home/docker/.ssh/ \u0026\u0026 \\ chmod 644 /home/docker/.ssh/authorized_keys \u0026\u0026 \\ chmod 700 /home/docker/.ssh EXPOSE 22 # Run ssh server daemon. CMD [\"/usr/sbin/sshd\", \"-D\"] 接下來將它包成 Image 並啟動他：\n$ docker build -t ansible-ubuntu-server . \u0026\u0026 docker run --name server1 -d -p 8888:22 ansible-ubuntu-server 64c51235e34a7ba42c0c45e690201dd80248c9aac76c3b855c99cf63f7f0af7c 可以用 exec 進入容器：\ndocker exec -it server1 /bin/bash 如何讓 Ansible 操控 Docker 容器？ 我們在工作目錄下，新增一個 ansible.cfg：\n[defaults] inventory = hosts remote_user = docker host_key_checking = False 設定 inventory hosts：\n[local] server1 ansible_ssh_host=127.0.0.1 ansible_ssh_port=8888 ansible_ssh_pass=docker 其中 8888 是我們在啟動時所開放的 Port，也可以自行更改。\nansible_ssh_host：設為本機的 IP。 ansible_ssh_port：設為 docker ps 取得的 SSH Port 也就是我們的 8888。 ansible_ssh_pass：因為我們沒有連線用的金鑰，所以直接使用密碼方式做連結。(建議只用於練習環境使用) Hello World On Managed Node 當我們都設置完成後，就可以使用 Terminal 用 Docker 建立好的 Ansible 來練習了！\n$ ansible all -m command -a 'echo Hello World on Docker.' server1 | CHANGED | rc=0 \u003e\u003e Hello World on Docker. ansible 安裝時常見問題\nQ1. server1 | FAILED | rc=-1 » to use the ‘ssh’ connection type with passwords or pkcs11_provider, you must install the sshpass program\nAns1. 會遇到這個問題是因為需要多安裝 sshpass，一般系統安裝 sshpass 很簡單，但在 macOS 上稍微麻煩，詳細可以參考這篇文章。\nQ2. ~paramiko/transport.py:236: CryptographyDeprecationWarning: Blowfish has been deprecated\nAns2. 在我安裝過程中，發現上前幾天才出現這個 Bug 詳細情形可以參考 GitHub issues，目前解決辦法有降板或是先將錯誤訊息給註解掉，之後再等新的版本出來再更新，大家可以自行選擇，我這邊是直接把出現問題的 transport.py 內容註解掉，大概位於 236 行，可以看下方圖片。\nCryptographyDeprecationWarning 錯誤訊息修正","ansible-是如何運作的#Ansible 是如何運作的？":"在 Ansible 世界裡，我們會透過 Inventory 檔案 來定義有哪些的 Managed Node，並藉由 SSH 與 Python 來進行溝通。那我們先來看一張圖：\nAnsible 運作原理 (圖片來源：七分鐘掌握 Ansible 核心觀念)\n誒 😱 突然多了很多新名詞，沒關係我來一一解釋，首先我們先從 Managed Node 是什麼，以及圖片中的 Control machine 開始說起吧！\n什麼是控制主機及被控節點？ 在 Ansible 裡，我們會把所有機器的角色做以下的區分：\n控制主機 (Control Machine)：顧名思義，這類主機可以透過運行 Ansible 的劇本 (Playbooks) 對被控節點進行部署。 被控節點 (Managed Node)：也稱為遙控節點 (Remote Node)。相對於控制主機，這類節點就是我們透過 Ansible 進行部署的對象。 所以代表我們在操作這邊就是 Control Machine，要部署的機器就是 Managed Node，透過 SSH 來做連線。但什麽是 Inventory 跟 Playbooks 呢？\n什麼是 Ansible Inventory Inventory 這個單字本身有詳細目錄、清單和列表的意思。在這裡我們可以把它理解成一份主機列表，可以透過它來定義每個 Managed Node 的代號、IP 位址、連線設定和群組。\n$ vim hosts # ansible_ssh_host：遠端 SSH 主機位址 # ansible_ssh_port：遠端 SSH Port # ansible_ssh_user：遠端 SSH 使用者名稱 # ansible_ssh_private_key_file：本機 SSH 私鑰檔案路徑 # ansible_ssh_pass：遠端 SSH 密碼 (建議使用私鑰) [local] server1 ansible_ssh_host=127.0.0.1 ansible_ssh_port=55000 ansible_ssh_pass=docker 所以我們可以在這邊輸入很多個主機來做管理，可以把它想成一個設定檔。\n什麼是 Ansible Playbooks 再談 Ansible Playbooks 之前，先說明我們要怎麼去操作 Ansible？一般來說，我們可以使用 Ad-Hoc Commands 和 Playbooks 兩種方式來操作 Ansible。\nAd-Hoc Commands 是什麼？ Ad hoc 可以翻譯成簡短地指令，也就是我們常用的指令模式，最常見的 ping和echo 為例。\nping $ ansible all -m ping server1 | SUCCESS =\u003e { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": false, \"ping\": \"pong\" } echo $ ansible all -m command -a \"echo Hello World\" server1 | CHANGED | rc=0 \u003e\u003e Hello World 從上面的例子中可以看到 Ad-Hoc Commands 一次只能處理一件事情，這是它與 Playbooks 最大的差異。\nPlaybooks 是什麼？ Playbooks 就是字面上的意思為劇本，我們可以先透過寫好的劇本 (Playbooks) 來讓各個 Managed Node 進行指定的動作 (Plays) 和任務 (Tasks)。\n簡而言之，Playbooks 就是 Ansible 的腳本 (Script)，而且比傳統 Shell Script 還要強大好幾百倍的腳本！此外它是使用 YAML 格式，寫 Code 就如同寫文件一樣，簡單易讀。\n有關詳細的動作 (Plays) 和任務 (Tasks)，等我們實際安裝好再來說明 😆","ansible-發送通知到-telegram-bot#Ansible 發送通知到 Telegram Bot":"剛剛看了很多內建的模組，當然 Ansible 還有很多好玩的模組可以使用，我們就跟 使用 Jenkins 設定 GitHub 觸發程序並通知 Telegram Bot 文章 一樣，將我們取得的內容傳送到 Telegram Bot 吧！那首先我們要先創造一個 Telegram Bot，在 Telegram 找到一個機器人叫 BotFather 的官方機器人帳號。並使用指令 /newbot，會看到一下畫面：\nTelegram 創建機器人\n他詢問你要幫機器人取叫什麼名稱，可以直接在輸入欄位輸入想要取的名稱，當然不能是別人已經取過的：\nTelegram 創建機器人\n看到它回覆你 Done! 代表成功了，接下來你會拿到一組 API Token，像我的是 5335968936:AAEDO_Tudhy0t577jtbF9TpgrzqOsL99h9c (已更換，大家放心 😂 )，接下來開啟瀏覽器輸入以下網址 https://api.telegram.org/bot{API Token}/getupdates，其中的 {API Token} 請帶入自己的 Token，直到出現 {\"ok\":true,\"result\":[]} 代表完成。\n接下來開啟你自己的 Bot ，打上 /start 指令，重新整理剛剛的網頁就可以看到以下這樣的文字：\n{\"ok\":true,\"result\":[{\"update_id\":606594112,\"message\":{\"message_id\":1,\"from\":{\"id\":493995679,\"is_bot\":false,\"first_name\":\"\\u54c1\\u6bc5\",\"last_name\":\"Ian\",\"username\":\"pinyichuchu\",\"language_code\":\"zh-hans\"},\"chat\":{\"id\":493995679,\"first_name\":\"\\u54c1\\u6bc5\",\"last_name\":\"Ian\",\"username\":\"pinyichuchu\",\"type\":\"private\"},\"date\":1652695148,\"text\":\"/start\",\"entities\":[{\"offset\":0,\"length\":6,\"type\":\"bot_command\"}]}} 這是你傳訊息給 Bot 它所收到的 API，資料很多沒關係，我們找到 id，像我的是 493995679，這個就是我跟機器人的聊天室，我們就先回到 Ansible 這邊吧！\n開啟一個新的檔案叫 send_notify_tg.yaml，打以下內容：\n--- - name: Send notify hosts: all tasks: - name: Send notify to Telegram community.general.telegram: token: \"9999999:XXXXXXXXXXXXXXXXXXXXXXX\" api_args: chat_id: 000000 parse_mode: \"markdown\" text: \"Your precious application has been deployed: https://example.com\" disable_web_page_preview: True disable_notification: True 可以看到我們使用的模組不是 Ansible 內建的，而是社群別人寫的，詳細可以參考 community.general.telegram module – module for sending notifications via telegram：\n其中 token 就是我們剛剛在 BotFather 那邊所拿到的 Token，chat_id 就是我們剛剛在網頁上看到的 id，把資料都輸入進去後，我們可以修改 text 內容，改成 “Send notify to Telegram 測試傳送通知”，接著執行 ansible-ploybook send_notify_tg.yaml ，看看能不能正常收到通知！\n發送通知至 Telegram Bot\n這樣子就成功透過 Ansible Module 傳送通知給 Telegram 囉！\n我們可能需要將機器人加入群組內，這時候需要更換一下 chat_id，先將機器人加入群組，再次到剛剛瀏覽器的網頁刷新，查看 chat 後面的 id 帶有 -，像是 -540226836 這樣，這個就是該群組的 ID，將 send_notify_tg.yaml 的 chat_id 修改成 -540226836 在測試看看，他就會在群組中發送通知囉！\n{\"update_id\":606594124,\"message\":{\"message_id\":14,\"from\":{\"id\":493995679,\"is_bot\":false,\"first_name\":\"\\u54c1\\u6bc5\",\"last_name\":\"Ian\",\"username\":\"pinyichuchu\",\"language_code\":\"zh-hans\"},\"chat\":{\"id\":-540226836,\"title\":\"\\u54c1\\u6bc5 \u0026 AnsibleSendMessageBot\",\"type\":\"group\",\"all_members_are_administrators\":true},\"date\":1652696181,\"group_chat_created\":true}} 發送通知至 Telegram 群組 Bot","使用-ansible-的-template-系統#使用 Ansible 的 Template 系統":"Template module 是常使用的檔案模組之一，我們在 常用的 Ansible Module 有哪些？ 文章中有提到，可以用它和變數 (Variables) 來操作檔案。\n我們只需要事先定義變數和模板 (Templates)，即可用它動態產生遠端的 Shell Script、設定檔 (Configure)等。換句話說，我們可以用一份 template 來開發 (Development)、測試 (Test)、正式環境 (Production) 等不同環境設定。\n舉例說明：\n建立 template 檔案 $ vim hello_world.txt.j2 Hello \"{{ dynamic_word }}\" 由於 Ansible 是就由 Jinja2 來實作 template 系統，所以需要使用 *.j2 的副檔名。 上面的 \"{{ dynamic_word }}\"\" 代表我們在 template 裡使用了名為 dynameic_word 的變數。 建立 playbook，並加入變數 vim template_demo.yaml --- - name: Play the template module hosts: localhost vars: dynamic_word: \"World\" tasks: - name: generation the hello_world.txt file ansible.builtin.template: src: hello_world.txt.j2 dest: /tmp/hello_world.txt - name: show file context command: cat /tmp/hello_world.txt register: result - name: print stdout debug: msg: \"{{ result.stdout_lines }}\" 在第 5 行，我們幫 dynamic_word 變數設了一個預設值 World。 在 8 行的第 1 個 task 裡，我們使用 template module，並指定了檔案的來源 (src) 和目的地 (dest)。 之後的 2 個 task 則是把 template module 產生的檔案給印出來。 直接使用 ansible-playbook template_demo.yaml 執行 Playbook。 Template Module 範例\n也可以透過 -e 參數將 dynamic_word 覆寫成 “ansible”\n$ ansible-playbook template_demo.yaml -e \"dynamic_word=ansible\" Template Module 使用 -e 覆寫參數\n如何切換不同環境 除了我們剛剛用 vars 來宣告變以外，還可以使用 vars_files 來 include 其他的變數檔：$ vim template_demo2.yaml --- - name: Play the template module hosts: localhost vars: env: \"development\" vars_files: - vars/{{ env }}.yml tasks: - name: generation the hello_world.txt file ansible.builtin.template: src: hello_world.txt.j2 dest: /tmp/hello_world.txt - name: show file context command: cat /tmp/hello_world.txt register: result - name: print stdout debug: msg: \"{{ result.stdout_lines }}\" 可以看到上面例子中第 7 行，就是我們使用 vars_files 來 include 其他的變數檔。\n建立 vars/development.yaml、vars/test.yaml、vars/production.yaml 檔案，接下來將依不同得環境 include 不同的檔案變數檔案 (vars files)，這樣就可以用一份 Playbook 切換環境了！ Development $ vim vars/development.yaml dynamic_word: \"development\" Test $ vim vars/test.yaml dynamic_word: \"test\" Production $ vim vars/production.yaml dynamic_word: \"production\" 執行 ansible-playbook template_demo2.yaml -e \"dynamic_word=Test\"，並有 -e 去修改各個環境。 Template Module 範例\nTemplate 系統是實務上很常見的手法之一，藉由它我們可以很輕鬆地讓開發、測試、正式環境無縫接軌。但若是在大型的 Playbook 裡切換環境，建議使用較為進階的 group_vars 跟 host_vars。","參考資料#參考資料":"現代 IT 人一定要知道的 Ansible 自動化組態技巧：https://chusiang.gitbooks.io/automate-with-ansible/content/\nAnsible 安裝：https://tso-liang-wu.gitbook.io/learn-ansible-and-jenkins-in-30-days/ansible/ansible/ansible-installation\n怎麼用 Docker 練習 Ansible？：https://chusiang.gitbooks.io/automate-with-ansible/content/05.how-to-practive-the-ansible-with-docker.html\ncommunity.general.telegram module – module for sending notifications via telegram：https://docs.ansible.com/ansible/latest/collections/community/general/telegram_module.html#ansible-collections-community-general-telegram-module","取得-managed-node-的-facts#取得 Managed node 的 facts":"還記得我們在執行任務 (Tasks) 時，明明只有兩個，但最後結果顯示三個嗎？是因為在使用 Playbooks 時，Ansible 會自動執行 Setup module 以蒐集各個 Managed node 的 facts。 這個 facts 就好比是系統變數一樣，從 IP 位址、作業系統、CPU 等資訊應有盡有。\nAd-Hoc Commands 通常我們都會先使用 Ad-Hoc Commands 來呼叫 setup 看看有哪些可用的資訊，這對於我們稍後撰寫較為複雜的 Playbooks 會很有幫助。\n可以藉由 less 快速搜尋所有的變數 $ ansible all -m setup | less server1 | SUCCESS =\u003e { \"ansible_facts\": { \"ansible_apparmor\": { \"status\": \"disabled\" }, \"ansible_architecture\": \"x86_64\", \"ansible_bios_date\": \"03/14/2014\", \"ansible_bios_vendor\": \"BHYVE\", \"ansible_bios_version\": \"1.00\", \"ansible_board_asset_tag\": \"NA\", \"ansible_board_name\": \"NA\", \"ansible_board_serial\": \"NA\", \"ansible_board_vendor\": \"NA\", \"ansible_board_version\": \"NA\", 搭配 filter 將發行版本 (distribution) 資訊給過濾出來 $ ansible all -m setup -a \"filter=ansible_distribution*\" server1 | SUCCESS =\u003e { \"ansible_facts\": { \"ansible_distribution\": \"Ubuntu\", \"ansible_distribution_file_parsed\": true, \"ansible_distribution_file_path\": \"/etc/os-release\", \"ansible_distribution_file_variety\": \"Debian\", \"ansible_distribution_major_version\": \"22\", \"ansible_distribution_release\": \"kinetic\", \"ansible_distribution_version\": \"22.10\", \"discovered_interpreter_python\": \"/usr/bin/python3\" }, \"changed\": false } 取得套件管理員的種類資訊，例子中取得的值是 apt $ ansible all -m setup -a \"filter=ansible_pkg_mgr\" server1 | SUCCESS =\u003e { \"ansible_facts\": { \"ansible_pkg_mgr\": \"apt\", \"discovered_interpreter_python\": \"/usr/bin/python3\" }, \"changed\": false } 轉寫 Playbooks 我來出個題目，我想要知道 Ansible 所使用的公鑰，並透過 Telegram Bot 發送到群組，要怎麼做呢！？\n首先要利用剛剛的 Ad-Hoc Commands filter，找到公鑰，再將公鑰透過 Telegram Bot 傳送，所以我們會有兩個 Tasks，那我們開始實作囉 🤓\n1.找到公鑰\n--- - name: Filter rsa_public \u0026 Send notify hosts: all tasks: - name: Filter setup rsa_public key ansible.builtin.setup: filter: - \"ansible_ssh_host_key_rsa_public\" register: result 可以看到我們將 filter setup 從 Ad-Hoc 轉成 Playbooks，並使用 result 來存在找到的公鑰。\n發送通知至 Telegram Bot - name: Send notify to Telegram community.general.telegram: token: \"5335968936:AAFhxxMRJy-rucGKgSE80Xss7qPq2iOHWlc\" api_args: chat_id: -540226836 parse_mode: \"markdown\" text: \"{{ result }}\" disable_web_page_preview: True disable_notification: True 老樣子，我們就使用上次 send_notify_tg.yaml 內的 Send notify to Telegram 任務來傳送通知。\n執行後，看看群組是否有收到我們找到的 ansible_ssh_host_key_rsa_public 通知。\n發送通知至 Telegram 群組 Bot","在-playbooks-使用-handlers#在 Playbooks 使用 Handlers":"Handlers 是我們在 Ansible Playbooks 裡很常用來重開系統服務 (Service) 的手法，我們這邊透過安裝 Nginx 來介紹它。\n那什麼是 Handlers 呢？Handler 本身是一種非同步的 callback function ; 在這裡則是指關聯於特定 tasks 的事件 (event) 觸發機制。當這些特定的 tasks 狀態為 被改變 (changed) 且都已被執行，才會觸發一次的 event。\n我們建立 setup_nginx.yaml --- - name: setup the nginx hosts: all become: true vars: username: \"PinYi\" mail: \"880831ian@gmail.com\" blog: \"https://pin-yi.me\" tasks: # 執行 'apt-get update' 指令。 - name: update apt repo cache apt: name=nginx update_cache=yes # 執行 'apt-get install nginx' 指令。 - name: install nginx with apt apt: name=nginx state=present # 於網頁根目錄 (DocumentRoot) 編輯 index.html。 - name: modify index.html ansible.builtin.template: src=templates/index.html.j2 dest=/var/www/html/index.html owner=www-data group=www-data mode=\"644\" backup=yes notify: restart nginx # handlers # # * 當確認事件有被觸發才會動作。 # * 一個 handler 可被多個 task 通知 (notify)，並於 tasks 跑完才會執行。 handlers: # 執行 'sudo service nginx restart' 指令。 - name: restart nginx service: name=nginx enabled=yes state=restarted # post_tasks: # # 在 tasks 之後執行的 tasks。 post_tasks: # 檢查網頁內容。 - name: review http state command: \"curl -s http://localhost\" register: web_context # 印出檢查結果。 - name: print http state debug: msg={{ web_context.stdout_lines }} 來說明一下上面這個 yaml 檔案：\n首先我們想要安裝 Nginx，我們給了三個參數，分別是 username、mail、blog，等等會帶入我們的 template。 我們一開始有 3 個 task，分別代表執行更新、安裝、編輯 index.html 檔案。 以及 1 個 handlers 他會等 modify index.html 有改變且執行後才會動作。 最後是 post_tasks 他是等 tasks 之後執行的 tasks。 接下建立 Nginx html 的 template：vim templates/index.html.j2 _____________________________________ / This is a ansible-playbook demo for \\ \\ automate-with-ansible at 2022/05/17./ ------------------------------------- \\ ^__^ \\ (oo)\\_______ (__)\\ )\\/\\ ||----w | || || [ {{ username }}@automate-with-ansible ~ ]$ [ {{ username }}@automate-with-ansible ~ ]$ [ {{ username }}@automate-with-ansible ~ ]$ cat .profile - {{ mail }} - {{ blog }} 執行 Playbook 可以看到因為我們 modify index.html 沒有被改變，notify 沒有通知 handlers，所以他不會執行 handlers 該段程式。(正常來說，修改 html 不需要重啟，此為範例 🤣 )\nHandlers 範例\n那我們修改一下 index.html 來測試一下會不會把 index.html 的狀態被改變，而讓 handlers 執行呢！我們隨意修改 index.html 內容，修改日期改成 05/17： Handlers 範例\n可以看到我們的 modify index.html 被改變了，所以 notify 通知 handlers 執行重新啟動。","在-playbooks-使用-loops#在 Playbooks 使用 loops":"在 Shell Script 中，我們會使用 for 和 while 等迴圈 (loop) 來簡化重複的程式碼，而在 Ansible 我們也可以使用 loop 來簡化重複的任務 (Tasks)。\n標準迴圈 首先我們先以簡單的方式重複印出三筆資料。\nShell Script 建立 for loop 的 Script $ vim bash_loop.sh #!/bin/bash for x in 0 1 2; do echo Loop $x done 在第 4 行，我們用 for，並 \b 代入 0,1,2 三個值到 $x 變數 在第 5 行，則用了 echo，印出訊息和 $x 變數 執行 Script：可以看到底下跑了 3 次的 loop $ chmod a+x bash_loop.sh $ ./bash_loop.sh Loop 0 Loop 1 Loop 2 Ansible Playbooks 我們需要透過 item 和 with_items 來使用 Ansible 的 loop，其 item 為預設名。在 Ansible 2.5 中添加了 loop，所以我們後續兩者都會提到 (目前兩者都可以使用！)\n建立 loop 的 playbook vim playbook_with_items.yaml --- - name: a basic loop with playbook hosts: localhost tasks: - name: print loop message ansible.builtin.debug: msg: \"Loop {{ item }}\" with_items: - 0 - 1 - 2 在第 6、7 行裡，我們用 debug module 來印出訊息，並定義 item 在第 8 ~ 11 行，則用了 with_item 將 0,1,2 的值傳入 item 執行 ansible-playbook playbook_with_items.yaml 後會得到： TASK [print loop message] ************************************************************************************************************* ok: [server1] =\u003e (item=0) =\u003e { \"msg\": \"Loop 0\" } ok: [server1] =\u003e (item=1) =\u003e { \"msg\": \"Loop 1\" } ok: [server1] =\u003e (item=2) =\u003e { \"msg\": \"Loop 2\" } 另一種 在 Ansible 新增的 loop\n建立 loop 的 playbook vim playbook_loop.yaml --- - name: a basic loop with playbook hosts: all tasks: - name: print loop message ansible.builtin.debug: msg: \"{{ item }} {{ my_idx }}\" loop: - Loop - Loop - Loop loop_control: index_var: my_idx 執行 ansible-playbook playbook_loop.yaml 後會得到： TASK [print loop message] ************************************************************************************************************* ok: [server1] =\u003e (item=0) =\u003e { \"msg\": \"Loop 0\" } ok: [server1] =\u003e (item=1) =\u003e { \"msg\": \"Loop 1\" } ok: [server1] =\u003e (item=2) =\u003e { \"msg\": \"Loop 2\" } 會使用 Loop 就可以減少我們在寫重複的程式碼，當然上面的只是簡單的範例，詳細請參考 Loops - Ansible Documentation。","常用的-ansible-module-有哪些#常用的 Ansible Module 有哪些？":"接下來簡單介紹一下比較常用到的 8 個 Module：\nansible.builtin.apt apt module 是給 Debian, Ubuntu 等作業系統使用的套件模組 (Packing Modules)，我們可以透過它管理 apt 套件。類似的有 apt-get、dpkg等。\n更新套件索引(快取)，等同於 apt-get update 指令 - name: Update repositories cache ansible.builtin.apt: update_cache: yes 安裝 vim 套件 - name: Install the package \"vim\" ansible.builtin.apt: name: vim state: present 移除 nano 套件 - name: Remove \"nano\" package ansible.builtin.apt: name: nano state: absent ansible.builtin.command command module 是可以在遠端上執行指令的指令模組，但它不支援變數 (variables) 和 \u003c、\u003e、|、;、\u0026，若有這類需求要改用 shell module。\n重新開機 - name: Reboot at now ansible.builtin.command: /sbin/shutdown -r now 當某個檔案不存在時才執行指令 - name: create .ssh directory ansible.builtin.command: mkdir .ssh creates=.ssh/ 先切換目錄再執行指令 - name: cat /etc/passwd ansible.builtin.command: cat passwd args: chdir: /etc ansible.builtin.copy copy moudule 是從本地複製檔案到遠端的檔案模組，若有使用變數需求，可以改用 template。它類似 Linux 指令的 scp。\n複製 ssh public key 到遠端 (chmod 644 /target/file) - name: copy ssh public key to remote node ansible.builtin.copy: src: files/id_rsa.pub dest: /home/docker/.ssh/authorized_keys owner: docker group: docker mode: 0644 複製 ssh public key 到遠端 (chmod u=rw,g=r,o=r /target/file) - name: copy ssh public key to remote node ansible.builtin.copy: src: files/id_rsa.pub dest: /home/docker/.ssh/authorized_keys owner: docker group: docker mode: \"u=rw,g=r,o=r\" 複製 nginx vhost 設定檔到遠端，並備份原有的檔案 - name: copy nginx vhost and backup the original ansible.builtin.copy: src: files/ironman.conf dest: /etc/nginx/sites-available/default owner: root group: root mode: 0644 backup: yes ansible.builtin.file file module 是在遠端建立和刪除檔案 (file)、目錄 (directory) 和軟連結 (symlinks) 的檔案模組。它類似的 Linux 指令為 chown、mkdir 和 touch。\n建立檔案 (touch)，並設定權限為 644 - name: touch a file, and set the permissions ansible.builtin.file: path: /etc/motd state: touch mode: \"u=rw,g=r,o=r\" 建立目錄 (mkdir)，並設定檔案擁有者為 docker - name: create a directory, and set the permissions ansible.builtin.file: path: /home/docker/.ssh/ state: directory owner: docker mode: \"700\" 建立軟連結 (ln) - name: create a symlink file ansible.builtin.file: src: /tmp dest: /home/docker/tmp state: link ansible.builtin.lineinfile lineinfile module 是個可用正規表示法對檔案進行插入或取代文字的檔案模組。它類似的 Linux 指令是 sed。\n移除 docker 使用者的 sudo 權限 - name: remove sudo permission of docker ansible.builtin.lineinfile: dest: /etc/sudoers state: absent regexp: \"^docker\" 在 /etc/hosts 檔案裡用 127.0.0.1 localhost 取代開頭為 127.0.0.1 的一行 - name: set localhost as 127.0.0.1 ansible.builtin.lineinfile: dest: /etc/hosts regexp: '^127\\.0\\.0\\.1' line: \"127.0.0.1 localhost\" owner: root group: root mode: 0644 ansible.builtin.service service module 是用來管理遠端系統服務的系統模組。它類似的 Linux 指令為 service。\n啟動 Nginx - name: start nginx service ansible.builtin.service: name: nginx state: started 停止 Nginx - name: stop nginx service ansible.builtin.service: name: nginx state: stopped 重開網路服務 - name: restart network service ansible.builtin.service: name: network state: restarted args: eth0 ansible.builtin.shell shell module 是可以在遠端用 /bin/sh 執行指令的指令模組，支援變數 (variables) 和 \u003c、\u003e、|、; 和 \u0026 等運算。\n藉由 ls 和 wc 檢查檔案數量 - name: check files number ansible.builtin.shell: ls /home/docker/ | wc -l 把所有的 Python 行程給砍掉 - name: kill all python process ansible.builtin.shell: kill -9 $(ps aux | grep python | awk '{ print $2 }') ansible.builtin.stat stat module 是用來檢查檔案狀態的檔案模組。其類似的 Linux 指令為 stat。\n檢查檔案是否存在，若不存在則建立他。 - name: check the 'vimrc' target exists ansible.builtin.stat: path: /home/docker/.vimrc register: stat_vimrc - name: touch vimrc file: path: /home/docker/.vimrc ansible.builtin.state: touch mode: \"u=rw,g=r,o=r\" when: stat_vimrc.stat.exists == false 取的某檔案的 md5sum - name: Use md5sum to calculate checksum ansible.builtin.stat: path: /path/to/something checksum_algorithm: md5sum 其他 其他還有很多可以使用的 Module ，詳情可以查看 Ansible.Builtin。","第一個-playbook#第一個 Playbook":"在我們都安裝好後，要來說說我們剛剛有偷偷提到的 Playbooks 的動作 (Plays) 和任務 (Tasks)。在一份 Playbooks 裡面，可以有多個 Play、多個 Task 和多個 Module：\nPlay：通常為某個特定的目的，例如： Setup a official website with Drupal 藉由 Drupal 建置官網 Restart the API Service 重開 API 服務 Task：要實行 Play 這個目的所需做的每個步驟，例如： Install the Nginx 安裝 Nginx Kill the djnago process 強制停止 django 的行程 Module：Ansible 所提供的各種操作方式，例如： apt: name=vim state=present 使用 apt 套件安裝 vim command: /sbin/shutdown -r now 使用 shutdown 的指令關機 有點聽不懂吧！我來舉個例子，我們最熟悉的 Hello World，先建立一個 helloworld.yaml 的檔案：\nhelloworld.yaml--- - name: say 'hello world' hosts: all tasks: - name: echo 'hello world' command: echo 'hello world' register: result - name: print stdout debug: msg: \"{{ result.stdout }}\" 可以看到這整個就是 Play，我們想要達到 say ‘hello world’ 的目的，其中有兩個 name 分別代表兩個 Task，也就是達成 Play 目的所需得步驟。最後 command 與 debug 就是我們的 Module 要怎麼達成這兩個步驟的操作方式。\nPlaybooks 組成結構\n我們使用 ansible-playbook 執行 Playbook，在這個範例中，我們執行了１個 Play、3 個 Tasks 和 2 個 Modules：\n$ ansible-playbook helloworld.yaml 執行 Playbooks\n我們剛剛明明只寫兩個 tasks，為什麼執行就變成三個 tasks？\n這是因為 Ansible 預設會使用 Setup task 來取得 Managed node 的 facts。關於 facts 的詳細說明，請滑到後面 取得-managed-node-的-facts 觀看 😬\n那如果沒有 Ansible 時，我們是怎麼操作的？我會附上 Shell Script 的做法，我們來比較看看吧！\nShell Script 建立 helloworld.sh 檔案 helloworld.sh#! /bin/bash echo \"Hello World\" 執行 helloworld.sh ./ helloworld.sh Hello World 看起來 Shell Script 已經夠用了，為什麼還要寫 Playbook 呢？這邊整理幾個理由給大家參考：\n用 Ansible 的 Module 可以把很多複雜的指令給標準化，例如不同的 Linux 發行版本在安裝套件時需代各種不同的參數。 在現有的雲原生 (cloud native) 的架構下，傳統的 Shell Script 已經不敷使用，一般而言 Shell Script 只能對一台機器 (instance) 進行操作。 "},"title":"Ansible 介紹與實作 (Inventory、Playbooks、Module、Template、Handlers)"},"/blog/git-or-cicd/git-introduce/":{"data":{"":"","git-常見問題#Git 常見問題":" Git 裡的 HEAD 是什麼？ HEAD 本身是一個指標，通常會指向某個本地端分支或是其他 Commit，所以也可以把 HEAD 當作目前所在的分支。\n刪除合併後的分支會發生什麼事情嗎？ 分支本身就像指標或是貼紙一樣，貼在某個 Commit 上面，分支並不是目錄或是檔案，所以當我們刪除已經合併過的分支，不會造成檔案或目錄跟著被刪除。","git-操作指令#Git 操作指令":"Git 的操作指令繁多，包含環境類、查看類、提交類、分支類、遠端類、合併類、還原類等等，所以才有了 Git GUI 工具，筆者很推薦 Gitkraken，雖然需要付費，但真的很方便，畫面也很乾淨簡潔。如果是學生的話，還可以使用 GitHub Student Developer Pack 免費使用歐！\nGit GUI 工具 (Gitkraken)\n接下來我們會依照環境類、查看類、提交類、分支類、遠端類、合併類、還原類依序下去介紹～\n環境類 使用每一個程式或工具，必須先把它安裝到自己電腦上對吧！但因為大家使用的系統都不一樣，所以這邊就不列出要怎麼進行安裝，可以參考 Git 安裝教學\n當我們安裝好後，我們就可以一起進入 Git 的世界囉！\ninit 首先，找一個你要開始進行版本控制 (Git) 的資料夾， 使用：\n$ git init 要記得要到版本控制的資料夾目錄下才使用這個指令歐！\n使用完後，會看到跑出下面這些文字：\nGit init\n此外資料夾內也會多一個隱藏檔案 .git ，他是用來存放 git 的紀錄，所以不要亂刪除歐：\n$ tree -a -d . └── .git ├── hooks ├── info ├── objects │ ├── info │ └── pack └── refs ├── heads └── tags 回到剛剛圖片，它說明默認會使用 master 這個來作為初始分支，並記得要使用 git config 來做設定，那 git config 是要做什麼用的呢！？\nconfig 在推送 Commit 的時候，會顯示使用者名稱以及電子郵件，所以要先在推送前設定好，這時就使用：\n$ git config --global user.name \"your name\" $ git config --global user.email \"your email\" 分別設定使用者名稱以及電子郵件，這樣共同使用版本控制的人，才分的出來誰是誰！ (若只要在單個專案下設定使用者名稱及電子郵件，就不需要設定 –global 參數)\n查看類 status 我們剛剛已經設定好 config ，如果想查看檔案的 git 狀態，就使用：\n$ git status 可以查看現在資料夾內有哪些檔案還沒加入版本控制，或是已經加入但還沒 Commit 變成新版本。\nGit status\n綠色代表已經加入版本控制但還沒有 Commit ，紅色代表尚未加入追蹤。\nlog 當我們想要查看 Commit 的幾個版本，或是是誰 Commit 的等等資訊，可以使用：\n$ git log Git log\n可以看到一串英文加數字，它是SHA-1 校驗碼也代表你推的這一個版本的識別 ID，也可以看到是由誰推送跟時間與推送的文字說明。\ndiff 當我們假設已經 Commit 後，想要比較不同版本內容的差異，就使用：\n$ git diff 116e 442c 116e 代表最新的版本，442c 代表上一個版本，可以看上面 log 的辨識 ID，因為校驗碼很長，最少需要前4個數字跟英文，才可以知道是哪一個版本。\nGit diff\n紅色代表最新版本因為我們是用 116e 最新版本來跟綠色上一個版本 442c 來做比較。\nreflog 如果我們在操作 Git 的時候執行錯誤，需要回復到前幾個版本，但還想要查看歷史紀錄，如果我們使用前面說的 git log 是看不到已經舊的紀錄，這時要使用：\n$ git reflog Git reflog (圖一)\nGit log (圖二)\n可以看到圖一是使用 reflog 就可以知道我們還原的紀錄，但使用 log 查看，會發現沒有辦法看到 test-2 的紀錄。\n提交類 add 當我們上面使用 git init 初始化資料夾後，還沒有開始進行版本控制，需要使用：\n$ git add . $ git add test.txt 將檔案加入版本控制的暫存區。它的格式是 git add [檔案名稱] ，如果想要把資料夾全部檔案都加入版本控制，可以使用 . 來加入。\ncommit 當我們新增完後，要將它提交出去，就要使用：\n$ git commit -m \"內容打這\" Git commit\n我們可以在雙引號內輸入我們修改的內容，方便其他人了解該版本的差異。\n分支類 branch 我們專案初始化後，一開始都只會有一個 master 分支，如果想要新增分支，可以使用：\n$ git branch \"分支名稱\" 會將所在分支的檔案狀態複製到新增的分支上，當該分支改動時，不會影響到原本的分支。\nGit branch\ncheckout 如果想要切換不同版本或是分支，就可以使用：\n$ git checkout \"分支名稱/分支ID\" 來切換不同的分支或是以前的版本。\nGit checkout\n如果想要同時建立分支並切換，可以使用：\n$ git chechout -b \"分支名稱\" -b 這個參數就等於是 git branch \"分支\" \u0026 git checkout \"分支\"。\nGit checkout -b\n遠端類 clone 如果我們遠端上已經有版本庫，想要下載到本地端，就可以使用：\n$ git clone [遠端網址] 會在下指令的當前路徑下，下載整個遠端的專案。\nremote 如果要新增遠端版本庫，就可以使用：\n$ git remote add [簡稱] [遠端網址] 取一個可以代表要新增的遠端 Git 版本庫簡稱。\nGit remote\n如果想要檢視已經設定好的遠端版本庫，就可以使用：\n$ git remote 他會列出每個遠端本版本庫的簡稱。\n也可以使用 git remote -v 指令，會顯示 Git 用來讀寫遠端簡稱時所用的網址：\nGit remote -v\npush 當我們已經設定好遠端版本庫的位址，我們如果想要將本地端的專案版本庫放到遠端，就可以使用：\n$ git push [簡稱] [分支名稱] 將本地端版本庫推到遠端的版本庫。\nGit push\n上面這張圖片，就是把本地端的 master 分支內容，推一份到 origin 這個地方 (可能是 GitHub 或公司內部 Git 伺服器)，並且在 origin 這個地方形成同名的 master 分支。\n但很多人不知道的是，其實 push 指令的完整型態長這樣：\n$ git push origin master:master 意思就是「把本地的 master 分支內容，推一份到 origin 上，並且在 origin 上建立一個 master 分支」\n如果我們把指令改為：\n$ git push origin master:dog 意思就會變成「把本地的 master 分支內容，推一份到 origin 上，並且在 origin 上建立一個 dog 分支」\npull 如果我們想要將遠端的專案 下載並合併 至本地端，就可以使用：\n$ git pull [簡稱] [分支名稱] 將遠端專案資料下載並合併到本地端。\nGit pull\nfetch 如果我們單純只想要將遠端的專案 下載 至本地端，就可以使用：\n$ git fetch [簡稱] [分支名稱] 將遠端專案資料下載到本地端。\nGit fetch\nclone、pull、fetch 差異 差異 clone fetch pull 功能 會把遠端整份專案都下載到本地端 只會下載，並不會合併 會下載且合併檔案 補充說明 適用於專案一開始時使用，如果 clone 之後要再更新，通常是執行 git fetch or git pull 假設我遠端叫 orgin，當執行時，Git 會比對本地端與遠端，會「下載 origin 上有但本地端沒有的檔案下來」 pull 與 fetch 做的事情差不多，多了一個進行合併的功能 合併類 merge 如果想要將不同分支內容合併，就可以使用：\n$ git merge [分支名稱] 像下面這張圖，我們將分支 123 合併到分支 master。(要記得先切換到要合併的主分支，才可以合併其他的分支進來)\nGit merge\nmerge (fast-fastward) merge 有一個參數叫做 fast-fastward，我們先看圖片再來說明：\nGit merge fast-fastward 圖一 (CS Visualized: Useful Git Commands)\nGit merge no fast-fastward 圖二 (CS Visualized: Useful Git Commands)\n圖一是我們的 fast-fastward，也是 Git 預設的合併方式，當我們要將 dev 合併到 master，fast-fastward 會將 dev 分支的 commit 紀錄合併到 master 上，然而圖二是不使用 fast-fastward 方式，會保留 dev 分支上的 commit 紀錄，並在 master 上新增一個。\nno fast-fastward 好處是可以完整保留每個分支的 commit 紀錄，壞處是假如 commit 紀錄只有一個，合併多次就會出現很多小叉路。要怎麽使用 no fast-fastward：\n$ git merge --no-ff [分支名稱] rebase (合併) 如果想要重新修改特定分支的「 基礎版本 」，要把另一個分支的變更，當成我這隻分支的基礎，就可以使用：\n$ git rebase [分支名稱] Git rebase (合併) (CS Visualized: Useful Git Commands)\nrebase (修改) 如果想要修改特定分支上任何一個版本資訊，就可以使用：\n$ git rebase -i [HEAD~?] 但要記得，如果再使用前，要先詢問是否有人正在使用此分支，因為 rebase 會改變歷史紀錄。\nGit rebase (修改) (CS Visualized: Useful Git Commands)\ncherry-pick 如果想要從某個分支，拉幾個 Commit 進來該分支，就可以使用：\n$ git cherry-pick [分支ID] 但做此動作，需要解決修改後的版本衝突。\n還原類 reset 如果想要還原任意 Commit，就可以使用：\n$ git reset [HEAD~?] 會還原選擇的 Commit，且檔案還是維持最新版本。\nreset 指令可以搭配參數使用，常見到的三種參數，方別是 --mixed、--soft、--hard，不同的參數執行之後會有稍微不太一樣的結果。\nmixed 模式：--mixed 是預設的參數，如果沒有特別加其他參數，got reset 會使用 --mix 模式。這個模式會把暫存區的檔案丟掉，但不會影響到工作目錄的檔案，也就是說 Commit 拆出來的檔案會留在工作目錄(實體的檔案)，但不會留在暫存區。 soft 模式：這個模式下的 reset，工作目錄跟暫存區檔案都不會被丟掉，所以看起來只有 HEAD 的移動而已。也因此，Commit 拆出來的檔案會直接放在暫存區。 hard 模式：在這個模式下，不管是工作目錄以及暫存區的檔案都會丟掉。 以下用表格在整理一次：\n模式 mixed 模式 soft 模式 hard 模式 工作目錄(實體的檔案) 不變 不變 丟掉 暫存區 丟掉 不變 丟掉 文字說明也不太懂對吧！沒錯我也是 😂，所以我整理了三種不同的範例，我們一起做看看吧！\n我們先開一個新專案，在 master 上面 commit 2 次，可以參考下方圖片：\nGit reset 示範\n我們用 git log 來看一下記錄：\nGit reset 示範 log 紀錄\n都設定好後，我們要來測試每個參數的不同之處，先以預設的 --mixed 來測試：\nreset - mixed 我們下 git reset --minxed 按 Tab 可以看要還原的 commit，我們之後的測試都是還原到 24aeb0d -- [HEAD^] add a.txt 這個，來觀察 add b.txt 這個 commit 的變化。\nGit reset mixed 模式\n所以我們的指令是 git reset --mixed 24aebo4，我們再來觀看看看，檔案狀態也就是 b.txt 以及暫存區狀態。\nGit reset mixed 模式\n可以看到使用 --mixed 模式，檔案 b.txt 還會存在，只是移除暫存區。\nreset - soft 我們指令是 git reset --soft 24aebo4：\nGit reset soft 模式\n可以看到使用 --soft 模式，檔案 b.txt 還會存在，且會在暫存區。\nreset - hard 我們指令是 git reset --hard 24aebo4：\nGit reset hard 模式\n可以看到使用 --hard 模式，檔案 b.txt 不見了，所以也不會在暫存區。\nrevert 如果想要還原任意 Commit，但又想保留在歷史紀錄，就可以使用：\n$ git revert [HEAD~?] 會還原選擇的 Commit，檔案也會還原到舊的版本\nreset 與 revert 差異 指令 reset revert 改變歷史狀態 是 否 說明 把目前狀態設定成某個指定的 Commit 狀態，通常適用於尚未推到遠端的 Commit 新增一個 Commit 來取消另一個 Commit 的內容，原本的 Commit 依舊會保留在歷史紀錄中。通常適用於已經推到遠端的 Commit 其他 tag 標籤是用於標記特定的點或是提交的歷史，通常會用來標記發佈版本的名稱或是編號，例如：v1.0。標籤看起來有點像是分支，但打上標籤的提交是固定的，不能隨意的變更位置。\nGit 中有兩種標籤類型：輕量標籤(lightweight tag)和標示標籤(annotated tag)，他們有什麼區別呢？我們分別列出他們不同之處。\n輕量標籤(lightweight tag)\n不可以變更的暫時標籤 可以添加名稱 標示標籤(annotated tag)\n可以添加打標籤者的名稱、email、日期 可以添加名稱 可以添加註解 可以添加簽名 一般情況下，標示標籤都會用在較為重要的提交上，如發布提交可以使用標示標籤來新增註解或簽名，另一方面，輕量標籤通常使用在本機端最為暫時性的使用或是一次性使用。\n我們分別來看一下要如何新增輕量標籤(lightweight tag)以及標示標籤(annotated tag)吧！\n我們先隨意在分支上推一次 commit ，如下圖，讓我們等等有 commit 可以來新增標籤：\n在隨意的分支推一個 commit\n輕量標籤 使用 tag 且不帶其他的參數來下指令：\n$ git tag lightweight bc4c597 lightweight 是我們 tag 名稱，bc4c597 是剛剛 commit 的 SHA-1\n接著我們使用 git show lightweight 來查看標籤：\n輕量標籤 lightweight\n可以發現因為我們使用「輕量標籤」，所以沒有存任何資訊，但可以在圖片第一行最後面看到我們使用的 tag。\n標示標籤 我們一樣使用 tag，但後面可以加上 -a -m 參數：\n$ git tag annotated bc4c597 -a -m \"可以備註\" -a 參數是請 Git 幫我們建立有附註的標籤，後面的 -m 則是跟我們 commit 一樣可以來輸入訊息\n接著我們使用 git show annotated 來查看標籤：\n標示標籤 annotated\n可以看到我們使用標示標籤，所以可以查看標籤是誰填寫、他的信箱、填寫時間以及他的備註內容。\n官方文件對於這兩種標籤的說明：\n有標示標籤主要用來做像是軟體版號之類的用途，而輕量標籤則是來於個人使用或暫時的標記用途。簡單來說，有標示標籤的好處是有更多關於這張標籤的資訊，假設不是很在乎這些資訊，使用一般的輕量標籤也是沒有問題的！","什麼是-git-#什麼是 Git ?":"不管是不是工程師，只要常常需要使用電腦工作，每天一定都會新增、修改、刪除許多檔案，我們看到這張圖：\n很多人的電腦裡面都有這樣的內容\n這張圖是一個菜鳥工程師在整理檔案時的方法，因為每一天都會對這份檔案做不同的處理，但為了保留以前的版本，所以也不會刪除舊的檔案，只好用日期或是版本來做分類，時間越久，檔案就累積越多，假如不小心刪除，也找不回來紀錄，也不清楚不同檔案的差異，所以有了 Git 這項工具。\nGit 為分散式版本控制系統，是為了更好管理Linux內核而開發的。 Git 的優點：免費開源、速度快、檔案體積小、分散式系統。 Git 的缺點：指令繁雜，但可以透過 GUI 工具解決。 Git 會紀錄哪些資料：更動前 vs 更動後的程式碼、修改者、修改時間、修改原因（修改者需要自行撰寫 commit message）。 我們也常常聽到 GitHub or GitLab 那跟 Git 是一樣的東西嗎？\nAns：GitHub(GitLab) 是基於 Web 的平台，結合了 Git 的版本控制功能，為開發團隊提供了儲存、分享、發布和合作開發項目的中心化雲存儲的場所。","參考資料#參考資料":"什麼是 Git？為什麼要學習它？：https://gitbook.tw/chapters/introduction/what-is-git\nGit：基本概念介紹與指令：https://medium.com/@tina2793778/git-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E4%BB%8B%E7%B4%B9%E8%88%87%E6%8C%87%E4%BB%A4-d5d85607cd7d\n[Git] 初始設定：https://ithelp.ithome.com.tw/articles/10240965\n連猴子都能懂的Git入門指南：https://backlog.com/git-tutorial/tw/\nGit教學】分支合併: merge 與 rebase 差異：https://www.maxlist.xyz/2020/05/02/git-merge-rebase/\nGit 面試題：https://gitbook.tw/interview\nReset、Revert 跟 Rebase 指令有什麼差別？：https://gitbook.tw/chapters/rewrite-history/reset-revert-and-rebase"},"title":"Git 介紹"},"/blog/git-or-cicd/git-merge-multiple-commit/":{"data":{"":"當我們在使用 Git 時，常常修改完內容後，會推 commit 到 github or gitlab，在一個分支上開發久了， commit 會累積很多，很雜且很亂，所以我們可以試著將 commit 給合併。\n大家可以使用這個檔案來做練習：點我 GoGo 😉\ngit commit\n可以看到上面這張圖，這個與範例檔案的 commit 相似(不同專案，所以 SHA-1 也會不同，為了模擬所以 commit 相同而已)，我們模擬在同一個分支底下，有很多的 commit，那我們試著把他給合併起來。先說明一下目前的 commit 狀況，我們在 master 分支上有 3 個 commit，且已經推到遠端上。所以我們本地修改後，還要讓遠端的也合併，這個步驟要怎麼做呢？大家可以先想想看，後面會告訴大家答案 🥰","參考資料#參考資料":"如何合併多個 commits：https://zerodie.github.io/blog/2012/01/19/git-rebase-i/\n【狀況題】聽說 git push -f 這個指令很可怕，什麼情況可以使用它呢？：https://gitbook.tw/chapters/github/using-force-push","合併本地端-commit#合併本地端 commit":"首先我們目的是想要讓 add 2.txt 與 add 3.txt 的 commit 合併成 add txt，可以先使用以下指令來找到他的 commit 的 SHA-1：\ngit log git log 查看 commit 的 SHA-1\n要怎麼合併呢？我們先使用 rebase 到不會變動的 commit，也就是 add 1.txt 這個 commit：\ngit rebase -i 3b5bab9d5fb65b965ae55236734103b178f9daf2 git rebase\n下完後，會跳出上面圖片內容，可以看到上面是 rebase interactive (-i) 要執行的指令，下面是每個指令的簡單說明，我們本次會使用的只有 pick 以及 squash，分別的意思是：\npick：會執行該 commit。 squash：會把這個版本的 commit 合併到前一個 commit。 所以我們要將它改成以下：\npick f8e5882 add 2.txt squash 3eb0ef4 add 3.txt 也就是將 3eb0ef4 這個版本的 commit 合併到 f8e5882 的 commit，對應我們的例子，將 add 3.txt 合併到 add 2.txt 這個 commit。\n儲存離開後，會跳出以下的畫面，他會告訴你原本兩個的 commit message 分別是 add 2.txt 以及 add 3.txt，這時候我們要輸入新的 commit message，也就是 add txt，建議可以把原本的訊息註解掉。\n輸入新的 commit\n儲存後，我們查看 git log，就可以看到我們將 add 2.txt 跟 add 3.txt 合併成 add txt 😝\n查看目前合併狀態的 git log","合併遠端-commit#合併遠端 commit":"可以看到下方是我們已經將本機端的 commit 給合併，但遠端還是一樣有 3 個 commit，如果我們就這樣直接推上去，只會多一次的 commit，所以我們該怎辦呢 ?\n遠端與本地端的 commit 不同\n我們就是要使用大家都害怕的：\ngit push -f 強制覆蓋掉分支上的內容，但切記切記，這個只適用於自己的分支上歐～不然會直接大爆炸 💣\n使用 git push -f 後的 commit"},"title":"如何合併多個 commit，且推到遠端呢？"},"/blog/git-or-cicd/gitlab-cicd/":{"data":{"":"自從上次學完 Jenkins 及 Ansible CI/CD，就覺得 CI/CD 實在太酷了！能夠自動化的去持續整合 (Continuous Integration, CI) 以及持續佈署 (Continuous Deployment,CD) 專案，再加上這幾天複習了 Git 的使用方法，突然想到，要怎麼設定我們將程式推到遠端的 Git Repo，能夠再搭配 CI/CD 去做測試，並且把程式碼自動部署到正式的服務機器設備上呢？\n那我們就開始囉！此篇會複習一下 CI/CD 並且說明 GitLab CI/CD 運作的原理，關於實作部分會放到下一篇文章 部署 Laravel 於 Heroku 搭配 GitLab CI/CD\n對了工商一下，剛剛有提到的 Jenkins 及 Ansible CI/CD 總共有 3 篇文章，還沒看過的可以先飛過去看一下歐 👇👇👇\nJenkins 及 Ansible IT 自動化 CI/CD 介紹\n使用 Jenkins 設定 GitHub 觸發程序並通知 Telegram Bot\nAnsible 介紹與實作 (Inventory、Playbooks、Module、Template、Handlers)\n老樣子文章也會同步到 Github，會附上範例中的程式碼，有需要的也可以去查看歐！ Github 程式碼連結 💓","gitlab-cicd#GitLab CI/CD":"GitLab CI/CD 是 GitLab 內建強大的工具，在 GitHub 被稱為 Github Actions，這個之後有空再來介紹 XD，回歸正題，GitLab CI/CD 可以讓我們持續整合和部署，且不需要使用第三方的應用程式來整合。我們來複習一下 持續整合 CI、持續部署 CD 他們是什麼吧：\n持續整合 CI 一個處於開發階段的專案或是軟體，它被我們放在 GitLab 的 Repository 裡，開發人員每天會推送不同的程式碼到 GitLab 上，GitLab 持續整合 CI 會在開發人員每次推送後，自動化的依照我們設定好的腳本進行建構與測試，從而減少開發中的專案發生錯誤的可能性。\n這種做法就被稱作持續整合，對於我們提交給專案的每一個更改、甚至是開發分支，它都是自動且持續地建構和測試，確保新加入的變動，符合我們在專案中所設計的所有測試。\n持續部署 CD 持續部署，讓我們不太需要手動的去部署專案服務，而是將其設置為自動化部署，完全不需要有人工去干涉，減少了人為的部署錯誤。\nGitLab CI/CD (圖片來源：GitLab Agile Planning)\n大致了解是如何運作之後，我們接著聊聊上面有提到的 設定好的腳本：\n.gitlab-ci.yml GitLab CI/CD 的工作原理是，要在專案根目錄新增一個名為 .gitlab-ci.yml 的文件 (記得文件名稱開頭有 . ，我一開始忘記要加，想說怎麼都沒有反應 😅 )，也就是我們上面說的 設定好的腳本，可以先將所需建構、測試和部署的腳本編寫完成，以及定義很多規則，例如執行命令的先後順序、部署應用程式的位置以及指定是否自動運行或是手動觸發腳本等。\n將 .gitlab-ci.yml 文件放入 Repository 裡，就會觸發 CI，負責管理的 GitLab-CI 就會依照 .gitlab-ci.yml 設定檔來啟動名為 GitLab Runner 的工具來運行腳本，這個 GitLab Runner 我們放到後面來說，我們先來說說 .gitlab-ci.yml 這個設定檔要怎麼編寫，以及編寫後的流程。\n這是一個示範的 .gitlab-ci.yml，選自優良的 GitLab XD，為了說明有小修改程式碼，程式碼也會放在 GitHub 上歐：\n.gitlab-ci.yml stages: - build - test - deploy cache: paths: - config/ build-job: stage: build script: - echo \"Hello, $GITLAB_USER_LOGIN!\" test-job1: stage: test script: - echo \"This job tests something\" test-job2: stage: test before_script: - echo \"This job tests something, but takes more time than test-job1.\" script: - echo \"After the echo commands complete, it runs the sleep command for 20 seconds\" - echo \"which simulates a test that runs 20 seconds longer than test-job1\" - sleep 20 deploy-prod: stage: deploy script: - echo \"This job deploys something from the $CI_COMMIT_BRANCH branch.\" 那我來簡單說明一下上面這些設定檔案的功能：\nstages：代表這個 CI 設定檔有三個 stage 要跑，一個是 build、一個 test、一個 deploy，他們的順序也決定 CI 運作的順序，由 build → test → deploy，假如 test 沒有通過，就不會執行 deploy。 cache：我們在寫 CI 時，常常需要裝 package，但我不想每次都重新跑一次，所以可以寫一個 cache，不要讓 GitLab 每次都重新拉新的 package。 build-job、test-job1、test-job2、deploy-prod：代表我有 4 個 job 要執行，每個 job 裡面有不同的任務，也是顯示在 Pipeline 的名稱。 stage：他現在要執行的階段，對應到 stages。 before_script：可以把它當先需要先執行的指令，後面才會執行主要的 script 指令。所以需要安裝的可以先寫在這裡面。 script：主指令，在實際運行的腳本中，通常會見到多行的指令被依序執行。 $CI_COMMIT_BRANC：當然 .gitlab-ci.yml 檔案也可以帶入參數，這個部分我們留到 部署 Laravel 於 Heroku 搭配 GitLab CI/CD 搭配實際操作來說明。 當然 .gitlab-ci.yml 有很多功能，上面只是簡單說明比較常用的，當你不確定自己寫的 CI 設定檔有沒有問題，沒關係就直接推上去，GitLab 還會先檢查一下設定檔是不是正確：\nGitLab CI/CD 檢查格式有錯\n當我們將 .gitlab-ci.yml 連同專案一起推到 GitLab 上後，我們可以看到它會開始執行我們所寫的腳本，會顯示整個執行過程：\nGitLab CI/CD 執行過程\n查看執行的狀態：\nGitLab CI/CD 狀態\n也可以在 GitLab Pipeline 看到執行的流程：\nGitLab CI/CD Pipeline\nGitLab CI Runner 我們上面有提到，我們在 CI 跑腳本，需要一個 Server 來代替 GitLab 來讓我們執行，這個 Server 我們稱為 Runner。我們來看一下整個執行的圖片：\nGitlab CI/CD 實際執行流程 (圖片來源：Gitlab-CI 入門實作教學 - 單元測試篇)\n那這個 Runner 有分成兩種：\n共享 Runner (Shared Runners) 自架 Runner (Specific Runners) 共享 Runner (Shared Runners) 因為本文章以及後續 部署 Laravel 於 Heroku 搭配 GitLab CI/CD 文章所使用的平台是 gitlab.com，由官方所提供，所以我們直接使用共享 Runner，可以在 repository Settings → CI / CD → Runners 中找到，有不少官方提供的共享 Runner 可以使用，也不需要做任何設定。\nGitLab CI/CD 共享 Runner\n但也有幾個缺點：\n因為是共享，所以 Server 資源也會共享，理論上多人使用的速度還是會比較慢。 以及如果是開源專案，是完全免費。但如果是私人專案，一個月有 400 分鐘的 CI 執行時間限制。 自架 Runner (Specific Runners) GitLab CI/CD 自架 Runner (圖片來源：Best Practice for DevOps on GitLab and GCP : GitLab Runner 簡介與安裝 - Day 7)\nGitLab Server 和 GitLab Runner 是 GitLab CI/CD 中不可或缺的兩者，但如果像公司是自架 GitLab，首先要先找一台電腦或是 Server 做為 Runner，那我們這邊以 Docker 作示範。\nGitLab Runner 的建議建置步驟如下：\n準備/安裝一個 GitLab Server (這邊我們直接使用 gitlab.com) 安裝一個與 GitLab Server 對應版本的 GitLab Runner 在安裝 GitLab Runner 的設備上設定 Executor 什麼是 Executor ?\n如果把 GitLab Runner 當成一個工廠來看，那 Executor 就是工廠內一個又一個的產線，同一個工廠內可以擁有不同種類的產線，Runner 與 Executor 之間的關係就是如此，這些產線會根據專案中 .gitlab-ci.yml 的內容，決定產線以及如何產出開發者期望的產品。\n另外 Executor 的種類非常多，可以看下方這些圖片，因為我們最常使用的就是 Docker，所以我們等等的範例，也是建置在 Docker 之上！\nGitLab Runner Executors\n那我們就開始來實作我們的 GitLab Runner 吧：\n首先，我們回去剛剛在 repository Settings → CI / CD → Runners 左側的 Specific runners GitLab Runner Executors\n可以看到一個註冊的 URL 以及 Token，這個我們在設定 Executor 會使用到！\n接下來開始安裝 GitLab Runner，我們使用 Docker，以下是 Docker 執行的指令：本此使用 gitlab-runner 版本是 alpine-v15.0.0 docker run -d --name gitlab-runner --restart always \\ -v ~/Shared/gitlab-runner/config:/etc/gitlab-runner \\ -v /var/run/docker.sock:/var/run/docker.sock \\ gitlab/gitlab-runner:alpine-v15.0.0 接著進入容器裡面，使用 docker exec -it gitlab-runner gitlab-runner register 來註冊，可以參考下方圖片，輸入 URL 以及 自己的 Token： GitLab Runner 註冊 Executors\n可以回到 gitlab.com 查看 Specific runners 下方是否多了我們剛剛所註冊的 GitLab-Runner GitLab Available specific runners\nGitLab CD GitLab CD 其實就是在 .gitlab-ci.yml 後面加上我們要部署的設定，透過 CI 整合完，我們可以設定他要部署到哪一台機器或是設備上這部分就放到下一篇文章直接用實作來告訴大家要怎麼使用吧！，請大家接續看下一篇 部署 Laravel 於 Heroku 搭配 GitLab CI/CD ，一起學習吧 GoGo !","參考資料#參考資料":"Get started with GitLab CI/CD：https://docs.gitlab.com/ee/ci/quick_start/\nBest Practice for DevOps on GitLab and GCP : GitLab CI/CD - Day 6：https://ithelp.ithome.com.tw/articles/10214114\nGitlab-CI 入門實作教學 - 單元測試篇：https://nick-chen.medium.com/gitlab-ci-%E5%85%A5%E9%96%80%E7%AD%86%E8%A8%98-%E5%96%AE%E5%85%83%E6%B8%AC%E8%A9%A6%E7%AF%87-156455e2ad9f\n如何使用 GitLab CI：https://medium.com/@mvpdw06/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-gitlab-ci-ebf0b68ce24b"},"title":"如何從頭打造專屬的 GitLab CI/CD"},"/blog/git-or-cicd/gitlab-package-registry-to-gcs/":{"data":{"":"今天接到一個案子，RD 部門之後想要使用 GitLab 的 Package Registry 功能來發布套件，且不想把它存在 GitLab 伺服器上，希望可以直接存到 GCP 的 Google Cloud Storage 上，所以才會有了此篇筆記來記錄一下整個過程。\n版本資訊\nGitLab 14.10 (有部分設定會於新版本棄用，請記得確認好自己的版本是否支援) 先說一下，我們的 GitLab 是使用 docker-compose 來建置，所以後續的實作內容都會以 docker-compose 的方式來介紹。","先查看尚未重啟的-gitlab-package#先查看尚未重啟的 GitLab Package":"由於公司 GitLab 預設有先開啟 packages_enabled，所以我就拿同事用 Helm 寫的 CI，來做測試。當更新 value.yaml 後會自動打包 Package 放到 Package Registry 中，我們直接進入到預設 Package Registry 的儲存位置，是在 /var/opt/gitlab/gitlab-rails/shared/packages/，用指令發現打包的 Package 的確存放於此 ，如下：\n檢查是否還有 Package 在預設儲存位置 (尚未遷移)","參考資料#參考資料":"GitLab Package Registry administration：https://docs.gitlab.com/14.10/ee/administration/packages/","啟動-gitlab-的-package-registry#啟動 GitLab 的 Package Registry":"首先，我們當然要先啟動這項 Package Registry 功能，才可以再之後使用它，我們先看一下 GitLab 啟動的 docker-compose.yml 檔案：\nversion: '3' services: gitlab: image: 'gitlab/gitlab-ee:14.10.5-ee.0' restart: always container_name: gitlab hostname: gitlab-pid logging: driver: \"json-file\" options: max-size: \"100m\" max-file: \"50\" environment: GITLAB_OMNIBUS_CONFIG: | external_url '${GITLAB_DOMAIN}' letsencrypt['enable'] = false gitlab_rails['initial_root_password'] = '${GITLAB_ROOT_PASSWORD}' gitlab_rails['gitlab_shell_ssh_port'] = '${GITLAB_HOST_SSH_PORT}' gitlab_rails['backup_keep_time'] = 79200 gitlab_rails['omniauth_allow_single_sign_on'] = ['google_oauth2'] gitlab_rails['omniauth_block_auto_created_users'] = false gitlab_rails['omniauth_sync_profile_from_provider'] = ['google_oauth2'] gitlab_rails['omniauth_sync_profile_attributes'] = ['name', 'email'] gitlab_rails['omniauth_providers'] = [ { 略過．．． } ] ports: - '${GITLAB_HOST_SSH_PORT}:22' - '${GITLAB_HOST_HTTP_PORT}:80' - '${GITLAB_HOST_HTTPS_PORT}:443' volumes: - './config:/etc/gitlab' - './logs:/var/log/gitlab' - './data:/var/opt/gitlab' 有些設定有略過或是省略不寫，大家就依照自己的設定來看就好～","新增-package-registry-設定#新增 Package Registry 設定":"我們在上方的 gitlab_rails['omniauth_providers'] = [ ... 略 ... ] 之後加上新增 Package Registry 設定內容：\ngitlab_rails['packages_enabled'] = true gitlab_rails['packages_object_store_enabled'] = true gitlab_rails['packages_object_store_remote_directory'] = \"GCS 名稱\" gitlab_rails['packages_object_store_direct_upload'] = true gitlab_rails['packages_object_store_background_upload'] = true gitlab_rails['packages_object_store_proxy_download'] = true gitlab_rails['packages_object_store_connection'] = { 'provider' =\u003e 'Google', 'google_project' =\u003e '專案 ID', 'google_json_key_location' =\u003e '/etc/gitlab/google_key.json' } packages_enabled：啟動 packages packages_object_store_enabled：啟動 packages 對象存儲 packages_object_store_remote_directory：設定 packages 對象存儲位置，這邊要輸入 GCS 的名稱 packages_object_store_direct_upload：設定是否可以直接上傳到對象存儲位置 packages_object_store_background_upload：設定是否以後台方式上傳到對象存儲位置 packages_object_store_proxy_download：設定是否可以透過代理伺服器進行套件下載 packages_object_store_connection：設定連接到對象存儲，由於我們要存到 GCS 上面，需要有這三項 provider、google_project、google_json_key_location 才可以將 packages 存到 GCS 上。如果想用其他的儲存位置，例如 Amazon S3、Azure Blob storage 可以參考 Object storage 詳細設定 ( 其中的 google_json_key_location 是要放可以讀寫 GCS 的 SA SECRET 檔案 ) ","重啟設定後再次檢查-gitlab-package#重啟設定後再次檢查 GitLab Package":"當我們重啟設定後，也有建立好可供我們權限 SA 的 GCS 後，會發現原本存在預設 /var/opt/gitlab/gitlab-rails/shared/packages/ 沒有自動跑到 GCS 上，是因為我們還需要手動下指令將他遷移過去，指令是 gitlab-rake \"gitlab:packages:migrate\"，最後等他跑完我們在檢查一下預設儲存位置就發現已經沒有 Package 了\n檢查是否還有 Package 在預設儲存位置 (已遷移)\n開 GCS 網站來看會發現原先在預設儲存位置的 Package 都可以跑到 GCS 上：\n查看已遷移到 Google Cloud Storage 的 Package"},"title":"如何啟用 GitLab 的 Package Registry 以及將儲存位置從伺服器改到 GCS 上"},"/blog/git-or-cicd/jenkins-ansible-it-cicd/":{"data":{"":"在軟體開發領域中，IT 自動化 (automation) 及 持續整合 (Continuous Integration, CI) 、持續佈署 (Continuous Deployment,CD) 是 DevOps 精神中很重要的兩個部分，此文章是參考 30 天入門 Ansible 及 Jenkins ，再加上自己測試後的筆記紀錄，歡迎大家可先閱讀作者原文，那我們就開始一起學習吧 👊\n我會分別介紹 Ansible 與 Jenkins 這兩個非常熱門的開源軟體再搭配實作來讓大家更了解他們，那在之前我們先來聊聊為什麼需要 IT 自動化以及什麼是持續整合/持續部署吧 🤠","ansible#Ansible":"Ansible 是一種 IT 自動化工具。它可以部署軟體、配置系統，並編排更高級的自動化任務，例如 CD (持續部署) 或 RollingUpdate 零停機的滾動更新。\n自動化簡化了複雜的任務，不僅使開發人員的工作更容易管理，也讓他們的注意力可以放在對團體更有價值的其他任務上。換句話說，它可以節省時間並提高效率。Ansible 使用簡單的 YAML 語法，且 Ansible 是一種輕量級且安全的解決方案，它的優點還有：\n使用 Ansible 不需要任何特殊的編程技能，因為使用的是 YAML 語法 Ansible 允許建立高複雜性的 IT 自動化。 因為不需要安裝其他套件，所以伺服器上有更多空間來容納應用服務的支援 Ansible 在設計上非常簡單且可靠一制性。 ","jenkins#Jenkins":"Jenkins 是使用 Java 編成語言編寫最受歡迎的開源自動化服務器。它促進了軟體開發過程中的持續整合、持續部署的自動化過程。\nJenkins 支持 1800 多個其他軟體套件，Jenkins 易於安裝和使用，它還提供方便瀏覽的項目管理儀表板，它的優點還有：\n免費開源 充滿活力的用戶社群 多種工具和技術集成 插件支持 易於安裝、配置和升級 監控外部工作 支持各種身份驗證方法、通知、版本控制等等 ","jenkins-與-ansible-介紹實作連結#Jenkins 與 Ansible 介紹實作連結":"由於 Jenkins 與 Ansible 介紹與實作教學文章較長，故個別分開一篇文章來做說明，大家可以去看自己有興趣的文章歐 😎\nJenkins：使用 Jenkins 設定 GitHub 觸發程序並通知 Telegram Bot Ansible：Ansible 介紹與實作 (Inventory、Playbooks、Module、Template、Handlers) ","jenkins-跟-ansible-比較#Jenkins 跟 Ansible 比較":" 名稱 Jenkins Ansible 套件 Jenkins 支持 1800 多種套件 支持較少套件 語言 支持 C、C++、Java、Perl、Python、Ruby 等 支持 C、Python、JavaScript、Ruby 等 費用 Jenkins 是免費的 Ansible 不是免費的，但有試用版(Red hat) 大小 重量級 輕量級 服務 基於伺服器的工具 基於雲上的工具 ","什麼是持續整合持續部署#什麼是持續整合/持續部署":"當環境搭建成功後，對於服務本身的維護以及監控也是開發流程中相當重要的一環。當我們從原始碼代管服務 (GitHub、GitLab)上取得原始碼後，要如何確保產品在發布前品質沒有問題，一直以來都是開發人員需要思考的一個課題。\n由於現在多數開發團隊都會透過版本控制來提交並整合開發人員各自修改的程式碼，若在合併分支時沒有把合併衝突 (conflict) 處理恰當，或是合併程式碼後產生某些邏輯錯誤，往往會到產品發佈後才發現不可預期的錯誤。\n所以有了持續整合/持續部署的機制下，我們可以透過高頻率的整合、測試並分析程式碼品質，在最短時間發現問題以及發生點，進而確保產品每一次的發布都是穩定且高品質的。\nCI/CD 流程圖 (作者打錯是 rsync 不是 rsyne) Day12 什麼是 CICD\n上面這張圖是簡化版的 CI/CD 流程圖，當我們 Developer 將程式 Push 到原始碼代管服務 (GitHub、GitLab)上，會經過 Webhook 給 Jenkins 這種自動化可以持續整合部署到各自的伺服器上。\n那 CI \u0026 CD 分別負責哪些工作呢？\n持續整合 Continuous Integration 持續整合的英文是 Continuous Integration 我們縮寫成 CI ，後續也會使用 CI 來做說明：\n流程： 程式建置\n開發人員在每一次的 Commit \u0026 Push 後，都能夠於統一的環境自動 Build 程式，透過此步驟可以避免每個開發人員因本機的環境或是套件版本不同導致出現異常。\n程式測試\n當程式編譯完後，透過單元測試測試新寫的功能是否正確，或者確定是否會影響現有功能，透過該步驟進行測試，可以避免開發人員遺忘先在本機檢查，作為雙重驗證之功用。\n目的： 降低人為疏失風險 減少人工手動的反覆動作 進行版本控制 增加系統一制性與透明化 持續佈署 Continuous Deployment 持續佈署的英文是 Continuous Deployment 我們縮寫成 CD ，後續也會使用 CD 來做說明：\n流程： 部署服務\n透過自動化方式，將寫好的程式碼更新到機器上並公開對外服務，另外需要確保套件版本＆資料庫資料的完整性，也會透過監控系統進行服務存活檢查，若服務異常會即時發送通知告知開發人員。\n目的： 保持每次更新程式都可以順暢完成 確保服務存活 我們了解了自動化與 CI/CD 的重要性與功用，那我們要怎麼去實現這些呢！我們先看下面這張圖：\nTop 5 DevOps Automation Tools in 2020\n這邊整理 2020 年適合用於 DevOps Automation 的工具，那我們本次教學會介紹最多人使用的 Jenkins 以及 Ansible 兩種：","參考資料#參考資料":"30 天入門 Ansible 及 Jenkins\nDay12 什麼是 CICD\nJenkins 和 Ansible 的對比和區別","為什麼需要-it-自動化-#為什麼需要 IT 自動化 ?":"當我們在開發任何軟體產品時，除了開發本身的過程需要花相當多的心力外，在產品部署的環節也是讓大家頭痛的一個部分。其環境的搭建或是參數的設定常常會因為一些小原因導致產品無法像在開發時一樣正常運作。尤其當需要部署的主機不只一台時，重複性的工作會花費我們大量的時間。再加上還會因為伺服器提供的作業環境不同、或是其他種種限制而必須做參數上的調整等等。\n這時候 IT 自動化 就顯得十分重要，透過自動化，不但可以幫助開發人員有效減少部署產品所需時間外，還可以在有限度的修改下分別針對不同環境去做調整。\nBest Automation Tools for DevOps"},"title":"Jenkins 及 Ansible IT 自動化 CI/CD 介紹"},"/blog/git-or-cicd/jenkins-github-tg-bot/":{"data":{"":"此文章是接續前面 Jenkins 及 Ansible IT 自動化 CI/CD 介紹 文章，此篇會實際安裝及實作 Jenkins，大家記得在學習前要先檢查自己的版本是否有新的更新！那我們開始囉 😘","jenkins-安裝與實作#Jenkins 安裝與實作":"我這次會使用 Docker-compose 來進行安裝，除了 Docker 以外也有不同的安裝方式，可以參考 Jenkins download and deployment，本次使用的環境版本如下：\n版本 macOS：11.6 Docker：Docker version 20.10.14, build a224086 Jenkins：jenkins/jenkins:lts-jdk11 yamllint：1.26.0 安裝 這邊會使用 Jenkins 提供的 官方 LTS 映像檔 來作為基底，因為我們要多安裝測試程式 yamllint，所以就自己寫一個 Docker-compose：(同樣的程式碼會放在 GitHub，也直接包成映像檔放在 DockerHub，歡迎大家自行取用)\nyamlint，它是語法檢查工具，可以用來檢查 yaml 檔案的語法是否正確以及符合規範，我們看一下實際操作的畫面：\nyamllint 測試\n可以看到如果不符合 yaml 規範就會跳出錯誤訊息。\n接下來先看一下整個 Docker-compose 結構以及各參數：\n. ├── Docker-compose.yaml ├── jenkins │ └── Dockerfile Docker-compose.yaml\nversion: \"3.8\" services: jenkins: build: ./jenkins/ container_name: jenkins ports: - 8080:8080 - 50000:50000 restart: always volumes: - ./jenkins_home:/var/jenkins_home 參數說明：\nbuild: ./jenkins/：因為要先安裝 yamllint，所以使用 Dockerfile 另外寫。 container_name:jenkins：容器的名稱。 ports: -8080:8080 - 50000:50000：8080 是待會我們瀏覽儀表板會使用到的 Port，如果本機上 8080 已經被佔用，可以自行更換，50000 是 Jenkins 所使用的 Port。 restart: always：當容器停止時，會自動重新啟動容器。 volumes: - ./jenkins_home:/var/jenkins_home：掛載目錄，就算刪除容器一樣可以保留其他設定。我將啟動 Docker-compose.yaml 的資料夾下多一個 jenkins_home 與容器內 /var/jenkins_home 做映射，大家可以自己去調整。 jenkins/Dockerfile\nFROM jenkins/jenkins:lts-jdk11 LABEL maintainer=\"880831ian@gmail.com\" USER root RUN apt-get upgrade -y\\ \u0026\u0026 apt-get update -y\\ \u0026\u0026 apt-get install yamllint -y 參數說明：\nFROM：我們使用 Jenkins 官方提供的 LTS 維護版本。 USER：因為要先安裝東西，所以直接給 root 權限。 RUN：先升級完後，再更新，最後再裝 yamllint。(-y 是同意所以詢問) 最後使用 docker-compose 來執行：\n$ docker-compose up -d 要在 Docker-compose.yaml 資料夾下指令才有用。\n下完指令後，他就會在背景開始安裝，可以試著用瀏覽器瀏覽 http://localhost:8080，查看有沒有跳出下面這個畫面：\n瀏覽器訪問 http://localhost:8080\n我們看到它需要輸入一組 Administrator password，我們要使用 docker logs 來查看，會發現最後會有寫 Please use the following password to proceed to installation 的地方：\n$ docker logs jenkins ************************************************************* ************************************************************* ************************************************************* Jenkins initial setup is required. An admin user has been created and a password generated. Please use the following password to proceed to installation: 70c0780f62a7441f90286be106908378 This may also be found at: /var/jenkins_home/secrets/initialAdminPassword ************************************************************* ************************************************************* ************************************************************* 其中的 70c0780f62a7441f90286be106908378 ，就是我們的 Administrator password，直接複製並貼到欄位後按 Continue。\n我們可以看到它詢問是否要安裝套件，我們選擇左邊 Install suggested plugins 安裝推薦的套件即可：\n安裝推薦的套件\n等待它安裝套件，安裝完後會自動跳到註冊畫面：\n等待安裝…\n輸入完基本的資料後，按 Save and Continue：\n創建 Admin 使用者\n最後看到下面這個畫面就代表我們安裝好囉！\nJenkins 儀表板\n建立第一個 Jenkins Job 我們已經成功安裝好並進入到 Jenkins 儀表板，我們先來建立第一個 Job，它的功用是告訴我們系統檔案的即時使用狀況，讓我們對 Jenkins 有初步的了解：\n點選儀表板的新增作業或是 Create a Job： 新增作業\n輸入 Job 專案名稱並選擇建立 Free-Style 軟體專案： 設定 Job 專案\n可以看到這邊有不同的專案類型可以選擇，Free-Style 以及 Pipeline 這兩種類型的專案基本上就涵蓋大部分的需求。Free-Style 類型的專案提供了非常大的彈性讓使用者來做原始碼管理以及建置。如果建置流程涉及多個專案，則可以使用 Pipeline 類型的專案來組合及定義建置邏輯。\n接下來設定專案組態： 設定專案組態\n要記得幫每一個專案都加上描述，讓其他人知道該專案的用途或是使用時機等。\n接著，在建置的下拉式欄位選擇 選擇建置步驟 \u003e 執行 Shell\n設定專案組態\n輸入 df -h 指令：\n設定專案組態\n我們這邊透過建置 執行 Shell 這個建置步驟來告訴 Jenkins，未來這個專案被建置，就會執行 df -h 這個指令。\n專案組態設置完後，我們點選左邊的馬上建置來建置剛剛建好的專案，如果我們設定上沒有問題，應該會在左下角的建置歷程這邊看到我們的第一個建置紀錄： 建置專案\n點進去後，再點 Console Output，可以看到這次建置的結果： 建置專案\n這樣我們的第一個 Jenkins Job 就設定完成囉！Jenkins 也確實的執行我們所設定的指令，並將系統的使用狀況呈現在終端機的輸出上。 以上就是我們第一個簡單的專案建置流程，當然，Jenkins 可以做到的事情不僅如此，在後面我們會透過安裝不同的套件來強化 Jenkins，來達到完整的持續整合 😁","原始碼管理與建置觸發程序#原始碼管理與建置觸發程序":"上面有提到 Jenkins 作為一個持續整合的工具，與原始碼管理系統的整合尤其重要。我們這一章節，會介紹如何在 Jenkins 上透過原始碼管理 (source code management,SCM) 系統，例如從 GitHub 獲得專案的原始碼，並設置建置觸發程序 (build triggers) 來實踐持續整合。\n建置專案 HTTPS 接下來我們先建立一個新的 Job，選擇 Free-style 模式，這次要在原始碼管理裡面選擇 Git，在 Repositories \u003e Repository URL 裡面輸入我們這次要測試的 repository URL：\n設定 Repository URL\nSSH 如果我們想要透過 SSH 來存取專案，我們會遇到以下狀況：\nSSH 尚未設定錯誤訊息\n會有錯誤訊息是因為我們還沒有把 Jenkins 與 GitHub 做 SSH 金鑰配對，所以 GitHub 拒絕 Jenkins 透過 SSH 存取。那要怎麼解決呢？\n最簡單的方法就是在 Jenkins 主機下建立 SSH 金鑰，並將公開金鑰 Key 加入到 GitHub 帳號中，有需要的朋友可以再自行使用，該範例使用 HTTPS 來做設定。\n建置觸發程序 由於我們還沒有定義任何建置的觸發程序，所以除非我們手動去操作 Jenkins，不然 Jenkins 並不會主動幫我們進行建置。因此，在建置觸發程序這個欄位內，我們可以自由設定我們希望 Jenkins 何時自動幫我們進行建置專案。那依照專案的屬性不同，我們也可以採用不同的建置時機。那最常見的有以下兩種：\n定期建置 在 Jenkins 中，我們是採用 Cron Format 的方式來定義建置行程。Cron Format 總共五個欄位，欄位與欄位之間可用空白或 Tab 鍵做區隔：\n分 (minute)：0 - 59 分 時 (hour)：0 - 23 時 日 (day of month)：1 - 31 日 月 (month)：1 - 12 月 星期 (day of week)：星期 0 - 7 (其中 0 與 7 都代表星期天) 假設我們希望每個 30 分鐘就建置一次當前專案，我們可以在定義規則裡面填入 H/15 * * * *：\n定期建置\nGitHub hook trigger for GITScm polling 這種觸發方式在持續整合時非常實用。我們可以讓 Jenkins 自動監測當在原始碼專案有任何的 push event 發生時就進行建置。為了要使用這種方式建置，需要以下幾個步驟來設定：\n新增 GitHub personal access token 進入 GitHub 首頁，點選右上角下拉選單，點選 Settings： GitHub hook trigger for GITScm polling 設定\n先點選左邊的 Developer settings \u003e 點選左邊的 Personal access tokens \u003e 點選右上角的 Generate new token，輸入 token 的描述並勾選 repo scope 以及 admin:repo_hook scope 跟 admin:org_hook scope，點選 Generate token： GitHub hook trigger for GITScm polling 設定\n會產生一個 token，請先把 token 複製下來，離開這個畫面後，token 就會看不到了！(此 token 已刪除 ✌️) GitHub hook trigger for GITScm polling 設定\n設定 Jenkins GitHub 進入 Jenkins 儀表板頁面，點選左邊管理 Jenkins \u003e System Configuration 的設定系統，往下滑找到 GitHub： GitHub hook trigger for GITScm polling 設定\n找到後，再 GitHub Servers 下拉欄位選擇 Add GitHub Server，會看到下面畫面，API URL 輸入 https://api.github.com，Credentials 點 Add： GitHub hook trigger for GITScm polling 設定\nKind 選擇 Secret Text，Secret 輸入剛剛存的 Personal Access Token，Description 簡單描述一下： GitHub hook trigger for GITScm polling 設定\n設定完後，點選一下右邊的 Test connection，如果我顯示 Credentials verified for user UserName, rate limit: xxx 就代表成功囉！ GitHub hook trigger for GITScm polling 設定\n設定專案組態 接著我們跳回來剛剛的專案組態，並勾選 GitHub hook trigger for GITScm polling： 設定專案組態\n寫一個 Shell Script 來建置專案： for file in $(find . -type f -name \"*yaml\") do yamllint $file done 設定專案組態\n這邊利用一個簡單的 Shell Script 迴圈來對所有 YAML file 進行 yamllint 的檢查，最後點選儲存離開。\nGitHub 上整合 Jenkins 先到 GitHub 被建置專案的頁面下點 Setting 標籤 \u003e 點選左邊的 Webhook \u003e 點選右上角的 Add webhook \u003e 輸入 Jenkins Hook URL 到 Payload URL：\n記得 Jenkins Hook URL 後面要加 /github-webhook/，小弟我卡在這裡很久 😢 😢\nWebhooks 設定\nJenkins Hook URL 設定\" 由於我們是將 Jenkins 運行在本機端，所以 Jenkins Hook URL http://localhost:8080 是 Private IP。GitHub 沒有辦法抓 Private IP，為了練習，我們可以透過 ngrok 這套簡單小工具來暫時將 http://localhost:8080 變成 Public IP。\nngrok 的使用方式很簡單，只要先下載 brew install ngrok/ngrok/ngrok ，並使用 ngrok http 8080 指令，將 Private IP 變成 Public IP：\nngrok 將 Private IP 變成 Public IP\n圖片中 https://2063-111-235-135-57.jp.ngrok.io 就是 Public 的 Jenkins Hook URL\n完成後，先點選 Recent Deliveries 檢查是否成功，底下的 Response 需要是 200，才是對的歐！(這裡一定要先檢查，不然後面會找問題到死 XD)\nWebhooks 設定\n測試 我們都設定好後，要開始來測試，我們可以直接先點選 馬上建置，來測試是否可以透過 GitHub personal access token，抓取 GitHub 的檔案。\n馬上建置\n可以看到在建置流程那邊發現建置失敗，點進去可以看詳細內容，\n建置失敗\n點選左側的 Console Output，可以看到我們有成功獲取 GitHub 上得專案，並且執行我們的 Shell 來檢查 yaml 的檔案格式，發現是因為格式有錯誤，所以建置才會失敗 ❌\nConsole Output\n接下來我們先修改一下 yaml 的檔案，後重新 push 到 Github 上，並觀察 Jenkins 會不會自動建置 ！(修改位置大家可以直接看 Commit 結果)\nConsole Output\n當你 push 完後，發現它會自動建置，請因為我們修改成正確格式，所以他也建置成功囉！\n也可以點選左側有一個新的 GitHub Hook Log ，可以看到我們成功透過 GitHub hook trigger for GITScm polling 偵測到有新的 event，透過 WebHook 讓 Jenkins 知道。\nGitHub Hook Log\n建置後觸發通知 當我們自動建置成功當然沒什麼問題，但如果失敗有可能就會影響後續程式的上線時間，所以我們希望建置完成後，可以收到通知，通知除了可以用 email 以外，也可以使用套件去串接我們常用的平台，例如 Telegram、Slack、Line 等等，接下來我會教大家要怎麼去串接這些服務，在開始之前要請大家先安裝兩個套件：\n安裝/設定 Build Timestamp Build Timestamp 這個套件可以幫我們在稍後傳送通知時加上當下的時間戳，那要怎麼安裝套件呢？先到儀表板首頁，點選左側的 管理 Jenkins \u003e 點選 管理外掛程式：\n管理 Jenkins \u003e 管理外掛程式\n再 Plugin Manager 的 可用的裡面搜尋 Build Timestamp，選擇後點下方的 Download now and install after restart，等待他安裝後會自動重啟。\n安裝 Build Timestamp\n重啟後從儀表板點選左側 管理 Jenkins \u003e 點選 設定系統，找到 Build Timestamp，開啟設定，並設定 Timezone 為 Asia/Taipei 以及 pattern yyyy-MM-dd HH:mm:ss z，這樣我們待會就可以使用 BUILD_TIMESTAMP 參數來獲取當下時間，記得要按下儲存歐！\n設定 Build Timestamp\n安裝/設定 Notify.Events Notify.Events 這個套件可以串接很多的平台，例如 Telegram、Slack、Line 等，也可以透過它寄發郵件，是一個十分方便的套件，但缺點是他需要註冊，免費版只有每個月 300 次的訊息傳輸量，但在我們測試階段已經十分夠用。一樣我們用剛剛的方法安裝 Notify.Events。\n安裝 Notify.Events\n安裝好後，Notify.Events 他不需要先設定，它可以依據不同的 Job 有不同的設定，所以我們開啟剛剛的 Job 組態，拉到最下面找到 建置後動作 ，選擇 Notify.Events：\n安裝 Notify.Events\n可以看到這邊要先輸入 Token，那 Token 就必須去官網註冊後設定。\n我們先開啟瀏覽器，搜尋 Notify.Events，註冊帳號後，在 Channels 點選 Create，輸入一下 Title 按下 Save。\nNotify.Events 官網設定\n完成後，應該可以看到以下畫面，這邊就可以讓我們選擇來源，以及要發送到哪裡：\nNotify.Events 官網設定\n我們先選擇 Sources，點選 Add source，可以看到很多來源，選擇 CI/CD and Version control ，再選擇 Jenkins：\nNotify.Events 官網設定\n點選 Next，就可以看到以下畫面，它告訴我們要將它提供的 Token 貼入設定檔，也就是我們剛剛在 Job 組態裡面的那個 Token：\nNotify.Events 官網設定\n設定好 Sources，接下來要設定接收方，回到剛剛 Notify.Events 的儀表板，點選 Subscribe，我們測試使用 Telegram 來當接收方，它會跳出一個視窗，告訴你要怎麼把他的機器人加成好友或是加入群組，這邊就依大家需要自行選擇，那我就將它加入群組，使用 /subscribe DRr0bIZ0 @NotifyEventsBot 指令來綁定\nNotify.Events 官網設定\n這時候我們都設定好了，我們回到 Job 組態的 Notify.Events 設定位置，將 Token 貼上去，它可以自訂訊息的模板，可以全部都一樣，也可以針對建置後的狀態，產生不同的訊息模板，我們來自定義設計一下：\nSuccess\n📢 Jenkins 建置通知 📣 時間：$BUILD_TIMESTAMP 🕐 名稱： \u003ca href=\"$PROJECT_URL\"\u003e$PROJECT_NAME\u003c/a\u003e 次數： \u003ca href=\"$BUILD_URL\"\u003e#$BUILD_NUMBER\u003c/a\u003e 建置狀態： 🟢 \u003cb\u003e$BUILD_STATUS\u003c/b\u003e 🟢 \u003ca href=\"$BUILD_URL/console\"\u003e建置日誌連結\u003c/a\u003e --------- 😍😍😍 --------- Notify.Events Success\nFailure\n📢 Jenkins 建置通知 📣 時間：$BUILD_TIMESTAMP 🕐 名稱： \u003ca href=\"$PROJECT_URL\"\u003e$PROJECT_NAME\u003c/a\u003e 次數： \u003ca href=\"$BUILD_URL\"\u003e#$BUILD_NUMBER\u003c/a\u003e 建置狀態： 🔴 \u003cb\u003e$BUILD_STATUS\u003c/b\u003e 🔴 \u003ca href=\"$BUILD_URL/console\"\u003e建置日誌連結\u003c/a\u003e --------- 😭😭😭 --------- Notify.Events Failure\nTelegram 通知測試 最後我們都設定好了，就來測試一下吧！我們先故意將程式碼格式用錯，讓他先跳出錯誤，再修改，來看看結果如何吧！(文字連結是對應的 Commit )\nTelegram 通知\n可以看到，我們分別兩次的測試，會依據我們建置後的結果，觸發不同的通知模板。","參考資料#參考資料":"30 天入門 Ansible 及 Jenkins\n[CI]設定 jenkins 連結 GitHub Private Repo by Webhook"},"title":"使用 Jenkins 設定 GitHub 觸發程序並通知 Telegram Bot"},"/blog/git-or-cicd/laravel-gitlab-cicd-heroku/":{"data":{"":"經過上一篇文章 如何從頭打造專屬的 GitLab CI/CD 的學習，讓我們了解到 GitLab CI/CD 的整個流程，接著我們本次要把 Laravel 給部署到 Heroku 透過 GitLab 的 CI/CD 去達成，不需要透過任何人工去測試，並上架程式到 HeroKu 上，全部都依賴 GitLab CI/CD，讓我們接著看下去吧！\n當然，此文章程式碼也會同步到 Github ，需要的也可以去查看歐！要記得先確定一下自己的版本 Github 程式碼連結 😏","gitlab-cd-建置#GitLab CD 建置":"我們玩完 CI 後，接著要把程式部署到伺服器或是雲端上，這時候我們不需要透過人工手動的方式，只需要有 CD 來幫我們自動化部署就可以拉！如果不太清楚，可以參考這張圖片：\nGitLab CI/CD workflow (圖片來源：GitLab)\n當我們剛剛進行 CI 的整合測試，最後經過 Review and approve 合併到主分支，這時候如果我們有設定 CD，CD 就會幫我們部署到服務上，我把 CD 流程轉成文字步驟說明：\n把新功能分支合併到 master 分支，代表功能已經可以上線 GitLab 觸發 Gitlab-CI 執行 pipeline Gitlab-CI 執行自動化測試 Gitlab-CI 測試成功後，執行部署到正式伺服器 回傳執行結果至 GitLab 那想要達成自動化部署之前，必須能在遠端用指令下達部署更新！簡單來說有兩件事情：\n要先整理再更新專案時需要哪些指令，並將其寫成腳本 需要獲得伺服器的授權，可以對伺服器下達更新專案的腳本 我們以現在 Laravel 專案來說，套用上面講的兩件事情：\n腳本製作：上線新版本大概要執行以下圖片的內容 Laravel 專案上線前會下達的指令\n對遠端伺服器下指令：通常使用 ssh 與 伺服器做溝通，所以先在伺服器產生授權金鑰給要遠端控制的電腦，如果要給 Gitlab-CI 控制的話，也需要把金鑰存在 GitLab 上，通常使用 ssh user@remote.server 'git pull' 來下達更新專案的指令 本篇我們要部署的是 PaaS 的 HeroKu，可以減少時間去架設環境，就可以達到我們想要的效果，那接著會帶大家從 Heroku 設定開始歐！先簡單介紹一下 Heroku：\nHeroku 是一個支援多種程式語言的雲平台即時服務(PaaS)， 是一種雲端運算服務，提供運算平台與解決方案服務，PaaS 提供使用者將雲端基礎設施部署與建立至使用者端，或者藉此獲得使用程式語言、程式庫與服務。使用者不需要管理與控制雲端基礎設施（包含網路、伺服器、作業系統或儲存），但需要控制上層的應用程式部署與應用代管的環境。\n創建 Heroku 專案 那要使用 Heroku 當然需要一組帳號拉，建立帳號我應該不用再多介紹了吧 🤡 我們直接到 Heroku 頁面，右上角 New，點選 Create new app，輸入本次專案名稱，我就取叫 laravel-gitlab-cicd-heroku (這個不能與別人重複，因為他會生成專屬網頁)， 進去後，點選右上角有一個 Open app，就會跳出這個專案專屬的網頁：\nHeroku 專屬網頁\n設定 HeroKu 與 GitLab 連線 先點選右上角個人頭像 → Account setrtings → 在 Account 往下滑 → API Key，點選 Reveal 並將該 API 記住，這是等等透過 GitLab 部署時會用到的 API Token：\n取得部署的 API Token\n回到 GitLab 專案底下，Settings → CI/CD → Variables，他可以將變數設定在這邊，再讓 .gitlab-ci.yml 來抓取變數，設定以下兩個變數：(詳細可以參考官網)\nKey 名稱(HEROKU_PRODUCTION_PROJECT_NAME)，Value 值(設定我們剛剛在 Heroku 部署的專案名稱，我的是 laravel-gitlab-cicd-heroku) Key 名稱(HEROKU_PRODUCTION_API_KEY)，Value 值(這個就是我們上面的 API Key，每個人都要用自己的歐！上面的我已經重設了 😎 ) gitLab 設定 Variables\n這邊要注意先把預設的 Protect variable 給關閉，他預設會只能在受保護的分支或標籤運行，但我們這此以簡單為主，所以這些設定都先關掉。 新增 Heroku 識別檔案 接下來我們要新增一個檔案名為 Procfile，它是 Heroku 部署更新時會啟動的對象，注意他沒有副檔名，我們在裡面輸入以下：(我們使用合併後的 master)\n新增 HeroKu 識別檔案\n它代表我們網頁服務使用 apache2 指令運行並把入口指向專案資料夾中的 laravel 專案的入口資料夾。\n修改 .gitlab-ci.yml 我們修改原本用來 CI 的腳本，來設定自動化部署的任務 Job 及腳本\n.gitlab-ci.ymlimage: lorisleiva/laravel-docker:latest Production_Deploy: stage: Production_Deploy before_script: - apk add ruby ruby-dev ruby-irb ruby-rake ruby-io-console ruby-bigdecimal ruby-json ruby-bundler yarn ruby-rdoc \u003e\u003e /dev/null - apk update - gem install dpl \u003e\u003e /dev/null script: - dpl --provider=heroku --app=$HEROKU_PRODUCTION_PROJECT_NAME --api-key=$HEROKU_PRODUCTION_API_KEY 最後上傳 GitLab 來觸發 Gitlab-CI 執行自動化部署 (上傳指令就不多說囉，想必大家都會了吧！，不會的話可以去看 Git 介紹，裡面有詳細的介紹 😍 )\n觸發 Gitlab-CI 執行自動化部署\n可以看到部署成功，我們也來看看 Runner 運作狀況：\nRunner 運作狀況\n看到他成功將服務給部署到 https://laravel-gitlab-cicd-heroku.herokuapp.com/。\n既然已經部署好了，當然要去看一下我們的網頁啊，但當我們打開部署好的網頁，會發現跳出 500 Error，雖然他與我們 CI/CD 沒有關係，但我們還是試著解決，那這個問題會發生是因為我們沒有給環境變數的 APP_KEY，這個 Key 可以在專案的 .env 取得，拿到後開啟 Heroku → Setting → Config vars 將 APP_KEY 設定上去。\nRunner 運作狀況\n最後重新整理 https://laravel-gitlab-cicd-heroku.herokuapp.com/，就可以看到我們部署上去的網站囉！\n透過 CD 部署到 Heroku 的 Laravel 首頁","gitlab-ci-建置#GitLab CI 建置":"上傳 Laravel 專案 接下來我們要上傳含有 Unit Test 專案到 GitLab 上，步驟如下，如果已經熟悉如何將專案推到 GitLab，可以直接跳到 在 GitLab 上執行單元測試\n在 gitlab.com 上點選建立專案，選擇 Create blank project，也可以直接瀏覽該網址 https://gitlab.com/projects/new#blank_project。 輸入專案名稱可以選擇 Project deployment target 為 Heroko，選擇 Public，最後按下 Create project 在 GitLab 上建立新專案\n於專案資料夾下加入 remote 遠端 GitLab，並 Push 將專案推上去。 將 Laravel 專案推到 GitLab 上\n成功推上去，可以到 GitLab 上，看到我們剛剛的專案！\n成功推到 GitLab 上\n在 GitLab 上執行單元測試 要在 GitLab 上執行 CI/CD 就需要有 Runner，這次我們選擇使用 gitlab.com 的 Shared runners，想要使用 Specific runners，可以查看上一篇 如何從頭打造專屬的 GitLab CI/CD 文章\n本次使用 Share runners\n接下來在專案的根目錄撰寫我們的 .gitlab-ci.yml 檔案，之後再次上傳 GitLab，當我們根目錄有此檔案，GitLab-CI 就會讀取並依照內容啟動 Runner 來執行工作：\n.gitlab-ci.ymlimage: lorisleiva/laravel-docker:latest Unit_test: before_script: - composer install --prefer-dist --no-ansi --no-interaction --no-progress --no-scripts script: - ./vendor/bin/phpunit --testsuit Unit --coverage-text --colors=never 說明一下這個 yml 檔內的設定是在做什麼：\nimage：因為我們執行 CI/CD 過程中，需要有 PHP、Compose、NPM 等工具，有這些套件管理工具就可以延伸去安裝更多套件，如果一開始沒有安裝，就會很麻煩，其中一個辦法就是去 Runner 環境修改並安裝，但因為方便以及我們這次使用 Share runners，所以不能修改別人的 Runner，另一個辦法是可以使用 image 關鍵字，可以讓 Runner 切換到另一個環境去執行工作 (Job)，我們這邊使用 lorisleiva/laravel-docker:latest ，他裡面已經幫我們安裝好上述的工具了！ Unit_test：這邊也是我們的 Job，那裡面主要是先用 composer install 去安裝我們需要的套件，最後在執行 phpunit 來做單元測試。 上傳 .gitlab-ci.yml 接著我們使用以下指令將含有 .gitlab-ci.yml 的專案上傳到 GitLab，並回到 GitLab 選擇 CI/CD，可以查看目前的 Pipelines，會有我們剛剛所新增的 Runner。\n將 .gitlab-ci.yml 推到 GitLab\n查看 Runner 已經進行執行單元測試檢測\n可以看到 Runner 先安裝我們的環境，再執行單元測試的腳本\n設置須通過測試才可以合併 當我們有了測試還不夠，要怎麼確保每隻要上線 (合併到主分支) 的程式都有經過測試才上線呢？\n接下來我們可以在 GitLab 裡面做這些設定，先到專案的 Setting → General → Merge requests → Merge checks 點選 Pipelines must succeed：\n點選 Pipelines must succeed 來確保程式合併前都必須經過測試\n測試是否可以阻擋未成功情況 我們先模擬要開發新功能，所以在 master 最新 commit 下，建立一個新分支 new\ngit checkout -b \"new\" 接著修改單元測試，故意新增錯誤的測試，開啟專案的 tests/Unit/ExampleTest.php，最下面加上紅色框框程式碼：\n新增錯誤測試，還模擬看看是否能成功擋住\nassertEquals 會檢查這兩個值是否相同，不同的話，就會跳出錯誤，所以我們故意輸入 1 和 2。\n並將它上傳到 GitLab，並發出 Merge Request 看看會有什麼結果！\n將新增錯誤的 ExampleTest 加入暫存，推到 GitLab\n並將 new 分支透過 Merge Request 來合併到 master\n可以看到我們合併在 Pipeline 測試時，因為 new 沒有通過測試，所以也沒有辦法進行合併！\n分支 new 沒有通過測試，所以沒有進行 Merge","參考資料#參考資料":"Gitlab-CI 入門實作教學 - 單元測試篇：https://nick-chen.medium.com/gitlab-ci-%E5%85%A5%E9%96%80%E7%AD%86%E8%A8%98-%E5%96%AE%E5%85%83%E6%B8%AC%E8%A9%A6%E7%AF%87-156455e2ad9f\nGitlab-CI 自動化部屬部署：https://medium.com/@nick03008/%E6%95%99%E5%AD%B8-gitlab-ci-%E5%85%A5%E9%96%80%E5%AF%A6%E4%BD%9C-%E8%87%AA%E5%8B%95%E5%8C%96%E9%83%A8%E7%BD%B2%E7%AF%87-ci-cd-%E7%B3%BB%E5%88%97%E5%88%86%E4%BA%AB%E6%96%87-cbb5100a73d4\n部署 Laravel 於 Heroku 搭配 Gitlab CI/CD：https://medium.com/@vip131430g/%E9%83%A8%E7%BD%B2-laravel-%E6%96%BC-heroku-%E6%90%AD%E9%85%8D-gitlab-ci-cd-6d59a66aebdb","建立-laravel-專案#建立 Laravel 專案":"請大家依照 Laravel 官方文件來建立 Laravel 環境，也可以看小弟我的文章拉 👆👆👆，請記得要先安裝好 php 以及 composer，接著按照以下步驟來建立。\n新建一個 Laravel 新專案\n這時候瀏覽 http://127.0.0.1:8000，如果都正確，應該會看到 Laravel 的首頁\nLaravel 首頁","測試本地-unit-test#測試本地 Unit Test":"接著我們剛剛有提到選用 Laravel 的原因是 Laravel 有 PHPUnit 單元測試可以使用，所以我們現在先在本地端來測試 Unit Test，專案預設有放一個單元測試在 tests/Unit/ExampleTest.php。我們先再次確認環境是否有安裝好，再來執行單元測試。\n在本地端執行單元測試\n執行後，應該都會是通過的畫面，如下圖：\n執行單元測試結果","版本資訊#版本資訊":" macOS：11.6 Docker：Docker version 20.10.14, build a224086 Laravel Installer：2.3.0 Laravel Framework：9.14.1 gitlab.com：GitLab Enterprise Edition 15.1.0-pre 首先，我們第一步驟就是先建立一個 Laravel 專案，至於為什麼要選擇用 Laravel 來當作 GitLab CI/CD 的範例呢？因為 Laravel 內建有 PHPUnit 的測試腳本，可以讓我們在 CI 測試時，更好的展現 CI 的功能！，有關於 Laravel 相關內容，這邊一樣推薦兩篇文章給大家閱讀：🤣\nLaravel 介紹 (使用 Laravel 從零到有開發出一個留言板功能並搭配 RESTful API 來實現 CRUD) Laravel 進階 (內建會員系統、驗證 RESTful API 是否登入、使用 Repository 設計模式) 又工商了一波 XD"},"title":"部署 Laravel 於 Heroku 搭配 GitLab CI/CD"},"/blog/golang/":{"data":{"":"此分類包含 Go (Golang) 相關的文章。\n用 Go 寫一個 Repository Restful API 的留言板 (gin、gorm 套件)發布日期：2022-03-27 Go (Golang) 介紹發布日期：2022-03-21 "},"title":"Golang"},"/blog/golang/go-introduce/":{"data":{"":"","channel#Channel":"Channel 有兩個強大的處理能力，等待以及變數共享。Channel 可以想成一條管線，這條管線可以推入數值，並且也可以將數值拉取出來。\n因為 Channel 會等待至另一端完成推入/拉出的動作後才會繼續往下處理，這樣的特性使其可以在 Goroutines 間可以同步的處理資料或是剛剛提到的等待\nChannel 基本操作 ch := make(chan int) //創建一個Channel ch ch \u003c- u //將值u傳送到 Channel ch裡 v := \u003c- ch //從Channel ch中接收數據 ，並且將其賦值給變數v close(ch) //關閉channel 多執行緒下的共享變數\n在執行緒間使用同樣的變數時，最重要的是確保變數在當前的正確性，在沒有控制的情況下極有可能發生問題。\nfunc main() { total := 0 for i := 0; i \u003c 1000; i++ { go func() { total++ }() } time.Sleep(time.Second) fmt.Println(total) } $ go run . 986 為什麼會明明是用for 跑 1000 累加，但最後只有 900 多呢？我們來看一下下面的例子\n多執行緒下的共享變數 - 錯誤 (Go 的並發：Goroutine 與 Channel 介紹)\n假設目前加到28，在多執行緒的情況下：\ngoroutine1 取值 28 做運算\ngoroutine2 有可能在 goroutine1 做 total++ 前就取 total 的值，因此有可能取到 28\n這樣的情況下做兩次加法的結果會是 29 而非 30\n在多個 goroutine 裡對同一個變數total做加法運算，在賦值時無法確保其為安全的而導致運算錯誤，此問題稱為 競爭危害 (Race condition)。\n使用 Channel 來保證變數的安全性\nfunc main() { total := 0 ch := make(chan int, 1) ch \u003c- total for i := 0; i \u003c 1000; i++ { go func() { ch \u003c- \u003c-ch + 1 }() } time.Sleep(time.Second) fmt.Println(\u003c-ch) } $ go run . 1000 goroutine1 拉出 total 後，Channel 中沒有資料了\n因為 Channel 中沒有資料，因此造成 goroutine2 等待\ngoroutine1 計算完成後，將 total 推入 Channel\ngoroutine2 等到 Channel 中有資料，拉出後結束等待，繼續做運算\n多執行緒下的共享變數 - Channel (Go 的並發：Goroutine 與 Channel 介紹)","go-介面-interface#Go 介面 Interface":"Interface 的概念有點像是藍圖，先定義某個方法的名稱 (function name)、會接收到的參數以及型別 (list of argument types)、會回傳的值與型別 (list of return types)。定義好藍圖之後，並不用去管實作的細節，實作的細節會由每個型別自行定義實作。\nempty interface\n沒有定義任何方法的 interface 稱作 empty interface，由於所有的 types 都能夠實作 empty interface，因此它的值會是 any type：(因為目前沒有被賦值，所以都會回傳 nil)\ntype value interface{} func main() { var v value describe(v) // (\u003cnil\u003e, \u003cnil\u003e) v = 42 describe(v) //(42, int) v = \"hello\" describe(v) // (hello, string) } func describe(v value) { fmt.Printf(\"(%v, %T)\\n\", v, v) } $ go run . value is \u003cnil\u003e type is \u003cnil\u003e type Person interface { getFullName() string getSalary() int } type Employee struct { firstName string lastName string salary int } func (e Employee) getFullName() string { return e.firstName + \" \" + e.lastName } func (e Employee) getSalary() int { return e.salary } func main() { var p Person = Employee{\"ian\", \"Zhuang\", 2000} fmt.Printf(\"full name : %v ,Salary : %v\\n\", p.getFullName(),p.getSalary()) } $ go run . full name : ian Zhuang ,Salary : 2000 ","go-控制流程#Go 控制流程":"if 結構裡會有一個條件，這個條件是個布林值，如果為 true，則會執行括號裡的程式碼，相反的，如果為 false，則會直接跳過：\nfunc main (){ var x = 2 var y = 1 fmt.Println(\"x = \",x,\",y = \",y) if x==y { fmt.Printf(\"%v 等於 %v\\n\",x,y) } fmt.Printf(\"%v 不等於 %v\\n\",x,y) } $ go run . x = 2 ,y = 1 2 不等於 1 switch switch 其目的是簡化 if 的條件，swtich 會檢查符合的條件，並且執行條件內的程式碼，如果都沒有符合，則會執行 default 內的程式碼：\nfunc main (){ x := 12 switch { case x \u003c 10: fmt.Printf(\"%v 小於 10\\n\",x) case x \u003c 20: fmt.Printf(\"%v 大於等於10, 小於 20\\n\",x) default: fmt.Printf(\"%v 大於 20\\n\",x) } } $ go run . 12 大於等於10, 小於 20 ","go-方法-method#Go 方法 Method":"Go 語言不像 Python 有 class ，但還是有提供可以在某種型態上定義方法(method)，method 其實是作用在接收器 (receiver) 上的一種函式，接收器要是某一種型別的變數，所以其實 method 也算是一種特殊型別的函式。\ntype Vertex struct { x,y float64 } func (v Vertex) Abs() float64 { return math.Sqrt(v.x * v.x + v.y * v.y) } func main() { v := Vertex{5,12} fmt.Println(v.Abs()) } $ go run . 13 一開始先宣告一個名為 Vertex 結構型態，裡面的屬性包含 ｘ、y (float64)，接著就是撰寫一個 method 了，這個 method 是以 Vertex 作為接收器， method 名稱是 Abs，最後回傳浮點數，接著 method 裡頭，即為對接收器的運算並回傳值。\n我們來看看如果使用 function 要怎麼來達成\n方法 (Method) vs 函式 (Function)\ntype Vertex struct { x, y float64 } func Abs(v Vertex) float64{ return math.Sqrt(v.x * v.x + v.y * v.y) } func main () { v := Vertex{5,12} fmt.Println(Abs(v)) } $ go run . 13 ","go-資料型態#Go 資料型態":"我們在學習 Go 語言之前，要先了解一下基本的資料型態，可以簡單分為 字串、字符、整數、浮點數、布林值、映射\n字串 String 在 Go 語言中，字串必須用雙引號給匡起來，也可以使用反引號來宣告。用雙引號刮起來的字串不能包含換行，但可以包含跳脫字元，例如 \\n 、\\t 。由於反引號內包含的是原始字串，可以跨越多行，所以跳脫符號在原始字串中沒有任何含義。\nfunc main() { var name = \"ian\" fmt.Printf(\"資料型態 name : %v(%T)\\n\", name,name) var address = `台中市太平區` fmt.Printf(\"資料型態 address : %v(%T)\\n\", address,address) } $ go run . 資料型態 name : ian(string) 資料型態 address : 台中市太平區(string) 字符 Character 字串中的每一個元素叫做字符，字符會使用單引號匡起來，像是 \"abc\" 這個字串，其中 'a'、'b'、'c' 就是字符，可以從字串元素中來獲得字符。\nGo 語言的字符有以下兩種：\n一種叫 byte 類型，可以叫做 uint8 類型，代表 ASCll 碼的一個字符。 另一種是 rune 類型，代表一個 UTF-8 字符，當需要處理中文、日文或是其他複合字符時，就會使用到 rune 類型。rune 類型等於 int32 類型。 func main (){ var a byte = 'A' var b rune = '嗨' fmt.Printf(\"%c %c\\n\",a,b) //%c 所表示的字符 fmt.Printf(\"%d(%T) %d(%T)\\n\",a,a,b,b) //%d 十進制表示,%T 輸出型態 fmt.Printf(\"%x %x\\n\",a,b) //%x 十六進制表示 fmt.Printf(\"%U %U\\n\",a,b) //%U 輸出格式為 Unicode 格式:U+hhhh的字串 } $ go run . A 嗨 65(uint8) 21992(int32) 41 55e8 U+0041 U+55E8 整數 Integer 整數用於儲存整數。 Go 具有多種大小不一的內建整數型別，用來儲存有號數和無號數。\n有號數 型別 大小 範圍 int 取決平台 取決平台 int 8 8 bits -128 到 127 int 16 16 bits -2^15 到 2^15-1 int 32 32 bits -2^31 到 2^31-1 int 64 64 bits -2^63 到 -2^63-1 無號數 型別 大小 範圍 uint 取決平台 取決平台 uint 8 8 bits 0 到 255 uint 16 16 bits 0 到 2^16-1 uint 32 32 bits 0 到 2^32-1 uint 64 64 bits 0 到 -2^64-1 int 、 uint 的型別大小取決於平台。在 32 位元系統上，它大小為 32 位元，64 位元系統上，它的大小為 64 位元。\n使用整數值時，除非確定會用到的大小跟範圍才使用有號數及無號數，否則應該都使用 int 資料型別。\nfunc main() { var myInt8 int8 = 97 var myInt = 1200 var myUint uint = 500 var myOctalNumber = 034 var myHexNumber = 0xFF fmt.Printf(\"%d, %d, %d, %#o, %#x\\n\", myInt8, myInt, myUint, myOctalNumber, myHexNumber) } $ go run . 97, 1200, 500, 034, 0xff 在 Go 中，也可以使用前綴 0 來宣告八進制數字，或是使用 0x 來宣告十六進制數字。\n浮點數 Float 浮點數型別用於儲存小數部分的數字。Go 有兩種浮點數型別：\nfloat32：在記憶體中佔用32位元，並以單精度浮點數格式儲存。 float64：在記憶體中佔用64位元，並以雙精度浮點數格式儲存。 浮點數的預設是 float64，除非初始化有為浮點數變數指定型別，否則編譯器將判定為 float64。\nfunc main() { var a = 245.4664 var b float32 = 1452.34 fmt.Printf(\"%f(%T)\\n%f(%T)\\n\", a,a,b,b) } $ go run . 245.466400(float64) 1452.339966(float32) 布林值 Bool Go 提供了一種稱為 bool 的資料型別來儲存布林值。它有兩個可能的值：true 和 false。\nfunc main() { var a = true var b bool = false fmt.Printf(\"%v(%T)\\n%v(%T)\\n\", a,a,b,b) } $ go run . true(bool) false(bool) 布林型別也可以使用運算子 \u0026\u0026 (與,and)、|| (和,or)、! (否定)\nfunc main() { var a = 4 \u003c= 7 var b = 10 != 10 var c = 10 \u003e 20 \u0026\u0026 5 == 5 var d = 2 * 2 == 4 || 10 / 3 == 3 fmt.Printf(\"%v\\n%v\\n%v\\n%v\\n\", a,b,c,d) } $ go run . true false false true 映射 Map 映射 (Map) 是 Go 內建的類型，是一種鍵值(key-value)的集合，可以透過 key 快速查詢並找到數據。\nfunc main (){ var map3 = map[int]string{99 : \"Go\", 87 : \"Python\", 79 : \"Java\", 93: \"Html\"} fmt.Println(map3) fmt.Println(\"map3[99] =\",map3[99],\"map3[79] =\",map3[79]) map3[79] = \"PHP\" fmt.Println(\"修改數據後，map3[99] =\",map3[99],\"map3[79] =\",map3[79]) } map[79:Java 87:Python 93:Html 99:Go] map3[99] = Go map3[79] = Java 修改數據後，map3[99] = Go map3[79] = PHP 數字型別的運算 Go 提供了多種用於數字、浮點數型別執行運算的運算子\n算術運算子：+、-、*、/、% 比較運算子：==、!=、\u003c、\u003e、\u003c=、\u003e= 位元運算子：\u0026、 |、 ^、 \u003c\u003c、\u003e\u003e 遞增和遞減運算子：++、-- 賦值運算子：+=、-=、*=、/=、%=、\u003c\u003c=、\u003e\u003e=、\u0026=、|=、^= import ( \"fmt\" \"math\" ) func main() { var a, b = 4, 5 var res1 = (a + b) * (a + b) / 2 a++ b += 10 var res2 = a ^ b var r = 3.5 var res3 = math.Pi * r * r fmt.Printf(\"res1 : %v, res2 : %v, res3 : %v\\n\", res1, res2, res3) } res1 : 40, res2 : 10, res3 : 38.48451000647496 型別轉換 Go 有一個強型別系統，它不允許混合型別。舉例來說：不能把 int 變數加到 float64 變數中，也不能將 int 變數加到 int64 變數中。\nfunc main() { var a int64 = 4 var b int = int(a) var c int = 500 fmt.Println(a, b,a+c) } $ go run . # command-line-arguments ./.:11:19: invalid operation: a + c (mismatched types int64 and int) 就會跳出錯誤說明別不同，無法直接做運算，那要怎麼辦呢！？\n使用型別轉換，將型別值轉成相同的\nfunc main() { var a int64 = 4 var b int = int(a) var c int = 500 fmt.Println(a, b,int(a)+c) } $ go run . 4 4 504 一般將值 v 轉換為型別 T 的語法是 T(V) 。","go-資料結構#Go 資料結構":"指標 Pointer 指標是程式語言中的資料結構及其物件或變數，用來表示或儲存記憶體位址，這個位址的值直接指向存在該位址物件的值。\nGo 支持指標，指標的聲明方式為 *T，可以藉由變數名稱前面加 \u0026 來獲得變數的位址，由於 Go 支持 GC ，在 Go 語言中不支持指標的運算。\n表示法\n使用 \u0026 來獲得指標位址 使用 * 來獲得指標所指向的值 func main (){ var a int = 2 fmt.Println(\"a 位址 = \",\u0026a) fmt.Printf(\"a 的值 = %v\\n\",a) var pInt *int = \u0026a fmt.Printf(\"pInt = %v\\n\", pInt); fmt.Printf(\"pInt 位址 = %v\\n\",\u0026pInt); fmt.Printf(\"pInt 指向的值 = %v\\n\",*pInt); } $ go run . a 位址 = 0xc0000b2008 a 的值 = 2 pInt = 0xc0000b2008 pInt 位址 = 0xc0000ac020 pInt 指向的值 = 2 陣列 Array 我們已經學會要怎麼宣告變數，以及如何使用變數來儲存值。但當我們今天想要儲存多個數值，使用原本的方式，需要創建許多變數才能儲存，因此有了陣列可以來儲存大量資料。\n陣列 (Array) 是由同類型的元素集合所組成的資料結構，分配一塊連續的記憶體來儲存。利用元素的索引可以計算出該元素所對應的儲存位址。\nfunc main (){ var a [2] float32 a [0] = 1.4 a [1] = 3.14 // 也可以寫成這樣 var b = [] int{10,20,99,333} fmt.Println(a,b) fmt.Println(len(a),len(b)) } $ go run . [1.4 3.14] [10 20 99 333] 2 4 上面有提到他會分配連續記憶體來儲存，我們來看看他是怎麼存的！ func main (){ a := [...]int{1, 2, 3} fmt.Printf(\"a 的記憶體分配位置 %p \\n\", \u0026a) fmt.Printf(\"陣列 a 的索引 0 記憶體分配位置 %p \\n\", \u0026a[0]) fmt.Printf(\"陣列 a 的索引 1 記憶體分配位置 %p \\n\", \u0026a[1]) fmt.Printf(\"陣列 a 的索引 2 記憶體分配位置 %p \\n\", \u0026a[2]) } $ go run . a 的記憶體分配位置 0xc0000180f0 陣列 a 的索引 0 記憶體分配位置 0xc0000180f0 陣列 a 的索引 1 記憶體分配位置 0xc0000180f8 陣列 a 的索引 2 記憶體分配位置 0xc000018100 切片 Slice 前面提到陣列的使用，陣列使用上是實值類型以及陣列長度不可變的情況下，間接限制了使用場景。\n切片 (slice) 是 Go 對陣列在進行一層的封裝，是一個擁有相同類型元素的可變長度序列，可以非常靈活運用，自動擴容，可以快速且方便的操作數據集合。\nfunc main (){ a := [5]int{55, 75, 58, 60, 66} b := a[1:4] //基於 a 陣列創建切片，等於 b 包含 a[1],a[2],a[3] fmt.Printf(\"%v(%T)\\n\",b,b) fmt.Printf(\"len = %v\\n\",len(b)) fmt.Printf(\"cap = %v\\n\",cap(b)) } [75 58 60]([]int) len = 3 cap = 4 len(b) 表示可見元素有幾個(直接打印元素看到的元素個數)，而 cap(b) 表示所有元素有幾個。 [1:4] 代表從第二個元素開始 (0為第一個元素，1位第二的元素)，取到第4個元素 (下標為 4-1=3，下標3代表第四個元素)\n使用 make 創建切片\nfunc main (){ a := make([]int,5,10) //創建長度 5,容量 10 的切片 fmt.Printf(\"a = %v, len(a) = %v, cap(a) = %v\\n\",a,len(a),cap(a)) } a = [0 0 0 0 0], len(a) = 5, cap(a) = 10 使用 make 創建長度5 ,容量 10 的切片\n使用 append 來達成新增元素\nfunc main (){ a := make([]int,2,2) //創建長度 5,容量 10 的切片 fmt.Printf(\"a = %v, len(a) = %v, cap(a) = %v\\n\",a,len(a),cap(a)) fmt.Printf(\"指標為 %p\\n\", a) a = append(a, 10) fmt.Printf(\"a = %v, len(a) = %v, cap(a) = %v\\n\",a,len(a),cap(a)) fmt.Printf(\"擴容後指標 = %p 改變\\n\", a) } a = [0 0], len(a) = 2, cap(a) = 2 指標為 0xc000014080 a = [0 0 10], len(a) = 3, cap(a) = 4 擴容後指標 = 0xc000022080 改變 結構 Struct 我們介紹的變數都是儲存單一的值或是多個相同型態的值，那如果要用變數表示較複雜的概念，像是紀錄一個人的名字、年齡或是身高時，由於這些是不同的資料型態，所以要記錄下來時，就必須使用不同的容器，這裡會介紹 Go 語言中的結構 Struct：\n建立結構\ntype Student struct { Id int Name string Score float64 } func main() { student := Student{1, \"ian\", 89.4} fmt.Println(student) } $ go run . {1 ian 89.4} 我們先宣告一個名為 Student 的 struct 結構，裡面屬性有 Id 和 Name 以及 Score，再以此結構宣告一個變數 student 並填入屬性，以顯示不同資料型態的值。","go-迴圈#Go 迴圈":"迴圈是每個程式語言必備的函式，藉以迴圈來達成反覆或是循環的動作。而 Go 語言的迴圈只有使用 for 迴圈來表達三種不同的迴圈 (for、while、loop)：\nfunc main (){ for i := 0 ; i \u003c= 10 ; i++ { fmt.Println(i) } } $ go run . 0 1 2 3 4 5 6 7 8 9 10 break\n也可以使用 break，依條件需求讓迴圈提早跳出結束：\nfunc main (){ for i := 0 ; i \u003c= 5 ; i++ { if i \u003e 3 { break } fmt.Println(i) } } $ go run . 0 1 2 3 Continue\n或是使用 continue 若符合條件，便會跳過到此迴圈，直接進入下個迭代：\nfunc main (){ for i := 1; i \u003c= 10; i++ { if i%2 == 0 { continue } fmt.Println(i) } } $ go run . 1 3 5 7 9 GoTo\n使用 goto 可以無條件轉移到程式中指定的行\nfunc main (){ fmt.Println(\"1\") fmt.Println(\"2\") goto labele1 fmt.Println(\"3\") labele1: fmt.Println(\"4\") fmt.Println(\"5\") fmt.Println(\"6\") } $ go run . 1 2 4 5 6 ","goroutine#Goroutine":"有人把 Go 比作 21 世紀的 C 語言，第一是因為 Go 語言設計簡單，第二，21 世紀最重要的就是並行程式設計，而 Go 從語言層面就支援了併發。\nGoroutine 是 Go 語言實現併發的一種方式。\nGoroutine 是一種非常輕量級的執行緒，它是Go語言併發設計的核心。執行 Goroutine 只需極少的記憶體，可同時執行成千上萬個併發的任務。\n單執行緒 我們先來看看一般的單執行緒\nfunc say(s string) { for i := 0; i \u003c 2; i++ { time.Sleep(100 * time.Millisecond) fmt.Println(s) } } func main() { say(\"world\") say(\"hello\") } $ go run . world world hello hello 在單執行緒下，每行程式碼都會依照順序執行。\n單執行緒 (Go 的並發：Goroutine 與 Channel 介紹)\n多執行緒 在多執行緒下，最多可以同時執行與 CPU 數相等的 Goroutine。要如何使用多執行緒，使用 goroutine 來執行多併發，只要使用 go 這個關鍵字來執行 func 就可以了 !\nfunc say(s string) { for i := 0; i \u003c 2; i++ { time.Sleep(100 * time.Millisecond) fmt.Println(s) } } func main() { go say(\"world\") say(\"hello\") } $ go run . hello world world hello 如此一來, say(\"world\") 會跑在另一個執行緒(Goroutine)上，使其並行執行。\n多執行緒 (Go 的並發：Goroutine 與 Channel 介紹)\n等待 多執行緒下，經常需要處理的是執行緒之間的狀態管理，其中一個經常發生的事情是等待。\n例如A執行緒需要等B執行緒計算並取得資料後才能繼續往下執行，在這情況下等待就變得十分重要。我們會介紹三個等待的方式，並說明其缺點。\ntime.sleep：休眠指定時間 sync.WaitGroup：等待直到指定數量的 Done() 呼叫 Channel 阻塞： Channel 阻塞機制，使用接收時等待的特性避免執行緒繼續執行 time.sleep 使 Goroutine 休眠，讓其他的 Goroutine 在 main 結束前有時間執行完成。\nfunc say(s string) { for i := 0; i \u003c 2; i++ { fmt.Println(s) } } func main() { go say(\"world\") go say(\"hello\") time.Sleep(5 * time.Second) } $ go run . world world hello hello 缺點：休息指定時間可能會比 Goroutine 需要執行的時間長或短，太長會耗費多餘的時間，太短會使其他 Goroutine 無法完成。\ntime.sleep (Go 的並發：Goroutine 與 Channel 介紹)\nsync.WaitGroup func say(s string, wg *sync.WaitGroup) { defer wg.Done() for i := 0; i \u003c 2; i++ { time.Sleep(100 * time.Millisecond) fmt.Println(s) } } func main() { wg := new(sync.WaitGroup) wg.Add(2) go say(\"world\", wg) go say(\"hello\", wg) wg.Wait() } $ go run . hello world world hello 在程式碼尾端加上 wg.Wait()，需要讓他達成一些條件，才可以往後執行，而這個條件，就是收到 wg.Done() 的呼叫次數。而這個次數，即是 wg.Add(2) 裡的數字2。\n優點：避免時間預估的錯誤 缺點：需要手動配置對應的數量 sync.WaitGroup (Go 的並發：Goroutine 與 Channel 介紹)\nChannel 阻塞 Channel 原為 Goroutine 溝通時使用的，但因其阻塞的特性，使其可以當作等待 Goroutine 的方法。\nfunc say(s string, c chan string) { for i := 0; i \u003c 2; i++ { time.Sleep(100 * time.Millisecond) fmt.Println(s) } c \u003c- \"FINISH\" } func main() { ch := make(chan string) go say(\"world\", ch) go say(\"hello\", ch) \u003c-ch \u003c-ch } $ go run . hello world hello world 兩個 Goroutine ( say(\"world\", ch) 、 say(\"hello\", ch) ) ，因此需要等待兩個 FINISH 推入 Channel 中才能結束 Main Goroutine。\n優點：避免時間預估的錯誤、語法簡潔 Channel 阻塞 (Go 的並發：Goroutine 與 Channel 介紹)","什麼是-go-#什麼是 Go ?":"Go 全名是 go programming language 又被稱為 Golang，是因為 go 這個詞太廣泛，不易搜尋，所以也可以叫 Golang。\nGo 是由 Google 開發並維護的編譯程式語言，支援垃圾回收與併發，由於開發人之一也是 C 語言的作者，所以 Go 也繼承了許多 C 的風格 其特點有以下幾點：\n靜態型別：因爲靜態型別的特性，可以在編譯期間就進行完整的型別檢查，可以找出大部分的型別錯誤。 編譯速度：因為 Go 語言先天優勢是架構設計非常單純，並不像物件導向語言龐大，在編譯時不用相依其他的 library，因此讓他有更好的執行效率。 語法簡潔：Go 關鍵字不多，不到30幾個，因為其關鍵字不少也與 C 的關鍵字重複，學習更容易上手。 垃圾回收：Go 有自動內存回收機制，不需要由開發人員來管理。垃圾回收是一種記憶體管理機制。當程式所佔用記憶體不再被該程式給存取時，會借助垃圾回收演算法，將記憶體空間歸還給作業系統。 原生支援併發：Go 語言支持併發，只需要透過 go 關鍵字來開啟 goroutine 將可。goroutine 是 Go 語言實現併發的一種方式，在執行的過程需要少量的記憶體，來暫存自己的上下文，就可在不同的時間點來分段執行程式。並且有 channel 可以跟 goroutine 進行資料溝通。 靜態型別/動態型別\n靜態型別的意思是指當宣告一個變數時，你必須同時宣告此變數所存放的資料型態為何\nvar age int // int var name string // string 動態型別指的是程式執行時，系統才可以看見的型別，什麼型別都可以\nvar i interface {} i = 18 i = \"Golang 程式設計\" 編譯式語言/直譯式語言\n靜態型別的意思是指當宣告一個變數時，你必須同時宣告此變數所存放的資料型態為何\n編譯式：當我們寫完程式時，我們需要將程式 compile (編譯) 成電腦看得懂的程式，再將程式拿去執行。 直譯式：當我們寫完程式時，直接使用直譯器一行一行翻譯成電腦語言並執行。 比較： 編譯式執行效率較佳 直譯式相對容易 Debug 編譯式語言編譯完後，可以直接在各類 OS 系統中執行，因為編譯完的程式，就是電腦看得懂的。 那我們大致了解 Go 後就來安裝 Go吧！","參考資料#參考資料":"Go 官網：https://go.dev/doc/tutorial/getting-started\ngolang後端入門分享：https://ithelp.ithome.com.tw/users/20137500/ironman/4184\n從一知半解到略懂 Go modules：https://myapollo.com.tw/zh-tw/golang-go-module-tutorial/\n30天就Go(3)：操作指令及Hello World!：https://ithelp.ithome.com.tw/articles/10186546\nGolang 基本型別、運算子和型別轉換：https://calvertyang.github.io/2019/11/05/golang-basic-types-operators-type-conversion/\nGo語言字符類型（byte和rune）：http://c.biancheng.net/view/18.html\ngolang初探：https://ithelp.ithome.com.tw/users/20129671/ironman/3326\nGo 的並發：Goroutine 與 Channel 介紹：https://peterhpchen.github.io/2020/03/08/goroutine-and-channel.html","安裝-go#安裝 Go":"本次作業系統為 macOS，所以後續都以 macOS 為主，如果是使用其他的作業系統，可以直接到官網來下載。\nmacOS 可以用 brew 等工具來下載，但這次我使用官網直接下載 pkg 來安裝。\nGo 官網下載位置\n下載完後，使用 go version 來檢查是否安裝成功：\n$ go version go version go1.18 darwin/amd64 接著我們來看一下他的環境變數，使用 go env，來查看：\n$ go env GO111MODULE=\"\" GOARCH=\"amd64\" GOBIN=\"\" GOCACHE=\"/Users/ian_zhuang/Library/Caches/go-build\" GOENV=\"/Users/ian_zhuang/Library/Application Support/go/env\" GOEXE=\"\" GOEXPERIMENT=\"\" GOFLAGS=\"\" GOHOSTARCH=\"amd64\" GOHOSTOS=\"darwin\" GOINSECURE=\"\" GOMODCACHE=\"/Users/ian_zhuang/go/pkg/mod\" GONOPROXY=\"\" GONOSUMDB=\"\" GOOS=\"darwin\" GOPATH=\"/Users/ian_zhuang/go\" GOPRIVATE=\"\" GOPROXY=\"https://proxy.golang.org,direct\" GOROOT=\"/usr/local/go\" 這邊簡單的列出來，比較重要的是\nGOPATH：他是有關管理程式碼和套件執行檔的地方，在 Go 1.8 版本以前，GOPATH 預設為空。從 1.8 以後，Go 安裝完後，都會直接給預設的路徑 說一下這個路徑的內容，我在依照預設的路徑，像我是在 Users/使用者/go，在 go 底下新增三個資料夾：\nsrc：主要放置專案的地方 pkg：套件主要儲存的資料夾 bin：存放編譯好的執行檔案 在 Go 1.11 後提供了 go modules 讓我們不一定要把專案程式碼放在 $GOPATH/src 中做開發，因此我們先來設定我在要放專案的資料夾，打開 .bash_profile：\nexport GOPATH=你專案的路徑/goworkspace $ source .bash_profile 接著我們來實作第一隻 Go 程式吧，我們會依照官網的示範，但為了要介紹 modules 是什麼，所以會小修改內容。","第一隻-go-程式#第一隻 Go 程式":"我們依照官網的示範教學，先建立一個資料夾(要放在我們剛剛的 GOPATH 目錄下方歐)，來放我們第一個 Go 程式 “印出 Hello world” (程式碼可以從此處下載)：\nmkdir helloworld cd helloworld 接著下指令來新增 Go module：\n$ go mod init helloworld go: creating new go.mod: module helloworld 如果成功，會產生一個 go.mod 檔案，我們來看看內容有什麼：\n$ cat go.mod module helloworld go 1.18 go.mod 是用來定義 module 的文件，用來標示此 module 的名稱、所使用的 go 版本以及相依的 Go module。\n我們分別再新增兩個資料夾，以及兩個 .go 檔，來建立我們範例所需要的環境：\nmkdir greeting cli touch greeting/greeting.go cli/say.go 到目前為止結構如下：\n. ├── cli │ └── say.go ├── go.mod └── greeting └── greeting.go 我們來修改一下 greeting.go 以及 say.go 程式碼吧。\ngreeting.go 是一個簡單的 package，用以顯示所傳入的字串 ; 而 say.go 則是以呼叫 greeting.go package 所提供的函式來顯示資料。\ngreeting.go 內容：\npackage greeting import \"fmt\" func Say(s string) { fmt.Println(s) } say.go 內容：\npackage main import ( \"helloworld/greeting\" ) func main(){ greeting.Say(\"Hello World\") } 順便來介紹一下程式裡面分別是什麼意思吧！\nPackage：package 主要分成兩種，一個是可執行，另一個則是可重複使用的，而 package main 就是可執行的檔案，像我們上面這個有包含 package main 的檔案，在編譯時，就會產生一個 say 的執行檔，電腦就是依照此檔案執行的。\nImport：當我們寫程式時，一定會引入其他人寫的套件。而 Go 語言的標準函式庫為開發團隊先寫好，提供一些常用的功能，當然也可以使用其他第三方套件，還滿足內建以及標準函式庫的不足。我們在 greeting.go 裡面引入的 fmt 就是開發團隊寫好的，然而在 say.go 裡面引入的就是greeting.go ，我們就可以使用其內容的函式來做使用。\nMain Function：每個 Go 語言的專案基本上都會有一個主程式，主程式裡的程式通常都為最核心的部分。\n最後使用 go run say.go 來將此程式運行起來：\n$ go run say.go Hello World 就可以看到程式成功將 Hello World 給印出來拉！\n常見指令 接下來要簡單介紹一下常用的另外3個指令，分別是 go build、go install、go clean：\ngo get：來下載套件到當前的模組，並安裝他們\n$ go get github.com/fatih/color go: downloading github.com/fatih/color v1.13.0 .... 省略 .... go build：還記得我們前面說 Go 是編譯式程式，所以我們可以將程式用 go build 來編譯成電腦看得懂的執行檔歐，檔案會存放在當前目錄或是指定目錄中 ~\n$ ls go.mod $ go build cli/say.go go.mod say $ ./say Hello World 多的這個 say 就是編譯後的執行檔，將他執行會顯示跟我們使用 run 來運行的一樣，顯示 Hello World。\ngo install：如果編譯沒有錯誤，一樣跟 build 會產生執行檔，不同的是，會將執行檔，產生於 $GOPATH/bin 內。\n$ ls /Users/ian_zhuang/go/bin dlv go-outline gomodifytags goplay gopls gotests impl staticcheck $ go install $ ls /Users/ian_zhuang/go/bin dlv go-outline gomodifytags goplay gopls gotests hello impl staticcheck go clean：執行後會將 build 產生的檔案都刪除 (install 的不會)\n$ ls go.mod hello . $ go clean $ ls go.mod . 套件相依性管理 Go modules 提供的另一個方便的功能則是套件相依性管理，接下來實際透過以下指令來安裝套件：\n$ go get github.com/fatih/color go: downloading github.com/fatih/color v1.13.0 go: downloading github.com/mattn/go-isatty v0.0.14 go: downloading github.com/mattn/go-colorable v0.1.9 go: downloading golang.org/x/sys v0.0.0-20210630005230-0f9fa26af87c go: added github.com/fatih/color v1.13.0 go: added github.com/mattn/go-colorable v0.1.9 go: added github.com/mattn/go-isatty v0.0.14 go: added golang.org/x/sys v0.0.0-20210630005230-0f9fa26af87c 安裝成功，可以再查看一下 go.mod：\nmodule github.com/880831ian/go/helloworld go 1.18 require github.com/fatih/color v1.13.0 require ( github.com/mattn/go-colorable v0.1.12 // indirect github.com/mattn/go-isatty v0.0.14 // indirect golang.org/x/sys v0.0.0-20220319134239-a9b59b0215f8 // indirect ) 會多了下面這些 require github.com/fatih/color v1.13.0 代表目前專案使用 v1.13.0 版本的 github.com/fatih/color\n下面的 indirect 指的是被相依的套件所使用的 package\n接著我們將 greeting.go、say.go 兩個檔案修改一下，使用我們剛剛所安裝的 package：\ngreeting.go\npackage greeting import ( \"fmt\" \"github.com/fatih/color\" ) func Say(s string) { fmt.Println(s) } func SayWithRed (s string) { color.Red(s) } func SayWithBlue (s string) { color.Blue(s) } func SayWithYellow (s string) { color.Yellow(s) } 我再多 import 了剛剛的 github.com/fatih.color，並使用該套件的函式 color 來分別顯示 Red、Blud、Yellow 三種顏色。\nsay.go\npackage main import ( \"github.com/880831ian/go/helloworld/greeting\" ) func main(){ greeting.Say(\"Hello World\") greeting.SayWithRed(\"Hello World\") greeting.SayWithBlue(\"Hello World\") greeting.SayWithYellow(\"Hello World\") } 我們將 greeting 三種顯示顏色的函式帶入。\n一樣我們來運行一下程式，來看看結果如何，這次我們直接編譯，使用 go build 來編譯，最後直接執行產生的執行檔：\n$ go build cli/say.go $ ./say Hello World Hello World //紅色 Hello World //藍色 Hello World //黃色 由於 Makedown 沒辦法於程式碼區域顯示正確顏色，用註解標示一下XD","變數#變數":"在使用變數做宣告時，要注意以下幾個 Go 保留字，不能拿來當變數名稱，其 Go 有三種宣告的方式：\nGo 保留字，不能拿來當變數名稱\n使用 := 來宣告 表示之前沒有進行宣告過。這是 Go 中最常見的變數宣告方式，但不能用縮寫方式來定義變數 (foo := bar) ，因為 package scope 的變數都是以 keyword 作為開頭。且只能在 function 中使用。\nfunc main (){ a := \"bar\" b := 4 c := true // 也可以簡寫成這樣 d,e,f := \"bar\",4,true fmt.Println(a,b,c); fmt.Println(d,e,f); } $ go run . bar 4 true bar 4 true 先宣告資料型態 當不知道變數的起始值，或是需要在 package scope 中宣告變數時可以使用。\nvar a string var b int // 也可以簡寫成這樣 var ( c string d float64 ) func main (){ a = \"Hello\" b = 123 c = \"ian\" d = 3.5 fmt.Println(a,b,c,d) } ⚠️ 不建議把變數寫在全域變數中 ⚠️\n$ go run . Hello 123 ian 3.5 直接宣告並賦值 func main (){ var ( a string = \"Hello\" b int = 9999 ) fmt.Println(a,b) } $ go run . Hello 9999 常見宣告錯誤 重複宣告變數 func main (){ name := \"ian\" name := \"pinyi\" } $ go run . # github.com/880831ian/go/test ./test.go:4:2: name declared but not used ./test.go:5:7: no new variables on left side of := 在 main 函式外賦值 var a int b := \"Hello\" func main (){ fmt.Println(a,b) } $ go run . # github.com/880831ian/go/test ./test.go:6:1: syntax error: non-declaration statement outside function body 我們可以在 main 函式外宣告變數，但無法在 main 函式外賦值\n沒有宣告就使用變數 func main (){ a = 123 b = true fmt.Println(a,b) } $ go run . # github.com/880831ian/go/test ./test.go:6:2: undefined: a ./test.go:7:2: undefined: b ./test.go:8:14: undefined: a ./test.go:8:16: undefined: b "},"title":"Go (Golang) 介紹"},"/blog/golang/go-restful-api-repository-messageboard/":{"data":{"":"本文章是使用 Go 來寫一個 Repository Restful API 的留言板，並且會使用 gin 以及 gorm (使用 Mysql)套件。\n建議可以先觀看 Go 介紹 文章來簡單學習 Go 語言。\n範例程式連結 點我 😘\n版本資訊\nmacOS：11.6 Go：go version go1.18 darwin/amd64 Mysql：mysql Ver 8.0.28 for macos11.6 on x86_64 (Homebrew) ","postman-測試#Postman 測試":"查詢全部留言 - 成功(無資料) 查詢留言 成功(無資料)\n查詢全部留言 - 成功(有資料) 查詢留言 成功(有資料)\n查詢{id}留言 - 成功 查詢{id}留言 成功\n查詢{id}留言 - 失敗 查詢{id}留言 失敗\n新增留言 - 成功 新增留言 成功\n修改{id}留言 - 成功 修改 {id}留言 成功\n修改{id}留言 - 失敗 修改 {id}留言 失敗\n刪除{id}留言 - 成功 刪除 {id}留言 成功\n執行結果 執行結果","參考資料#參考資料":"基於Gin+Gorm框架搭建MVC模式的Go語言後端系統：https://iter01.com/609571.html","實作#實作":"檔案結構 . ├── controller │ └── controller.go ├── go.mod ├── go.sum ├── main.go ├── model │ └── model.go ├── repository │ └── repository.go ├── router │ └── router.go └── sql ├── connect.yaml └── sql.go 我們來說明一下上面的資料夾個別功能與作用\nsql：放置連線資料庫檔案。 controller：商用邏輯控制。 model：定義資料表資料型態。 repository：處理與資料庫進行交握。 router：設定網站網址路由。 go.mod 一開始我們創好資料夾後，要先來設定 go.mod 的 module\n$ go mod init message go.mod 檔案 module message go 1.18 接著使用 go get 來引入 gin、gorm、mysql、yaml 套件\n$ go get -u github.com/gin-gonic/gin $ go get -u gorm.io/gorm $ go get -u gorm.io/driver/mysql $ go get -u gopkg.in/yaml.v2 可以在查看一下 go.mod 檔案是否多了很多 indirect\nmain.go package main import ( \"fmt\" \"message/model\" \"message/router\" \"message/sql\" ) func main() { //連線資料庫 if err := sql.InitMySql(); err != nil { panic(err) } //連結模型 sql.Connect.AutoMigrate(\u0026model.Message{}) //sql.Connect.Table(\"message\") //也可以使用連線已有資料表方式 //註冊路由 r := router.SetRouter() //啟動埠為8081的專案 fmt.Println(\"開啟127.0.0.0.1:8081...\") r.Run(\"127.0.0.1:8081\") } 引入我們 Repository 架構，將 config、model、router 導入，先測試是否可以連線資料庫，使用 AutoMigrate 來新增資料表(如果沒有才新增)，或是使用 Table 來連線已有資料表，註冊網址路由，最後啟動專案，我們將 Port 設定成 8081。\nsql 我們剛剛有引入 yaml 套件，因為我們設定檔案會使用 yaml 來編輯\nconnect.yaml host: 127.0.0.1 username: root password: \"密碼\" dbname: \"資料庫名稱\" port: 3306 我們把 mysql 連線的資訊寫在此處。\nsql.go (下面為一個檔案，但長度有點長，分開說明) package sql import ( \"io/ioutil\" \"fmt\" \"gopkg.in/yaml.v2\" \"gorm.io/gorm\" \"gorm.io/driver/mysql\" ) import 會使用到的套件。\nvar Connect *gorm.DB type conf struct { Host string `yaml:\"host\"` UserName string `yaml:\"username\"` Password string `yaml:\"password\"` DbName string `yaml:\"dbname\"` Port string `yaml:\"port\"` } func (c *conf) getConf() *conf { //讀取config/connect.yaml檔案 yamlFile, err := ioutil.ReadFile(\"sql/connect.yaml\") //若出現錯誤，列印錯誤訊息 if err != nil { fmt.Println(err.Error()) } //將讀取的字串轉換成結構體conf err = yaml.Unmarshal(yamlFile, c) if err != nil { fmt.Println(err.Error()) } return c } 設定資料庫連線的 conf 來讀取 yaml 檔案。\n//初始化連線資料庫 func InitMySql() (err error) { var c conf //獲取yaml配置引數 conf := c.getConf() //將yaml配置引數拼接成連線資料庫的url dsn := fmt.Sprintf(\"%s:%s@tcp(%s:%s)/%s?charset=utf8mb4\u0026parseTime=True\u0026loc=Local\", conf.UserName, conf.Password, conf.Host, conf.Port, conf.DbName, ) //連線資料庫 Connect, err = gorm.Open(mysql.New(mysql.Config{DSN: dsn}), \u0026gorm.Config{}) return } 初始化資料庫，會把剛剛讀取 yaml 的 conf 串接成可以連接資料庫的 url ，最後連線資料庫。\nrouter.go package router import ( \"message/controller\" \"github.com/gin-gonic/gin\" ) func SetRouter() *gin.Engine { //顯示 debug 模式 gin.SetMode(gin.ReleaseMode) r := gin.Default() v1 := r.Group(\"api/v1\") { //新增留言 v1.POST(\"/message\", controller.Create) //查詢全部留言 v1.GET(\"/message\", controller.GetAll) //查詢 {id} 留言 v1.GET(\"/message/:id\", controller.Get) //修改 {id} 留言 v1.PATCH(\"/message/:id\", controller.Update) //刪除 {id} 留言 v1.DELETE(\"/message/:id\", controller.Delete) } return r } 設定路由，版本 v1 網址是 api/v1 ，分別是新增留言、查詢全部留言、查詢 {id} 留言、修改 {id} 留言、刪除 {id} 留言，連接到不同的 controller function 。\nmodel.go package model import \"gorm.io/gorm\" func (Message) TableName() string { return \"message\" } type Message struct { Id int `gorm:\"primary_key,type:INT;not null;AUTO_INCREMENT\"` User_Id int `json:\"User_Id\" binding:\"required\"` Content string `json:\"Content\" binding:\"required\"` Version int `gorm:\"default:0\"` // 包含 CreatedAt 和 UpdatedAt 和 DeletedAt 欄位 gorm.Model } 設定資料表的結構，使用 gorm.Model 預設裡面會包含 CreatedAt 和 UpdatedAt 和 DeletedAt 欄位。\ncontroller.go (下面為一個檔案，但長度有點長，分開說明)\npackage controller import ( \"message/model\" \"message/repository\" \"net/http\" \"unicode/utf8\" \"github.com/gin-gonic/gin\" ) import 會使用到的套件。\n查詢留言功能\nfunc GetAll(c *gin.Context) { message, err := repository.GetAllMessage() if err != nil { c.JSON(http.StatusBadRequest, gin.H{\"message\": err.Error()}) return } c.JSON(http.StatusOK, gin.H{\"message\": message}) } func Get(c *gin.Context) { var message model.Message if err := repository.GetMessage(\u0026message, c.Param(\"id\")); err != nil { c.JSON(http.StatusNotFound, gin.H{\"message\": \"找不到留言\"}) return } c.JSON(http.StatusOK, gin.H{\"message\": message}) } GetAll() 會使用到 repository.GetAllMessage() 查詢並回傳顯示查詢的資料。\nc.Param(\"id\") 是網址讀入後的 id，網址是http://127.0.0.1:8081/api/v1/message/{id} ，將輸入的 id 透過 repository.GetMessage() 查詢並回傳顯示查詢的資料。\n新增留言功能\nfunc Create(c *gin.Context) { var message model.Message if c.PostForm(\"Content\") == \"\" || utf8.RuneCountInString(c.PostForm(\"Content\")) \u003e= 20 { c.JSON(http.StatusBadRequest, gin.H{\"message\": \"沒有輸入內容或長度超過20個字元\"}) return } c.Bind(\u0026message) repository.CreateMessage(\u0026message) c.JSON(http.StatusCreated, gin.H{\"message\": message}) } 使用 Gin 框架中的 Bind 函數，可以將 url 的查詢參數 query parameter，http 的 Header、body 中提交的數據給取出，透過 repository.CreateMessage() 將要新增的資料帶入，如果失敗就顯示 http.StatusBadRequest，如果成功就顯示 http.StatusCreated 以及新增的資料。\n修改留言功能\nfunc Update(c *gin.Context) { var message model.Message if c.PostForm(\"Content\") == \"\" || utf8.RuneCountInString(c.PostForm(\"Content\")) \u003e= 20 { c.JSON(http.StatusBadRequest, gin.H{\"message\": \"沒有輸入內容或長度超過20個字元\"}) return } if err := repository.UpdateMessage(\u0026message, c.PostForm(\"Content\"), c.Param(\"id\")); err != nil { c.JSON(http.StatusNotFound, gin.H{\"message\": \"找不到留言\"}) return } c.JSON(http.StatusOK, gin.H{\"message\": message}) } 先使用 repository.GetMessage() 以及 c.Param(\"id\") 來查詢此 id 是否存在，再帶入要修改的 Content ，透過 repository.UpdateMessage() 將資料修改，，如果失敗就顯示 http.StatusNotFound 以及找不到留言，如果成功就顯示 http.StatusOK 以及修改的資料。\n刪除留言功能\nfunc Delete(c *gin.Context) { var message model.Message if err := repository.DeleteMessage(\u0026message, c.Param(\"id\")); err != nil { c.JSON(http.StatusNotFound, gin.H{\"message\": \"找不到留言\"}) return } c.JSON(http.StatusNoContent, gin.H{\"message\": \"刪除留言成功\"}) } 透過 repository.DeleteMessage() 將資料刪除，如果失敗就顯示 http.StatusNotFound 以及找不到留言，如果成功就顯示 http.StatusNoContent。\nrepository.go (下面為一個檔案，但長度有點長，分開說明)\n所有的邏輯判斷都要在 controller 處理，所以 repository.go 就單純對資料庫就 CRUD：\npackage repository import ( \"message/model\" \"message/sql\" ) import 會使用到的套件。\n查詢留言資料讀取\n//查詢全部留言 func GetAllMessage() (message []*model.Message, err error) { err = sql.Connect.Find(\u0026message).Error return } //查詢 {id} 留言 func GetMessage(message *model.Message, id string) (err error) { err = sql.Connect.Where(\"id=?\", id).First(\u0026message).Error return } 新增留言資料讀取\n//新增留言 func CreateMessage(message *model.Message) (err error) { err = sql.Connect.Create(\u0026message).Error return } 修改留言資料讀取\n//更新 {id} 留言 func UpdateMessage(message *model.Message, content, id string) (err error) { err = sql.Connect.Where(\"id=?\", id).First(\u0026message).Update(\"content\", content).Error return } 刪除留言資料讀取\n//刪除 {id} 留言 func DeleteMessage(message *model.Message, id string) (err error) { err = sql.Connect.Where(\"id=?\", id).First(\u0026message).Delete(\u0026message).Error return } "},"title":"用 Go 寫一個 Repository Restful API 的留言板 (gin、gorm 套件)"},"/blog/kubernetes/":{"data":{"":"此分類包含 Kubernetes 相關的文章。\nKubernetes 開啟 Region 後，如何減少跨 Zone 網路費用發布日期：2025-05-15 K8s Node Log Stdout Logrotate 回收機制發布日期：2025-03-12 KubeDNS vs CoreDNS 比較發布日期：2024-07-23 Pod 出現 cURL error 6: Could not resolve host發布日期：2024-07-19 在正式環境上踩到 StatefulSet 的雷，拿到 P1 的教訓發布日期：2023-10-27 部署 Pod 遇到 container veth name provided (eth0) already exists 錯誤發布日期：2023-09-13 Kubernetes (K8s) 自定義 PHP HorizontalPodAutoscaler (HPA) 指標發布日期：2022-07-18 Kubernetes (K8s) HorizontalPodAutoscaler (HPA) 原理與實作發布日期：2022-07-12 用大型社區來介紹 Kubernetes 元件發布日期：2022-06-02 Kubernetes (K8s) 搭配 EFK 實作 (Deployment、StatefulSet、DaemonSet)發布日期：2022-05-10 Kubernetes (K8s) 介紹 - 進階 (Service、Ingress、StatefulSet、Deployment、ReplicaSet、ConfigMap)發布日期：2022-05-03 Kubernetes (K8s) 介紹 - 基本發布日期：2022-04-28 "},"title":"Kubernetes"},"/blog/kubernetes/gcp-k8s-hpa-php-custom-metrics/":{"data":{"":"此篇要介紹 HorizontalPodAutoscaler 的自定義指標，K8s 內建的指標 (metrics) 只支援 CPU 以及 Memory，如果我們今天想要使用其他的指標來讓 HPA 擴縮呢！？ 不知道什麼是 HorizontalPodAutoscaler 嗎？可以先查看：\nKubernetes (K8s) HorizontalPodAutoscaler (HPA) 原理與實作 這時候我們就必須使用自定義指標，我們一樣來說說他的工作原理吧：HorizontalPodAutoscaler 是怎麼取得自定義指標，以及是跟誰拿到指標的呢？我們從下圖得知：\nKubernetes HPA : ExternalMetrics+Prometheus\nHorizontalPodAutoscaler 會先訪問 K8s 的 API，並向 API 取得指標資料。這邊的 API 就是 custom.k8s.io/v1beta1。\n大致了解後，我們就來進入今天的重點，也就是透過自定義化 PHP 的指標來讓 HorizontalPodAutoscaler 進行擴縮，這次使用的平台是 Google Cloud Platform，前面介紹 GCP 服務的大家可以參考 Google Cloud Platform (GCP) 百科全書 - 介紹與開頭 [ EP.0 ]，我們這邊就直接跳到程式碼與操作部分。\n此文章程式碼也會同步到 Github ，需要的也可以去 clone 使用歐！ Github 程式碼連結 😆","參考資料#參考資料":"Autoscaling Deployments with Cloud Monitoring metric：https://cloud.google.com/kubernetes-engine/docs/tutorials/autoscaling-metrics#custom-prometheus_1\nGoogleCloudPlatform/k8s-stackdriver：https://github.com/GoogleCloudPlatform/k8s-stackdriver/tree/master/custom-metrics-stackdriver-adapter\nhipages/php-fpm_exporter：https://github.com/hipages/php-fpm_exporter\nScaling PHP-FPM with custom metrics on GKE/kubernetes：https://www.ashsmith.io/scaling-phpfpm-with-custom-metrics-gke","實作#實作":"安裝自定義的 Adapter 我們要先 Apply 自定義的 Adapter，這邊我們使用 Google 提供的 Stackdriver Adapter 來使用 (也可以直接使用 Github 程式碼中的 adapter_new_resource_model.yaml 檔案歐)：\nkubectl apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/k8s-stackdriver/master/custom-metrics-stackdriver-adapter/deploy/production/adapter_new_resource_model.yaml Deployment apiVersion: apps/v1 kind: Deployment metadata: name: demo labels: app: demo spec: replicas: 1 selector: matchLabels: app: demo template: metadata: labels: app: demo spec: containers: - name: php-fpm image: php:fpm workingDir: /var/www/service ports: - containerPort: 9000 resources: requests: cpu: 100m memory: 1G limits: cpu: 100m memory: 1G volumeMounts: - name: application mountPath: /var/www/service/ - name: php-fpm-config mountPath: /usr/local/etc/php-fpm.d/www.conf subPath: www.conf - name: nginx image: nginx:alpine workingDir: /var/www/service ports: - containerPort: 80 volumeMounts: - name: application mountPath: /var/www/service/ - name: nginx-config mountPath: /etc/nginx/conf.d/ - name: phpfpm-exporter image: hipages/php-fpm_exporter:latest env: - name: PHP_FPM_SCRAPE_URI value: \"tcp://localhost:9000/status\" - name: PHP_FPM_FIX_PROCESS_COUNT value: \"true\" resources: requests: cpu: 10m limits: cpu: 10m - name: prometheus-to-sd image: gcr.io/google-containers/prometheus-to-sd:v0.9.0 ports: - containerPort: 6060 protocol: TCP command: - /monitor - --stackdriver-prefix=custom.googleapis.com - --monitored-resource-type-prefix=k8s_ - --source=:http://localhost:9253 - --pod-id=$(POD_NAME) - --namespace-id=$(POD_NAMESPACE) resources: requests: cpu: 10m limits: cpu: 10m env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace volumes: - name: application emptyDir: {} - name: php-fpm-config configMap: name: php-fpm-conf - name: nginx-config configMap: name: nginx-conf 我來簡單說明一下，這是一個 Deployment，我們在每一個 Pod 裡面都放 4 個 Container，分別是 php-fpm、nginx、phpfpm-exporter、prometheus-to-sd\nphp-fpm 就是我們要使用的 php，nginx 會提供 9000 Port 讓 phpfpm-exporter 去抓到目前的 Process 數值，最後丟給 prometheus-to-sd，讓他去通知我們剛剛所安裝的 Adapter，就可以透過 API 讓 HPA 知道！聽不太懂嗎？沒關係，幫大家畫了一張圖，請參考下方圖片：\n架構圖\n可以看到 Deployment 裡面，我們使用 ConfigMap 來掛載 php 的 www.conf 以及 nginx 的設定檔，那我們接下來就寫一份 ConfigMap 吧！\nConfigMap apiVersion: v1 kind: ConfigMap metadata: name: php-fpm-conf data: www.conf: | [www] user = 900 group = 900 listen = 9000 listen.owner = 900 listen.group = 900 listen.mode = 0660 pm = dynamic pm.max_children = 150 pm.max_requests = 300 pm.start_servers = 24 pm.min_spare_servers = 24 pm.max_spare_servers = 126 pm.status_path = /status ping.path = /ping ping.response = OK catch_workers_output = yes request_terminate_timeout = 300 clear_env = no --- apiVersion: v1 kind: ConfigMap metadata: name: nginx-conf namespace: ian data: nginx.conf: | server { listen 80; listen [::]:80; server_name _; root /var/www/service/; index index.php; location / { try_files $uri $uri/ /index.php$is_args$args; } location ~ ^/(status|ping)$ { fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; } } 這份 ConfigMap.yaml 檔案裡面分成 php-fpm-conf 來放 www.conf，以及 nginx-conf 來放 nginx.conf 檔案，這邊要注意的是 www.conf 記得要加上 pm.status_path = /status，phpfpm-exporter 是透過這個頁面來獲得 Process 數量。\nHorizontalPodAutoscaler apiVersion: autoscaling/v2beta2 kind: HorizontalPodAutoscaler metadata: name: demo-hpa spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: demo minReplicas: 1 maxReplicas: 10 metrics: - type: Pods pods: metric: name: phpfpm_active_processes target: averageValue: 50 type: AverageValue - type: Pods pods: metric: name: phpfpm_idle_processes target: averageValue: 50 type: AverageValue - type: Pods pods: metric: name: phpfpm_total_processes target: averageValue: 50 type: AverageValue - type: Pods pods: metric: name: phpfpm_accepted_connections target: averageValue: 50 type: AverageValue 這份 HorizontalPodAutoscaler 我們使用的版本是 autoscaling/v2beta2， v2beta1 跟 v2beta2 的設定檔語法有些不同！\n可以看到我們 metrics.pods.metric.name 分別是 phpfpm_active_processes (活動的進程個數)、phpfpm_idle_processes (空閒的進程個數)、phpfpm_total_processes (總共的進程個數)、phpfpm_accepted_connections (當前的連接數量)，如果超過我們所設定的 target 值，HPA 就會作動。\n我們依序把 Deployment \u003e ConfigMap \u003e HorizontalPodAutoscaler 的 yaml 檔案給 Apply，可以觀察一下 Pod 是否都有正常啟動：\nPod 是否正常啟動\n我們到 HorizontalPodAutoscaler 查看我們所設定的 metrics 是否都有抓到目前的值：\nmetrics 是否都有抓到目前的值\n我們也可以用 Google Cloud Platform 的監控來查看：\nGoogle Cloud Platform 的監控\n以上就完成 自定義 HorizontalPodAutoscaler 指標囉～ 😍"},"title":"Kubernetes (K8s) 自定義 PHP HorizontalPodAutoscaler (HPA) 指標"},"/blog/kubernetes/k8s-advanced/":{"data":{"":"前面我們在基本篇中，為了使 Pod 能夠與本機連線，使用了 port-forward，還有另一種方法就是今天要介紹的第一個主題：Service\n那我們會先複習一下 port-forward，再來介紹 Service，當然後面也會有實際操作，請大家跟我繼續一起學下去吧 ！","port-forward#port-forward":"port-forward 簡單來說就是把本機的某一個 Port 與 Pod 所開放對外的 Port 做映射，就像是我們在 Docker 跑 container 時會使用 -p 來連結機器與 container 的 port 一樣～\n使用的方法也很簡單且方便，使用 kubectl port-forward \u003cpod\u003e \u003cexternal-port\u003e:\u003cpod-port\u003e，我們拿 Kubernetes 基本篇最後的範例，來做說明：\n$ kubectl port-forward kubernetes-demo-pod 3000:3000 Forwarding from 127.0.0.1:3000 -\u003e 3000 Forwarding from [::1]:3000 -\u003e 3000 Handling connection for 3000 我們把 kubernetes-demo-pod 這個 pod，用 port-forward 設定本機 3000 port 與 pod 3000 port 做映射，當我們瀏覽 http://localhost:3000 就可以看到 pod 裡面的內容了！\n雖然很方便，可以馬上就開好要映射的 port，但缺點就是每次建立 pod 時都需要手動去打指令來設定 port，且時間久了，也會忘記本機上哪些 port 有被使用到，因此這邊推薦使用 Service 來取代 port-forward，那我們來看看 Service 是什麼吧。","什麼是-configmap-#什麼是 ConfigMap ?":"看到 Config 應該會聯想到與設定檔有關，沒錯 ConfigMap 通常都是用來存放設定檔用的，換句話來說這個物件會直接連結一個或多個檔案，而 ConfigMap 通常都是用來存放偏向部署面的設定檔，像是資料庫的初始化設定、nginx 設定檔等等，這種不用被包進去 image 內，但需要讓 container 可以使用的檔案。\nConfigMap 特性 一個 ConfigMap 物件可以放一個或多個設定檔：我們上面有提到它是用來存放設定檔用的，會直接連接該設定檔。 無需修改程式碼，可以替換不同環境的設定檔：由於設定檔都交由 ConfigMap 管理，並不是包在 image 內，因此可以藉由修改 ConfigMap 的方式來達到不用更新 Pod 內容就可以更換設定檔的作用。 統一存放所有的設定檔：一個 ConfigMap 可以連結一個以上的設定檔，因此也可以將該專案會用到的所有設定檔通通存放在同一個 ConfigMap 物件中進行管理。 如何建立 ConfigMap？ 由於 ConfigMap 可以直接存入設定檔，所以我們以現有設定檔為基準，接下來要用 create 這個參數來建立 ConfipMap 物件出來，這時可能會想，之前都是使用 apply 怎麼這次要改用 create呢！？ 雖然兩者都有建立的意思，但背後實作的技術完全不同：\ncreate 使用的是 Imperative Management，Imperative Management 會告訴 Kubernetes 我目前的動作要做什麼，例如：create、delete、replace 某個物件。 apply 是使用 Declarative Management，Declarative Management 是用宣告的方式來建立物件，更白話一點就是我希望這個物件要長怎麼樣，所以 apply 通常都會搭配 yaml 檔，而這份 yaml 檔就會在 kind 這個設定值告訴 Kubernetes 這個物件要長成什麼樣子。 因為我們這次要直接使用現有的設定檔來建立 ConfigMap，所以這時候不能使用 apply 的方式，只能使用 create 來建立，指令也很簡單：\n$ kubectl create configmap \u003cconfigmapName\u003e --from-file=\u003cfilePath\u003e 建立 ConfigMap\n建立完成使用 get 來查詢是否正確建立 ConfigMap：\n使用 get 查詢是否正確 ConfigMap\n最後可以下 describe 這個參數來查看 ConfigMap 的內容，會發現裡面就是我們設定檔的完整內容：\n使用 describe 查看 ConfigMap 的內容","什麼是-deployment-#什麼是 Deployment ?":"Deployment 是一種負責管理 ReplicaSet 以及控制 Pod 更新的物件，在先前的文章都沒有提到 Pod 的更新，是因為 Pod 無法直接做更新，必須砍掉重建才會是新的內容，有了 Deployment 之後我們就可以很方便的進行 Pod 的更新了！\n由於 ReplicaSet 本身也會控制 Pod，所以整個看起來會像是 Deployment 控制 Pod，但其實真正控制 Pod 的是 ReplicaSet ~\nDeployment 與 ReplicaSet 架構圖\nDeployment 的特性 部署一個應用服務 上面我們提到 Deployment 可以幫助 Pod 進行更新，通常在開發一個產品的時候一定會不斷的更新，透過 Deployment 我們可以快速的更新 Pod 內部的 container，所以通常在部署應用的時候都會使用 Deployment 來進行部署。\n在服務升級過程中可以達成無停機服務遷移 (Zero downtime deployment) 在 Deployment 幫 Pod 內部 container 進行更新的過程有一個專有名稱叫做 RollingUpdate ，RollingUpdate 翻成中文的意思是滾動更新，在更新的過程中 Deployment 會先產生一個新的 ReplicaSet 而這個 ReplicaSet 內部的 Pod 會運行新的內容，待新的 Pod 被 Kubernetes 確認可以正常運行後 Deployment 才會將舊的 ReplicaSet 進行取代的動作，這樣就完成了無停機服務遷移了。\n可以 Rollback 到先前版本 每一次的 Deployment 在進行 RollingUpdate 的時候都會把當前的 ReplicaSet 做一個版本控制的紀錄，就像是 git commit 一樣，所以我們也可以利用這些紀錄來快速恢復成以前的版本，這些 Pod 也就會變成先前的內容。\n講完基本的介紹後，接下來要介紹的是要如何撰寫以及建立一個 Deployment：\nDeployment 搭配 ReplicaSet 寫法 Github 程式碼連結\napiVersion: apps/v1 kind: Deployment metadata: name: kubernetes-deployment spec: replicas: 3 strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 1 minReadySeconds: 60 revisionHistoryLimit: 10 template: metadata: labels: app: demo spec: containers: - name: kubernetes-demo-container image: 880831ian/kubernetes-demo ports: - containerPort: 3000 selector: matchLabels: app: demo 看到 Deployment 是不是覺得跟 Replication Controller 非常相似呢？其實 Deployment 就多了再 RollingUpdate 時的設定以及 ReplicaSet 的設定而已，下面來說明一下這些設定：\n首先一開始的 apiVersion 的值已經不是 v1 了，改成 apps/v1，由於 Kubernetes 針對 Deployment、RollingUpdate、ReplicaSet 等等設定了新的 apiVersion 值，通常都用 apps/v1 都是用來設定跟應用程式有關的架設，所以 Deployment 這邊要記得改成 apps/v1 歐！\n在 spec 的地方有看到 strategy 新的設定值，這個主要用來設定 Deployment 更新的策略，這裡的 strategy.type 有兩種設定：\nRollingUpdate 此為預設值，先建立新的 ReplicaSet 並控制新內容的 Pod，待新 Pod 也可以正常運作後，才會通知 ReplicaSet 將原有的 Pod 給移除，由於過程中會有新舊兩種 Pod 同時上線，因此會有一段時間是新舊內容會隨機出現的情形發生。\n這邊可以看到除了 type 以外還寫了 maxSurage 以及 maxUnavailablle，這兩個設定值為 RollingUpdate 過程的設定，接下來一樣說明一下兩個設定的功能：\nmacSurge：代表 ReplicaSet 可以產生比 Deployment 設定中的 replica 所設定的數量還多幾個出來，多新增 Pod 的好處是在 RollingUpdate 過程中可以減少舊內容顯示在頁面的機率。\nmacUnavailable：代表在 RollingUpdate 過程中，可以允許多少的 Pod 無法使用，假設 macSurge 設定非 0，maxUnavailable 也要設定非 0。\nRecreate 先通知當前 ReplicaSet 把舊的 Pod 砍掉再產生新的 ReplicaSet 並控制新內容的 Pod，由於先砍掉 Pod 才建立新的 Pod ，所以中間有一段時間伺服器會無法連線。\n也因為 Recreate 會砍掉重建，因此 Recreate 無法像 RollingUpdate 設定 maxSurge 以及 macUnavailable。\n講完 Deployment 的更新流程設定後，接下來要講 Deployment 完成更新後的設定，這邊有兩種設定：\nminReadySeconds minReadySeconds 代表當新的 Pod 建立好並且運行的 container 沒有 crash 的情況下，最少需要多少時間可以開始接受 Request，預設為 0 秒，代表當 Pod 一被建立起來，就可以馬上開始接受 Request，假設今天 container 在剛運行的時後需要花時間做初始化，這時候就可以利用 minReadySeconds 讓此 Pod 不會馬上接受到 Request ，這個是選填的設定。\nrevisionHistoryLimit 每次 Deployment 在進行更新的時候，都會產生一個新的 ReplicaSet 用來進行版本控制，在 Deployment 中這個專有名稱為 revision，所以這個設定就是要設定最多只會有多少個 revision，這個也是選填的設定。\nDeployment 建立 老樣子，使用 apply 來建立 Deployment，我們可使用 kubectl get deploy 來查看是否建立成功：\n$ kubectl get deploy NAME READY UP-TO-DATE AVAILABLE AGE kubernetes-deployment 3/3 3 3 8m31s 由於 Deployment 直接管理 ReplcaSet，因此我們可以查看 ReplcaSet 是否也有被建立起來：\n$ kubectl get rs NAME DESIRED CURRENT READY AGE kubernetes-deployment-dc5c59fdb 3 3 3 10m 可以看到 ReplcaSet 後面會自動加上一小段亂數，這邊是 Deployment 在建立 ReplicaSet 的時候加進去的，這樣之後可以更方便的利用 ReplcaSet 進行 rollback 的動作。\n由於 ReplicaSet 會直接管理 Pod，因此我們也可以查看 Pod 是否有被建立起來：\n$ kubectl get po NAME READY STATUS RESTARTS AGE kubernetes-deployment-dc5c59fdb-cjbz6 1/1 Running 0 12m kubernetes-deployment-dc5c59fdb-mjvd7 1/1 Running 0 12m kubernetes-deployment-dc5c59fdb-r92zt 1/1 Running 0 11m 那我們用 minikube dashboard 來查看一下：\nminikube dashboard 查看 Deployment 與 ReplicaSet\nDeployment 的最後要來說的是要如何更新底下的 Pod 呢，大家就接著往下看囉！\n如何更新 Deployment 內部的 Pod 大家都知道 Pod 是 Kubernetes 最小的運行單位，所以更新 Pod 的意思就是把內部運行的 container 進行更新，也就是說我們只要更新 Pod 的 image 就可以順利的讓 Pod 運行最新的內容，Deployment 就是運用這個原理才進行 Pod 的內容更新，方法也很簡單只要利用 Set 這個參數就可以了，可以使用 kubectl set -h 可以看 set 這個參數真正的用法。\nkubectl set\n可以看到 set 後面還要接 SUBCOMMAND，而 SUBCOMMAND 就是 Available Commands 的內容，由於我們要更新的是 image 所以這邊的 SUBCOMMAND 會是 image，完整指令是：\n$ kubectl set image deployment/kubernetes-deployment kubernetes-demo-container=880831ian/kubernetes-demo:v1 --record deployment.apps/kubernetes-deployment image updated 使用 set 來更新 Pod，格式是 kubectl set image deployment/\u003cdeployment name\u003e \u003cpod name\u003e=\u003c要更新的 image\u003e，後面加上 --record 參數，這樣會紀錄每次更新的時候到底更新哪些內容，這樣日後要進行 rollback 也會比較容易知道要 rollback 回哪個 revision，由於要顯示差異，所以在 dockerhub 上又多推一個版本 v1，將原本柴犬的圖片改成 kubernetes logo，等於我們更新是從 kubernetes-demo:latest 更新至 kubernetes-demo:v1，我們來看看它的變化吧！\n這是還沒更新 Pod 前的 Deployment 與 ReplicaSet：\nDeployment 與 ReplicaSet\n我們用 set 下完更新指令後，可以查看 ReplicaSet 以及 Pod 在更新過程中的變化：\nReplicaSet 以及 Pod 在更新過程中的變化\n可以發現 strategy 為 RollingUpdate 的時候並不會把舊有的 Pod 移除反而會讓新舊 Pod 同時上線，以達到無停機服務的作用，但這樣在網頁中就有可能會同時出現新舊內容。\n因為 strategy 為 RollingUpdate，所以會同時出現新舊內容\n最後等到新的 Pod 已經建立完成，且正常運作，ReplicaSet 就會把舊的 Pod 給移掉：\n新的 Pod 建立成功，並刪除舊 Pod\n再次瀏覽就可以發現已經變成新的內容了！代表我們完成更新動作～\n更新至 kubernetes-demo:v1\nDeployment 回朔版本 我們講完更新後，接著要講如何 rollback 回以前的版本，首先我們必須使用 rollout 這個參數，一樣使用 kubectl rollout -h 可以查看 rollout 的用法，跟 set 十分相似。\n更新至 kubernetes-demo:v1\n由於我們這邊要 rollback ，所以 SUBCOMMAND 會使用 undo，我們上面有用 --record 可以查看要 rollback 的版本，所以我們這邊寫法會長像這樣：\n$ kubectl rollout undo deployment/kubernetes-deployment --to-revision=1 deployment.apps/kubernetes-deployment rolled back 格式是 kubectl rollout undo deployment/\u003cdeployment name\u003e --to-revision=\u003c--record 版本\u003e\n就可以看到又回復成以前的柴犬囉 \u003e\u003c\n成功還原柴犬\n為甚麼不用 Replication Controller 最後要討論的是為什麼要不用 Replication Controller 而是改用 ReplicaSet + Deployment？\n由於實際再使用 Kubernetes 時架構會比現在練習的還要複雜，所以用 ReplicaSet 讓 selector 用更彈性的方式選取 Pod 會是比較好的做法。","什麼是-ingress-#什麼是 Ingress ?":"還記得我們在 Service - NodePort 時，需要打 \u003cip\u003e:\u003cport\u003e ，但現在網站除了網域以外，基本上不會需要自己去打 IP 以及 Port 了吧！那為了解決這個問題，有了 Ingreess。\nIngress 可以幫助我們統一對外的 port number，並根據 hostname 或是 pathname 來決定請求要轉發到哪一個 Service 上，之後就可以利用該 Service 連接到 Pod 來處理服務。\n我們先來看一下一般的 Service ：\nService 圖片來源：[Day 19] 在 Kubernetes 中實現負載平衡 - Ingress Controller\n可以看到當多個 Service 同時運行時，Node 都需要有對應的 port number 去對應每個 Server 的 port number。像是 GCP 這種雲端服務，每台機器都會配置屬於自己的防火牆。這也代表，不論新增、刪除 Service 物件，都必須要額外多調整防火牆的設定，Port 的管理也想對複雜。\n若是使用 Ingress，我們只需要開放一個對外的 port numer，Ingree 可以在設定檔中設定不同的路徑，決定要將使用者的請求傳送到哪一個 Service 物件上：\nIngress 圖片來源：[Day 19] 在 Kubernetes 中實現負載平衡 - Ingress Controller\n這樣的設計，除了讓維運人員不需要維護多個 port 或是頻繁的更改防火牆外，可以自訂條件的功能，也使得請求的導向可以更加彈性。\nIngress 功能 將不同\"路徑\"的請求對應到不同的 Service 物件 若沒有設定網域，則該機器上的所有網域只要透過此路徑都可以連接到指定的 Service 物件。\n將不同\"網域\"的請求對應到不同的 Service 物件 若沒有設定路徑，則會以 / 路徑連接到指定的 Service 物件。\n支援 SSL Termination SSL 全名是傳輸層安全性協定，而網站通常都會利用 https 進行加密以確保資料安全，但 Service 與 Pod 之間的溝通都是以無加密方式傳輸，所以 Ingress 就支援解密，讓 Service 與 Pod 可以正常溝通傳遞資料。\nminikube 啟動 Ingress 由於 minikube 預設沒有啟動 Ingress 功能，因此需要額外使用 minikube addons enable ingress 讓 minikube 啟動 Ingress (Ingress 也需要先安裝 Hyperfix)：\nIngress 圖片來源：[Day 19] 在 Kubernetes 中實現負載平衡 - Ingress Controller\n設定 /etc/hosts 加入 Ingress 基本上就需要網域才可以使用，但我們在本機上做練習，所以只要修改本機的 host 檔案就可以了(加入 minikube ip 以及想要的網域名稱)。\n$ vim /etc/hosts 192.168.64.11 test.tw 192.168.64.11 test-test.tw 因為有兩種方法，第一種 (設定網域以及路徑)跟第二種 (只設定路徑沒有設定網域)，所以底下也會分成兩種來說明！\n設定網域以及路徑 Github 程式碼連結\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: kubernetes-demo-ingress spec: rules: - host: \"test.tw\" http: paths: - path: / pathType: Prefix backend: service: name: kubernetes-demo-service port: number: 3000 只設定路徑沒有設定網域 Github 程式碼連結\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: kubernetes-demo-ingress spec: rules: - http: paths: - path: / pathType: Prefix backend: service: name: kubernetes-demo-service port: number: 3000 Ingress 整體寫法與 Service 差不多，只差在 spec 的細部設定，我們就來說一下 spec 設定吧：\n對了～ Ingress 的 apiVersion 不像是 Pod 跟 Service 一樣使用 v1 ，因為目前支援 Ingress 的 API 只有 networking.k8s.io/v1，詳細可以參考 kubernetes Ingress 官網\nrules 這代表這個 Ingress 的轉發規則，此 Ingress 所有的設定都必須寫載 rules 內。\nhost 設定可以連接到 Service 物件的網路名稱。\npath 設定可以連接到 Service 物件的路徑名稱。\npathType 分為 Prefix 和 Exact 兩種，Prefix：前綴符合就符合規則 ; Exact：需要完全一致才行，包含大小寫。\ntype path request path macth Prefix / 全部路徑 Yes Exact /aa /aa Yes Exact /bb /cc No Prefix /aa /aa,/aa/ Yes Prefix /aa/cc /aa/ccc No service.name 設定連接到的 Service 名稱，這裡要填寫的就是 Service 中在 metadata 內寫的 name。\nservice.port.number 設定要經由哪個 port 連接到 Service 物件，就像是 Service 的 Port 要連接到 Pod 的 targetPort。\n建立 Ingress 一樣我們分成兩個做說明\n設定網域以及路徑 一樣使用 apply 來建立：\n$ kubectl apply -f ingress.yaml ingress.networking.k8s.io/kubernetes-demo-ingress created 使用 kubectl get ing 來查詢 Ingress 狀況：\n$ kubectl get ing NAME CLASS HOSTS ADDRESS PORTS AGE kubernetes-demo-ingress nginx test.tw 192.168.64.11 80 10m 接下來我們分別測試寫在 /etc/host 裡面的兩個網域，使用瀏覽器搜尋 test.tw 跟 test-test.tw：\ntest.tw 成功顯示柴犬\ntest-test.tw 顯示 404\n從上面結果可以知道，因為我們有設定 domain，所以只有符合的才會連接到 Service 。\n只設定路徑沒有設定網域 一樣使用 apply 來建立：\n$ kubectl apply -f ingress.yaml ingress.networking.k8s.io/kubernetes-demo-ingress created 使用 kubectl get ing 來查詢 Ingress 狀況：\n$ kubectl get ing NAME CLASS HOSTS ADDRESS PORTS AGE kubernetes-demo-ingress nginx * 192.168.64.11 80 5m56s 接下來我們分別測試寫在 /etc/host 裡面的兩個網域，使用瀏覽器搜尋 test.tw 跟 test-test.tw：\ntest.tw 成功顯示柴犬\ntest-test.tw 顯示 404\n從上面結果可以知道，可以發現 hosts 的部分是 * ，這代表所有背後指向 192.168.64.11 這個 IP 的網域都可以直接連接到 Service 物件。","什麼是-replicaset-#什麼是 ReplicaSet ?":"看到 Replica 大家應該就知道這個跟控制 Pod 的數量有關係了！其實 Replica 跟 Replication Controller 很像，ReplcaSet 提供了更彈性的 selector，在 Replication Controller 中我們必須要把完整的 key/value Label 寫上去，在 ReplicaSet 不用這個麻煩，但 ReplicaSet 一樣也可以寫上完整的 Label ，這個就看開發者要怎麼去設計了！\nReplcaSet 的特性 上面有提到 ReplcaSet 非常彈性的 selector，這邊要說的是 ReplcaSet 是如何讓 selector 變得更加彈性，這裡一共要介紹兩種 ReplcaSet 的 selector 寫法：\nmatchLabels matchLabels 其實跟一般的 selector 做的事情一模一樣，也要寫出完整的 Label 出來，整體大概會長像這樣：\nselector: matchLabels: app: demo matchExpressions matchExpressions 則是提供更彈性的選取條件，每一筆條件主要由 key、operator、value 組成，並且使用一個 { } 包起來，看起來很像 JS 的 物件型態，整體大概會長像這樣：\nselector: matchExpression: - { key: app, operator: In, values: [demo, test] } 看起來似乎很複雜但其實很容易理解，上面的示範中 Expression 翻成中文就是只要 Label 的 key 是 app 且值符合 [demo, test] 陣列中的其中一個值的 Pod 就會被選取到。\n我們在隨便取得例子：假設有一個 matchExpression 長得像這樣：\nselector: matchExpression: - { key: env, operator: NotIn, values: [hello, hi] } 我們可以得知要選取 Label 的 key 為 env 且值不能是 [hello, hi] 其中一個值的 Pod。","什麼是-replication-controller-#什麼是 Replication Controller ?":"大家看到 Controller 就知道 Replication Controller 也是一種 Controller 負責控制 Replication，而 Replication 翻成中文是複製的意思，在 Kubernetes 中 Replication 代表同一種 Pod 的複製品。\n這邊要帶給大家認識一個重要的設定：replica，replica 就是複製品的意思，透過這個設定我們就可以快速產生一樣內容的 Pod，舉例來說：今天設定了 replica: 3 就代表會產生兩個內容一樣的 Pod 出來。\nKubernetes StatefulSet 架構\nReplication Controller 用途 上面有提到 Replication Controller 可以利用設定 replica 的方式快速建立 Pod 數量，除了建立之外 Replication Controller 也確保 Pod 數量與我們設定的 replica 一致，假如今天不小心刪除其中一個 Pod，這時候 Replication Controller 會自動再產生一個新的 Pod 來補齊刪除的 Pod 空缺，所以我們可以善用 Replication Controller 來讓系統更佳穩定。\nReplication Controller 寫法 我們來修改一下之前的 kubernetes-demo.yaml Pod 檔案：(Github 程式碼連結)\napiVersion: v1 kind: ReplicationController metadata: name: kubernetes-demo spec: replicas: 3 selector: app: demo template: metadata: labels: app: demo spec: containers: - name: kubernetes-demo-container image: 880831ian/kubernetes-demo ports: - containerPort: 3000 這個設定檔看似複雜但其實很簡單，可以發現 template 區塊內的設定基本上就是 Pod 的設定，再加上一些屬於 Replication Controller 的設定。\n由於我們要建立的是 Replication Controller，因此在一開始的 spec 要填的是 Replication Controller 的設定，所以 replica 會擺在第一個 spec 內。\n可以再看到 selector，前面提到 Replication Controller 要控制的就是 Pod 的數量，所以這邊的 selector 就是要選取 Pod，就跟我們在 Service 要選取 Pod 的一樣。\n最後一個新的設定：template，template 就是用來定義 Pod 的資訊，所以 Pod 的內容像是 metadata、spec 等等都會寫在 template 內，所以可以把 template 想像成不需要寫 apiVersion 跟 kind 的 Pod 壓模檔，有了這個觀念再來看 template 內的描述就很簡單，只是把 Pod 的內容複製過來而已，而 template 內的 spec 就是寫上 Pod 的 container 資訊。\nReplication Controller 建立 老樣子，也是使用 apply 來建立壓模檔：\n$ kubectl apply -f kubernetes-demo.yaml replicationcontroller/kubernetes-demo created 來查看一下 Replication Controller 是否有成功建立起來，可以使用 kubectl get rc 來查詢：\n$ kubectl get rc NAME DESIRED CURRENT READY AGE kubernetes-demo 3 3 3 28s 接下來可以查看 Pod 是否有出現 3 個，所以使用 kubectl get po 來查詢：\n$ kubectl get po NAME READY STATUS RESTARTS AGE kubernetes-demo-4zkxm 1/1 Running 0 37s kubernetes-demo-cp8jt 1/1 Running 0 37s kubernetes-demo-pt9px 1/1 Running 0 37s 可以看到為了不要讓名稱重複，所以 Replication Controller 會在每一個 Pod 名稱後面加入亂數。\n接下來我們用 minikube dashboard 來測試一下，是否刪除其中一個 Pod 後，Replication Controller 會自動建立新的：\n刪除隨機一個 Pod\n當我們隨機刪除一個 Pod 時，被刪除的 Pod 會 Terminating 準備刪除，且啟動一個新的 Pod ContainerCreating：\nPod 服務\n當新的 Pod 啟動成功後，舊的 Pod 才會被刪除，所以可以確保我們的服務穩定度。\nPod 服務\n綜上所述，可以知道 Replication Controller 真的會控制 Pod 數量，那我們刪掉一個 Pod 他就重生一個，這樣不會永遠都刪不完嗎？其實我們可以把 Replication Controller 砍掉就好了，而 Replication Controller 刪除時，也會自動終止底下的 Pod ，最後 Pod 都會自動刪除。\n但其實 Kubernetes 官方不建議使用 Replication Controller 的方式來控制 Pod，而是建議使用 Deployment 搭配 ReplicaSet 來控制，我們接下來要介紹的主題就是：Deployment 跟 ReplicaSet 。","什麼是-secrets-#什麼是 Secrets ?":"看到 Secrets 這個名字就知道這是非常機密的物件，相較於 ConfigMap 是用來存放部署面的檔案，Secrets 通常都是用來存機密的資料，像是使用者帳號、SSL 憑證等。\nSecrets 特性 上面 ConfigMap 提到的特性 Secrets 一樣，比較特別的是 Secrets 會將內部的資料進行 base64 編碼。因為重新編碼所以可以確保資料相較於 ConfigMap 下安全一些，所以建議如果是機密性的資料就存在 Secrets 裡面吧！\n如何建立 Secrets？ 我們一樣用現有的檔案來做基準作為示範，由於上面 ConfigMap 只存入一個檔案而已，所以這邊 Secrets 我們改成存入多個檔案：\n一樣用 create 的參數進行 Secrets 建立，但這邊要多加一個 SUBCOMMAND 叫 generic，generic 代表意思是從本機檔案、目錄建立 Secrets，接下來只要下：\nkubectl create secret generic \u003csecretName\u003e --from-file=\u003cfilePath\u003e 建立 Secrets\n建立完成使用 get 來查詢是否正確建立 Secrets：\n使用 get 查詢是否正確 Secrets\n最後一樣可以下 describe 這個參數來查看 Secrets 的內容，但因為加密所以不會顯示原本內容，只會看到的確有兩個檔案：\n使用 describe 查看 Secrets 的內容","什麼是-service-#什麼是 Service ?":"service 他其實就是建立的一個網路連線通道，可以讓應用程式正確的連結到正在運行的 pods，而 service 又有 4 種的表現形式，我們接下來會一個一個簡單介紹：\nClusterIP 它是 service 的預設值，所以沒有設定時，預設就是使用該方式做連線，它代表這個 service 只能在相同的 cluster 內使用，無法讓外部做使用。\nNodePort 簡單來說它可以從外部連線到內部使用。假設本機有其他服務，例如：nginx 之類的服務，還有架一個 K8s 的 cluster ，這時候只要設定好 NodePort，就可以讓本機使用 K8s cluster 來使用內部的服務。\nExternalName 主要是為了讓不同 namespace ，以 ClusterIP 所生成的 service 可以利用 ExternalName 設定外部名稱，藉以連到指定的 namespace Service。\nLoadBalancer 這個屬性是強化版的 NodePort，除了 NodePort 可以讓外部連線的優點以外，同時也建立負載平衡的機制來分散流量，很可惜 LoadBalaner 只提供雲端服務，例如：GCP、AWS 等等都有支援，目前 minikube 要使用 LoadBalancer 需要先啟動 tunnel 才能做使用。tunnel 是什麼呢？我們後面會說明！\nk8s service 流程圖 Kubernetes 那些事 — Service 篇\nNodePort 實作 那我們來用 Service (NodePort) 改寫基本篇的連線問題 ：(Github 程式碼連結)\nservice.yaml apiVersion: v1 kind: Service metadata: name: kubernetes-demo-service labels: app: demo spec: type: NodePort ports: - protocol: TCP port: 3000 targetPort: 3000 nodePort: 30001 selector: app: demo 結構與 kubernetes-demo.yaml 相同，以下簡單說明不同之處：\nkind\n該元件的屬性，此設定檔的類型是：Service\nspec\ntype：指定此 Service 要使用的方法，這邊我們使用 NodePort。 ports.protocol：此為連線的網路協議，預設值為 TCP，當然也可以使用 UDP。 ports.port：此為建立好的 Service 要以哪個 Port 連接到 Pod 上。 ports.targetPort：此為目標 Pod 的 Port ，通常 port 跟 targetPort 一樣。 ports.nodePort：此為機器上的 Port 要對應到該 Service 上，這個設定要 nodePort 形式的 Service 才會有效果，假設今天沒有設定 nodePort ，Kubernetes 會自動開一個機器上的 Port 來對應該 Service ，範圍是在 30000 - 37267 之間。 selector.app：如果要使 Service 連接到正確的 Pod 就必須利用 selector，只要原封不動的把 Pod 的 Labels 複製上去即可。 接下來使用 kubectl apply 建立 service：\n$ kubectl apply -f service.yaml k8s 建立 service (NodePort)\n接下來取得 minikube Node IP，可以使用：\n$ minikube ip 192.168.64.11 打開瀏覽器搜尋 192.168.64.11:30001 就可以看到我們可愛的柴犬囉 \u003e\u003c\n成功顯示柴犬\nLoadBalancer 實作 那我們來用 Service (LoadBalancer) 改寫基本篇的連線問題 ：(Github 程式碼連結)\nservice.yaml apiVersion: v1 kind: Service metadata: name: kubernetes-demo-service labels: app: demo spec: type: LoadBalancer ports: - protocol: TCP port: 3000 targetPort: 3000 selector: app: demo 結構與 kubernetes-demo.yaml 相同，以下簡單說明不同之處：\nkind\n該元件的屬性，此設定檔的類型是：Service\nspec\ntype：指定此 Service 要使用的方法，這邊我們使用 LoadBalancer。 ports.protocol：此為連線的網路協議，預設值為 TCP，當然也可以使用 UDP。 ports.port：此為建立好的 Service 要以哪個 Port 連接到 Pod 上。 ports.targetPort：此為目標 Pod 的 Port ，通常 port 跟 targetPort 一樣。 selector.app：如果要使 Service 連接到正確的 Pod 就必須利用 selector，只要原封不動的把 Pod 的 Labels 複製上去即可。 接下來使用 kubectl apply 建立 service：\n$ kubectl apply -f service.yaml k8s 建立 service (LoadBalancer)\n可以看到 dashboard 有我們剛剛啟動的 Service，但是啟動後前面的燈是黃色的，是因為 minikube LoadBalancer 需要透過 tunnel 才可以使用，可以參考 minikube 官網說明：\nminikube 官網 說明 LoadBalancer minikube tunnel\n所以我們需要使用 minikube tunnel 來啟動 tunnel\n$ minikube tunnel ✅ Tunnel successfully started 📌 NOTE: Please do not close this terminal as this process must stay alive for the tunnel to be accessible ... 🏃 Starting tunnel for service kubernetes-demo-service. 就可以看到燈號已經變成綠燈，在外部 Endpoints 多一個連結，可以直接點開\nk8s service\n就可以看到我們可愛的柴犬囉 \u003e\u003c\n成功顯示柴犬","什麼是-stateless-和-stateful#什麼是 Stateless 和 Stateful":"Stateless Stateless 顧名思義就是無狀態，我們可以想成我們每次與伺服器要資料的過程中都不會被伺服器記錄狀態，每一次的 Request 都是獨立的，彼此是沒有關聯性的，也就是我們當下獲得的資料只能當下使用沒有辦法保存，靜態網頁通常都是一種 Stateless 的應用。\n舉個例子來說：今天我想要查詢火車時刻表，我可以藉由 Google 搜尋火車時刻表，並點選連結，或是直接在瀏覽器輸入 https://tip.railway.gov.tw/ ，這兩種結果最後都會一致，並不會因為我的操作不同而產生不同結果，這就是一種 Stateless 的表現。\nStateful Stateful 就是 Stateless 的相反，也就是每次的 Request 都會被記錄下來，日後都可以進行存取，Stateful 最常見的例子是資料庫，所以我們可以理解成 Stateful 背後一定會有一個負責更新內容的儲存空間。\n幾個例子來說：今天我們想要查看 Google 雲端硬碟的內容，我們必須先登入自己的帳號才可以查看內容，這種有操作先後順序才會有結果的就是一種 Stateful 的表現。\nStateless vs Stateful\n那為什麼要提到 Stateless 跟 Stateful 呢？\n因為跟 Pod 有很大的關係，在 Kubernetes 中 Pod 就屬於 Stateless 的，我們前面有提到 Stateless 的特性就是每次的 Request 都是獨立的，這樣有一個好處是可以快速的擴充。\n在 Kubernetes - 基本篇中的 Pod 有提到：Pod 是 Kubernetes 中最小的單位，由於 Pod 是屬於 Stateless 的，即便今天同一種內容的 Pod 有很多個也沒有關係，因為每次的 Request 都是獨立的，多個 Pod 就多個連線的端點而已。\nKubernetes 的 Stateful 上面有說到 Kubernetes 的 Pod 是 Stateless 的，那難道 Kubernetes 沒有辦法做 Stateful 應用嗎？其實是可以的，Kubernetes 為了 Stateful 有特別開啟一個類別叫：StatefulSet\n這邊會簡單說明一下 StatefulSet 的架構：\nStatefulSet StatefulSet 一共有兩個重要的部分：\nPersistent Volume Claim 前面有說到 Stateful 背後有一個更新內容的儲存空間，在 Kubernetes 中負責管理儲存的空間是 Volume，作用與 Docker 的 Volume 幾乎一模一樣，但 Kuberntes 的 Volume 只是在 Pod 中暫時存放的儲存空間，當 Pod 移除之後這個儲存空間就會消失，為了要在 Kubernetes 中建立一個像是資料庫可以永久儲存的空間，這個 Volume 不能被包含在 Pod 中，而這個就是 Persistent Volume (PV)。\nPersistent Volume Claim (PVC) 就是負責連接 Persistent Volume (PV) 的物件，所以可以想像一下今天有多少的 Persistent Volume 就會有多少的 Persistent Volume Claim。\nHeadless Service 還記得在 Service 有提到 ClusterIP 嗎？其實每個 Service 都會有自己一組的 ClusterIP (ExternalName 形式的除外)，所以 Headless 的意思其實就是不要有 ClusterIP，方法也很簡單，直接在設定檔中加入 ClusterIP: None 就可以了！\n這麼做有什麼好處？由於 Headless Service 並沒有直接跟 Pod 有對應關係，因此 Service 本身沒有 ClusterIP，所以 Kubernetes 內部在溝通時就沒有辦法把我們設定好的 Service 名稱進行 IP 轉換，不過 Headless Service 會將內部的 Pod 的都建立屬於自己的 domain，所以我們可以自由的選擇要連接到哪一個 Pod。\n這時候你會說可以用手動來連接呀？但因為 Service 一般是跟著 Pod 的 Label ，所以一個 Service 都會連接許多個 Pod，這樣我們就沒有辦法針對某個 Pod 來做事情，所以 Headless Service 在 Stateful 中也會被建立。\n我們一直強調說 Kubernetes 最小的單位是 Pod，即便是 StatefulSet 也會有 Pod，只是這個 Pod 會歸 StatefulSet 管理，綜合上面所述可以知道一個 StatefulSet 裡面除了執行的 Pod 外還會有負責跟 Persistent Volume 連接的 Persistent Volume Claim，整體的 StatefulSet 架構會長得像這樣：\nKubernetes StatefulSet 架構\n基本上 StatefulSet 中在 Pod 的管理上都是與 Deployment 相同，基於相同的 container spec 來進行; 而其中的差別在於 StatefulSet controller 會為每一個 Pod 產生一個固定識別資訊，不會因為 Pod 改變後而有所變動。\n什麼時候需要使用 StatefulSet ? 如何研判哪些 Application 需要使用 StatefulSet 來部署？只要符合以下條件，就需要使用 StatefulSet 來進行部署\n需要穩定 \u0026 唯一的網路識別 (pod 改變後的 pod name \u0026 hostname 都不會變動) 需要穩定的 Persistent storage (pod 改變後還是能存取相同的資料，基本上用 PVC 就可以解決) 部署 \u0026 擴展的時候，每個 Pod 的產生都是有其順序且逐一慢慢完成的 進行更新操作時，也是與上面的需求相同 StatefulSet 有什麼限制？ v1.5 以前版本不支援，v1.5 ~ v1.9 之間是 beta，v1.9 後正式支援 storage 的部分一定要綁定 PVC，並綁定到特定的 StorageClass or 預先配置好的 Persistent Volume，確保 Pod 被刪除後資料依然存在。 需要額外定義一個 Headless Service 與 StatefulSet 搭配，確保 Pod 有固定的 network identity。 network identity：\n代表可以直接透過 domain name 直接取的 Pod IP; 實現的方法則是部署一個 ClusterIP=None 的 Service，讓 Cluster 內部存取 Service 時，可以直接連到 Pod 而不是 Service IP。\nStatefulSet 寫法 要寫一個 StatefulSet ，有幾個重要的部分必須涵蓋：\nApplication \u0026 Persistent Volume Claim Headless Service .spec.selector 所定義的內容 (matchLabels) 必須與 .spec.template.metadata.labels 相同。 其他部分都與 Deployment 幾乎相同 ~\n我們先來看看要怎麼定義 Application \u0026 Persistent Volume Claim (把 Service 跟 StatefulSet 寫在一起) (Github 程式碼連結)\napiVersion: v1 kind: Service metadata: name: nginx labels: app: nginx spec: ports: - port: 80 name: web clusterIP: None selector: app: nginx --- apiVersion: apps/v1 kind: StatefulSet metadata: name: web spec: serviceName: \"nginx\" replicas: 2 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: k8s.gcr.io/nginx-slim:0.8 ports: - containerPort: 80 name: web volumeMounts: - name: www mountPath: /usr/share/nginx/html volumeClaimTemplates: - metadata: name: www spec: accessModes: [\"ReadWriteOnce\"] resources: requests: storage: 1Gi 我們先打開一個 Terminal 來觀察 StatefulSet 創建 Pod 的過程：\n$ kubectl get pods -w -l app=nginx 我們一樣使用 kubectl apply 來創建定義在 web.yaml 中的 Headless Service 和 StatefulSet。\n$ kubectl apply -f web.yaml 我們看一下剛剛的 StatefulSet 創建 Pod 的過程，可以發現我們擁有 N 的副本的 StatefulSet ， Pod 部署時會按照 {0 …. N-1} 的序號依序創建。\nNAME READY STATUS RESTARTS AGE web-0 0/1 Pending 0 0s web-0 0/1 Pending 0 0s web-0 0/1 ContainerCreating 0 0s web-0 1/1 Running 0 2s web-1 0/1 Pending 0 0s web-1 0/1 Pending 0 0s web-1 0/1 ContainerCreating 0 0s web-1 1/1 Running 0 3s 可以發現 web-1 Pod 是在 web-0 Pod 處在 Running 狀態才會被啟動。此外，可以發現就跟我們上面講的一樣，StatefulSet 中的 Pod 擁有一個獨一無二的身份標記，基於 StatefulSet 控制器分配給每個 Pod 的唯一順序索引。\nPod 名稱的格式是 \u003c statefulset name \u003e-\u003c ordinal index \u003e，像我們 web 這個 StatefulSet 有兩個副本，所以它創建了兩個 Pod：web-0、web-1。\nStatefulSet 測試 我們知道 StatefulSet 它有使用穩定的網路身份以及 PV 的永久儲存，那我們就分別來測試看看：\nStatefulSet 穩定的網路身份 我們先使用 kubectl exec 在每個 Pod 中執行 hostname\n$ for i in 0 1; do kubectl exec \"web-$i\" -- sh -c 'hostname'; done web-0 web-1 再使用 kubectl run 來運行一個提供 nslookup 命令的容器，通過對 Pod 的主機名執行 nslookup，我們可以檢查他在集群內部的 DNS 位置。\n$ kubectl run -i --tty --image busybox:1.28 dns-test --restart=Never --rm /bin/sh 啟動一個新的 shell ，並運行 nslookup web-0.nginx 跟 nslookup web-1.nginx：\n/ # nslookup web-0.nginx Server: 10.96.0.10 Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local Name: web-0.nginx Address 1: 172.17.0.5 web-0.nginx.default.svc.cluster.local / # nslookup web-1.nginx Server: 10.96.0.10 Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local Name: web-1.nginx Address 1: 172.17.0.6 web-1.nginx.default.svc.cluster.local 可以看到 Headless Service 的 CHANCE 指向 SRV 記錄 (記錄每個 Running 的 Pod)。SRV 紀錄指向一個包含 Pod IP 位址的記錄表。\n我們使用 kubectl delete pod -l app=nginx 刪除 Pod 後，會發現 Pod 的序號、主機名、SRV 條目和記錄名稱都沒有改變！\nStatefulSet 永久儲存 我們先查看 web-0 跟 web-1 的 PersistentVolumeClaims：\n$ kubectl get pvc -l app=nginx NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE www-web-0 Bound pvc-e9a23104-d018-4d73-8cd9-89e5ea67f96c 1Gi RWO standard 39m www-web-1 Bound pvc-00dd6c87-6d95-4d04-9c1b-49b43441f4a1 1Gi RWO standard 38m StatefulSet 控制器創建兩個 PersistentVolumeClaims，綁定兩個 PersistentVolumes，因為我們配置是動態提供 PersistentVolume，所有的 PersistentVolume 都是自動創建和綁定的。\nNginx web 服務器默認會加載位於 /usr/share/nginx/html/index.html 的 index 文件。因此我們在 spec 中的 volumeMounts 將 /usr/share/nginx/html 資料夾由一個 PersistentVolume 支持。\n那我們將 Pod 主機名稱寫入 index.html ，再刪掉 Pod 看看寫入內容是否還會存在：\n$ for i in 0 1; do kubectl exec \"web-$i\" -- sh -c 'echo \"$(hostname)\" \u003e /usr/share/nginx/html/index.html'; done $ for i in 0 1; do kubectl exec -i -t \"web-$i\" -- curl http://localhost/; done web-0 web-1 當我們刪除 Pod 後，如果沒有使用 PersistentVolumeClaims 去綁定 PersistentVolumes 的話，資料就會消失，那我們來看看有綁定的結果：\n使用 kubectl delete pod -l app=nginx 刪除 Pod：\npod \"web-0\" deleted pod \"web-1\" deleted 再次使用 kubectl get pod -w -l app=nginx 來檢查 Pod 的狀態：\nweb-0 0/1 ContainerCreating 0 0s web-0 1/1 Running 0 2s web-1 0/1 Pending 0 0s web-1 0/1 Pending 0 0s web-1 0/1 ContainerCreating 0 0s web-1 1/1 Running 0 1s 一樣使用 for i in 0 1; do kubectl exec -i -t \"web-$i\" -- curl http://localhost/; done 來查看：\nweb-0 web-1 可以發現 web-0、web-1 雖然重新啟動，但依舊會監聽它們主機名，因為和它們的 PersistentVolumeClaim 相關聯的 PersistentVolume 被重新掛載到了各自的 volumeMount 上。","參考資料#參考資料":"Kubernetes 那些事 — Service 篇：https://medium.com/andy-blog/kubernetes-%E9%82%A3%E4%BA%9B%E4%BA%8B-service-%E7%AF%87-d19d4c6e945f\nKubernetes 那些事 — Ingress 篇（一）：https://medium.com/andy-blog/kubernetes-%E9%82%A3%E4%BA%9B%E4%BA%8B-ingress-%E7%AF%87-%E4%B8%80-92944d4bf97d\nKubernetes 那些事 — Ingress 篇（二）：https://medium.com/andy-blog/kubernetes-%E9%82%A3%E4%BA%9B%E4%BA%8B-ingress-%E7%AF%87-%E4%BA%8C-559c7a41404b\nKubernetes 那些事 — Stateless 與 Stateful：https://medium.com/andy-blog/kubernetes-%E9%82%A3%E4%BA%9B%E4%BA%8B-stateless-%E8%88%87stateful-2c68cebdd635\nkubernetes ReplicationController：https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/\nKubernetes 那些事 — Replication Controller：https://medium.com/andy-blog/kubernetes-%E9%82%A3%E4%BA%9B%E4%BA%8B-replication-controller-5c8592d37083\nKubernetes 那些事 — Deployment 與 ReplicaSet（一）：https://medium.com/andy-blog/kubernetes-%E9%82%A3%E4%BA%9B%E4%BA%8B-deployment-%E8%88%87-replicaset-%E4%B8%80-406234a63d43\nKubernetes 那些事 — Deployment 與 ReplicaSet（二）：https://medium.com/andy-blog/kubernetes-%E9%82%A3%E4%BA%9B%E4%BA%8B-deployment-%E8%88%87-replicaset-%E4%BA%8C-f60146c878e4\nKubernetes 那些事 — Deployment 與 ReplicaSet（三）：https://medium.com/andy-blog/kubernetes-%E9%82%A3%E4%BA%9B%E4%BA%8B-deployment-%E8%88%87-replicaset-%E4%B8%89-142b2863eb94\nkubernetes Deployments：https://kubernetes.io/docs/concepts/workloads/controllers/deployment/\n[Kubernetes] StatefulSet Overview：https://godleon.github.io/blog/Kubernetes/k8s-StatefulSets-Overview/"},"title":"Kubernetes (K8s) 介紹 - 進階 (Service、Ingress、StatefulSet、Deployment、ReplicaSet、ConfigMap)"},"/blog/kubernetes/k8s-efk/":{"data":{"":"這是我們 Kubernetes 文章的第三篇，此文章會紀錄我將 EFK 建在 Kubernetes 上面，最後也會統整實作過程中，可能會遇到的一些問題，讓大家在學習時，可以更有效率，不用 debug 到死 😎😎\n那由於本文會直接帶入程式，觀念部分，可以先查看：\nKubernetes : Kubernetes (K8s) 介紹 - 基本 kubernetes : Kubernetes (K8s) 介紹 - 進階 (Service、Ingress、StatefulSet、Deployment、ReplicaSet、ConfigMap) EFK : 用 EFK 收集容器日誌 (HAProxy、Redis Sentinel、Docker-compose) 此文章程式碼也會同步到 Github ，需要的也可以去查看歐！要記得先確定一下自己的版本 Github 程式碼連結 😆","參考資料#參考資料":"How To Set Up an Elasticsearch, Fluentd and Kibana (EFK) Logging Stack on Kubernetes","實作#實作":"創建命名空間 在我們開始之前，我們首先建立一個命名空間，我們會將 EFK 所有的工具都安裝在此。那我想創建一個名為 kube-logging 的 namespace，先查詢現有的命名空間是否有重複：\n$ kubectl get namespaces 我們可以看到這些已經存在的 namespace ：\nNAME STATUS AGE default Active 92m kube-logging Active 89m kube-node-lease Active 92m kube-public Active 92m kube-system Active 92m kubernetes-dashboard Active 92m 那要怎麼創建一個命名空間呢？先打開編輯器編輯名為 kube-logging.yaml：\napiVersion: v1 kind: Namespace metadata: name: kube-logging 完成後，使用 apply 來創建命名空間：\n$ kubectl apply -f kube-logging.yaml namespace/kube-logging created 再使用 kubectl get namespaces 查看是否多了一個名為 kube-logging 的命名空間：\n$ kubectl get namespaces NAME STATUS AGE default Active 2m23s kube-logging Active 88s kube-node-lease Active 2m24s kube-public Active 2m24s kube-system Active 2m24s kubernetes-dashboard Active 2m19s 創建 Elasticsearch StatefulSet 我們已經創建好一個命名空間來放我們的 EFK，首先先部署副本數有 3 個的 Elasticsearch Pod。為什麼要使用 3 個 呢？使用 3 個 Elasticsearch Pod 是為了避免在高可用性、多節點叢集時出現錯誤，當其中一個 Elasticsearch Pod 故障，其他 2 個就會選舉後來接替，保證叢集可以繼續運行。\n創建 Headless Service 我們先創建名為 elasticsearch_svc.yaml 的 yaml 檔，用來處理 service 的問題：\napiVersion: v1 kind: Service metadata: name: elasticsearch namespace: kube-logging labels: app: elasticsearch spec: selector: app: elasticsearch clusterIP: None ports: - port: 9200 name: rest - port: 9300 name: inter-node 我們有創建一個命名空間，所以要先在 metadata 加入 namespace: kube-logging。記得要設定標籤，當我們將 Elasticsearch StatefulSet 與此 Service 關聯時，Service 會返回指向的帶有標籤的 Elasticsearch Pod。然後我們設置 clusterIP: None 定義該 Service 為 Headless Service。最後定義 Port 9200 為 REST API、Port 9300 為 Node 之間的通信。\n一樣使用 apply 來建立 Service：\n$ kubectl apply -f elasticsearch-svc.yaml service/elasticsearch created 我們這次直接查看 minikube dashboard：\nelasticsearch service\n創建 StatefulSet 創建名為 elasticsearch_statefulset.yaml 的 yaml 檔案 (因為程式長度，所以分開說明，要完整請參考 Github 程式碼連結)：\napiVersion: apps/v1 kind: StatefulSet metadata: name: es-cluster namespace: kube-logging spec: serviceName: elasticsearch replicas: 3 selector: matchLabels: app: elasticsearch template: metadata: labels: app: elasticsearch 名稱取名為 es-cluster ，我一樣使用 kube-logging 的 namespace，設定 ServiceName elasticsearch 是確保 StatefulSet 中的每一個 Pod 都可以使用以下 DNS 位址進行訪問：es-cluster-[0,1,2].elasticsearch.kube-logging.svc.cluster.local ，其中 [0,1,2] 對應 Pod 分配的整數序號。\n我們指定副本數為 3，並將 matchLabels 選擇器設定 app: elasticseach，我們在將其鏡像到該 .spec.template.metadata，.spec.selector.matchLabels 跟 .spec.template.metadata.labels 必須相同。\n接續…\nspec: containers: - name: elasticsearch image: elasticsearch:8.1.3 resources: limits: cpu: 1000m requests: cpu: 100m ports: - containerPort: 9200 name: rest protocol: TCP - containerPort: 9300 name: inter-node protocol: TCP volumeMounts: - name: data mountPath: /usr/share/elasticsearch/data env: - name: cluster.name value: k8s-logs - name: node.name valueFrom: fieldRef: fieldPath: metadata.name - name: discovery.seed_hosts value: \"es-cluster-0.elasticsearch,es-cluster-1.elasticsearch,es-cluster-2.elasticsearch\" - name: cluster.initial_master_nodes value: \"es-cluster-0,es-cluster-1,es-cluster-2\" - name: ES_JAVA_OPTS value: \"-Xms512m -Xmx512m\" - name: xpack.security.enabled # 記得要加上，因爲 Elasticsearch 8.x版本後會自動開啟SSL，如果沒有設定他就會一直重新啟動 value: \"false\" 我們定義容器名稱是 elasticsearch，映像檔是 elasticsearch:8.1.3 ，要記得這個版本要與後面的 kibana 相同，我們在 resources 設置容器至少需要 0.1 CPU，最多可以到 1 個 CPU。一樣設定 Port 9200 為 REST API、Port 9300 為 Node 之間的通信，並將名為 data 的PersistentVolume 的容器掛載到容器的 /usr/share/elasticsearch/data。\n最後幫容器設置一些環境變數：\ncluster.name：Elasticsearch 叢集的名稱，我們設定為 k8s-logs。 node.name：節點名稱，我們設置 valueFrom 使用 .metadata.name，它會解析為 es-cluster-[0,1,2]，取決節點的指定順序。 discovery.seed_hosts：設置叢集中主節點列表，這些節點將會為節點發現過程中提供 Pod，但由於我們配置的 Headless Service，所以我們的 Pod 具有 es-cluster-[0,1,2].elasticsearch.kube-logging.svc.cluster.local Kubernetes DNS 解析。 cluster.initial_master_nodes：這邊指定將參與主節點的選舉過程節點列表，這邊是通過節點 node.name 來辨識，不是透過主機名。 ES_JAVA_OPTS：這邊我們設置 -Xms512m -Xmx512m，告訴 JVM 使用最大跟最小 512 MB，可以依據資源來做調配。 xpack.security.enabled：這是也是我在 elasticsearch 卡很久的一個設定，詳細的會在最後的常見問題中提到，大家只要記得 elasticsearch 8.x 以後都需要多這個。 接續…\ninitContainers: - name: fix-permissions image: busybox command: [\"sh\", \"-c\", \"chown -R 1000:1000 /usr/share/elasticsearch/data\"] securityContext: privileged: true volumeMounts: - name: data mountPath: /usr/share/elasticsearch/data - name: increase-vm-max-map image: busybox command: [\"sysctl\", \"-w\", \"vm.max_map_count=262144\"] securityContext: privileged: true - name: increase-fd-ulimit image: busybox command: [\"sh\", \"-c\", \"ulimit -n 65536\"] securityContext: privileged: true 我們這個區塊定義了主容器運行前的初始化設定：\nfix-permissions：因為默認情況下，Kubernetes 會將數據目錄掛載為 root，導致 Elasticsearch 無法訪問，所以才會多這個運行 chown 將 Elasticsearch 數據目錄的所有者和組更改為 1000:1000 /usr/share/elasticsearch/data。詳細可以參考 Notes for Production Use and Defaults。 increase-vm-max-map：這邊是因為默認情況下內存會太低，所以多這個用 sysctl -w 來調整內存大小。詳細可以參考 Elasticsearch documentation。 iincrease-fd-ulimit：它運行 ulimit 調整打開文件描述的最大數量。詳細可以參考 Notes for Production Use and Defaults。 接續…\nvolumeClaimTemplates: - metadata: name: data labels: app: elasticsearch spec: accessModes: [\"ReadWriteOnce\"] resources: requests: storage: 3Gi 這邊定義了 StatefulSet 的 volumeClaimTemplates。Kubernetes 會幫 Pod 創建 PersistentVolume，我們在上面的命名將它取為 data，並與 app: elasticsearch StatefulSet 相同標籤。 我們將訪問的模式設定為 ReadWriteOnce，代表我們只能被單個節點以讀寫方式掛載，最後我們設定每個 PersistentVolume 的大小為 3GiB。\n都完成後，我們一樣用 apply 部署 StatefulSet：\n$ kubectl apply -f elasticsearch-statefulset.yaml statefulset.apps/es-cluster created 我們可以查看 minikube dashboard：\nelasticsearch statefulset\n可以看到已經成功建好，接著我們使用 port-forward 來測試是否正常運作：\n$ kubectl port-forward es-cluster-0 9200:9200 --namespace=kube-logging $ curl http://localhost:9200/ { \"name\" : \"es-cluster-0\", \"cluster_name\" : \"k8s-logs\", \"cluster_uuid\" : \"aGxBystkQFW-xvjJ98Pxcw\", \"version\" : { \"number\" : \"8.1.3\", \"build_flavor\" : \"default\", \"build_type\" : \"docker\", \"build_hash\" : \"39afaa3c0fe7db4869a161985e240bd7182d7a07\", \"build_date\" : \"2022-04-19T08:13:25.444693396Z\", \"build_snapshot\" : false, \"lucene_version\" : \"9.0.0\", \"minimum_wire_compatibility_version\" : \"7.17.0\", \"minimum_index_compatibility_version\" : \"7.0.0\" }, \"tagline\" : \"You Know, for Search\" } 如果像我上面一樣有跳出 ”You Know, for Search“ 就代表 Elasticsearch 已經在正常運作囉 🥳\n創建 Kibana Deployment Service 要在 Kubernetes 上啟動 Kibana，我們要先創建一個名為 Service kibana，以及包含一個副本的 Deployment。我們先創建名為 kibana.yaml 的 yaml 檔：\napiVersion: v1 kind: Service metadata: name: kibana namespace: kube-logging labels: app: kibana spec: ports: - port: 5601 selector: app: kibana --- apiVersion: apps/v1 kind: Deployment metadata: name: kibana namespace: kube-logging labels: app: kibana spec: replicas: 1 selector: matchLabels: app: kibana template: metadata: labels: app: kibana spec: containers: - name: kibana image: kibana:8.1.3 resources: limits: cpu: 1000m requests: cpu: 100m env: - name: ELASTICSEARCH_URL value: http://elasticsearch:9200 ports: - containerPort: 5601 一樣我們要把 kibana 加入 kube-logging 的命名空間，讓它以去調用其他服務，並賦予 app: kibana 標籤。並指定本機訪問 Port 為 5601，並使用 app: kibana 標籤來選擇服務的 Pod。我們在 Deployment 定義 1 個 Pod 副本，我們使用 kibana:8.1.3 image，記得要跟 Elasticsearch 使用相同版本，此外我們還有設定 Pod 最少使用 0.1 個 CPU、最多使用 1 個 CPU。最後在環境變數中使用 ELASTICSEARCH_URL 設定 Elasticsearch 的叢集以及 Port。\n都完成後，我們來開始部署：\n$ kubectl apply -f kibana.yaml service/kibana created deployment.apps/kibana created 一樣我們用 minikube dashboard 來查看是否部署成功：\nkibana Deployments\n沒有問題後，我們用 port-forward 將本地 Port 轉發到 Pod 上：\n$ kubectl port-forward kibana-75cbbfcd9c-nr4r8 5601:5601 --namespace=kube-logging Forwarding from 127.0.0.1:5601 -\u003e 5601 Forwarding from [::1]:5601 -\u003e 5601 Handling connection for 5601 開啟瀏覽器瀏覽 http://localhost:5601，如果可以進入 Kibana，就代表成功將 Kibana 部署到 Kubernetes 叢集中：\nkibana Deployments\n創建 Fluentd DaemonSet 我們要將 Fluentd 設置成 DaemonSet，讓它在 Kubernetes 叢集中每個節點上運行 Pod 副本。用 DaemonSet 控制器，可以將叢集中每個節點部署 Fluentd Pod，詳細可以參考 Using a node logging agent。在 Kubernetes 中，容器化的應用程式會透過 stdout 將日誌 log 定向到節點上的 JSON 文件。Fluentd Pod 會追蹤這些日誌文件、過濾日誌事件、轉換日誌的數據，並發送到我們部署的 Elasticsearch 後端。\n除了容器的日誌，Fluentd 還會追蹤 Kubernetes 系統日誌，例如：kubelet、kube-proxy 和 Docker 日誌。\n先創建一個 fluentd.yaml 的 yaml 檔 (因為程式長度，所以分開說明，要完整請參考 Github 程式碼連結)：\n創建 ServiceAccount apiVersion: v1 kind: ServiceAccount metadata: name: fluentd namespace: kube-logging labels: app: fluentd 我們先創建一個服務帳號 fluentd，Fluentd Pod 將使用它來訪問 Kubernetes API。我們在 kube-logging namespace 中創建它並再次賦予它 label app: fluentd。\n創建 ClusterRole 接著…\napiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: fluentd labels: app: fluentd rules: - apiGroups: - \"\" resources: - pods - namespaces verbs: - get - list - watch 在這邊我們定義一個 ClusterRole fluentd，設定我們對叢集範圍的 Kubernetes 資源（如節點）的訪問權限，我們設定 get、list、watch 等權限。\n創建 ClusterRoleBinding 接著…\nkind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: fluentd roleRef: kind: ClusterRole name: fluentd apiGroup: rbac.authorization.k8s.io subjects: - kind: ServiceAccount name: fluentd namespace: kube-logging 我們定義一個將 ClusterRole 綁定到 ServiceAccount 的 ClusterRoleBinding 調用。\n創建 DaemonSet 接著…\napiVersion: apps/v1 kind: DaemonSet metadata: name: fluentd namespace: kube-logging labels: app: fluentd 定義一個可以在 kube-logging namespace 中調用的 DaemonSet，並給它一個 app: fluentd 標籤。\n接著…\nspec: selector: matchLabels: app: fluentd template: metadata: labels: app: fluentd spec: serviceAccount: fluentd serviceAccountName: fluentd tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule containers: - name: fluentd image: fluent/fluentd-kubernetes-daemonset:v1.14.6-debian-elasticsearch7-1.0 env: - name: FLUENT_ELASTICSEARCH_HOST value: \"elasticsearch.kube-logging.svc.cluster.local\" - name: FLUENT_ELASTICSEARCH_PORT value: \"9200\" - name: FLUENT_ELASTICSEARCH_SCHEME value: \"http\" - name: FLUENTD_SYSTEMD_CONF value: disable 我們先匹配 .metadata.labels 定義的標籤 app: fluentd ，然後為 DaemonSet 分配 fluentd Service Account。選擇 app: fluentd 作為這個 DaemonSet 管理的 Pod。\n我們定義 NoSchedule 容忍度來匹配 Kubernetes master node 上的等效污點。他可以確保 DaemonSet 也被部署到 Kubernetes 主服務器。\n接下來定義 Pod 容器，我們將名稱取為 fluentd，我們使用的映像檔是 fluent/fluentd-kubernetes-daemonset:v1.14.6-debian-elasticsearch7-1.0，最後配置一些環境變數：\nFLUENT_ELASTICSEARCH_HOST：設置我們之前定義的 Elasticsearch Headless 位址。elasticsearch.kube-logging.svc.cluster.local 會解析 3 個 Elasticsearch Pod 的 IP 地址列表。 FLUENT_ELASTICSEARCH_PORT：設置我們之前定義的 Elasticsearch 9200 Port。 FLUENT_ELASTICSEARCH_SCHEME：我們設置 http。 FLUENTD_SYSTEMD_CONF：我們將 systemd 在容器中設定相關的輸出設置為 disable。 接著…\nresources: limits: memory: 512Mi requests: cpu: 100m memory: 200Mi volumeMounts: - name: varlog mountPath: /var/log - name: varlibdockercontainers mountPath: /var/lib/docker/containers readOnly: true terminationGracePeriodSeconds: 30 volumes: - name: varlog hostPath: path: /var/log - name: varlibdockercontainers hostPath: path: /var/lib/docker/containers 我們設置 Fluentd Pod 上使用 512 MiB 的內存限制，並保證 0.1 個 CPU 跟 200 MiB 的內存。我們將 varlog /var/log 以及 varlibdockercontainers var/lib/docker/containers 一併掛載進容器內。最後一個設定是 Fluentd 在收到信號 terminationGracePeriodSeconds 後有 30 秒的時間可以優雅的關閉。\n都定義完成後，我們部署 Fluentd DaemonSet：\n$ kubectl apply -f fluentd.yaml serviceaccount/fluentd created clusterrole.rbac.authorization.k8s.io/fluentd created clusterrolebinding.rbac.authorization.k8s.io/fluentd created daemonset.apps/fluentd created 一樣我們用 minikube dashboard 來查看是否部署成功：\nfluentd DaemonSet\n我們使用剛剛的 kibana port-forward 將本地 Port 轉發到 Pod 上：\n$ kubectl port-forward kibana-75cbbfcd9c-nr4r8 5601:5601 --namespace=kube-logging Forwarding from 127.0.0.1:5601 -\u003e 5601 Forwarding from [::1]:5601 -\u003e 5601 Handling connection for 5601 開啟瀏覽器瀏覽 http://localhost:5601，點選 Management \u003e Stack Management：\nfluentd 設定\n點選 Kibana \u003e Data Views，會看到跳出一個視窗，有一個按鈕寫 Create data view：\nfluentd 設定\nName 輸入 logstash* ，並選擇 @timestamp 來用時間過濾日誌，最後按下 Create date view：\nfluentd 設定\n設定好 logstash* 的 Data views，再點選左邊欄位的 Discover：\nfluentd 設定\n就可以看到顯示容器的 log 日誌拉！\n顯示容器 log","版本資訊#版本資訊":" macOS：11.6 Minikube：v1.25.2 hyperkit：0.20200908 Kubectl：Client Version：v1.22.5、Server Version：v1.23.3 Elasticsearch：8.1.3 Fluentd：v1.14.6-debian-elasticsearch7-1.0 Kibana：8.1.3 ","部署常見問題及解決辦法#部署常見問題及解決辦法":" 常見問題\nQ：Elasticsearch 部署成功後會一直重新啟動？\nAns：原因是 Elasticsearch 從 8.x 版本後，會自動開啟 SSL 認證，我們在 env 環境變數設定時，如果沒有多加 SSL Key 等設定值，Elasticsearch 這個 Pod 會啟動後，一直重新啟動，導致服務無法正常使用，只需要在環境變數中加入 xpack.security.enabled ，設定為 false 就可以解決。"},"title":"Kubernetes (K8s) 搭配 EFK 實作 (Deployment、StatefulSet、DaemonSet)"},"/blog/kubernetes/k8s-hpa/":{"data":{"":"此篇是要介紹 HorizontalPodAutoscaler (HPA) 的原理以及實作內容，那我們先來說明一下 HorizontalPodAutoscaler 是什麼吧！","horizontalpodautoscaler-原理#HorizontalPodAutoscaler 原理":"HorizontalPodAutoscaler (HPA) 中文可以叫水平 Pod 自動擴縮，他會自動更新工作負載資源 (Deployment 或 StatefulSet)，其目的是透過自動擴縮工作負載來滿足使用需求。\n簡單來說他會根據你現在的負載去調整你的 Pod 數量，當目前的負載超過配置的設定時，HPA 會指示工作負載資源 (Deployment 或 StatefulSet) 擴增，來避免塞爆同一個負載資源。\n如果負載減少，且 Pod 數量高於配置的最小值，HPA 也會指示工作負載資源 (Deployment 或 StatefulSet) 慢慢縮減。其中水平 Pod 自動擴縮不適用 DaemonSet 工作負載資源。\nHPA 是如何工作呢？ HorizontalPodAutoscaler 工作流程圖\nKubernetes 將水平 Pod 自動擴縮定義為一個間歇運行的控制迴路，他不是連續的，其間隔是由 kube-controller-manager 的 --horizontal-pod-autoscaler-sync-period 參數來配置，預設是 15 秒，controller-manager 會依據每一個 HPA 定義的 scaleTargetRef 來找到是哪一個工作負載資源需要進行水平 Pod 自動擴縮，然後根據目標資源的 .spec.selector 標籤選擇對應的 Pod，並從資源指標 API 或自定義指標獲取 API，目前總共有三種的資源指標，分別是 CPU、Memory、自定義指標。\n有關於其他 Kubernetes 觀念部分，可以先查看：\nKubernetes : Kubernetes (K8s) 介紹 - 基本 kubernetes : Kubernetes (K8s) 介紹 - 進階 (Service、Ingress、StatefulSet、Deployment、ReplicaSet、ConfigMap) 此文章程式碼也會同步到 Github ，需要的也可以去 clone 使用歐！要記得先確定一下自己的版本 Github 程式碼連結 😆","參考資料#參考資料":"Horizontal Pod Autoscaling：https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/\nHorizontalPodAutoscaler Walkthrough：https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/","實作#實作":"這次實作要使用的叢集是 Minikube，所以照以前文章一樣，我們先啟動 Minikube。本次會使用 K8s 的管理工具：k8slens 來做為輔助，大家有興趣也可以先去下載來使用歐 😘\n啟動 Minikube 叢集 啟動 Minikube 叢集 minikube start --vm-driver=docker 啟動 Minikube\n設定 Metrics Server 由於我們 HorizontalPodAutoscaler 會根據現在的負載來判斷是否要新增新的 Pod 來解決負載資源用完的問題，所以第一個條件就是要先獲的目前的負載資源使用量，這時候我們必須先在 K8s 叢集上安裝 Metrics Server，透過他讓我們可以知道目前的負載使用量！\n先到 kubernetes-sigs/metrics-server 下載最新的 components.yaml 檔案下來，以我這次示範的版本為例，大家可以點我下載 👇\n下載完，需要先修改兩個地方，才能 apply 到 Minikube 叢集，第一個是修改 kubelet-preferred-address-types=InternalIP，以及新增 kubelet-insecure-tls=ture 讓 Metrics Server 禁用 TLS 證書驗證，詳細可以參考以下照片：\n修改 components.yaml\n接著 apply 這個 yaml 檔案： apply components.yaml\n可以檢查一下 deployment.apps/metrics-server 是否有成功建立或是 Pod 是否有問題：\n檢查 metrics-server 是否有問題\n開始撰寫實作檔案 接著我們就依照官方的 HorizontalPodAutoscaler Walkthrough 的文章開始囉！首先先寫一個 index.php，這個檔案是用於後續測試 HPA 負載使用量的程式： index.php\n新增 Dockerfile，我們使用 php:5-apache 的 image，並複製剛剛寫的 index.php 到容器內： Dockerfile\n將該 Dockerfile Build 起來，推到 DockerHub 上，這部分就不多說明，有興趣可以參考以前文章，大家可以直接使用 880831ian/php-apache 我推好的 image 來使用。 新增 Deployment.yaml，裡面會使用我們剛剛打包推到 DockerHub 上的 image： apiVersion: apps/v1 kind: Deployment metadata: name: php-apache spec: selector: matchLabels: run: php-apache replicas: 1 template: metadata: labels: run: php-apache spec: containers: - name: php-apache image: 880831ian/php-apache ports: - containerPort: 80 resources: limits: cpu: 500m requests: cpu: 200m --- apiVersion: v1 kind: Service metadata: name: php-apache labels: run: php-apache spec: ports: - port: 80 selector: run: php-apache 這邊大家可以先記得我們在 containers.resources 有設定 cpu 的 requests 為 200m。後續在計算副本數時會再提到 😬\n最後將 Deployment.yaml 給 apply，檢查一下是否有正常啟動： k8slens 檢查是否正常\n使用 kubectl autoscale 來幫助我們創建 HorizontalPodAutoscaler： kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10 我們在 Deployment.yaml 裡面 containers.resources 有設定 cpu 的 requests 是 200m，也就是 200 milli-cores，當我們現在設定 HPA 的平均 CPU 使用率為 50%，所以我們只要超過 200m / 2 = 100m，也就是 requests 超過 100 milli-cores 就會產生新的 Pod。這邊最少 Pod 為 1，最多為 10。後續等測試時，會帶大家計算他是如何產生 Pod 的 😏 查看目前 HPA 使用量，因為我們這個 php-apache 還沒有任何的訪問，所以是 0% / 50% (承上面所說，所以這邊的值是 0 / 100 milli-cores)，後面也可以看到我們所設定最高跟最低的 Pod，以及目前的副本數。 查看目前 HPA 使用量\n當然我們除了用剛剛的指令以外，我們也可以自己寫 HPA 的 yaml 檔案。我這邊先使用 kubectl get hpa php-apache -o yaml \u003e hpa.yaml 來將剛剛用指令 run 起來的 HPA 變成 yaml 檔，我們來看看裡面有哪些內容吧： apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: creationTimestamp: \"2022-07-12T03:16:11Z\" name: php-apache namespace: default resourceVersion: \"19454\" uid: dde68e68-9b6e-46a8-b50f-5525b8ec3bdf spec: maxReplicas: 10 metrics: - resource: name: cpu target: averageUtilization: 50 type: Utilization type: Resource minReplicas: 1 scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: php-apache 後面狀態省略.... apiVersion：要記得使用 autoscaling/v2 kind：HorizontalPodAutoscaler maxReplicas：是我們剛剛用指令的最大 Pod 副本數 metrics：我們指標設定 cpu resource，其設定平均使用率為 50 % (百分比) minReplicas：剛剛用指令的最小 Pod 副本數 scaleTargetRef：設定我們這個 HPA 是依照 php-apache 這個 Deployment。 ","常見問題及解決辦法#常見問題及解決辦法":" ℹ️ Q1 . 出現 x509: cannot validate certificate for 192.168.XXX.XXX because it doesn’t contain any IP SANs 錯誤\nAns 1：是因爲沒有加入 kubelet-insecure-tls=ture 讓 Metrics Server 禁用 TLS 證書驗證，才導致錯誤發生。","測試-hpa#測試 HPA":" 接下來我們都設定好後，我們要來模擬增加負載，看看 HPA 的後續動作，首先我們先使用以下指令來持續觀察 HPA： kubectl get hpa php-apache --watch NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE php-apache Deployment/php-apache 0%/50% 1 10 1 4h40m 以及執行以下指令，該指令是建一個新的 Pod，由新的 Pod 無限循環的去向 php-apahe 服務發出請求 kubectl run -i --tty load-generator --rm --image=busybox:1.28 --restart=Never -- /bin/sh -c \"while sleep 0.01; do wget -q -O- http://php-apache; done\" 模擬增加負載\n我們使用 k8slens 觀察，會發現當負載使用量超過我們前面所設定的 50%，也就是 100 milli-cores 時，他就會自動長新的 Pod 出來： HPA 自動長 Pod\n我們切回去看觀察 HPA 指令 kubectl get hpa php-apache --watch： 觀察 HPA\n我們剛剛有說超過 50%，也就是 100 milli-cores 時，會長新的 Pod。我們以上面圖片的來說明，來計算期望需要幾個副本?\n官方有提供一個公式： 期望副本数 = ceil[目前副本数 * (目前指標 / 期望指標)] ，可以看到我們負載從 0% 從到 250%，也就是我們實際上是從 0 變成 500 milli-cores (250 / 50 _ 100 )。我們將值帶入公式內，目前的副本：1 _ (目前指標：500 / 期望指標是：100)得出來的值是 5，如果有小數，因為前面有ceil，所以會取整數(不可能開半個 Pod 對吧 🙄)，最後以這個例子來說，最終得到的 期望副本数 = 5\n根據我們計算出來的副本數，他就會依照計算結果，幫我們自動生成該數量的 Pod，來減緩同一個 Pod 的負載量，接著我們先中斷測試指令，再繼續觀察 HPA ： 觀察 HPA\n可以發現因為我們將測試指令中斷後，負載使用量會慢慢降低，但負載降低後，副本數不會馬上變回去，因為怕如果又有大量的使用量會導致 Pod 來不及長出來，所以預設是 5 分鐘 (--horizontal-pod-autoscaler-downscale-stabilization) 才會減至原來的 Pod 數量\nPod 自動減少","版本資訊#版本資訊":" macOS：11.6 Minikube：v1.25.2 Kubectl：Client Version：v1.24.1、Server Version：v1.23.3 Metrics Server 3.8.2 "},"title":"Kubernetes (K8s) HorizontalPodAutoscaler (HPA) 原理與實作"},"/blog/kubernetes/k8s-node-log-stdout-logrotate/":{"data":{"":"我們知道可觀測性的三大支柱是 Log、Metics、Trace，其中 Log 是最基礎的一環，透過 Log 我們可以了解應用程式的運作狀況，當應用程式出現問題時，透過 Log 可以快速定位問題，進而解決問題。\n以我們公司為例，我們主要使用 GCP，現在也開始用 AWS 來當作 DR 的環境。在 GCP 我們原本會使用 Log Exporter 來查看服務及系統的 Log，但為了更方便的管理，我們改使用 Datadog 來統一收集 Log (AWS 也是)。\n要如何傳遞給 Datadog，我們依照 Datadog 官方文件建議，將 Log Stdout，讓 Datadog agent 去收集 Log，這樣就可以統一管理 Log 了，詳細可參考 Datadog Kubernetes log collection。","node-logrotate#Node Logrotate":"當然，有在使用過相關 Log 工具，對 Logrotate 一定不陌生，第一個想法就想說，會不會 Node 上有對應的 Logrotate 設定，去定期的去處理 /var/log 的檔案，因次我們這邊針對 GKE(GCP) 以及 EKS(AWS) 來做說明：\nGKE 首先，我們先連線到 GKE Node 上，進到 /etc/logrotate.d 底下，可以看到以下圖片的設定，在 GKE 裡面只有一個 Logrotate 設定檔，他會針對整個 /var/log 底下的 log 檔案做處理。\nGKE /etc/logrotate.d 底下檔案設定\nEKS 一樣的步驟，進到 /etc/logrotate.d 底下，可以看到以下圖片的設定，EKS 會有多個 Logrotate 設定檔，但卻沒有對 /var/log/pods 底下的 log 檔案做處理，所以 EKS 需要額外去寫 DameonSet 來處理 /var/log/pods 底下的 log 檔案回收嗎？\nEKS /etc/logrotate.d 底下檔案設定","參考資料#參考資料":"Logging Architecture 日誌架構：https://kubernetes.io/docs/concepts/cluster-administration/logging/#log-rotation\nDatadog Kubernetes log collection：https://docs.datadoghq.com/containers/kubernetes/log/?tab=datadogoperator","尋找答案#尋找答案":"最後在 Kubernetes 官方文件中 Logging Architecture 有看到兩個設定值，分別是 containerLogMaxSize (預設 10Mi)，以及 containerLogMaxFiles(預設 5)。kubelet 會根據這兩個設定值來決定是否要回收 Log。\n如果 Log 檔案大小超過 containerLogMaxSize，kubelet 就會重新開始寫新的 Log 檔案，如果 Log 檔案數量超過 containerLogMaxFiles，kubelet 會刪除最舊的 Log 檔案。\n那要怎麼查看當前 node 的 kubelet 設定值呢？可以透過以下指令：\njournalctl -u kubelet --no-pager | grep \"container-log-max\" 就可以看到底下兩個設定值：（圖片上面是 EKS，下面是 GKE）\n查看 containerLogMaxSize 跟 containerLogMaxFiles 當前設定值\n可以發現都跟預設值一樣。\n那我們也直接進到 /var/log/pods 底下，來查看是否是依照這個規律去運作。\n進入 /var/log/pods\n可以看到，就跟上面說的資料夾名稱是由 {namespace}_{pod name}_xxx 來命名，我們進去到隨機一個資料夾來查看 Log 檔案。\nGKE EKS 可以看到，GKE 跟 EKS 檔案數量都是 5 個，且每個都是小於 10Mi (因為計算誤差，所以有些會顯示 11、12)，所以可以確定是依照 containerLogMaxSize 跟 containerLogMaxFiles 在運作的。\n也可以看到 GKE Log，第一次查看最舊的檔案是 0.log.20250317-081814.gz，第二次查看最舊的檔案就變成 0.log.20250317-081844.gz，就代表超過 5 份 Log 檔案，kubelet 就會刪除最舊的 Log 檔案。","疑問#疑問":"我之前ㄧ直以為 Container Log Stdout 不會寫實體檔，後來發現，其實它還是會寫，但當 Node 重啟，或是 Pod 被刪除時，才會移除，要怎麼查看，我們可以進到 GKE or EKS 的 node 機器上，到 /var/log/pods/ 路徑底下，就可以看到運作在該 node 上的 pod，資料夾名稱會是以 {namespace}_{pod name}_xxx 來命名。\nHow nodes handle container logs 圖片：Logging Architecture\n所以就好奇，是誰來管理這些 Log 的回收機制呢？如果沒有回收機制，Log 不會把 Node 的 Disk 給寫爆嗎？"},"title":"K8s Node Log Stdout Logrotate 回收機制"},"/blog/kubernetes/k8s-open-region-how-to-reduce-cross-zone-costs/":{"data":{"":"由於最近公司開始導入 Multi Zone 架構，這時候會出現一個問題，就是明明我是同一個 GKE，但因為有多個 Zone，導致開始會有一些跨 Zone 的網路傳輸費用產生 (每 Gib 會收 0.01 美元)，可以參考下方圖片：\n跨域費用說明\n所以此文章會針對要如何減少跨 Zone 網路費用進行測試以及提供解決辦法。","參考資料#參考資料":"Google Cloud 內部 VM 之間的資料移轉定價：https://cloud.google.com/vpc/network-pricing?hl=zh-TW\nTopology Aware Routing：https://kubernetes.io/docs/concepts/services-networking/topology-aware-routing/\nAWS 相關文件 (AWS 也會有類似問題)：https://aws.amazon.com/tw/blogs/containers/exploring-the-effect-of-topology-aware-hints-on-network-traffic-in-amazon-elastic-kubernetes-service/","建置-gke-環境#建置 GKE 環境":" GKE 資訊：\nTier / Mode：Standard\n版本：1.32.2-gke.1297002\nLocation type：Regional\nDefault node zones：[\"asia-east1-a\", \"asia-east1-b\"]\n使用的 Module 請參考：880831ian/IaC terraform { source = \"${get_path_to_repo_root()}/modules/gke\" } include { path = find_in_parent_folders(\"root.hcl\") } inputs = { name = \"test\" regional = true region = \"asia-east1\" zones = [\"asia-east1-a\", \"asia-east1-b\"] release_channel = \"REGULAR\" master_ipv4_cidr_block = \"172.16.1.16/28\" network_project_id = \"xxxxx\" network = \"default\" subnetwork = \"default-asia-east1\" gcs_fuse_csi_driver = false enable_maintenance = false monitoring_enable_managed_prometheus = false deletion_protection = false node_pools = [ { name = \"test-pool\" machine_type = \"e2-small\" min_count = 1 max_count = 1 } ] } (這邊小提醒，如果使用多個 Zone，在 node 的 CA 記得要用 min_count 跟 max_count，他才是依照 Zone 去長，也就是 test-pool 會分別在 Zone A 跟 Zone B 去長歐)","測試連線#測試連線":"原生 Service 測試 建立完後，啟動兩個 nginx 服務，分別叫做 nginx-a 跟 nginx-b，各會有兩個 pod，並使用 podAntiAffinity 來限制同一個服務能建立在兩個 Zone 上面。\n相關的程式碼都會放在 880831ian/k8s-open-region-how-to-reduce-cross-zone-costs\n程式碼 下面附上其中一個 nginx-a 服務 yaml (nginx-b 也跟 nginx-a 一樣，只有名稱改變)\napiVersion: apps/v1 kind: Deployment metadata: name: nginx-a spec: replicas: 2 selector: matchLabels: app: nginx-a template: metadata: labels: app: nginx-a spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - nginx-a topologyKey: topology.kubernetes.io/zone containers: - name: nginx image: nginx:latest ports: - containerPort: 80 volumeMounts: - name: nginx-conf mountPath: /etc/nginx/conf.d volumes: - name: nginx-conf configMap: name: nginx-conf --- apiVersion: v1 kind: Service metadata: name: nginx-a spec: selector: app: nginx-a ports: - protocol: TCP port: 80 targetPort: 80 --- apiVersion: v1 kind: ConfigMap metadata: name: nginx-conf data: default.conf: | server { listen 80; location / { return 200 \"$hostname\\n\"; } } 顯示部署完的服務狀態 我們使用指令，列出四個 pod 各別的 node 以及所在的 Zone\nprintf \"%-30s %-35s %-20s\\n\" \"POD\" \"NODE\" \"ZONE\" echo \"---------------------------------------------------------------------------------------------\" kubectl get pod -o=custom-columns='POD:metadata.name,NODE:spec.nodeName' --no-headers | while read pod node; do zone=$(kubectl get node \"$node\" -o=jsonpath='{.metadata.labels.topology\\.kubernetes\\.io/zone}') printf \"%-30s %-35s %-20s\\n\" \"$pod\" \"$node\" \"$zone\" done Pod 所在 Node 以及 Zone\n接著，我們從 nginx-a-98f749dc6-jzl8m (Zone A) 發起連線打 nginx-b 的 svc，觀察一下他會打到同樣是 Zone A 的 nginx-b-795d6c85b8-c2k45，還是 Zone B 的 nginx-b-795d6c85b8-tncfm，或是隨機分配呢？\n我們使用以下指令打 1000 次並觀察次數\nfor i in {1..1000}; do curl -s http://nginx-b.default.svc.cluster.local done | sort | uniq -c Pod 所在 Node 以及 Zone\n為了測試準確性，我們重複測試 3 次\n同 nginx-a Zone A 打 nginx-b svc nginx-b Zone A nginx-b Zone B 第一次打 1000 次 482 518 第二次打 1000 次 493 507 第三次打 1000 次 498 502 就上面測試結果可以得知，他是隨機去分配到 Zone A 或是 Zone B 的 Nginx-b Pod，因為 Service 本來就會依照預設的 Round-Robin 來去分配。\n使用 Topology Aware Routing 測試 簡單說明一下，Topology Aware Routing 縮寫是 (TAR/TAH) 中文叫做：拓樸感知路由\n在 K8s 1.27 以前叫作 Topology Aware Hints，所以縮寫才會是 TAH，邏輯是需要在 Service 的 annotation 加上 service.kubernetes.io/topology-mode: Auto （1.27 Version），以前版本設定方式會不同。\n它可以幫助網路流量盡可能的保持在同一個 Zone 裡面，Pod 之前優先使用同 Zone 流量來提高可靠性、性能以及減少成本。\n設定完成後，會顯示來至 endpoint-slice-controller\n當設定上去後，因為 EndpointSlice 這邊會記錄每個 Pod 是屬於哪個 Zone，當並提供給 kube-proxy 知道，進而讓流量往同樣的 Zone 走。\n顯示部署完的服務狀態 那我們一樣用同樣的步驟再來測試一次：\n使用一樣的指令，先列出四個 pod 各別的 node 以及所在的 zone (服務都有重啟過)\nprintf \"%-30s %-35s %-20s\\n\" \"POD\" \"NODE\" \"ZONE\" echo \"---------------------------------------------------------------------------------------------\" kubectl get pod -o=custom-columns='POD:metadata.name,NODE:spec.nodeName' --no-headers | while read pod node; do zone=$(kubectl get node \"$node\" -o=jsonpath='{.metadata.labels.topology\\.kubernetes\\.io/zone}') printf \"%-30s %-35s %-20s\\n\" \"$pod\" \"$node\" \"$zone\" done 測試結果\n我們從 nginx-a-fd5bdb699-6qnwb (Zone A) 發起連線打 nginx-b 的 svc，一樣觀察他會打到同樣是 zone A 的 nginx-b-795d6c85b8-2q7m2，還是 Zone B 的 nginx-b-795d6c85b8-82427？\n我們使用以下指令打 1000 次並觀察次數\nfor i in {1..1000}; do curl -s http://nginx-b.default.svc.cluster.local done | sort | uniq -c Pod 所在 Node 以及 Zone\n為了公平性，我們也比照原本測試的，重複測試 3 次\n同 nginx-a Zone A 打 nginx-b svc nginx-b Zone A nginx-b Zone B 第一次打 1000 次 1000 0 第二次打 1000 次 1000 0 第三次打 1000 次 1000 0 所以可以得知：\n只要 Service 有加上該 annotation，就可以透過 EndpointSlice 走到同樣的 Zone。\n另外，我們也來測試看看，如果打的過程中，將 nginx-b Zone A nginx-b-795d6c85b8-2q7m2 Pod 給移除，那觀察流量會不會自動轉到 Zone B 呢？！\nPod 所在 Node 以及 Zone\n測試結果：當一開始進到 nginx-b-795d6c85b8-2q7m2 的流量，遇到 nginx-b-795d6c85b8-2q7m2 服務中斷，由於該 Nginx-b 在 Zone A 沒有其他 Pod，所以就會切回去使用 Zone B 的 Nginx-b 服務 (nginx-b-795d6c85b8-82427)，但當 Zone A 的 Nginx-b 服務好了，又會再走回 Zone A (nginx-b-795d6c85b8-w2vr4) \u003c 是後面長出來的 Pod。\n使用的建議 確保 K8s 版本在 1.27 以上，1.27 以下需要使用 service.kubernetes.io/topology-aware-hints。\n每個服務要均勻分配在每個 Zone 上面。\n確保服務在更新時，同 Zone 上面的數量足夠，避免切到其他 Zone 導致費用產生。"},"title":"Kubernetes 開啟 Region 後，如何減少跨 Zone 網路費用"},"/blog/kubernetes/k8s-plain/":{"data":{"":"前陣子對於 Kubernetes 部分內容還不是很清楚，在網路上閒逛的時候發現一篇很有趣的文章，標題是 『 給行銷跟業務的 Kubernetes 101 中翻中介紹 』，點進去後才發現，作者 Phil Huang 將 Kubernetes 元件的內容用大型住戶社區來介紹，讓我更清楚每一個元件的意思，以下我會用我理解的以及作者的思考邏輯來去寫這篇筆記，再次感謝作者文章 😍","kubernetes-元件白話文#Kubernetes 元件白話文":"\n大型社區示意圖 (圖片來源：蘋果地產)\n可以看到上面這張圖片，他是一個很典型的大型社區，我們這次的 Kubernetes 元件白話文，會以大型社區來當作現實中的元件，以社區的例子來說明 Kubernetes 。\n當然，我們之前的文章也有介紹過 Kubernetes，有興趣可以先飛回去看看！\nKubernetes (K8s) 介紹 - 基本\nKubernetes (K8s) 介紹 - 進階 (Service、Ingress、StatefulSet、Deployment、ReplicaSet、ConfigMap)\nKubernetes (K8s) 搭配 EFK 實作 (Deployment、StatefulSet、DaemonSet)\n我們先想像一下我們建立 Kubernetes 完整的叢集服務，就好比是建立一個大型的社區，所以以下會將名詞與社區來做連結，那．．．開始囉！\nKubernetes 元件 Kubernetes：建立這個大型社區的藍圖，規劃的社區內的大大小小的設計。 虛擬化平台、公有雲平台、實體機器：就是我們要蓋社區的地皮。(虛擬化平台：vSphere/Proxmox/VMware、公有雲平台：AWS/GCP/Azuer、實體機器：就是實體機器 😂) OS 作業系統：每棟大樓的骨架和地基。 Master Node：住戶管委會所居住大樓 (真好自己有所屬的大樓ＸＤ)，為了保證他們不會吵架，所以建議最少需要三棟。 Etcd Cluster：管委會的人，一樣為了怕一黨獨大(?，所以建議最少需要有三位管委，且互相投票選出一個頭頭。 Worker Node：就是偶們住戶所居住的大樓。 Pod：住戶，所以我們一棟 Worker Node 大樓，可以有很多 Pod 住戶。 Pod IP：每個住戶的門牌，既然是門牌，代表他也不會重複。 Container Registry：包裹集中的存放中心。 Container Images：還沒有被打開的包裹。 Containers：已經被打開且正在使用的包裹，那每個 Pod 住戶裡面，可以有一個或很多個以上的 Containers 包裹。 Service：社區裡面的社團，例如：羽球社、麻將社等等，可以集合大家的地方。 Service Mesh：社團的聯絡名冊，會記住誰是哪一個社團的成員。 Ingress Controller：社區的大門，可以指定讓社區成員都固定走同一個或是多個門的入口管控。 Egress Controller：社區的後門，可以指定讓社區成員都固定走同一個或是多個門的出口管控。 Internal DNS / Service Discovery：大樓住戶的電話簿。 External DNS：指向各大樓的路標。 OCI (Open Container Initiative)：制定大樓鋼筋水泥或是行李箱標準的組織。 CRI (Container Runtime Interface)：大樓鋼筋水泥的廠商。 CNI (Container Network Interface)：大樓水電系統的廠商。 CSI (Container Storage Interface)：大樓空間規劃的廠商。 Bastion：專門維護社區的工程車。 常用套件 Promethus：社區整體的監控中心 (Meterics)，一堆攝影機的管理室 XD Grafana：監控中心裡面大型的 LED 儀表板。 Elaticsearch：社區整體的情資中心 (Logging)，這應該是一群愛八卦的大媽吧 😏 Kibana：情資中心裡面大型的 LED 訊息版。 ","參考資料#參考資料":"給行銷跟業務的 Kubernetes 101 中翻中介紹","常見問題#常見問題":"作者也有整理了一些常見的問題，我把它整理一下挑選出幾個我自己一開始也會有疑問的問題，我們一起來看看吧！一樣我會用我所理解的意思來介紹 ~\nQ1：為什麼 Kubernetes 的最小單位是 Pod 這個要怎麼理解？ Ans ： 試想一下，難道管委會會管你的包裹裡面內容物放什麼嗎 XD\nQ2：Docker 在 Kubernetes 的角色是什麼？ Ans ： 是眾多的 CRI (Container Runtime Interface) 選擇之一，也就是大樓的鋼筋水泥廠商有很多間，有一間叫做 Docker 的廠商特別有名。\nQ3：呈上，那 CRI/CNI/CSI 是不是也可以替換成其他的廠商？ Ans ： 當然可以，現在可以看到越來越多廠商都開始支援 Kubernetes 就是因為這個原因，因為一般情況下，Kubernetes 並不會特別限定 大樓鋼筋水泥廠商、大樓水電系統廠商、大樓空間規劃廠商，只要有符合特定的標準即可，但要留意 Kubernetes 的版號也會受到這三個介面支援發行的版號所影響，要留意相容性的問題！\nQ4：整個住戶社區最重要的角色是什麼？ Ans ： 那三棟管委會大樓，和裡面的的三位委員，三位掛掉一位還可以維持正常運作，掛掉兩位會維持唯獨運作。\nQ5：Kubernetes、VM、Container 的差異性 Ans ： 我們可以建立好一個大樓(VM)，隨意放置一個或是多個包裹(Containers)，必須手動管理這些包裹，如果資源不足或是這棟大樓倒了，就沒有辦法自動轉移這些包裹的內容了。\n但如果是 Kubernetes，我們就可以建立多個大樓(VM)，透過 Kubernetes 所規定的放置計畫 (例如：Deployment、DaemonSet)，我們將可以統一調度這些 Pod，當某棟大樓資源不夠或是這棟大樓倒時，可以根據 Kubernetes 規則來進行搬遷或是擴充大樓。\nQ6：為什麼有人會把 Container 跟 Pod 混在一起講 Ans ： 因為大部分的情況下，都是一個住戶(Pod)放一個包裹(Container)，，才會導致這樣的誤會。但實際上，一個住戶(Pod)是可以放一個或多個包裹在裡面的！\nQ7：什麼是 Node Scaling ? Ans ： 可以把它理解成，當住戶太多時，Kubernetes 會自動或手動興建大樓，來把多出來的用戶給塞進去。\nQ8：在 Q5 有提到放置計畫是什麼意思？ Ans ： 只要你有需要將包裹放在社區內，都必須提出部署計畫(Deployment) 給管委會審核，只要通過審核，他們依照你事先聲明的計畫，盡最大可能性來放置包裹。\n所以當我們有任何變更計畫時，都需要重新提交一份新的部署計畫給管委會重新審核和接受。"},"title":"用大型社區來介紹 Kubernetes 元件"},"/blog/kubernetes/k8s-statefulset-podmanagementpolicy/":{"data":{"":"此文章要來記錄一下前陣子在公司的正式環境踩到 StatefulSet 的雷，事情是這樣的，我們有些服務，是使用 StatefulSet 來建置，至於為什麼不用 Deployment，這個說來話長 (也不是因為需要特定的 Pod 名稱、或是網路標記等等)，我們這邊先不討論，這個 StatefulSet 服務是 Nginx + PHP-FPM，為了避免流量進入到 processes 已經被用光的 Pod 中，我們在 StatefulSet 的 PHP Container 上有設定 readiness，readiness 的設定長得像以下：\nreadinessProbe: exec: command: - \"/bin/bash\" - \"-c\" - | CHECK_INFO=$(curl -s -w 'http code:\\t%{http_code}\\n' 127.0.0.1/status) HTTP_CODE=$(echo -e \"${CHECK_INFO}\" | awk '/http code:/ {print $3}') IDLE_PROCESSES=$(echo -e \"${CHECK_INFO}\" | awk '/idle processes:/ {print $3}') [[ $HTTP_CODE -eq 200 \u0026\u0026 $IDLE_PROCESSES -ge 10 ]] || exit 1 我們會用 curl 來打 /status，檢查回傳的 http code 是否為 200，以及 idle processes 是否大於等於 10，如果不符合，就會回傳 1，讓他被標記不健康，讓 Kubernetes 停止流量到不健康的容器，以確保流量被路由到其他健康的副本。","參考資料#參考資料":"Kubernetes — 健康檢查：https://medium.com/learn-or-die/kubernetes-%E5%81%A5%E5%BA%B7%E6%AA%A2%E6%9F%A5-59ee2a798115\nPod Management Policies：https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#pod-management-policies","問題#問題":"當天遇到的情況是，我們上程式後，Pod 都一切正常，當流量開始進來後，發現 10 個 Pod 會開始偶發的噴 Readiness probe failed，查看監控發現 processes 越來越低，最後被反應服務有問題，我們查看 Hpa 的紀錄的確有觸發到 40 個 Pod，只是查看 Pod 數還是依樣卡在 10 個，當下我們有嘗試使用調整 yaml 在 apply，發現 StatefulSet 的 yaml 也已經更新了，但 Pod 還是一樣卡在 10 個，也有使用 kubectl 下 kubectl scale sts [服務名稱] --replicas=0，想要切換 Pod 數也沒有辦法。\n當下我們有先 Call Google 的 Support 一起找原因，Google 是建議我們 readiness 的條件不要設的太嚴格，可以加上 timeoutSeconds: 秒數，但對於 Pod 卡住，還是沒有找到原因，後來我們查了一下 StatefulSet 的文件發現，StatefulSet 有一個設定 podManagementPolicy，預設是 OrderedReady，他必須等待前面的 Pod 是 Ready 狀態，才會再繼續建立新的，也就是說我們的 StatefulSet 已經卡住，導致就算 Hpa 觸發要長到 40 個 Pod 也沒有用。","測試結果#測試結果":"最後我們就使用兩種模式來測試看看，已下是測試結果(透過 P1 才知道的設定ＱＱ)：\n有將測試的 StatefulSet 放在 Github，可以點我查看 (可以調整 readinessProbe 的 httpGet.Path 故意把他用壞)\n使用 OrderedReady 模式 StatefulSet 在 podManagementPolicy 預設 OrderedReady 的模式，故意讓 readiness 卡住時 (Pod 卡住時)：\n當下的 StatefulSet 設定： StatefulSet 設定\nPod 狀態： Pod 狀態\n使用指令調整 Pod 數量 我們這時候下指令調整 Pod 數量，看看會發生什麼事：\nkubectl scale sts my-statefulset --replicas=5 我們先看 StatefulSet 的 yaml 可以看到 Pod replicas 已經改變，也可以看 generation 有更新，代表 StatefulSet 本身有接收到調整設定的請求。\n下指令調整後的 StatefulSet 設定\n看了一下 Pod 數量，也是一樣卡住，且 Pod 數量也沒有變化。\n下指令調整後的 Pod 狀態\n使用 yaml 調整 Pod 數量 我們直接調整 StatefulSet yaml 的 Pod 數量，看看會發生什麼事：\n一樣我們先看 StatefulSet 的 yaml 可以看到 Pod replicas 已經改變(這裡應該切別的 Pod 數量，切回 3 個好像沒有意義 xD)，也可以看 generation 有更新。\n使用 yaml 調整後的 StatefulSet 設定\n看了一下 Pod 數量，也是一樣卡住，且 Pod 數量也沒有變化。\n使用 yaml 調整後的 Pod 狀態\n所以代表在 OrderedReady 的模式下，Pod 卡住時，無法對 Pod 進行任何操作，必須要手動刪除卡住的 Pod 才吃得到最新的設定。\n使用 Parallel 模式 StatefulSet 在 podManagementPolicy Parallel 的模式，故意讓 readiness 卡住時 (Pod 卡住時)：\n當下的 StatefulSet 設定： StatefulSet 設定\nPod 狀態： Pod 狀態\n使用指令調整 Pod 數量 我們這時候下指令調整 Pod 數量，看看會發生什麼事：\nkubectl scale sts my-statefulset --replicas=5 我們先看 StatefulSet 的 yaml 可以看到 Pod replicas 已經改變，也可以看 generation 有更新，代表 StatefulSet 本身有接收到調整設定的請求。\n下指令調整後的 StatefulSet 設定\n看了一下 Pod 數量，就算 my-statefulset-2 卡住，還是可以擴到 5 個 Pod。\n下指令調整後的 Pod 狀態\n使用 yaml 調整 Pod 數量 我們直接調整 StatefulSet yaml 的 Pod 數量，看看會發生什麼事：\n一樣我們先看 StatefulSet 的 yaml 可以看到 Pod replicas 已經改變，也可以看 generation 有更新。\n使用 yaml 調整後的 StatefulSet 設定\n看了一下 Pod 數量，也不會管其他 Pod 是否 Ready，一樣可以縮小成 2 個 Pod。\n使用 yaml 調整後的 Pod 狀態","結論#結論":"後來我們重新檢查了一下為什麼 processes 會用完，結果發現是 RD 的程式邏輯，導致每筆 Request 必須等待前一筆 Request 做完，才會開始動作，讓 processes 一直被占用，沒辦法即時消化，導致 processes 用完，又加上服務是使用 StatefulSet，預設模式的 OrderedReady，必須等待前一個 Pod 是 Ready 才可以自動擴縮，所以當我們 Hpa 想要擴縮，來增加可用的 processes 數量，也因為沒辦法擴縮，最後導致這一連串的問題 😕。\n另外，如果想要從 OrderedReady 模式切成 Parallel 模式 (反正過來也是)，必須先將原本的 StatefulSet 給刪除，才可以調整：\nOrderedReady 模式切成 Parallel 模式","解決辦法#解決辦法":"當下想趕快解決 readiness 這個問題，調整 timeoutSeconds 後，單純 apply 是沒有用的，要記得刪掉卡住的 Pod，讓他重新建立，才會套用新的設定 (但我們當下太在意為甚麼 Pod 會卡住，沒有想到要先把 readiness 問題修掉 xD，我們當下的解法是先將流量導到地端正常的服務上)。\n另外 Google 也說，假如我們還是必須使用 StatefulSet 來建立服務，建議我們把 podManagementPolicy 改成 Parallel，它會有點像是 Deployment 的感覺，不會等待其他 Pod 變成 Ready 狀態，所以可以讓我們就算在 readiness 卡住的情況下，也可以自動擴縮服務。\nℹ️ StatefulSet podManagementPolicy 參數說明\nOrderedReady (預設) Pods 會按照順序一個接一個地被創建。即，n+1 號 Pod 不會在 n 號 Pod 成功創建且 Ready 之前開始創建。 在縮小 StatefulSet 的大小時，Pods 會按照反向順序一個接一個地被終止。即，n 號 Pod 不會在 n+1 號 Pod 完全終止之前開始終止。 這確保了 Pods 的啟動和終止的順序性。\nParallel 所有 Pods 會同時地被創建或終止。 當 StatefulSet 擴展時，新的 Pods 會立即開始創建，不用等待其他 Pods 成為 Ready 狀態。 當縮小 StatefulSet 的大小時，要終止的 Pods 會立即開始終止，不用等待其他 Pods 先終止。 這種策略提供了快速的擴展和縮小操作，但缺乏順序性保證。"},"title":"正式環境上踩到 StatefulSet 的雷，拿到 P1 的教訓"},"/blog/kubernetes/k8s/":{"data":{"":"","kubernetes-元件介紹與說明#Kubernetes 元件介紹與說明":"Kubernetes 是如何幫我們管理以及部署 Container ? 要了解 Kubernetes 如何運作，就要先了解它的元件以及架構：\n那我們由小的往大的來做介紹：依序是 Pod、Worker Node、Master Node、Cluster\nPod Kubernetes 運作中最小的單位，一個 Pod 會對應到一個應用服務 (Application)，舉例來說一個 Pod 可能會對應到一個 NginxServer。\n每個 Pod 都有一個定義文件，也就是屬於這個 Pod 的 yaml 檔。 一個 Pod 裡面可以有一個或多個 Container，但一般情況一個 Pod 最好只有一個 Container。 同一個 Pod 中的 Containers 共享相同的資源以及網路，彼此透過 local port number 溝通。 Worker Node Kubernetes 運作的最小硬體單位，一個 Worker Node (簡稱 Node) 對應到一台機器，可以是實體例如你的筆電、或是虛擬機例如 GCP 上的一台 Computer Engine。\n每一個 Node 都有三個組件所組成：kubelet、kube-proxy、Container Runtime\nkubelet 該 Node 的管理員，負責管理該 Node 上的所有 Pods 的狀態並負責與 Master Node 溝通。\nkube-proxy 該 Node 的傳訊員，負責更新 Node 的 iptables，讓 Kubernetes 中不在該 Node 的其他物件可以得知該 Node 上的所有 Pods 的最新狀態。\nContainer Runtime 該 Node 真正負責容器執行的程式，K8s 預設是 Docker，但也支援其他 Runtime Engine，例如 Mirantis Container Runtime、CRI-O、containerd\n常見誤解：\n很多人認為 Kubernetes 是 docker container 的管理工具，包含我一開始也是這樣認為，但其實 Kubernetes 是用來管理容器化 (containerized applications) 並不是專屬於 docker 獨享，作為一個 container orchestrator 的角色，Kubernetes 希望能夠管理所有容器化的應用程式\nMaster Node Kubernetes 運作的指揮中心，可以簡化看成一個特殊化的 Node 來負責管理所有其他的 Node。\n一個 Master Node (簡稱 Master) 中有四個組件組成：kube-apiserver、etcd、kube-scheduler、kube-controller-manager\nkube-apiserver 管理整個 Kubernetes 所需 API 的接口 (Endpoint)，例如從 Command Line 下 kubectl 指令就會把指令送到這裡。 負責 Node 之間的構通橋樑，每個 Node 彼此不能直接溝通，必須要透過 apiserver 轉介。 負責 Kubernetes 中的請求的身份認證與授權。 etcd etcd 是兼具一制性和高可用性的分散式鍵值數據庫，可以保存 Kubernetes 所有 Cluster 的後台數據庫。 kube-scheduler 整個 Kubernetes 的 Pods 調度員，scheduler 會監視新建立但還沒有被指定要跑在哪個 Node 上的 Pod ，並根據每個 Node 上面資源規定，硬體限制等條件去協調出一個最適合放置 Node 來讓該 Pod 運行。 kube-controller-manager 負責管理並運行 Kubernetes controller 的組件，簡單來說 controller 就是 Kubernetes 裡一個負責監視 Cluster 狀態的 Process，例如：Node Controller、Replication Controller。 這些 Process 會在 Cluster 與預期狀態 (desire state) 不符時嘗試更新現有狀態 (current state)。例如：現在要多開一台機器以應付突然增加的流量，那我的預期狀態就會更新成 N+1，現有狀態是 N，這時相對應的 controller 就會想辦法多開一台機器。 controller-manager 的監視與嘗試更新也都需要透過訪問 kube-apiserver 達成。 Cluster Cluster 也叫叢集，可以管理眾多機器的存在，在一般的系統架設中我們不會只有一台機器而已，通常都是多個機器一起運行同一種內容，在沒有 Kubernetes 的時候就必須要土法煉鋼的一台一台機器去更新，但有了 Kubernetes 我們就可以透過 Cluster 進行控管，只要更新 Master 旗下的機器，也會一併將更新的內容放上去，十分方便。在 Kubernetes 中多個 Worker Node 與 Master Node 的集合。","什麼是-annotation-#什麼是 Annotation ?":"前面提到的 Label 功用其目的是要讓 Kubernetes 知道可以去更方便管理的，那我們如果想要貼標籤但不想讓 Kubernetes 知道，要怎麼做呢？\n這時我們就可以用 Annotation，透過 Annotation 可以將標籤單純給開發人員查看，那聽起來 Annotation 好像沒有什麼實質上的用途，因為 Kubernetes 不會採用這些標籤，但其實 Annotation 還是有用的歐！後續文章會再提到 \u003e\u003c\n那既然 Label 跟 Annotation 有相似，所以寫法想必也是差不多吧：\nannotations: author: Pin-YI contact: 880831ian@gmail.com 一樣也是定義一組具有辨識度的 key/value ，我們這邊就先放 author、contact\n那 Label 與 Annotation 要放在 Pod 的哪一處呢？\n還記得我們上面說 metadata 是用來擺描述性資料的地方嗎，所以不管是 Label 或是 Annotation 都是放在 metadata 中歐！\nmetadata: name: kubernetes-demo-pod labels: app: demo annotations: author: Pin-YI contact: 880831ian@gmail.com ","什麼是-kubernetes-k8s#什麼是 Kubernetes (K8s)?":"Kubernetes 也可以叫 K8s，這個名稱來源希臘語，意思是舵手或是飛行員，所以我們可以看到它的 logo 是一個船舵的標誌，之所以叫 K8s 是因為 Kubernetes 的 k 到 s 中間有 8 的英文字母，為了方便，大家常以這個名稱來稱呼他！\nkubernetes logo\nKubernetes 是一種開源可用來自動化部屬、擴展以及管理多個容器的系統，適用於當容器數量增加，需要穩定容器環境，以及管理資源或權限分配的狀況。\n我們之前在 Docker 介紹 文章中，已經有介紹以往傳統虛擬機以及容器化的 Docker 差異以及優點，那當我們在管理容器時，其中一個容器出現故障，則需要啟動另一個容器，如果要用手動，會十分麻煩，所以這時就是 Kubernetes 的厲害的地方了，Kubernetes 提供：\n服務發現和負載平衡：K8s 可以使用 DNS 名稱或是自己的 IP 位址來公開容器。如果容器流量過高，Kubernetes 能夠使用負載平衡和分配網路流量，能使部署更穩定。 編排儲存：Kubernetes 允許使用自動掛載來選擇儲存系統，例如使用本地儲存，或是公共雲等。 自動部署、刪除：可以使用 Kubernetes 來幫我們自動化部屬新的容器、刪除現有的容器並將其資源用於新容器。 自動打包：當我們為 Kubernetes 提供一個節點叢集，它可以用來運行容器化的任務，告訴 Kubernetes 每個容器需要多少 CPU 和 RAM。Kubernetes 可以將容器安裝到節點上，充分利用資源。 自動修復：Kubernetes 會重新啟動失敗的容器、替換容器、刪除不回應用戶的不健康容器，並且在容器準備好服務之前不會通知客戶端。 機密和配置管理：Kubernetes 允許儲存和管理敏感訊息，例如密碼、OAuth token 和 SSH 金鑰。可以部署和更新機密的應用程序配置。 kubernetes 官網\nKubernetes 很常被拿來與 Docker Swarm 做比較，兩者不同的是，Docker Swarm 必須建構在 Docker 的架構下，功能侷限、無法跳脫。\nKubernetes 則因為功能較為廣泛，而逐漸取代 Docker Swarm 在市場上的地位。下方有簡易的比較表格：\n比較 Kubernetes Docker Swarm 說明 Kubernetes 是一個開源容器的編排平台，Kubernetes 的叢集結構比 Docker Swarm 更為複雜。\nKubernetes 通常有建構器和工作節點，還可進一步分為 Pod、命名空間、配置映射等。 Docker Swarm 是一個由 Docker 構建和維護的開源容器編排平台。\n一個 Docker Swarm 叢集通常包含三個項目：Nodes、Services and tasks、Load balancers。 優點 它有龐大的開源社群，由 Google 支持。\n它可以維持和管理大型架構和複雜的工作負載。\n它是自動化，並支持自動化擴展的自我修護能力。\n它有內建監控和廣泛的可用集成。 Docker Swarm 安裝簡單，它輕量化且容易學習使用。\nDocker Swarm 與 Docker CLI 一起運作，因此不需要多運行或是安裝新的 CLI 缺點 它複雜的安裝過程以及較難學習\n它需要安裝單獨的 CLI 工具並且學習每一項功能 它是輕量級且與 Docker API 相關聯，與 Kubernetes 相比，Docker Swarm 被限制很多功能，且自動化也沒有 Kubernetes 強大。 ","什麼是-label-#什麼是 Label ?":"Label 顧名思義就是標籤，可以為每一個 Pod 貼上標籤，讓 Kubernetes 更方便的管控這些 Pod。\nLabel 的寫法很簡單，可以自己自訂一對具有辨識度的 key/value，舉我們上面的例子來說：我們可以在 labels 內加入 app: demo，那 Label 有什麼好處呢?\n這邊要稍微提一下 Selector，它的功用是選取對應的物件。為了要方便選取到我們設定好的 Pod，這時候 Label 就派上用場了！\nSelector 的寫法也很簡單，只要把我們在 Label 定義的 key/value 直接完整的貼過來就可以了～\n就像這樣：\nselectors: app: demo 那選取後有什麼功用呢！請看 Kubernetes - 進階 - Service\n講完 Label 後，順邊提一下跟 Label 有相似的：","參考資料#參考資料":"kubernetes 官網：https://kubernetes.io/\nKubernetes（K8s）是什麼？基礎介紹+3 大優點解析：https://www.sysage.com.tw/news/technology/293\nDocker Swarm vs Kubernetes: how to choose a container orchestration tool：https://circleci.com/blog/docker-swarm-vs-kubernetes/\nKubernetes 基礎教學（一）原理介紹：https://cwhu.medium.com/kubernetes-basic-concept-tutorial-e033e3504ec0\nKubernetes 那些事 — 基礎觀念與操作：https://medium.com/andy-blog/kubernetes%E9%82%A3%E4%BA%9B%E4%BA%8B-%E5%9F%BA%E7%A4%8E%E8%A7%80%E5%BF%B5%E8%88%87%E6%93%8D%E4%BD%9C-97cc203a2660\nKubernetes 那些事 — Pod 篇：https://medium.com/andy-blog/kubernetes-%E9%82%A3%E4%BA%9B%E4%BA%8B-pod-%E7%AF%87-57475cec22f3\nKubernetes 那些事 — Label 篇：https://medium.com/andy-blog/kubernetes-%E9%82%A3%E4%BA%9B%E4%BA%8B-label-%E7%AF%87-4186af2af556","基本運作#基本運作":" kubernetes 組件 Kubernetes 基礎教學（一）原理介紹\n接下來我們用 「Kuberntes 是如何建立一個 Pod ？」來複習一下整個 Kubernetes 的架構。\n(上圖是一個簡易的 Kubernetes Cluster ，通常 Cluster 為了高穩定性都會有多個 Master 作為備援，但為了簡化我們只顯示一個。)\n當使用者要部署一個新的 Pod 到 Kubernetes Cluster 時，使用者要先透過 User Command (kubectl) 輸入建立 Pod 對應的指令 (後面會說明要如何實際的動手建立一個 Pod)。此時指令會經過一層確認使用者身份後，傳遞到 Master Node 中的 API Server，API Server 會把指令備份到 etcd。 controller-manager 會從 API Server 收到需要創建一個新的 Pod 的訊息，並檢查如果資源許可，就會建立一個新的 Pod。最後 Scheduler 在定期訪問 API Server 時，會詢問 controller-manager 是否有建置新的 Pod，如果發現新建立的 Pod 時，Scheduler 就會負責把 Pod 配送到最適合的 Node 上面。 雖然上面基本的運作看起來十分複雜，但其實我們在實際操作時，只是需入一行指令後，剩下的都是 Kubernetes 會自動幫我們完成後續的動作。","如何建立一個-pod#如何建立一個 Pod":"版本資訊\nMinikube：v1.25.2 hyperkit：0.20200908 Kubectl：Client Version：v1.22.5、Server Version：v1.23.3 下載完 Minikube 後，我們可以先透過 Minikube 來查詢全部的指令，由於我們前面有安裝 Hyperkit 這個驅動程式，啟動 Minikube 預設是使用 Docker，我們這邊要利用 Hyperkit 來啟動，所以使用 Minikube start --vm-driver=hyperkit 來啟動 Minikube。\nMinikube 其他指令介紹：\n顯示 minikube 狀態 minikube status\n停止 minikube 運行 minikube stop\nssh 進入 minikube 中 minikube ssh\n查詢 minikube 對外的 ip minikube ip\n使用 minikube 所提供的瀏覽器 GUI minikube dashboard (可以加 - - url 看網址歐)\n我們啟動 minikube 後，我們要打做一個可以在 Pod 運行的小程式。這個小程式是一個 Node.js 的 Web 程式，他會建立一個 Server 來監聽 3000 Port，收到 request 進來後會渲染 index.html 這個檔案，這個檔案裡面會有一隻可愛的小柴犬。\n因為本文章是在介紹 kubernetes 所以在程式部分就不多做說明，我把程式碼放在 Github，以及附上 Dockerhub 的 Repository 可以直接使用包好的 image 來做測試！\nPod yaml 檔案說明 接下來我們要先撰寫一個 Pod 的定義文件 (.yaml) 檔，這個 .yaml 檔就可以建立出 Pod 了！\nkubernetes-demo.yaml (程式縮排要正確，不然會無法執行歐！) apiVersion: v1 kind: Pod metadata: name: kubernetes-demo-pod labels: app: demo spec: containers: - name: kubernetes-demo-container image: 880831ian/kubernetes-demo ports: - containerPort: 3000 apiVersion\n該元件的版本號，必須依照 Server 上 K8s 版本來做設定 (想要知道 k8s 版本，可以使用 kubectl version 指令來查詢，會顯示 client 跟 server 的版本訊息，client 代表 kubectl 版本訊息，server 代表的是 master node 的 k8s 版本訊息)，目前 k8s 都使用 1.23 版本以上，所以 apiVersion 直接寫 v1 即可。\nkind\n該元件的屬性，用來決定此設定檔的類型，常見的有 Pod、Node、Service、Namespace、ReplicationController 等等\nmetadata\n用來擺放描述性資料的地方，像是 Pod 名稱或是標籤等等都會放在此處。\nname：指定該 Pod 的名稱 labels：指定該 Pod 的標籤 spec\n用來描述物件生成的細節，像是 Pod 內其實是跑 Dokcer container，所以在 Pod 的 spec 內就會描述 container 的細節。\ncontainer.name：指定運行的 Container 的名稱 container.image：指定 Container 要使用哪個 Image，這裡會從 DockerHub 上搜尋 container.ports：指定該 Container 有哪些 Port number 是允許外部存取的 使用 kubectl 建立 Pod 當我們有了定義文件後，我們就可以使用 kubectl 的指令來建立 Pod\nkubectl create -f kubernetes-demo.yaml kubectl apply -f kubernetes-demo.yaml 可以使用 create or apply 來建立 Pod ，那這兩個的差異是什麼呢？\nkubectl create kubectl create 是所謂的 “命令式管理” (Imperative Management)。通過這種方式，可以告訴 Kubernets API 你要建立、更新、刪除的內容。\nkubectl create 命令是先刪除所有現有的東西，重新根據 YAML 文件生成新的 Pod。所以要求 YAML 文件中的配置必須完整。\nkubectl create 命令，使用同一個 YAML 文件重複建立會失敗。\nkubectl apply kubectl apply 是 “聲明式管理” (Declarative Management)方法的一部分。在該方法中，即使對目標用了其他更新，也可以保持你對目標應用的更新。\nkubectl apply 命令，根據配置文件裡面出來的內容，生成就有的。所以 YAML 文件的內容可以只寫需要升級的欄位。\n如果看到 pod/kubernetes-demo-pod created 就代表我們成功建立第一個 Pod 了，接下來我們可以使用：\nkubectl get pods 可以查看我們運行中的 Pod：\nNAME READY STATUS RESTARTS AGE kubernetes-demo-pod 1/1 Running 0 3m5s Pod 指令介紹：\n查詢現有 Pod 狀態 kubectl get po/pod/pods\n查看該 Pod 詳細資訊 kubectl describe pods \u003cpod-name\u003e\n刪除 Pod kubectl delete pods\n查看 Pod log kubectl logs \u003cpod-name\u003e\n也可以使用 minikube 圖形化頁面來看一下是否成功！\nminikube dashboard --url 🤔 Verifying dashboard health ... 🚀 Launching proxy ... 🤔 Verifying proxy health ... http://127.0.0.1:55991/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/ minikube dashboard\n連線到 Pod 的服務 當我們建立好 Pod 之後，打開瀏覽器 localhost:3000 會發現，什麼都沒有，是因為我們剛剛在 .yaml 裡面設定的是 Pod 的 Port ，它與本機的 Port 是不相通的，因此我們需要使用 kubectl port-forward \u003cpod\u003e \u003cexternal-port\u003e:\u003cpod-port\u003e ，來將 Pod 與本機端做 mapping。\nkubectl port-forward kubernetes-demo-pod 3000:3000 Forwarding from 127.0.0.1:3000 -\u003e 3000 Forwarding from [::1]:3000 -\u003e 3000 Handling connection for 3000 我們在此瀏覽 localhost:3000，就會看到可愛的柴犬囉！\n成功顯示柴犬\n前面我們已經創建屬於我們第一個 Pod 了，但當我們 Pod 越建越多時，要怎麼快速的得知每個 Pod 在做什麼事情？除了用 Pod 的 metadata name 來命名外，還有另一種方式：","安裝-kubernetes#安裝 Kubernetes":"在我們開始操作 Kubernetes 之前，需要先下載 Minikube、Hyperkit、Kubectl 套件：\nMinikube 一個 Google 發佈的輕量級工具，讓開發者可以輕鬆體驗一個 Kubernetes Cluster。(僅限開發測試環境)\n安裝 minikube\n(Mac 專用) Hyperkit Hyperkit 是 MacOS 系統細部設定的驅動程式。\n安裝 hyperkit\nKubectl Kubectl 是 Kubernetes 的 Command Line 工具，我們之後會透過 Kubectl 去操作我們的 Kubernetes Cluster。\n安裝 kubectl"},"title":"Kubernetes (K8s) 介紹 - 基本"},"/blog/kubernetes/kubedns-vs-coredns/":{"data":{"":"前情提要：由於上一篇筆記 Pod 出現 cURL error 6: Could not resolve host 的後續規劃中，有提出我們想研究看看 Core DNS 來取代 Kube DNS，因此，這篇筆記要來比較一下 Kube DNS 以及 Core DNS 的差異以及兩者的優缺點，另外還會說明一下 DNS 的最佳實踐做法。\n前面概論介紹 K8s 內的 DNS 以及 CoreDNS 主要都是參考 探秘K8S DNS：解密IP查詢與服務發現、CoreDNS簡單除錯：解決你遇到的一般問題，詳細可以查看原文。","gke-上的-dns可以改用-coredns-嗎#GKE 上的 DNS，可以改用 CoreDNS 嗎？":"目前 GKE 上的 DNS 預設是 KubeDNS，或是使用 GCP 提供的另一個 Cloud DNS 服務，但是上次我們有提到，因為公司之後想走多雲架構，會開始使用 AWS，而 AWS 的 EKS 預設的 DNS 服務是 CoreDNS (K8s 現在預設也是 CoreDNS 😿)\n所以我們想說，是不是可以在 GKE 上也改使用 CoreDNS，這樣之後維護可以更一致。\n2025/08/05 更新：\n發現下方 「Google 有一篇 knowledge 文章」文章連結已經失效，並且最近與 Google TAM 確認一些問題，以下分享給大家： A1：是否可以自行調整 kubelet cluster-dns-ip 設定？ Q1：使用者無法自行調整。 A2：是否能夠自己建立 node-local-dns，而不使用 GCP 提供的方式安裝？ Q2：因為無法調整 kubelet 的內容(cluster-dns-ip)，所以也不能自建 node-local-dns。 A3：KubeDNS 如果想要調整 pod 數量，我們知道要調整 kube-dns-autoscaler configmap，我們想將這個納入版控進行控制，想確認這個有可能會因為更新 cluster 而被還原嗎？ https://cloud.google.com/kubernetes-engine/docs/how-to/nodelocal-dns-cache?hl=zh-tw#scaling_up_kube-dns Q3：deployment 會被還原是因為 KubeDNS 後面還有一個 Controller 在控制，Configmap 設計目的的確是可以讓我們自行接管來調整，且不會被還原。 原本自定義 KubeDNS、或是改成其他 DNS 供應商，可以參考此篇文件：設定自訂 kube-dns 部署作業 另外，目前有針對 GKE 的四種 KubeDNS、Cloud DNS、NodeLocal DNSCache 組合有進行測試，詳細可以查看以下連結： GKE KubeDNS 運作測試 GKE KubeDNS + NodeLocal DNSCache 運作測試 GKE Cloud DNS 運作測試 GKE Cloud DNS + NodeLocal DNSCache 運作測試\n在爬文的過程中有發現，Google 有一篇 knowledge 文章 How to run CoreDNS on Kubernetes Engine? 有提到 Google 預設就是使用 KubeDNS，沒辦法把 KubeDNS 縮小到 0 並完全替換，如果只是單純想要使用 CoreDNS 的快取功能，可以啟用 GKE 上的 NodeLocal DNSCache。\n如果想要把 CronDNS 解析功能加到 GKE 上，要先部署一個 CoreDNS Pod，並透過 service 公開它，並為 KubeDNS 配置一個 stub domain，將其指向 CoreDNS 服務 IP。\n所有與 stub domain 後綴相符的流量都會路由到 CoreDNS Pod。不符合的流量將繼續由 KubeDNS 解析。","kubedns-vs-coredns-優缺點#KubeDNS vs CoreDNS 優缺點":"KubeDNS 優點：有 dnsmesq，在性能上有一定的保障。 缺點： dnsmesq 如果重啟，會先刪掉 process 才會重新起服務，中間可能會出現查詢失敗。在確認內部檔案時，如果數量過多或是太頻繁更新 DNS 有可能反而導致 dnsmasq 不穩，這時候就需要重啟 dnsmasq 服務。 CoreDNS 優點：可以自行根據需求使用自訂的 Plugins，記憶體的佔用情況也比 KubeDNS 好。 缺點：Cache 功能不如 dnsmasq，內部解析速度可能會比 KubeDNS 慢。 ","參考資料#參考資料":"探秘K8S DNS：解密IP查詢與服務發現：https://weng-albert.medium.com/%E6%8E%A2%E7%A7%98k8s-dns-%E8%A7%A3%E5%AF%86ip%E6%9F%A5%E8%A9%A2%E8%88%87%E6%9C%8D%E5%8B%99%E7%99%BC%E7%8F%BE-034de2e72abe\nCoreDNS簡單除錯：解決你遇到的一般問題：https://weng-albert.medium.com/coredns%E7%B0%A1%E5%96%AE%E9%99%A4%E9%8C%AF-%E8%A7%A3%E6%B1%BA%E4%BD%A0%E9%81%87%E5%88%B0%E7%9A%84%E4%B8%80%E8%88%AC%E5%95%8F%E9%A1%8C-71d255e39548\nConnecting the Dots: Understanding How Pods Talk in Kubernetes Networks：https://medium.com/@seifeddinerajhi/connecting-the-dots-understanding-how-pods-talk-in-kubernetes-networks-992fa69fbbeb\nHow to run CoreDNS on Kubernetes Engine?：https://cloud.google.com/knowledge/kb/how-to-run-coredns-on-kubernetes-engine-000004698","應用程式-dns-查詢流程#應用程式 DNS 查詢流程":"\n應用程式 DNS 查詢流程 圖片：Connecting the Dots: Understanding How Pods Talk in Kubernetes Networks\n下面列出一下應用程式在 K8s 內的 DNS 查詢流程：\n當 Pod 執行 DNS 查詢時，會先查詢 Pod 裡面的 resolv.conf 檔案，再來查詢 Pod 所在的 node 上的 resolv.conf 檔案，這個檔案有設定 NodeLocal DNSCache 伺服器被設定為預設遞迴 DNS 解析器，充當快取 (在 GKE 需要另外開啟)。 如果此快取不包含所請求主機名稱的 IP 位址，則查詢將轉送至叢集 DNS 伺服器 (CoreDNS)，目前 GKE 是 KubeDNS。 此 DNS 伺服器透過查詢 Kubernetes 服務註冊表來決定 IP 位址。此註冊表包含服務名稱到對應 IP 位址的對應。這允許叢集 DNS 伺服器將正確的 IP 位址傳回給請求的 pod。 任何被查詢但不在 Kubernetes 服務註冊表中的網域都會轉送到上游 DNS 伺服器。 ","概論-coredns#概論 (CoreDNS)":"這邊就說明一下我們想要研究的 CoreDNS 吧，其實早就應該用 CoreDNS 來取代 KubeDNS，只是 GKE 不知道為什麼還不支援\nCoreDNS 是一個用 Go 語言寫的 DNS 伺服器，它是一個 CNCF 的專案，目前已經成為 K8s 的預設 DNS 伺服器。\n跟其他 DNS 比較不一樣的是他非常靈活彈性，並且所有功能都是透過 Plugins 來實現，這樣就可以根據需求來自訂自己的 DNS 伺服器。\n它的特點是：\nPlugins (插件化) Service Discovery (服務發現) Fast and Flexible (快速且靈活) Simplicity (簡單) 這邊來說一下 常見的 Plugins 如下：\nloadbalance：提供基於 DNS 的負載均衡功能 loop：檢測在 DNS 解析過程中出現的簡單循環問題 cache：提供前端快取功能 health：對 Endpoint 進行健康檢查 kubernetes：提供對 Kubernetes API 的訪問，並將其轉換為 DNS 記錄 etcd：提供對 etcd 的訪問，並將其轉換為 DNS 記錄 reload：定時自動重新加載 Corefile 配置的內容 forward：轉發域名查詢到上游 DNS 伺服器 proxy：轉發特定的域名查詢到多個其他 DNS 伺服器，同時提供到多個 DNS 服務器的負載均衡功能 prometheus：爲 Prometheus 系統提供採集性能指摽數據的 URL log：對 DNS 查詢進行日誌記錄 errors：對 DNS 查詢錯誤進行日誌記錄 CoreDNS 使用的 Plugins/架構 圖片：A Brief Look at CoreDNS\nCoreDNS 的 helm：https://github.com/coredns/helm/tree/master/charts/coredns","概論-k8s-dns#概論 (K8s DNS)":"K8s 會有 DNS 服務，主要是因為 K8s 連線是透過 Pod IP 來連線，但每當 Pod 重啟後 IP 會變動，所以就需要透過 Service 這個 Type 先建立持久性的名稱來讓其他 Pod 連線到後端服務上。\n當我們建立 Service 時，K8s DNS 就會自動產生一個對應的 A record 將 service dns name 與 IP 位址配對。之後，Pod 就可以透過 DNS 名稱來進行連線。\n而 DNS 負責動態更新 A record 與 Service IP 的異動。\n就如上面有提到，目前 K8s 的 DNS 已經從 KubeDNS 改成 CoreDNS，那兩者都有些相同的功能，如下：\nKubeDNS svc 可以建立一個或多個 Pod KubeDNS svc 監控 Kubernetes API 的 service 和 endpoint 事件，並根據需要變更其 DNS 項目。當透過建立、編輯或刪除操作修改這些 Kubernetes 服務及其相關 pod 時，這些事件會自動觸發。 Kubelet 會將 KubeDNS svc 的叢集 IP 指派給每個新 Pod 的 /etc/resolv.conf 名稱伺服器檔案內，以及適當的搜尋設定以允許更短的主機名稱，如下：。 可以看到 /etc/resolv.conf 與 KubeDNS 的 cluster IP ㄧ致\nK8s 服務的整個 DNS A record 如下：\nService DNS name： [service].[namespace].svc.cluster.local service、namespace 可以自行替換，這也是我們最常使用的方式。\n當然 Pod 也有自己的 DNS name，只是因爲 Pod 每次重啟 IP 會變動，所以我們通常不會使用 Pod DNS name：\nPod DNS name： [pod cluster IP].[namespace].pod.cluster.local pod cluster IP 這邊要將 IP 從 . 的分隔符號換成 -，例如：10-166-65-136.default.pod.cluster.local\n可以看到 curl 的 pod cluster IP 與 svc 的 endpoint ㄧ致\n所以，程式可以透過簡單且一致的主機名稱來存取 cluster 其他服務或是 Pod。\n也不需要使用完整的 hostname 來存取服務，因為在 resolv.conf 會設定好 domain 的 suffixes，如果同一個 namespace 底下服務溝通，只要設定以下即可：\n[service] 跨到其他 nampesace 就要改成：\n[service].[namespace] ","概論-kubedns#概論 (KubeDNS)":"這邊就來說一下目前 GKE 在使用的 KubeDNS 架構，主要由這三個 container 組成：\nKubeDNS 架構 圖片：DNS\n目前 GKE 1.29 KubeDNS\nKubeDNS：DNS 服務核心元件，主要由 KubeDNS 以及 SkyDNS 兩個元件組成： KubeDNS 負責監聽 Kubernetes API 的 service 和 endpoint 事件，並將相關訊息更新到 SkyDNS 中。 SkyDNS 負責 DNS 解析，監聽在 10053 Port (tcp/udp)，也同時監聽在 10055 Port 提供 metrics。 SkyDNS Metrics Port 以及監聽 10053 Port\ndnsmasq (有換名稱)：負責啟動 dnsmasq，並在變化時重啟 dnsmasq：\ndnsmasq 的 Upstream DNS Server 是 SkyDNS，cluster 內部的 DNS 查詢都會由 SkyDNS 負責。 sidecar：負責健康檢查和提供 DNS metrics 監聽在 10054 Port。\nsidecar 健康檢查以及監聽 10054 Port"},"title":"KubeDNS vs CoreDNS 比較"},"/blog/kubernetes/pod-curl-error-6-could-not-resolve-host/":{"data":{"":"","參考資料#參考資料":"k8s dns 故障 Pod 无法解析主机名 Couldn‘t resolve host：https://blog.csdn.net/hknaruto/article/details/109361342\nSetting up NodeLocal DNSCache/Scaling up kube-dns：https://cloud.google.com/kubernetes-engine/docs/how-to/nodelocal-dns-cache#scaling_up_kube-dns\nDNS on GKE: Everything you need to know：https://medium.com/google-cloud/dns-on-gke-everything-you-need-to-know-b961303f9153\nMonitoring kube-dns pods in GKE：https://medium.com/@prageeshag/monitoring-kube-dns-pods-in-gke-83b76a0ef3d9","問題發生#問題發生":"","解決辦法--後續規劃#解決辦法 \u0026amp; 後續規劃":"此篇主要是來記錄一下在正式環境服務出現 cURL error 6: Could not resolve host 的問題，以及如何解決。\n問題發生 當天 RD 在公窗反應說，API 程式去打其他單位或是 Google BigQuery 的 API 時，會出現 cURL error 6: Could not resolve host 的錯誤，但這個錯誤是偶發性錯誤，且出現錯誤的 Pod 也不是固定，也會出現在不同的 node 上。\n當下檢查服務是正常的，也沒有任何異常，因此我們馬上開了 Google Support Ticket 來詢問。\n解決辦法 \u0026 後續規劃 自行測試 在等待 Google Support 回覆同時，也先在網路上搜尋出現 cURL error 6: Could not resolve host 的相關錯誤，主要都是因為 DNS 導致，因此我們也先檢查了一下目前 DNS 的設定。\n參考了 Google 的 kube-dns 文件後 Setting up NodeLocal DNSCache/Scaling up kube-dns，我們嘗試調整 kube-dns-autoscaler configmap 的設定：\nkube-dns-autoscaler configmap\n並觀察一陣子後發現，出現 cURL error 6: Could not resolve host 的錯誤 Log 已經沒有再出現，因此我們認為是 DNS 的問題，並回報給 Google Support。\nKube-dns 的 Pod 是由 kube-dns-autoscaler 的 configmap 來控制 Pod 數量，如圖：\nkube-dns-autoscaler configmap\nkube-dns-autoscaler configmap 設定 我們來說明一下這個設定內容是什麼 (下面會先以 GCP 的 kube-dns 官網文件來補充說明，而不是上面圖片)：\nconfigmap kube-dns-autoscalerlinear: '{ \"coresPerReplica\": 256, \"includeUnschedulableNodes\": true, \"nodesPerReplica\": 16, \"min\": 1, \"max\": 5, \"preventSinglePointFailure\": true }' coresPerReplica：多少個 cores 啟動一個 kube-dns pod (預設 256，也就是每 256 core 才會啟動一個 kube-dns pod) nodesPerReplica：多少個 node 啟動一個 kube-dns pod (預設 16，也就是每 16 node 才會啟動一個 kube-dns pod) min：最小 kube-dns pod 數量 max：最大 kube-dns pod 數量 GCP kube-dns 官網文件建議：\n要對 nodesPerReplica 設定比較低的值，確保在叢集節點擴充時，kube-dns pod 數量也會跟著增加。 也強烈建議設定一個明確的 max 值，以確保 GKE control plane 不會因大量的 kube-dns pod 去使用 Kubernetes API 而過載。 kube-dns 計算方式 kube-dns 的 pod 數量計算方式如下：\nreplicas = max( ceil( cores × 1/coresPerReplica ) , ceil( nodes × 1/nodesPerReplica ), maxValue ) 情境一：假設我現在有 16 個 node，core 數量會是 50，那麼 kube-dns pod 數量就會是：\nreplicas = max( ceil(50 × 1/256) , ceil(16 × 1/16)) \u003e replicas = max( 1 , 1) \u003e replicas = 1 情境二：假設我現在有 32 個 node，core 數量會是 100，那麼 kube-dns pod 數量就會是：\nreplicas = max( ceil(100 × 1/256) , ceil(32 × 1/16)) \u003e replicas = max( 1 , 2) \u003e replicas = 2 情境三：假設我現在有 96 個 node，core 數量會是 300，有多設定 max 是 5，那麼 kube-dns pod 數量就會是：\nreplicas = max( ceil(300 × 1/256) , ceil(96 × 1/16)) \u003e replicas = max( 2 , 6) \u003e replicas = 6 但是我們有設定 max 是 5，所以最後 kube-dns pod 數量就會是 5，不會超過 5。\n為了確保叢集 DNS 可用性的水平，也建議幫 kube-dns 設定 mix 值。\n從上面計算方法也可以知道，當叢集具有多個核心的節點時，coresPerReplica 會佔主導地位。當叢集使用核心數較少的節點數，nodesPerReplica 佔主導地位。通常應該會以 nodesPerReplica 來設定。\n調整完 kube-dns-autoscaler 的 configmap 設定，不需要重啟服務，kube-dns 就會自動 scale up or scale down pod 數\n最後等待 Google Support 回覆，並告知已經調整 kube-dns-autoscaler configmap 的設定，並觀察一陣子後，問題已經解決。\nGoogle Support 回覆 Google Support 回覆：\n簡單解釋一下為什麼 scale up kube-dns pods 數量會有幫助：\n當 Pod 做 DNS 查詢時，因為你們有啟用 NodeLocal DNSCache，所以他會先找 cache，如果沒有 cache 就會回 kube-dns 查詢。\n我們在表象上先觀察到 NodeLocal DNSCache latency 很高，而這有兩個可能，一個當然可能是自身問題，但更大的可能性是當 cache 要回源跟 kube-dns 查找時，kube-dns 已經沒辦法再承接更多 request，所以 NodeLocal DNSCache 會一直嘗試 retry，我們在指標上也看到 latency 拉高並持平在一個水位顯示出某個瓶頸。\n在經過 kube-dns 增加 pod 操作之後，看起來後續 DNS 查詢可以順利被處理跟 cache，所以問題就大幅緩解。\nGoogle Support 建議 架構優化：未來可以評估用 CloudDNS 取代 kube-dns，CloudDNS 為託管服務，可以自動 scale up 來處理效能問題。參考文件 監控：如果短期暫時無法替換 CloudDNS，建議可在 kube-dns 安裝 agent 做監控，當接近效能瓶頸時，可以提前 scale up kube-dns pod。非官方參考文件 後續規劃 但由於我們公司最近也在考慮 multi-cloud 的方案，所以暫時不考慮用 GCP 雲供應商的 CloudDNS 取代 kube-dns，我們想研究看看另一套 coreDNS 是否能夠解決這個問題。\n有興趣可以參考下一篇文章：Kube DNS vs Core DNS。"},"title":"Pod 出現 cURL error 6: Could not resolve host"},"/blog/kubernetes/pod-veth-name-provided-eth0-already-exists/":{"data":{"":"此文章要來記錄一下公司同事在正式服務上遇到的問題，會詳細說明遇到事情的經過，以及開單詢問 google support 最後討論出的暫時解決的辦法：\n簡單列出正式站當下服務環境：\ngke master version：1.25.10-gke.2700 gke node version：1.25.8-gke.1000 該問題發生的 node pool 有設定 taint 發生問題的 Pod 是用 Statefulset 建立的服務 ","事情發生的經過#事情發生的經過":" RD 同仁反應，發現使用 Statefulset 建立的排程服務有問題，下 kubectl delete 指令想要刪除 Pod，讓 Pod 重新長，卻卡在 Terminating，等待一段時間後，決定下 kubectl delete --force --grace-period=0 來強制刪除 Pod，這時候狀態會卡在 ContainerCreating，使用 Describe 查看，會出現以下錯誤： Warning (combined from similar events): Failed to create pod sandbox: rpo error: code = Unknown desc = failed to setup network for sandbox \"14fe0cd3d688aed4ffed4c36ffab1a145230449881bcbe4cac6478a63412b0c*: plugin type=*gke\" failed (add): container veth name provided (etho) already exists 我們 SRE 協助查看後，也有嘗試去下 kubectl delete --force --grace-period=0 來刪除 Pod，但還是一樣卡在 ContainerCreating，最後是先開一個新的 Node 並讓該 Pod 建立到新的 Node 上，才解決問題。為了方便 google support 協助檢查出問題的 Node，先將 Node 設定成 cordon，避免其他 Pod 被調度到該問題 node 上。 Node 設定成 cordon\nNode 可以設定 cordon、drain 和 delete 三個指定都會使 Node 停止被調度，只是每個的操作暴力程度不同：\ncordon：影響最小，只會將 Node 標示為 SchedulingDisabled 不可調度狀態，但不會影響到已經在該 Node 上的 Pod，使用 kubectl cordon [node name] 來停止調度，使用 kubectl uncordon [node name] 來恢復調度。\ndrain：會先驅逐在 Node 上的 Pod，再將 Node 標示為 SchedulingDisabled 不可調度狀態，使用 kubectl drain [node name] --ignore-daemonsets --delete-local-data 來停止調度，使用 kubectl uncordon [node name] 來恢復調度。\ndelete：會先驅逐 Node 上的 Pod，再刪除 Node 節點，它是一種暴力刪除 Node 的作法，在驅逐 Pod 時，會強制 Kill 容器進程，沒辦法優雅的終止 Pod。\n我們隨後開單詢問 goolge support。 ","參考資料#參考資料":"Node 節點禁止調度（平滑維護）方式- cordon，drain，delete：https://www.cnblogs.com/kevingrace/p/14412254.html","與-google-support-討論內容#與 Google Support 討論內容":"Google Support 經過查詢後，回覆說：這個問題是因為 Pod 被強制刪除導致，強制刪除是一種危險的操作，不建議這樣處理，下面有詳細討論。\n一開始卡在 Terminating 狀態，我們也有請 RD 說明一下當下遇到的問題以及處理動作：RD 當時想要刪除 Pod 是因為該程式當下有 Bug，將 redis 與 db 連線給關閉，程式找不到就會一直 retry，導致相關進程無法結束，再加上 terminationGracePeriodSeconds 我們設定 14400，也就是 4 小時，才會卡在 Terminating 狀態。 (terminationGracePeriodSeconds 設定這麼久是希望如果有被 on call，工程師上來時，可以查看該 Pod 的錯誤原因)\n因為卡在 Terminating 太久，RD 有執行 kubectl delete --force，就是因為下了 --force 才造成相關資源問題 (例如 container proccess, sandbox, 以及網路資源)沒有刪乾淨。所以引起了此次的報錯 “container veth name provided (eth0) already exists”。 (因為我們服務使用 Statefulset，Pod 名稱相同，導致 eth0 這個網路資源名稱重複，所以造成錯誤，可以用 deployment 來改善這個問題，只是資源如果沒有清理乾淨會佔用 IP，所以單純調整成 deployment 也不是最佳解)\nGoogle 產品團隊建議，如果 Pod 處於 Running 狀態時，想要快速刪除 Pod 時，一開始就先使用 kubectl delete pod --grace-period=number[秒數] 來刪除，如果已經是 Terminating 狀態則無效。(SRE 同仁已測試過，與 Google Support 說明相同)\n那如果已經處於 Terminating 狀態，要怎麽讓 Pod 被順利刪除，這部分 Google Support 後續會在測試並給出建議，目前測試是：進去卡住的 Pod Container，手動刪除主進程 (pkill) 就可以了。\nGoogle Support 回覆"},"title":"部署 Pod 遇到 container veth name provided (eth0) already exists 錯誤"},"/blog/nginx/":{"data":{"":"此分類包含 Nginx 相關的文章。\nNginx Proxy Set Header 注意事項發布日期：2024-08-06 想使用 Nginx Upstream Proxy 到外部服務，並帶入對應的 header 該怎麼做？發布日期：2023-10-16 Soketi WebSocket Server LOG 不定時出現 502 error 以及 connect() failed (111: Connection refused)發布日期：2023-09-20 "},"title":"Nginx"},"/blog/nginx/nginx-upstream-set-host-header/":{"data":{"":"此文章要來記錄一下最近在公司服務入口遇到的一些小問題，以及解決的方法。簡單說明一下，我們的服務入口是用 Nginx 來當作 proxy server，將不同路徑或是 servername 導到對應的後端程式，或是外部的服務上(例如 AWS cloudfront.net)，本篇要測試的是如果使用要同時使用 upstream 到外部服務，且需要帶 host header 該怎麼做。\nNginx 的 upstream 是什麼？\n通常我們 proxy_pass 的寫法會是這樣：\nlocation /aaa { proxy_pass http://aaa.example.com; } 當 Nginx 收到的 request 是 /aaa 時，就會將 request 轉發到 http://aaa.example.com。\n但假如後端有多台機器或是服務，可以處理同一種 request，這時候就可以使用 upstream 來處理：\nupstream backend_hosts { server aaa.example.com; server bbb.example.com; server ccc.example.com; } location /aaa { proxy_pass http://backend_hosts; } 這樣子的好處是可以有多個機器或是後端服務可以分散請求，做到負載平衡的效果。","參考資料#參考資料":"Make nginx to pass hostname of the upstream when reverseproxying：https://serverfault.com/questions/598202/make-nginx-to-pass-hostname-of-the-upstream-when-reverseproxying","問題#問題":"那如果我們使用 Nginx upstream 時，還想要同時帶 host 的 header 到後端該怎麼做呢？我們先來看一下目前的寫法：\n( 測試範例是使用 docker 來模擬，可以參考程式碼 \u003e 點我前往 github，會有三個 nginx，其中一個是負責 proxy 的 nginx 名為 proxy，另外兩台是 upstream 後的服務，名為 upstream_server1、upstream_server2 )\nnginx-old.conf upstream upstream_server { server upstream_server1; server upstream_server2; } server { listen 80; server_name localhost; location /upstream_server/ { proxy_pass http://upstream_server; proxy_set_header Host \"upstream_server1\"; proxy_set_header Host \"upstream_server2\"; access_log /var/log/nginx/access.log upstream_log; } } } 可以看到我們希望 Nginx 收到 request 是 /upstream_server 時，將 request 轉發到 http://upstream_server，而 upstream_server 後面有兩個 server，並且在 proxy 時，帶入兩個不同的 host header。但如果真的這樣寫，可以達到我們想要得效果嗎？我們實際跑看看程式 (範例可以使用 nginx-old.conf)：\nnginx 原本寫法\n從上面的 LOG 可以發現，我們 call /upstream_server 時，後端的 upstream_server1、upstream_server2 收到的 host 只會收到第一個設定的 Host，且服務會出現 400 Bad Request，查了一下網路文章，發現出現 400 Bad Request，可能跟 header 送太多資訊過去，詳細可以參考 解決網站出現 400 Bad Request 狀態的方法。\n這邊推測應該是後端如果也是用 nginx 直接接收才會遇到 400 的問題，還好目前公司服務還是正常的 xDD，檢查一下後發現，其實後端根本沒有要求對應 header 才能接收(應該是對方忘記加上此限制)。","解決#解決":"好，不管是否需要對應 header，我們還是找看看有沒有辦法同時使用 upstream，並帶入對應 host 的方法呢？\n最後參考網路上的文章，似乎只能使用兩層的 proxy，才能完成這兩個需求，我們來看看要怎麼寫吧 (範例可以使用 nginx.conf)：\nnginx.conf server { listen 777; server_name localhost; location / { proxy_pass http://upstream_server1; proxy_set_header Host \"upstream_server1\"; access_log /var/log/nginx/access.log upstream_log; } } server { listen 888; server_name localhost; location / { proxy_pass http://upstream_server2; proxy_set_header Host \"upstream_server2\"; access_log /var/log/nginx/access.log upstream_log; } } upstream upstream_server { server 127.0.0.1:777; server 127.0.0.1:888; } server { listen 80; server_name localhost; location /upstream_server/ { proxy_pass http://upstream_server; access_log /var/log/nginx/access.log upstream_log; } } 可以看到上面的程式碼，我們透過兩層的 proxy，來達到我們想要的效果，這樣子就可以同時使用 upstream，並且帶入對應的 host header。\n首先在 28 ~ 36 行，我們一樣如果 Nginx 收到 request 是 /upstream_server 時，會 proxy 到 upstream_server 這個 upstream 中，而 upstream_server 有兩個 server，分別是 127.0.0.1:777、127.0.0.1:888，但實際上沒有這兩個 port，所以我們需要再寫一層一般的 proxy 設定，分別是 1 ~ 10 行、12 ~ 21 行，這樣子就可以達到我們想要的效果。\n但這個方法比較適用於 upstream 後端沒有太多個服務或是機器的情況，如果有很多個服務或是機器，就需要寫很多的 proxy，這樣子會變得很麻煩，所以如果有更好的方法，也歡迎留言跟我分享 🤣。\n最後我們來看一下實際執行的結果：\n使用多層的 nginx proxy 處理"},"title":"想使用 Nginx Upstream Proxy 到外部服務，並帶入對應的 header 該怎麼做？"},"/blog/nginx/proxy-set-header/":{"data":{"":"","參考資料#參考資料":"关于 proxy_set_header 的注意点：https://keepmoving.ren/nginx/about-proxy-set-header/\nModule ngx_http_proxy_module#proxy_set_header：https://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_set_header","注意事項#注意事項":"那假設我有很多個 path 都需要帶同樣的 header，例如：\nserver { listen 80; server_name localhost; location /api { proxy_pass http://api-svc; proxy_set_header Host $host; } location /blog { proxy_pass http://blog-svc; proxy_set_header Host $host; } location /ian { proxy_pass http://ian-svc; proxy_set_header Host $host; } } 那我們可以將 proxy_set_header 改寫到 server block 中，這樣就可以避免重複寫很多次。\nserver { listen 80; server_name localhost; proxy_set_header Host $host; location /api { proxy_pass http://api-svc; } location /blog { proxy_pass http://blog-svc; } location /ian { proxy_pass http://ian-svc; } } 官方文件說明：\nAllows redefining or appending fields to the request header passed to the proxied server. The value can contain text, variables, and their combinations.\n因爲 proxy_set_header 是一個全域設定，它可以寫在 http、server、location block 中，所以如果有多台 server 都需要帶同樣的 header，可以寫在 http block 中。\n官方文件介紹 proxy_set_header\n但是 (っ・Д・)っ 如果原本的 location 有設定 proxy_set_header，那麼在 server block 中的 proxy_set_header 是沒有辦法同時使用，後端只會收到 location 的 proxy_set_header，所以要注意這個地方。\nserver { listen 80; server_name localhost; proxy_set_header Host $host; location /api { proxy_pass http://api-svc; } location /blog { proxy_pass http://blog-svc; } location /ian { proxy_pass http://ian-svc; proxy_set_header Test helloworld; } } 上面這個例子，/api 跟 /blog 都會帶 Host header，但是 /ian 只會帶 Test header。\n官方文件說明：\nThese directives are inherited from the previous configuration level if and only if there are no proxy_set_header directives defined on the current level. By default, only two fields are redefined:\n只有當前層級沒有定義 proxy_set_header，才會繼承上一層級，所以如果當前層級有定義，就繼承不到上一層級的設定。","說明#說明":"在 Nginx 中，我們可以透過 proxy_set_header 來設定 header，這樣子可以讓我們在 proxy 的過程中，將一些 header 帶到後端的服務上，除了新增，也可以蓋掉前面原有的 header。\n通常都長得像：\nserver { listen 80; server_name localhost; location / { proxy_pass http://your_service; proxy_set_header Host $host; } } "},"title":"Nginx Proxy Set Header 注意事項"},"/blog/nginx/soketi-log-502-error/":{"data":{"":"此文章要來記錄一下 RD 同仁前陣子有反應使用 Soketi 這個 WebSocket Server 會不定時在 LOG 出現 502 error 錯誤訊息以及 connect() failed (111: Connection refused) while connecting to upstream，雖然說服務使用上不會影響很大，但還是希望我們可以協助找出 502 的原因。\n出錯的 LOG\n在開始找問題前，先簡單介紹一下 Soketi 是什麼東西好了，根據官網的說明，他是簡單、快速且有彈性的開源 WebSockets server，想要了解更多的可以到它官網去查看。\n另外會把程式碼相關放到 GitHub » 點我前往","參考資料#參考資料":"[Nginx] 解決 connect() failed (111: Connection refused) while connecting to upstream：https://wshs0713.github.io/posts/8c1276a7/\nWebSocket proxying：http://nginx.org/en/docs/http/websocket.html\nday 10 Pod(3)-生命週期, 容器探測：https://ithelp.ithome.com.tw/articles/10236314","壓測#壓測":"最後調整完，我們來測試看看是否在 Pod 自動重啟 or 更新 Deployment 的時候(並且有大量連線時)還會噴 502 error 或是 connect() failed (111: Connection refused)，我們這邊使用 k6 來做 websocket 服務的壓測，有簡單寫一個壓測程式如下：\nk6 壓測\nk6 是一個開源的壓測工具，可以用來測試 API、WebSocket、gRPC 等服務，可以到它的官網查看更多資訊。\nMacOS 安裝方式：brew install k6\nwebsocket.jsimport ws from \"k6/ws\"; import { check } from \"k6\"; export const options = { vus: 1000, duration: \"30s\", }; export default function () { const url = \"wss://socket.XXX.com/app/hex-ws?protocol=7\u0026client=js\u0026version=7.4.1\u0026flash=false\"; const res = ws.connect(url, function (socket) { socket.on(\"open\", () =\u003e console.log(\"connected\")); socket.on(\"message\", (data) =\u003e console.log(\"Message received: \", data)); socket.on(\"close\", () =\u003e console.log(\"disconnected\")); }); check(res, { \"status is 101\": (r) =\u003e r \u0026\u0026 r.status === 101 }); } 簡單說明一下上面程式在寫什麼，我們在 const 設定 vus 代表有 1000 個虛擬使用者，會在 duration 30s 內完成測試，下面的 default 就是測試連線 ws 以及 message 跟 close 等動作，最後需要回傳 101 (ws 交握)\n執行 k6 run websocket.js 後，就會開始壓測，可以看到會開始執行剛剛在上面提到 default 的動作：\nk6 壓測過程\n等到跑完，就會告訴你 1000 筆裡面有多少的 http 101，這邊顯示 status is 101，就代表都是 101，代表都有連線成功，沒有出現 502 error 或是 connect() failed (111: Connection refused) 的錯誤，這樣就代表我們的問題解決了。\nk6 壓測結果","解決過程#解決過程":"我們可以看到上方錯誤 LOG 中，發現有出現 502 error 以及 connect() failed (111: Connection refused) while connecting to upstream，這兩個錯誤都是由 Nginx 所產生的，那我們先來理解一下，Nginx 與 Soketi 之間的關係。\n在使用上，RD 的程式會打 Soketi 專用的 Subdomain 來使用這個 WebSocket 服務，而在我們的架構上，這個 Subdomain 會經過用 nginx proxy server，來轉發到 Soketi WebSocket Server (走 k8s svc)，設定檔如下圖：\n入口 nginx 設定\n然後會出現 connect() failed (111: Connection refused) while connecting to upstream 的錯誤訊息，代表我們的 Nginx 設定少了一個重要的一行設定，就是 proxy_http_version 1.1;，這個設定要讓 Nginx 作為 proxy 可以和 upstream 的後端服務也是用 keepalive，必須使用 http 1.1，但如果沒有設定預設是 1.0，也要記得設定 proxy_set_header Upgrade、proxy_set_header Connection。調整過後就變成：\nws.confserver { server_name socket.XXX.com; listen 80 ; listen [::]:80 ; listen 443 ssl; listen [::]:443 ssl; ssl_certificate /etc/nginx/ingress.gcp.cert; ssl_certificate_key /etc/nginx/ingress.gcp.key; access_log /var/log/nginx/access.log main; location / { proxy_pass http://soketi-ws-ci:6001; proxy_connect_timeout 10s; proxy_read_timeout 1800s; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \"Upgrade\"; proxy_set_header X-Real-IP $remote_addr; } } 解決完 connect() failed (111: Connection refused) 這個問題後，接下來就是要解決 502 error 這個問題，會導致 502 代表 Nginx 這個 proxy server 連不上後端的 Soketi WebSocket Server，再觀察 LOG 以及測試後發現，當 Pod 自動重啟，或是手動重啟 Deployment 的時候，就會有 502 的錯誤，代表 Nginx 在 proxy 到後面的 Soketi svc 再到 Pod 的時候，有一段時間是連不上的，所以就會出現 502 的錯誤，可以推測是流量進到正在關閉的 Pod 或是進到還沒有啟動好的 Pod 才導致的。\n那我們先來看一下 Soketi WebSocket Server 的服務 yaml 檔案：\ndeployment.yaml deployment.yaml spec: terminationGracePeriodSeconds: 30 securityContext: {} containers: - name: soketi securityContext: {} image: \"quay.io/soketi/soketi::1.6.0-16-alpine\" ... 省略 (可以到 github 看 code)... livenessProbe: failureThreshold: 3 httpGet: httpHeaders: - name: X-Kube-Healthcheck value: \"Yes\" path: / port: 6001 initialDelaySeconds: 5 periodSeconds: 2 successThreshold: 1 可以看到原來的設定只有 livenessProbe 而已，因此我們為了要避免流量進到正在關閉的 Pod 或是進到還沒有啟動好的 Pod，所以我們需要加上 readinessProbe 以及 preStop，讓 Pod 確定啟動完畢，或是等待 Service 的 endpoint list 中移除 Pod，才開始接收流量，這樣就可以避免出現 502 的錯誤。\ndeployment.yaml spec: terminationGracePeriodSeconds: 30 securityContext: {} containers: - name: soketi securityContext: {} image: \"quay.io/soketi/soketi::1.6.0-16-alpine\" ... 省略 (可以到 github 看 code)... livenessProbe: failureThreshold: 3 httpGet: httpHeaders: - name: X-Kube-Healthcheck value: \"Yes\" path: / port: 6001 initialDelaySeconds: 5 periodSeconds: 2 successThreshold: 1 readinessProbe: failureThreshold: 3 httpGet: httpHeaders: - name: X-Kube-Healthcheck value: \"Yes\" path: /ready port: 6001 initialDelaySeconds: 5 periodSeconds: 2 successThreshold: 1 lifecycle: preStop: exec: command: [\"/bin/sh\", \"-c\", \"sleep 20\"] Pod 終止的過程"},"title":"Soketi WebSocket Server LOG 不定時出現 502 error 以及 connect() failed (111: Connection refused)"},"/blog/nodejs/":{"data":{"":"此分類包含 Node.js 相關的文章。\n用 Node.js寫一個 Repository Restful API 的留言板 (express、sequelize 套件)發布日期：2022-04-11 Node.js 介紹發布日期：2022-03-30 "},"title":"Node.js"},"/blog/nodejs/nodejs-introduce/":{"data":{"":"","express-框架#Express 框架":"Node.js 在實作上不會單獨使用，通常會搭配框架去使用，像是 Express JS 後端框架，可以讓開發人員在寫同一個功能時，少寫很多程式的工具。\nExpress 是 Node.js 環境下提供的輕量後端架構，自由度極高，透過豐富的 HTTP 工具，能快速發開後端應用程式，它提供：\n替不同 HTTP Method、不同 URL 路徑的 requests 編寫不同的處理方法。 透過整合「畫面」的渲染引擎來達到插入資料到樣板產生 response。 設定常見的 web 應用程式，例如：連線用的 Port 和產生 response 樣板的位置。 在 request 的處理流程中增加而外的中間層 (Middleware) 進行處理。 第一個 Express Hello world 程式：\nconst express = require('express') const app = express() const port = 3000 app.get('/', (req, res) =\u003e { res.send('Hello World!') }) app.listen(port, () =\u003e { console.log(`Example app listening on port ${port}`) }) $ node app.js Example app listening on port 3000 使用 node 來啟動伺服器，並使用 Port 3000 來連線。應用程式指向 URL (/) 的路由，以 “Hello World!” 回應如果是其他路徑，res 就會回應 404 找不到。\n畫面 (view) 剛剛有提到說它可以使用整合「畫面」的渲染引擎來顯示到樣板，我們可以透過 --view 指令來產生樣板以及應用程式的目錄：\n$ express --view=pub create : public/ create : public/javascripts/ create : public/images/ create : public/stylesheets/ create : public/stylesheets/style.css create : routes/ create : routes/index.js create : routes/users.js create : views/ create : app.js create : package.json create : bin/ create : bin/www 路由 (router) 路由是判斷應用程式如何回應用戶端對特定端點的要求，而特定端點是一個 URL 或是路徑，與一個特定的 HTTP 要求方法 (GET、POST) 等，路由定義的結構如下：\napp.METHOD(PATH, HANDLER) 其中\napp 是 express 的實例。 METHOD 是 HTTP 要求的方法。 PATH 是伺服器上的路徑。 HANDLER 是當路由相符時要執行的函數。 以下範例簡單說明不同 HTTP 要求的方法：\n首頁中以 Hello World! 回應。\napp.get('/', function (req, res) { res.send('Hello World!'); }); 對根路由 (/)（應用程式的首頁）發出 POST 要求時的回應：\napp.post('/', function (req, res) { res.send('Got a POST request'); }); 對 /user 路由發出 PUT 要求時的回應：\napp.put('/user', function (req, res) { res.send('Got a PUT request at /user'); }); 對 /user 路由發出 DELETE 要求時的回應：\napp.delete('/user', function (req, res) { res.send('Got a DELETE request at /user'); }); ","npm#NPM":"NPM 是跟 Node.js 一起安裝的線上套件庫，可以下載各式各樣的 JavaScript 套件來使用，能解決 Node.js 代碼部署上的很多問題。\n安裝好後，可以使用 npm -v 來檢查版本：\n$ npm -v 8.5.0 跟 NPM 息息相關的是 package.json 這個檔案，他是掌管專案資訊的重要檔案，我們可以使用 init 指令來設定 package.json：\n$ npm init This utility will walk you through creating a package.json file. It only covers the most common items, and tries to guess sensible defaults. .... 省略 .... Press ^C at any time to quit. package name: (message) demo version: (1.0.0) description: entry point: (index.js) test command: git repository: keywords: author: ian \u003c880831ian@gmail.com\u003e license: (ISC) name: 就是該專案的名字，它預設就是該目錄名。 description: 專案描述。 entry point: 專案切入點，這有點複雜，之後再說。 test command: 專案測試指令，之後說。 git repository: 專案原始碼的版本控管位置。 keywoard: 專案關鍵字 author: 專案作者，以 author-name author@email.com 寫之。 license: 專案版權。 設定好後，專案資料夾就會多一個 package.json 的檔案，打開後可以看到，是我們剛剛所設定好的資訊：\n{ \"name\": \"demo\", \"version\": \"1.0.0\", \"description\": \"\", \"main\": \"index.js\", \"scripts\": { \"test\": \"echo \\\"Error: no test specified\\\" \u0026\u0026 exit 1\" }, \"author\": \"ian \u003c880831ian@gmail.com\u003e\", \"license\": \"ISC\" } 接下來，要如何下載網路上的模組，要使用 install 指令來下載，我們下載 Node.js 最小又靈活的 Web 應用程式框架 express 來做示範：\n$ npm install express added 50 packages, and audited 51 packages in 1s 2 packages are looking for funding run `npm fund` for details found 0 vulnerabilities 下載好後，可以看到剛剛 package.json 檔案多了 dependencies 欄位，它裡面會紀錄我們安裝了哪些套件，所以未來我們想知道專案使用了哪些套件，我們可以從 dependencies 這個欄位知道。\n{ \"name\": \"demo\", \"version\": \"1.0.0\", \"description\": \"\", \"main\": \"index.js\", \"scripts\": { \"test\": \"echo \\\"Error: no test specified\\\" \u0026\u0026 exit 1\" }, \"author\": \"ian \u003c880831ian@gmail.com\u003e\", \"license\": \"ISC\", \"dependencies\": { \"express\": \"^4.17.3\" } } 除此之外，還會多一個 node_modules 資料夾，這個資料夾就會存放我們所下載的套件\n$ ls node_modules/ accepts cookie-signature etag inherits mime-types qs setprototypeof array-flatten debug express ipaddr.js ms range-parser statuses body-parser depd finalhandler media-typer negotiator raw-body toidentifier bytes destroy forwarded merge-descriptors on-finished safe-buffer type-is content-disposition ee-first fresh methods parseurl safer-buffer unpipe content-type encodeurl http-errors mime path-to-regexp send utils-merge cookie escape-html iconv-lite mime-db proxy-addr serve-static vary 當我們都安裝好後，express 已經包在 node_modules 目錄內，在專案裡面，就可以使用 require(‘套件名稱‘) 來使用套件囉！\nvar express = require('express'); 因為這些套件都可以直接在網路上下載到，所以在推送 git 專案時，可以使用 .gitignore 來隱藏不想被 push 的檔案，當我們想要下載套件回來時，只要使用：\n$ npm install 就可以依照 package.json 裡面的 dependencies 來下載套件！","repl#REPL":"Node.js REPL (交互式解釋器)：表示一個電腦環境，類似 Windoes 系統的終端或是 Unix/Linux 的 Shell，我們可以在終端機上輸入命令，並接收系統的響應。\nNode 自帶了交互式解釋器，可以執行以下任務：\n讀取：讀取用戶輸入，解析輸入的 JavaScript 數據結構並儲存在內存中。 執行：執行輸入的數據結構。 顯示：輸出結果。 循環：循環以上任務直到用戶按下兩次的 ctrl+c 按鈕退出。 我們可以輸入以下命令來啟動 Node 的終端：\n$ node Welcome to Node.js v16.14.2. Type \".help\" for more information. \u003e 可以執行簡單的數學運算：\n$ node Welcome to Node.js v16.14.2. Type \".help\" for more information. \u003e 1+4 5 \u003e 5/2 2.5 \u003e 3*7 21 \u003e 4-3 1 \u003e 1 + (2*4) -5 4 \u003e 也可以將數據存在變數中，在需要時使用它。變數宣告需要使用 var 關鍵字，如果沒有使用關鍵字，會直接顯示出來。也可以使用 console.log() 來輸出變數\n$ node Welcome to Node.js v16.14.2. Type \".help\" for more information. \u003e x = 19 19 \u003e var y = 10 undefined \u003e x + y 29 \u003e console.log(\"Hello\") Hello undefined \u003e Node REPL 也支持輸入多行程式，我們試著寫一個 do-while 迴圈：\nWelcome to Node.js v16.14.2. Type \".help\" for more information. \u003e var x = 0 undefined \u003e do { ... x++; ... console.log(\"x:\" + x); ... } while (x\u003c5); x:1 x:2 x:3 x:4 x:5 undefined \u003e 也可以使用下底線(_)來獲得上一個程式的運算結果：\n$ node Welcome to Node.js v16.14.2. Type \".help\" for more information. \u003e var x = 10 undefined \u003e var y = 20 undefined \u003e x + y 30 \u003e var sum = _ undefined \u003e console.log(sum) 30 undefined \u003e ","什麼是-nodejs-#什麼是 Node.js ?":"Node.js 是能夠在伺服器端運行 JavaScript 的開放原始碼、跨平台執行環境。\nNode.js 的出現，讓前端網站開發人員可以使用 JaveScript 來做後端或是系統層面的工作。讓前端開發網站開發人員使用已懂的 JavaScript 語言就可以自行設定網站伺服器。\nNode.js 採用 Google 開發的 Chrome V8 JavaScript 引擎和 libuv 函式庫，可以用指令去執行 JavaScript，使用非阻塞輸入輸出、非同步、事件驅動等技術來提高效能，可最佳化應用程式的傳輸量和規模。這些技術通常用於資料密集的即時應用程式。\n主要的 Node.js 組件 (An Intro to Node.js That You May Have Missed)\n其他開源、低級組件，主要用 C/C++ 編寫：\nc-ares : 用於非同步 DNS 請求的 C 函式庫，用於 Node.js 中的一些 DNS 請求。 http-parser：一個輕量級的 HTTP 請求/回應解析器。 OpenSSL：一個著名的通用密碼庫。用於 tls 和 cryto 模組。 zlib：無損數據壓縮庫。 Node.js 能快速的原因是因為他對資源的調校不同，當程式收到一筆連線，\n相較於 PHP 的每次連線都會新生成一個執行緒，當連線數量暴增時很快就會消耗掉系統的資源，並且容易產生阻塞 (block)，\n而 Node.js 則是會通知作業系統透過 epoll、kqueue、/dev/poll、select 等將連線保留，並放入 heap 中配置，先讓連線進入休眠 (Sleep) 狀態，等系統通知才觸發連線的 callback。\n這種處理方式只會佔用記憶體，並不會使用到 CPU 資源。另外因為 JavaScript 語言的特性，每一個 request 都會有一個 callback，可以避免發生阻塞的狀況發生。\n以下會先從非阻塞輸入輸出、非同步，再談到事件驅動。\n非阻塞 (non-blocking) 輸入輸出(I/O) 是程式跟系統記憶體或網路的互動，例如發送 HTTP 請求、對資料庫CRUD 操作等等。\n以網站開發者角度來看，大部分的網站程式都不需要太多的 CPU 計算，反而是在等待大量的 I/O 處理完畢 (HTTP 請求、資料庫的取得資料或是更新資料等)，所以處理 I/O 的速度會是網頁程式效能的關鍵。\n那要怎麼才能讓等待 I/O 的時間，不要卡住後續的程式碼呢？可以讓程式一邊等 I/O 處理，一邊繼續執行其他部分的程式碼，主要有兩種方法：\n多執行緒 (multi-threaded)：使用阻塞 (blocking) I/O 的設計。 單執行緒 (single-threaded)：使用非阻塞 (non-blocking) I/O 的設計 + 非同步 (asynchronous) 處理。 阻塞就是 I/O 的處理阻擋了其他後續程式碼的執行。舉個例子：\n阻塞 (blocking) 非阻塞 (non-blocking) 阻塞後續程式碼的執行，就好像是我們去附近買烤肉，交給老闆後，為了想吃到熱騰騰的食物，所以只能留在原地，不能先去其他地方 非阻塞不會阻擋後續程式碼的執行，就好像是我們去百貨公司美食街點餐，點完餐後會拿到一個呼叫器，就可以先離開，等到呼叫器響了再回去拿即可 像是 Python、Ruby 等語言是使用多執行緒 (multi-threaded)，使用阻塞 I/O ：\n程式會等網路或是記憶體的作業結束後才會繼續往下，等待時間這個作業中的執行緒不會去做其他事情。\n如果想要達到『等待 I/O 期間不要卡住其他程式碼』，的做法就是新開一個執行緒，直到任務完成，再告訴主執行緒說( 我完成囉！) 即可。\nNode.js 使用單執行緒 (single-threaded) ，非阻塞 I/O + 非同步函式：\nNode.js 使用非阻塞設計，那要怎麼去操作資料庫或是 HTTP 請求的輸入輸出呢？就要透過非同步 (asynchronous) 來處理囉！\n非同步 (asynchronous) 非同步也可以稱為異步，它的作用就是讓程式不要被阻擋等 I/O 處理完，才可以跑下一行程式碼，是直到函式中的 callback 被呼叫的時候再執行後續要做的事情。\n同步 vs. 非同步\n非同步 (asynchronous) 可以想像一下你去咖啡廳買拿鐵跟黑咖啡，可能會發生的情況是：\n你點了拿鐵跟黑咖啡 店員在收銀機上輸入點餐內容 店員請同事 A 準備拿鐵、請同事 B 準備黑咖啡，並告知做完後，要提醒店員 黑咖啡製作會比較快，B 同事會先完成，而剛好店員剛幫你結帳完沒事，所以把黑咖啡拿給你 拿鐵製作包含較多步驟，花費時間較久，等 A 同事完成後，店員剛好沒事，所以把拿鐵拿給你 可以看到，櫃檯店員一次只能做一件事情，但為了節省時間，店員將工作分配給其他同事，在下完指令後，店員會繼續幫你結帳，等同事們各自完成後會告知店員，店員在依序把飲料交給你 - 最終等待時間減少，也不會浪費閒置的資源，這就是現實生活中非同步的情況。\n同步 (synchronous) 一樣我們點了拿鐵跟黑咖啡，換成同步的話：\n店員在收銀機上輸入點餐內容 店員請同事 A 開始準備拿鐵 A 同事準備完拿鐵，店員轉交給你 店員請同事 B 開始準備黑咖啡 B 同事準備完黑咖啡，店員交給你 店員幫你刷載具、打統編、找錢等等 同樣的餐點內容，如果是同步處理，代表要等每一件事情做完，才可以做下一步。也就是說同事 A 完成拿鐵後，店員才請 B 同事準備黑咖啡。相對於非同步來說，會花費不少時間以及浪費不少閒置資源。\n從咖啡店的例子中可以發現：\n櫃檯店員手上一次能做的事情只有一件 (Single thread 單執行緒)，只是在非同步的例子中，將製作咖啡的事情委派給其他同事處理，讓自己可以繼續幫你結帳，來提高效率 — JavaScript 是 Single thread，一次只能做一件事情。 在非同步的例子中，櫃檯店員將製作咖啡的事情委派出去，其實店員也不知道哪一個任務會先被完成，但店員還是可以繼續完成結帳的任務，不會因為同事 A、B 還在製作咖啡，就不能接下去動作 (non-blocking) — JavaScript 一次只能做一件事，但藉由 Node 提供的 API 協助，在背後處理這些事件 (同事在背後製作咖啡)，可以等待製作同時，不會被阻塞 (blocking) 到下一件事情的執行。 非同步的例子中，店員委派事情的流程很簡單：就是請同事完成製作咖啡 (event) + 在收到同事通知完成後接手咖啡，並轉交給你 (callback function) — 若是採用非同步處理，會有 callback function 來指定事件完成後要接續做什麼：它不會立即被執行，而是等待委託的事情被完成後才觸發。 當同事 A、B 分別通知完成後，就會依序把咖啡放在店員的旁邊排成一排 (event queue)。想向店員有一個小助手 (event loop) ，他的工作內容是確認店員結帳完了沒有：如果結帳完了，就會把隊伍中第一杯咖啡叫給店員，讓店員交給你 (觸發 callback function) ; 如果店員還在結帳，就會讓隊伍中的咖啡擺在旁邊繼續等待。 JaveScript 實現非同步的方法不斷演進著：從 callback、promise 到最新的 async-await 函式。\nCallback 什麼是 Callback 假設有 A、B、C 三件工作，其中 B 必須等待 C 做完才能執行。大部份的人幾乎都是做 A，再做 C，等待 C 做完以後最後做 B。但對於可多工的人來說，卻可能是同時做 A 與 C（多工），等待 C 完成後做 B。\nCallback function 是一個被作為參數帶入另一個函式中的「函式」，這個被作為參數帶入的函式將在「未來某個時間點」被呼叫和執行 — 這是處理非同步事件的一種方式。\n再次舉 A、B、C 三件工作的例子，其中 B 必須等待 C 做完才能執行，於是我們將 B 放到 C 的 callback 中，讓宿主環境在收到 C 完成的回應時後 B 放到佇列中準備執行。\ndoA(); doC(function() { doB(); }); 常見的例子：\n使用瀏覽器所提供的 setTimeout()或是 setInterval() setTimeout(() =\u003e { console.log('這個訊息將在三秒後被印出來') }, 3000) 提供一個匿名函式作為參數帶入 setTimeout() 函式中，目的就是請 setTimeout() 在未來某個時間點（三秒後）呼叫和執行這個匿名函式。\nDOM 的事件監聽 const btn = document.querySelector('button') btn.addEventListener('click', callbackFunctionName) callbackFunctionName 做為參數被帶入 addEventListener() 中，callbackFunctionName 不會立即被執行，而是未來按鈕被點擊時才會執行。\nCallback 主要有一個缺點：回呼地獄\n回呼地獄 (Callback Hell) 回呼地獄 (Callback Hell) 又稱為「毀滅金字塔」，指的是層次太深的巢狀 Callback，讓程式變的更複雜且難以預測或是追蹤。\n向遠端伺服器發出請求並獲得資訊後，執行 Callback，再發出請求，獲得資訊後執行 Callback，再發出請求，獲得資訊後執行 Callback，就會不小心一層包一層，變成所謂的 Callback Hell。\ndoA(function() { doB(); doC(function() { doD(); }); doE(); }); doF(); 缺點：\n可讀性低：如果程式碼出錯，要回頭慢慢找錯誤的地方 可維護性低：如果要修改其中一組函式，牽一髮而動全身 那在同步執行情況下，可以使用 try...catch 來捕捉錯誤訊息，但如果是在非同步情況下，要怎麼處理錯誤或是例外訊息呢！？ 主要有兩種方式：\n分別回呼 (Split Callback) 分別的回呼要設定兩個 Callback，一個用於成功通知，另一個則用於錯誤通知。如下，第一個參數是用於成功的 Callback，第二個參數是用於失敗的 Callback：\nfunction success(data) { console.log(data); } function failure(err) { console.error(error); } ajax('http://sample.url', success, failure); 那如果在 Callback 中發生錯誤，要怎麼辦呢!?\nfunction success(data) { console.log(x); } function failure(err) { console.error(error); } ajax('http://sample.url', success, failure); // Uncaught (in promise) ReferenceError: x is not defined 會直接報錯，並不會進入到 failure 這個 Callback 裡面，也就是說，如果是在 Callback 內發生錯誤，是不會被捕捉到的。\n錯誤優先處理 (Error-First Style) Node.js 的 API 常見這樣的設計方式，第一個參數是 error ，第二個參數是回應的資料 (data)。檢查 error 是否有值或是 true，否則就接續處理 data。\nfunction response(err, data) { if(err) { console.error(err); } else { console.log(data); } } ajax('http://sample.url', response); 接下來我們來看 Promise，可以解決 callback 可讀性低的 Callback Hell 問題。\nPromise Promise：Callback 以外的另一種方式來處理非同步事件，且可讀性與可維護性比 Callback 好很多。 Promise 是一個物件，代表著一個尚未完成，但最終會完成的一個動作 - 在一個非同步處理流程中，它只是一個暫存的值。\n我們一樣來說剛剛咖啡店的例子： 當我們點完拿鐵跟黑咖啡後，店員會給你一張印有號碼的收據，然後告訴你等等聽到號碼，就可以來領咖啡了，而這張收據就是 Promise，代表這個任務完成後，就可以接著執行接下來的動作了。\n在等待過程中，其實無法百分百確定最後一定會拿到咖啡 (Promise) ; 店員可能順利做完咖啡交到你手上 (Resolved) ; 可能牛奶或是咖啡豆沒了，所以店員告訴你今天做不出來咖啡 (Rejected)。\nPromise 就像上面的例子中，會處在三個任意階段中：\nPedning：等待事情完成中，但不確定最終會順利完成或失敗 Resolved（或稱 Fulfilled）：代表順利完成了，並轉交結果 Rejectesd：代表失敗了，並告知失敗原因 const getData = new Promise((resolve, reject) =\u003e { // 製作咖啡..... // 作業完成，並回傳錯誤訊息時 if (error) { return reject('牛奶或是咖啡豆沒了') } // 作業成功完成 resolve({ data: '咖啡交到你手上', }) }) resolve 函式：當非同步作業成功完成時，將結果做為參數帶入執行。 reject 函式：當非同步作業失敗時，將錯誤訊息作為參數帶入執行。 創建出來的 Promise 物件在實例上有兩個重要的方法：\n.then() 方法 當成功從 resolve() 獲得結果時 - 狀態會由 Pending 轉為 Resolved - then() 方法就會被調用。\ngetData // 使用 then 方法，並將成功訊息印出來 .then(data =\u003e {console.log('成功資料', data)}) 以剛剛咖啡店的例子來看， then() 方法就像你會去聽咖啡人員是否叫號，並確認咖啡是否成功準備好了，就領取咖啡來喝。\n.catch() 方法 當從 reject() 獲得錯誤訊息時 - 狀態由 Pending 轉為 Rejected - catch() 方法就會被調用來處理錯誤。\ngetData // 使用 then 方法，並將成功訊息印出來 .then(data =\u003e {console.log('成功資料', data)}) // 使用 catch 方法，並將錯誤訊息印出來 .catch(error =\u003e console.log('錯誤訊息', error)) 一樣以剛剛咖啡店的例子來看， catch() 方法就像你會去聽咖啡店店員是否有告知咖啡做不出來的訊息，如果有，你就會去櫃檯退錢或是改其他產品代替。\n串接 then() 方法 還記得上面我們示範的 callback hell ，因為我們依序要向不同的伺服器或資料庫取得資料，也要依序處理多個非同步的作業，所以會有一層一層的 callback 去達成。\n從上面 then() 和 catch() 方法例子中可以發現，非同步作業執行完成後的處理步驟被獨立開來，這樣程式碼的可讀性就會高很多 - 在依序處理多個非同步作業時會更為明顯，我們可以看到下面我們用 then() 方法做串接：\ngetData // 使用 then 方法，並將成功訊息印出來 .then(data =\u003e { console.log(data.data1) // abc return data.data1 + 'def' }) // 獲得前一個 then() 回傳的結果 .then(data =\u003e { console.log(data) // abcdef return data + 'ghi' }) // 獲得前一個 then() 回傳的結果 .then(data =\u003e { console.log(data) // abcdefghi }) // 使用 catch 方法，並將錯誤訊息印出來 .catch(error =\u003e console.log('錯誤訊息', error)) Promise.all([….]) 方法 當在處理「多個非同步事件」時，Promise.all() 方法會等所有 Promise 都被順利完成 (Resolved) 後，才會執行接下去的動作 ; 一旦收到某一個 Promise 回傳錯誤 (Rejected)，就會立即執行後續的錯誤處理流程。\n我們以剛剛咖啡店的例子來說，我們點完咖啡後，又因為嘴饞，多買了一個麵包來吃，這時我們手上就有兩個領餐號碼，那 Promise.all([...]) 就是當你等到兩個號碼都被叫到後，才會去領餐。\n那 Promise.all([....]) 的好處什麼呢!? 它可以縮短等待時間：\nconst oneSecond = new Promise((resolve, reject) =\u003e { setTimeout(() =\u003e { // 一秒後回傳資料 resolve('one second') }, 1000); }) const twoSecond = new Promise((resolve, reject) =\u003e { setTimeout(() =\u003e { // 兩秒後回傳資料 resolve('two second') }, 2000); }) const threeSecond = new Promise((resolve, reject) =\u003e { setTimeout(() =\u003e { // 三秒後回傳資料 resolve('three second') }, 3000); }) // 等到三個 Promise 都成功回傳後，才執行接下去的流程 Promise.all([oneSecond, twoSecond, threeSecond]) .then(([oneSecond, twoSecond, threeSecond]) =\u003e { console.log(oneSecond, twoSecond, threeSecond) }) 我們模擬一下我們要分別向三個資料庫請求資料 (我們以 setTimeout() 示意非同步處理事件)，可以看到我們要先等第一個資料庫成功回傳資料後，才展開第二次請求，等到成功收到回傳後才開始第三次請求。而使用 Promise.all() 方法，我們能先依序向資料庫發出請求，並在成功獲得全部回傳資料後，才會做後續的動作。\nAsync/Await 那我們剛剛透過 Promise 包裝和使用，的確避免了 callback hell 讓整個流程變得很清楚，提升了程式碼的易讀性與可維護性。\nAsync/Await 是所謂的語法糖衣，是一種新的語法撰寫方式，來處理「非同步事件」，讓非同步的程式碼讀起來更像在寫「同步程式碼」。\nAsync/Await 是用來簡單化和清楚化 Promise 串連 then 這種相對複雜的結構，他回傳的一樣也是 Promise 物件，只是針對 promise-based 寫法進行包裝。\nAsync 關鍵字\nasync 關鍵字可以放在任意函式前面，它代表「我們正宣告一個非同步的函式，且這個函式會回傳一個 Promise 物件」：\n// 一般函式 function getGroupInfo() { return data } // 使用 async 函式 async function getGroupInfo() { return data } Await 關鍵字\n在 async 函式中使用 await 關鍵字，代表「我們請 JavaScript 等待這個非同步的作業完成後，才展開後續的動作」，換句話說：await 讓 async 函式的執行動作暫停，等到它獲得回傳的 Promise 物件後 - 無論執行成功 (resolve) 或是失敗 (rejected) -才會恢復執行 async 函式。\n優點：可以直接使用 await 後獲得的回傳值存於一個變數中做後續使用，而不是在呼叫 then() 方法一個一個串，讓程式碼看起來像是在處裡一般的同步程式碼，提升了易讀性與可維護性。 Async/Await 範例 我們在後續的範例，一樣使用 setTimeout() 來模擬資料庫請求和等待資料的非同步：\nfunction getFirstInfo() { return new Promise((resolve, reject) =\u003e { setTimeout(() =\u003e { resolve('first data') }, 1000); }) } function getSecondInfo() { return new Promise((resolve, reject) =\u003e { setTimeout(() =\u003e { resolve('second data') }, 2000); }) } async function getGroupInfo() { // 代表等到第一筆資料回傳後，才印出結果和請求第二筆資料 const firstInfo = await getFirstInfo() console.log(firstInfo) // 代表等到第二筆資料回傳後，才印出結果 const secondInfo = await getSecondInfo() console.log(secondInfo) } getGroupInfo() 在 getGroupInfo() 函式前加上 async 關鍵字，來告知值這是一個非同步函示， getFirstInfo() 跟 getSecondInfo() 是兩個要處理非同步的地方，因此分別在兩個呼叫函式前加上 await 關鍵字。\nawait 搭配 Promise.all([…]) 方法 function getFirstInfo() { return new Promise((resolve, reject) =\u003e { setTimeout(() =\u003e { // 1秒後回傳結果 resolve('first data') }, 1000); }) } function getSecondInfo() { return new Promise((resolve, reject) =\u003e { setTimeout(() =\u003e { // 2秒後回傳結果 resolve('second data') }, 2000); }) } async function getGroupInfo() { try { const firstInfo = getFirstInfo() const secondInfo = getSecondInfo() // 同步發出非同步請求，並等到兩秒後（非三秒）成功獲得兩筆回傳後，才印出結果 const [firstData, secondData] = await Promise.all([firstInfo, secondInfo]) console.log(firstData, secondData) } catch (error) { // 處理錯誤回傳 console.log(error) } } getGroupInfo() 透過 await 關鍵字搭配 Promise.all() 方法，同步向兩個資料庫獲取資料，並等到兩筆資料都成功回傳後，才印出結果 - 省時效率提高！\n我們可以看到下面這張圖，他是 Node.js 運行時的縮圖，一旦你的 Node.js 應用程序啟動，它會先開始一個初始化階段，運行啟動腳本，包括請求模組和註冊事件 callback。然後應用程序進入事件循環（也稱為主線程、事件線程等），從概念上講，它是為通過執行適當的 JS callback 來響應傳入的客戶端請求而構建的。\nNode.js 運行時的圖示 (An Intro to Node.js That You May Have Missed)\nlibuv 是非同步處理的函式庫（以 C 語言為主)，在面對非同步作業時，會開啟執行緒池 (thread pool / worker pool) ，預設共有四個執行緒 (也就是 worker threads)，運算這些 I/O 用的方式是事件迴圈 (event loop)。\n所以就算 V8 處理程式碼是單執行緒，一但進入到 libuv 手上，它還是會幫你把會阻塞的 I/O 分工到執行緒池中交給不同的執行緒去處理，直到 callback 發生才會丟回應讓程式知道。\n那我們來看看 libuv 如何透過事件迴圈有效的處理非同步 I/O。\n事件驅動 (event-driven) 在講事件驅動前，我們先來了解一下什麼是事件：\n事件 (event) 事件是指用戶或是系統作出的動作，例如使用者點選按鈕，或是檔案讀取完成、某種錯誤產生等等，都叫做事件。\n事件驅動 (event-driven) 事件驅動是一種程式執行模型，表示程式的進行是依據事件的發生而定，監聽到事件就處理、處理完就執行 callback ，透過不斷的監聽跟回應事件執行程式。\n而事件驅動在不同的地方有不同的實現。瀏覽器 (前端) 和 Node.js (後端) 基於不同的技術實現了各自的事件迴圈。就 Ndoe.js 來說，事件就是交給 libuv 去處理 ; 至於瀏覽器的事件迴圈在 HTML 5 的規範中有定義。\n事件迴圈 (event loop) 因為 Node.js 只有一個執行緒，所以當 libuv 把非同步事件處理完後，callback 要被丟回應用程式中排隊，等待主執行緒的 stack 為空的時候，才會開始執行。這個排隊的地方就是事件佇列 (event queue)。\nlibuv 會不斷檢查有沒有 callback 需要被執行，有的話分配到主執行緒結束手邊的程式後處理，因此這個過程稱為 『事件迴圈』。\n事件迴圈(event loop) (Node.js 101 — 單執行緒、非同步、非阻塞 I/O 與事件迴圈)\nlibuv 事件迴圈有哪些階段呢？\nlibuv 事件迴圈 (nexocode)\nlibuv 的事件迴圈共有六個階段，每個階段的作用如下：\nTimers：等計時器 (setTimeout、setInterval) 的時間一到，會把他們的 callback 放到這裡等待執行。\nPending callbacks：會把作業系統層級的錯誤給callback (TCP errors、sockets 連線被拒絕)。\nIdle, prepare：內部使用。\nPoll：最重要的一個階段。\n如果 Queue 不為空，依次取出 callback 函數執行，直到 Queue 為空或是抵達系統最大限制。 如果 Queue 為空但有設置 「setImmediate」，就進入 check 階段。 如果 Queue 為空但沒有設置 「setImmediate」，就會在 Poll 階段等到直到 Queue 有東西或是 Timers 時間抵達。 Check：處理 setImmediate 的 callback。\nClose callbacks：執行 close 事件的 callback，利如 socket.destroy()。\n事件迴圈就是不斷重複以上階段。每個階段都有自己的 callback 佇列，在進入某個階段時，都會從所屬的佇列中取出 callback 來執行，當佇列為空或者被執行 callback 的數量達到系統的最大數量時，就會進入下一階段。\n根據以上提到事件驅動、單執行緒和非同步、非阻塞的 I/O 處理特性，Node.js 很適合拿來開發 I/O 密集型應用程式，如影音串流、即時互動、在線聊天、遊戲、協作工具、股票行情等軟體。","參考資料#參考資料":"Node.js 官網：https://nodejs.dev/learn/introduction-to-nodejs\nNode.js 101 — 單執行緒、非同步、非阻塞 I/O 與事件迴圈：https://medium.com/wenchin-rolls-around/node-js-101-%E5%96%AE%E5%9F%B7%E8%A1%8C%E7%B7%92-%E9%9D%9E%E5%90%8C%E6%AD%A5-%E9%9D%9E%E9%98%BB%E5%A1%9E-i-o-%E8%88%87%E4%BA%8B%E4%BB%B6%E8%BF%B4%E5%9C%88-ef94f8359eee\nAn Intro to Node.js That You May Have Missed：https://itnext.io/an-intro-to-node-js-that-you-may-have-missed-b175ef4277f7\nSequelize：https://sequelize.org/v6/index.html\n透過 sequelize 來達成 DB Schema Migration：https://hackmd.io/@TSMI_E7ORNeP8YBbWm-lFA/ryCtaVW_M?print-pdf#%E4%BD%BF%E7%94%A8sequelize%E5%BB%BA%E7%AB%8B%E4%B8%80%E5%BC%B5user-table\n認識同步與非同步 — Callback + Promise + Async/Await：https://medium.com/%E9%BA%A5%E5%85%8B%E7%9A%84%E5%8D%8A%E8%B7%AF%E5%87%BA%E5%AE%B6%E7%AD%86%E8%A8%98/%E5%BF%83%E5%BE%97-%E8%AA%8D%E8%AD%98%E5%90%8C%E6%AD%A5%E8%88%87%E9%9D%9E%E5%90%8C%E6%AD%A5-callback-promise-async-await-640ea491ea64\n你懂 JavaScript 嗎？#23 Callback：https://ithelp.ithome.com.tw/articles/10206555\nHow to create JOIN queries with Sequelize：https://sebhastian.com/sequelize-join/"},"title":"Node.js 介紹"},"/blog/nodejs/nodejs-restful-api-repository-messageboard/":{"data":{"":"本文章是使用 Node.js 來寫一個 Repository Restful API 的留言板，並且會使用 express 以及 sequelize (使用 Mysql)套件。\n建議可以先觀看 Node.js 介紹 文章來簡單學習 Node 語言。\n範例程式連結 點我 😘 版本資訊\nmacOS：11.6 node：v16.14.2 npm：8.5.0 Mysql：mysql Ver 8.0.28 for macos11.6 on x86_64 (Homebrew) ","postman-測試#Postman 測試":"註冊 - 成功 註冊 成功\n註冊 - 失敗(無輸入) 註冊 失敗(無輸入)\n註冊 - 失敗(已經註冊過) 註冊 失敗(已經註冊過)\n登入 - 成功 登入 成功\n登入 - 失敗 登入 失敗\n查詢全部留言 - 成功(無資料) 查詢留言 成功(無資料)\n查詢全部留言 - 成功(有資料) 查詢留言 成功(有資料)\n查詢{id}留言 - 成功 查詢{id}留言 成功\n查詢{id}留言 - 失敗 查詢{id}留言 失敗\n新增留言 - 成功 新增留言 成功\n新增留言 - 失敗 新增留言 失敗\n修改{id}留言 - 成功 修改 {id}留言 成功\n修改{id}留言 - 失敗 修改 {id}留言 失敗\n刪除{id}留言 - 成功 刪除 {id}留言 成功\n刪除{id}留言 - 失敗 刪除 {id}留言 失敗\n新增回覆 - 成功 新增回覆 成功\n新增回覆 - 失敗 新增回覆 失敗\n修改{id}回覆 - 成功 修改 {id}回覆 成功\n修改{id}回覆 - 失敗 修改 {id}回覆 失敗\n刪除{id}回覆 - 成功 刪除 {id}回覆 成功\n刪除{id}回覆 - 失敗 刪除 {id}回覆 失敗","參考資料#參考資料":"Node.js 官網：https://nodejs.dev/learn/introduction-to-nodejs\nSequelize：https://sequelize.org/v6/index.html\n透過 sequelize 來達成 DB Schema Migration：https://hackmd.io/@TSMI_E7ORNeP8YBbWm-lFA/ryCtaVW_M?print-pdf#%E4%BD%BF%E7%94%A8sequelize%E5%BB%BA%E7%AB%8B%E4%B8%80%E5%BC%B5user-table\nHow to create JOIN queries with Sequelize：https://sebhastian.com/sequelize-join/","實作#實作":"檔案結構 . ├── app.js ├── config │ └── config.json ├── controllers │ ├── auth.js │ ├── message.js │ └── reply.js ├── middleware │ └── index.js ├── migrations │ ├── 20220331054531-create-user.js │ ├── 20220401093019-create-message.js │ └── 20220404041905-create-reply.js ├── models │ ├── index.js │ ├── message.js │ ├── reply.js │ └── user.js ├── node_modules(以下檔案省略) ├── package-lock.json ├── package.json ├── repositories │ ├── auth.js │ ├── message.js │ └── reply.js └── router └── index.js 我們來說明一下上面的資料夾以及檔案各別功能與作用\napp.js：程式的啟動入口，裡面會放置有關程式系統需要呼叫哪些套件等等。 config：放置資料庫連線資料 (使用 sequelize-cli 套件自動產生)。 controllers：商用邏輯控制。 middleware：用來檢查登入權限。 migrations：放置產生不同 Model 資料表 (使用 sequelize-cli 套件自動產生)。 models：定義資料表資料型態 (使用 sequelize-cli 套件自動產生)。 node_modules：放置下載使用的套件位置。 package.json/package-lock.json：專案資訊的重要檔案 (使用 npm init 自動產生)。 repositories：處理與資料庫進行交握。 router：設定網站網址路由。 以下詳細說明部分，只會說明 app.js、config、controllers、middleware、migrations、models、repositories、router (介紹會依照程式流程來介紹)。\nconfig { \"development\": { \"username\": \"root\", \"password\": \"\", \"database\": \"node\", \"host\": \"127.0.0.1\", \"dialect\": \"mysql\", \"dialectOptions\": { \"dateStrings\": true, \"typeCast\": true }, \"timezone\": \"+08:00\" }, \"test\": { \"username\": \"root\", \"password\": \"\", \"database\": \"node\", \"host\": \"127.0.0.1\", \"dialect\": \"mysql\", \"dialectOptions\": { \"dateStrings\": true, \"typeCast\": true }, \"timezone\": \"+08:00\" }, \"production\": { \"username\": \"root\", \"password\": \"\", \"database\": \"node\", \"host\": \"127.0.0.1\", \"dialect\": \"mysql\", \"dialectOptions\": { \"dateStrings\": true, \"typeCast\": true }, \"timezone\": \"+08:00\" } } 由 sequelize-cli 套件自動產生，可以依照不同程式狀態(開發 development、測試 test、上線 production)來修改連線時的參數。\nmigrations 檔案基本上都雷同，舉其中一個為說明。migrations 也是由 sequelize-cli 套件自動產生，相關指令會統一放在最後。\n20220401093019-create-message.js \"use strict\"; module.exports = { async up(queryInterface, Sequelize) { await queryInterface.createTable(\"message\", { id: { allowNull: false, autoIncrement: true, primaryKey: true, type: Sequelize.INTEGER, }, user_id: { allowNull: false, type: Sequelize.INTEGER, references: { model: \"user\", key: \"id\", }, }, content: { allowNull: false, type: Sequelize.STRING, }, version: { type: Sequelize.INTEGER, defaultValue: 0, }, createdAt: { allowNull: false, type: Sequelize.DATE, }, updatedAt: { allowNull: true, type: Sequelize.DATE, }, deletedAt: { allowNull: true, type: Sequelize.DATE, }, }); }, async down(queryInterface, Sequelize) { await queryInterface.dropTable(\"message\"); }, }; 這個檔案的格式也是透過指令產生，會有 up 跟 down，up 就是執行指令後會產生什麼，主要都會是新增資料表或是修改資料表，down 則是回復的功能，可以將資料表給 dropTable 。我們主要會修改的地方是 createTable 內的資料，裡面的資料代表資料表的欄位，以下列出常用的格式：\nallowNull：是否為空值 autoIncrement：自動累加 primaryKey：主鍵 type：裡面就放欄位類型，例如 INTEGER、STRING、DATE等等 app.js \"use strict\"; const express = require(\"express\"); const sessions = require(\"express-session\"); const app = express(); const port = 8888; // 設定 Session const oneDay = 1000 * 60 * 60 * 1; app.use( sessions({ secret: \"mySecret\", name: \"user\", saveUninitialized: false, rolling: true, cookie: { maxAge: oneDay }, resave: false, }) ); // 註冊路由 app.use(\"/api/v1\", require(\"./router\")); // 檢查是否有table，沒有就建立 // const Message = require(\"./models\").message; // const User = require(\"./models\").user; // Message.sync(); // User.sync(); // 開啟監聽 app.listen(port, console.log(\"啟動 Server,Port:\" + port)); 先引入會使用的套件 (express：node web 框架、express-session：session 套件) 設定 app 為 express() 實例，port 為 8888。 設定 session 失效時間、名稱等設定。 secret(必要)：用來簽章 sessionID 的cookie, 可以是一secret字串或是多個secret組成的一個陣列。 name：在response中，設定的 sessionID cookie 名字。預設是 connect.sid。 saveUninitialized：強制將未初始化的session存回 session store，未初始化的意思是它是新的而且未被修改。 rolling：強制在每一次回應時，重新設置一個sessionID cookie。 cookie：設定sessionID 的cookie相關選項。 resave：強制將session存回 session store, 即使它沒有被修改。 註冊路由，連線 http://127.0.0.1/api/v1 後面會導向 router 檔案。 註解部分為可以每次啟動後先檢查是否有table，沒有就建立。 開啟 port(8888) 監聽。 router \"use strict\"; const express = require(\"express\"); const middleware = require(\"../middleware\"); const router = express(); const { register, login, lojsut } = require(\"../controllers/auth\"); const { getAllMessage, getMessage, createMessage, updateMessage, deleteMessage, } = require(\"../controllers/message\"); const { createReply, updateReply, deleteReply, } = require(\"../controllers/reply\"); // 註冊、登入、登出 router.post(\"/register\", express.json(), register); router.post(\"/login\", express.json(), login); router.post(\"/lojsut\", lojsut); // 查詢留言 router.get(\"/message\", getAllMessage); router.get(\"/message/:message_id\", getMessage); //需要驗證才可以使用（新增留言、修改留言、刪除留言、新增留言回覆、修改留言回覆、刪除留言回覆） router.use(middleware); router.post(\"/message\", express.json(), createMessage); router.patch(\"/message/:message_id\", express.json(), updateMessage); router.delete(\"/message/:message_id\", deleteMessage); router.post(\"/message/:message_id\", express.json(), createReply); router.patch(\"/message/:message_id/:reply_id\", express.json(), updateReply); router.delete(\"/message/:message_id/:reply_id\", deleteReply); module.exports = router; 設定路由，分別是註冊、登入、登出、新增留言、查詢全部留言、查詢 {id} 留言、修改 {id} 留言、刪除 {id} 留言，連接到不同的 controller function。express.json() 函數是為了要讓 body-parser 解析帶有 JSON 傳入後面的 controller，其中比較特別的是因為新增留言、修改留言、刪除留言、新增留言回覆、修改留言回覆、刪除留言回覆需要登入後才可以使用，所以多一個 middleware 來驗證是否登入。\nmiddleware \"use strict\"; module.exports = (req, res, next) =\u003e { if (!req.session.userid) { return res.status(401).json({ message: \"用戶需要認證\" }); } next(); }; 使用 session 來驗證是否有登入。\nmodels 會放置與資料表資料型態有關的資訊，其中 index.js 是由 sequelize-cli 套件自動產生，用來讀取目前寫在 config 的連線設定檔等資訊。\nindex.js \"use strict\"; const fs = require(\"fs\"); const path = require(\"path\"); const Sequelize = require(\"sequelize\"); const basename = path.basename(__filename); const env = process.env.NODE_ENV || \"development\"; const config = require(__dirname + \"/../config/config.json\")[env]; const db = {}; let sequelize; if (config.use_env_variable) { sequelize = new Sequelize(process.env[config.use_env_variable], config); } else { sequelize = new Sequelize( config.database, config.username, config.password, config ); } fs.readdirSync(__dirname) .filter((file) =\u003e { return ( file.indexOf(\".\") !== 0 \u0026\u0026 file !== basename \u0026\u0026 file.slice(-3) === \".js\" ); }) .forEach((file) =\u003e { const model = require(path.join(__dirname, file))( sequelize, Sequelize.DataTypes ); db[model.name] = model; }); Object.keys(db).forEach((modelName) =\u003e { if (db[modelName].associate) { db[modelName].associate(db); } }); db.sequelize = sequelize; db.Sequelize = Sequelize; module.exports = db; user.js \"use strict\"; const { Model } = require(\"sequelize\"); module.exports = (sequelize, DataTypes) =\u003e { class user extends Model { static associate(models) { user.hasMany(models.message, { foreignKey: \"user_id\", }); } } user.init( { username: { type: DataTypes.STRING, unique: true, }, password: { type: DataTypes.STRING, }, }, { sequelize, paranoid: true, freezeTableName: true, modelName: \"user\", } ); return user; }; 此為 user 資料表的欄位資料，有預設的 init 初始化，比較特別的是如果有用到關聯性的外鍵等等要記得在 associate 做設定，user.hasMany(models.message,{foreignKey: \"user_id\",}) 代表 user 這張表可以有很多個 message，其 message 外鍵是 user_id。\nparanoid：代表會執行軟刪除而不是硬刪除，但必須要多一個 deletedAt 來存放軟刪除時間。 freezeTableName：因為 sequelize 會自動再產生資料表時加上複數，如果不想要就必須使用它讓 sequelize 不會自動加入 s (複數)。 modelName：此 model 名稱。 message.js \"use strict\"; const { Model } = require(\"sequelize\"); module.exports = (sequelize, DataTypes) =\u003e { class message extends Model { static associate(models) { message.hasMany(models.reply, { foreignKey: \"message_id\", }); message.belongsTo(models.user, { foreignKey: \"user_id\", }); } } message.init( { user_id: { type: DataTypes.INTEGER, allowNull: false, }, content: { type: DataTypes.STRING, allowNull: false, }, version: { type: DataTypes.STRING, }, }, { sequelize, paranoid: true, freezeTableName: true, modelName: \"message\", } ); return message; }; 此為 message 資料表的欄位資料，associate 設定有 message.hasMany(models.reply,{foreignKey: \"message_id\",}) 代表 message 這張表可以有很多個 reply，其 reply 外鍵是 message_id。以及 message.belongsTo(models.user, {foreignKey: \"user_id\",}); 代表 message 存在一對一的關係，外鍵是 user_id。\nreply.js \"use strict\"; const { Model } = require(\"sequelize\"); module.exports = (sequelize, DataTypes) =\u003e { class reply extends Model { static associate(models) { reply.belongsTo(models.message, { foreignKey: \"message_id\", }); reply.belongsTo(models.user, { foreignKey: \"user_id\", }); } } reply.init( { message_id: { type: DataTypes.INTEGER, allowNull: false, }, user_id: { type: DataTypes.INTEGER, allowNull: false, }, content: { type: DataTypes.STRING, allowNull: false, }, version: { type: DataTypes.STRING, }, }, { sequelize, paranoid: true, freezeTableName: true, modelName: \"reply\", } ); return reply; }; 此為 reply 資料表的欄位資料，associate 設定有reply.belongsTo(models.message, {foreignKey: \"message_id\",}); 代表 reply 存在一對一的關係，外鍵是 message_id，以及 reply.belongsTo(models.user, {foreignKey: \"user_id\",}); 代表 reply 存在一對一的關係，外鍵是 user_id。\ncontrollers 我們遵循 MVC 的設計規範，所有的商用邏輯都會放在 controller 內，repository 只負責與資料庫進行交握。\nauth.js const Auth = require(\"../repositories/auth\"); const bcrypt = require(\"bcrypt\"); // 註冊 const register = async (req, res) =\u003e { if (!req.body.username || !req.body.password) { return res.status(400).json({ user: \"沒有正確輸入帳號或密碼\" }); } const salt = await bcrypt.genSalt(10); const bcrypt_password = await bcrypt.hash(req.body.password, salt); user = await Auth.register(req.body.username, bcrypt_password); if (!user[1]) { return res.status(400).json({ user: \"username已存在\" }); } return res.status(201).json({ user: user }); }; // 登入 const login = async (req, res) =\u003e { if (!req.body.username || !req.body.password) { return res.status(400).json({ user: \"沒有正確輸入帳號或密碼\" }); } user = await Auth.login(req.body.username, req.body.password); if (!user) { return res.status(400).json({ user: \"帳號或密碼錯誤\" }); } session = req.session; session.userid = user.id; return res.status(200).json({ user: \"登入成功\" }); }; // 登出 const logout = async (req, res) =\u003e { session = req.session; session.destroy(); return res.status(200).json({ user: \"登出成功\" }); }; module.exports = { register, login, logout, }; 主要有3 個部份，註冊、登入、登出：\n註冊：會先驗證是否正確輸入資料(包含欄位是否錯誤等)，如果有錯，就會回應 400 以及沒有正確輸入帳號或密碼。接下來會使用到 bcrypt 來幫密碼進行加密處理，再把加密後的密碼以及帳號丟到 Auth.register repository 來進行資料庫的存取，如果錯誤會回應 400 以及 username 已存在，成功會回應 201 以及新增的帳號資訊。 登入：一樣會先檢查輸入資料，接著把資料丟到 Auth.login repository 來進行資料庫的驗證，如果錯誤會回應 400 以及帳號或密碼錯誤，成功會回應 200 以及登出成功。 登出：將 session 給清除，然後回應 200 以及登出成功。 message.js (由於內容較多，所以依照功能拆分後說明)\n查詢留言功能\nconst Message = require(\"../repositories/message\"); const Reply = require(\"../repositories/reply\"); // 查詢所有留言 const getAllMessage = async (req, res) =\u003e { message = await Message.getAll(); return res.status(200).json({ message: message }); }; //查詢{id}留言 const getMessage = async (req, res) =\u003e { if (!(message = await Message.get(req.params.message_id))) { return res.status(404).json({ message: \"找不到留言\" }); } return res.status(200).json({ message: message }); }; 先載入要使用的 repository ，查詢所有留言會使用 Message.getAll() repository 來進行資料庫的讀取，會回應 200 以及查詢內容，若是尚未有留言則會顯示空陣列。查詢{id}留言會將 req.params.message_id 丟到 Message.get repository 來進行資料庫的讀取，如果錯誤會回應 404 以及 找不到留言，成功會回應 200 以及查詢內容。\n新增留言功能\n//新增留言 const createMessage = async (req, res) =\u003e { if (!req.body.content || req.body.content.length \u003e 20) { return res.status(400).json({ message: \"沒有輸入內容或長度超過20個字元\" }); } message = await Message.create(req.session.userid, req.body.content); return res.status(201).json({ message: message }); }; 新增留言會先檢查輸入內容是否為空以及不能大於20個字元，如果錯誤就回應 400 以及沒有輸入內容或長度超過20個字元，再將 req.session.userid、req.body.content 丟到 Message.create repository 來進行資料庫的存取，成功會回應 201 以及新增的留言。\n修改留言功能\n//修改留言 const updateMessage = async (req, res) =\u003e { if (!(message = await Message.get(req.params.message_id))) { return res.status(404).json({ message: \"找不到留言\" }); } if (!req.body.content || req.body.content.length \u003e 20) { return res.status(400).json({ message: \"沒有輸入內容或長度超過20個字元\" }); } if ( (await Message.update( req.params.message_id, req.session.userid, req.body.content, message[\"version\"] )) == \"0\" ) { return res.status(400).json({ message: \"修改留言失敗\" }); } return res.status(200).json({ message: \"修改留言成功\" }); }; 修改留言因為需要先從資料中取得 version 來檢查樂觀鎖，所以會先將 req.params.message_id 丟到 Message.get repository 來進行資料庫的查詢，錯誤會回應 404 以及找不到留言。 先檢查輸入內容是否為空以及不能大於20個字元，如果錯誤就回應 400 以及沒有輸入內容或長度超過20個字元，再將 req.params.message_id、req.session.userid、req.body.content、message[\"version\"] 丟到 Message.update repository 來進行資料庫的更新，錯誤會回應 400 以及修改留言失敗，成功會回應 200 以及修改留言成功。\n刪除留言功能\n//刪除留言 const deleteMessage = async (req, res) =\u003e { if (!(await Message.delete(req.params.message_id, req.session.userid))) { return res.status(400).json({ message: \"刪除留言失敗\" }); } // 刪除留言時同步刪除所有回覆 await Reply.deleteMessage(req.params.message_id); return res.status(204).json({ message: \"刪除留言成功\" }); }; 刪除留言會將 req.params.message_id、req.session.userid 丟到 Message.delete repository 來進行資料庫的軟刪除，錯誤會回應 400 以及刪除留言失敗，因為我們刪除留言後，不能在對該留言進行回覆的任何功能，所以一同軟刪除所有的回覆，成功後會回應 204 以及刪除留言成功。\nreply.js (由於內容較多，所以依照功能拆分後說明)\n新增回覆功能\nconst Reply = require(\"../repositories/reply\"); //新增回覆 const createReply = async (req, res) =\u003e { if (!req.body.content || req.body.content.length \u003e 20) { return res.status(400).json({ message: \"沒有輸入回覆或長度超過20個字元\" }); } if ( !(reply = await Reply.create( req.params.message_id, req.session.userid, req.body.content )) ) { return res.status(400).json({ message: \"新增回覆失敗\" }); } return res.status(201).json({ message: reply }); }; 先載入要使用的 repository，新增回覆先檢查輸入內容是否為空以及不能大於20個字元，如果錯誤就回應 400 以及沒有輸入內容或長度超過20個字元，再將 req.params.message_id、req.session.userid、req.body.content 丟到 Reply.create repository 來進行資料庫的存取，錯誤會回應 400 以及新增留言失敗，成功會回應 201 以及新增回覆內容。\n修改回覆功能\n//修改回覆 const updateReply = async (req, res) =\u003e { if (!(reply = await Reply.get(req.params.reply_id, req.params.message_id))) { return res.status(404).json({ message: \"找不到回覆\" }); } if (!req.body.content || req.body.content.length \u003e 20) { return res.status(400).json({ message: \"沒有輸入回覆或長度超過20個字元\" }); } if ( (await Reply.update( req.params.reply_id, req.params.message_id, req.session.userid, req.body.content, reply[\"version\"] )) == \"0\" ) { return res.status(400).json({ message: \"修改回覆失敗\" }); } return res.status(200).json({ message: \"修改回覆成功\" }); }; 修改留言因為需要先從資料中取得 version 來檢查樂觀鎖，所以會先將 req.params.reply_id、req.params.message_id 丟到 Reply.get repository 來進行資料庫的查詢，錯誤會回應 404 以及找不到留言。 先檢查輸入內容是否為空以及不能大於20個字元，如果錯誤就回應 400 以及沒有輸入內容或長度超過20個字元，再將 req.params.reply_id 、req.params.message_id、req.session.userid、req.body.content、reply[\"version\"] 丟到 Reply.update repository 來進行資料庫的更新，錯誤會回應 400 以及修改回覆失敗，成功會回應 200 以及修改回覆成功。\n刪除回覆功能\n//刪除回覆 const deleteReply = async (req, res) =\u003e { if ( !(await Reply.delete( req.params.reply_id, req.params.message_id, req.session.userid )) ) { return res.status(400).json({ message: \"刪除回覆失敗\" }); } return res.status(204).json({ message: \"刪除回覆成功\" }); }; 刪除回覆會將 req.params.reply_id、req.params.message_id、req.session.userid 丟到 Reply.delete repository 來進行資料庫的軟刪除，錯誤會回應 400 以及刪除回覆失敗，成功後會回應 204 以及刪除回覆成功。\nrepositories 此會放置與資料庫進行交握的程式。\nauth.js const User = require(\"../models\").user; const bcrypt = require(\"bcrypt\"); const auth = { // 註冊 async register(username, bcrypt_password) { return await User.findOrCreate({ where: { username: username }, defaults: { username: username, password: bcrypt_password }, }); }, // 登入 async login(username, password) { const user = await User.findOne({ where: { username: username } }); if (user) { return (await bcrypt.compare(password, user.password)) ? user : false; } }, }; module.exports = auth; auth 內主要有兩個與資料庫進行互動，分別是，註冊以及登入，註冊會將傳入的帳號以及加密過後密碼使用 User.findOrCreate 來進行查詢或新增，如果帳號不存在才會進行新增動作。登入會將傳入的帳號及密碼使用 User.findOne 來檢查是否存在，如果存在再檢查密碼與資料庫是否正確。\nmessage.js const Message = require(\"../models\").message; const Reply = require(\"../models\").reply; const repository = { async getAll() { return await Message.findAll({ include: { model: Reply } }); }, async get(message_id) { return await Message.findOne({ include: { model: Reply, }, where: { id: message_id }, }); }, async create(user_id, content) { return await Message.create({ user_id: user_id, content: content }); }, async update(message_id, user_id, content, version) { return await Message.update( { content: content, version: version + 1, }, { where: { id: message_id, user_id: user_id, version: version, }, } ); }, async delete(message_id, user_id) { return await Message.destroy({ where: { id: message_id, user_id: user_id }, }); }, }; module.exports = repository; message.js 裡面有會查詢全部留言、查詢{id}留言、新增留言、修改{id}留言、刪除{id}留言等，以下會說明各別負責功用：\n查詢全部留言 getAll()：使用 Message.findAll 來顯示查詢結果，並且 include model Reply 回覆內容。 查詢{id}留言 get()：使用 Message.findOne 查詢 message.id 等於 message_id 的結果，並且 include model Reply 回覆內容。 新增留言 create()：使用 Message.create 新增 message.user_id 以及 message.content。 修改{id}留言 update()：使用 Message.update 更新 content 以及 version，且 message.id 要等於 message_id 及 message.user_id 等於 user_id 及 message.version 等於 version。 刪除{id}留言 delete()：使用 Message.destroy 刪除符合 message.id 等於 message_id 及 message.user_id 等於 user_id。 reply.js const Message = require(\"../models\").message; const Reply = require(\"../models\").reply; const reply = { async get(reply_id, message_id) { return await Reply.findOne({ where: { id: reply_id, message_id: message_id }, }); }, async create(message_id, user_id, content) { is_exist = await Message.findOne({ where: { id: message_id }, }); if (is_exist) { return await Reply.create({ message_id: message_id, user_id: user_id, content: content, }); } }, async update(reply_id, message_id, user_id, content, version) { return await Reply.update( { content: content, version: version + 1, }, { where: { id: reply_id, message_id: message_id, user_id: user_id, version: version, }, } ); }, async delete(reply_id, message_id, user_id) { return await Reply.destroy({ where: { id: reply_id, message_id: message_id, user_id: user_id, }, }); }, async deleteMessage(message_id) { return await Reply.destroy({ where: { message_id: message_id, }, }); }, }; module.exports = reply; reply.js 裡面有會新增回覆、修改{id}回覆、刪除{id}回覆等，除此之外還多了兩個 get、deleteMessage 用來取得 version 樂觀鎖以及同步刪除回覆功能，那以下會說明各別負責功用：\n新增回覆 create()：因為要先確認留言是否被刪除，所以先使用 Message.findOne 檢查留言是否被刪除，在用 Reply.create 新增 reply.message_id 跟 reply.user_id 以及 reply.content。 修改{id}回覆 update()：使用 Reply.update 更新 content 以及 version，且 reply.id 要等於 reply_id 及 reply.message_id 等於 message_id 及 reply.user_id 等於 user_id 及 reply.version 等於 version。 刪除{id}回覆 delete()：使用 Reply.destroy 刪除符合 reply.id 等於 reply_id 跟 reply.message_id 等於 message_id 及 reply.user_id 等於 user_id。 ","常用指令-sequelize-cli#常用指令 (sequelize-cli)":" sequelize db:migrate：將資料表依照 up 內容執行(migrate 檔案)。 sequelize db:migrate:undo:all：將資料表依照 down 內容執行(migrate 檔案)。 sequelize db:seed：產生假資料 "},"title":"用 Node.js寫一個 Repository Restful API 的留言板 (express、sequelize 套件)"},"/blog/opentelemetry/":{"data":{"":"此分類包含 Opentelemetry 相關的文章。\n如何透過 OpenTelemetry 來收集 Ingress Nginx Controller 的 Metrics 與 Traces 並送到 Datadog 上發布日期：2023-09-13 什麼是 Opentelemetry？可觀測性 (Observability) 又是什麼？發布日期：2023-09-06 "},"title":"Opentelemetry"},"/blog/opentelemetry/opentelemetry-ingress-nginx-controller/":{"data":{"":"由於最近公司想要導入 Datadog，在測試過程中順便導入 OpenTelemetry 來收集 Metrics 與 Traces 並送到 Datadog 上 ～\n🔥 這個範例比較特別，因為 Datadog 有提供 Ingress Nginx Controller 的 integrations，可以透過 Datadog Agent 來收集 Metrics，不需要透過 OpenTelemetry Collector 來收集。 ( Datadog Agent 請參考：https://docs.datadoghq.com/containers/kubernetes/ )\n程式部分也同步上傳到 github 上，可以點我前往","參考資料#參考資料":"Configure Nginx Ingress Controller to use JSON log format：https://dev.to/bzon/send-gke-nginx-ingress-controller-logs-to-stackdriver-2ih4\n淺談 OpenTelemetry - Collector Compoents：https://ithelp.ithome.com.tw/articles/10290703","執行步驟#執行步驟":" 先 clone 這個 repo (廢話 xD)\n先建立 OpenTelemetry Collector，執行以下指令：\nhelm upgrade collector \\ opentelemetry-collector \\ --repo https://open-telemetry.github.io/opentelemetry-helm-charts \\ --install \\ --create-namespace \\ --namespace opentelemetry \\ -f \"otel-collector.yaml\" 再建立 Ingress Nginx Controller，執行以下指令：\nhelm upgrade ingress-nginx \\ ingress-nginx \\ --repo https://kubernetes.github.io/ingress-nginx \\ --install \\ --create-namespace \\ --namespace ingress-nginx \\ -f \"ingress-nginx-values.yaml\" 接著建立測試用 Nginx 服務，執行以下指令：\nkubectl apply -f nginx.yaml ","檔案說明#檔案說明":" otel-collector.yaml： OpenTelemetry Collector 的設定檔，主要是設定要收集哪些 metrics、traces，並且要送到哪個 exporter，要注意的是 exporters 的 datadog 需要設定 site、api_key，以及 image 要記得用 otel/opentelemetry-collector-contrib，才會有 datadog 的 exporter。\ningress-nginx-values.yaml： Ingress Nginx Controller 的設定檔，這邊的 podAnnotations 是為了讓 Ingress Nginx Controller 的 Pod 能夠透過 Datadog agent 收集 metrics 到 Datadog 才加上的。\nconfig 裡面的設定有很多，主要都是 openTelemetry 的設定，要注意的是 enable-opentelemetry 要設為 true，另外 otlp-collector-host 以及 otlp-collector-port 要送到哪個 collector 等等也要記得設定。 另外如果想要將 LOG 與 Trace 串再一起，記得要把 log-format 設為 json，並且帶入，trace_id 與 span_id ( 這邊有多帶 dd.trace_id 是為了讓 datadog 可以自動串接 LOG \u0026 Trace )。\nnginx.yaml： 一個簡單的 Nginx 整套服務 (Deployment、Service、Ingress)，要注意的是 Ingress 需要設定 annotations kubernetes.io/ingress.class: nginx (這個是 Ingress Nginx Controller 的預設 class name)，才會被 Ingress Nginx Controller 接管 (才會有 Load Balancer 的 IP)","測試#測試":"當你執行完上面的步驟後，你會發現有產生兩個 namespace，一個是 ingress-nginx，另一個是 opentelemetry，並且會有 OpenTelemetry Collector、Ingress Nginx Controller、Nginx 等服務，如下：\n啟動服務\n我們試著打 http://nginx.example.com/ (測試網址，需要先在 /etc/hosts 綁定 Ingress Nginx Controller 咬住的 Load Balancer IP)，查看一下 Datadog 的 LOG，看看是否有收到 Nginx 的 LOG (此收集 LOG 的方式是透過在 cluster 上安裝 Datadog 的 agent)，如下：\nDatadog LOG\n接著查看 Datadog APM 的 trace，如下：\nDatadog APM\n由於我們在後面目前沒有串其他服務，所以只有一個 span，之後還有另外兩篇文章是介紹如何串其他服務 (會增加服務以及部分設定)，可以參考看看：opentelemetry-roadrunner、opentelemetry-nodejs\n順便看一下透過 Datadog Agent 收集的 Ingress Nginx Controller 的 Metrics，如下：\nDatadog Ingress Nginx Controller 的 Metrics\n可以用這些 Metrics 來做 Dashboard，如下：\nDatadog Dashboard","結論#結論":"透過 OpenTelemetry Collector 來收集 Ingress Nginx Controller 的 Metrics 與 Traces 並送到 Datadog 上，這樣就可以透過 Ingress Nginx Controller 的 Metrics 來做監控了，對於 RD 再開發上，有 Traces 也更方便 RD 他們找到程式的瓶頸 (有可能是服務導致的)。"},"title":"如何透過 OpenTelemetry 來收集 Ingress Nginx Controller 的 Metrics 與 Traces 並送到 Datadog 上"},"/blog/opentelemetry/opentelemetry-observability/":{"data":{"":"在介紹 Opentelemetry 之前，我們要先了解一下目前軟體架構以及基礎設施的演進：\n軟體架構以及基礎設施的演進\n第一階段在軟體架構設計上較為簡單，不會有什麼特別需要拆分出來的程式，所以都是一整包的程式，再測試以及除錯也比較不會有什麼問題。基礎設施都是使用 VM 或是使用放在 IDC 的機房來當 Server。\n第二階段隨著雲端技術的推出，會開始將服務搬上雲供應商提供的 IaaS 服務，或是使用私有雲給企業放置較機密的內容，其他則放置公有雲上，達成混合雲的模式。\n第三階段隨著雲端技術越來越成熟，有更多的雲端 IaC 以及功能推出，會開始考慮使用分散式的系統架構，將 DB 等服務也改用 Cloud SQL 的方式。在基礎設施上也隨著容器化的技術成熟而進入新的時代。\n第四階段已經使用 docker 來管理好一陣子，但發現虛擬容器技術在管理上十分不方便，因此 K8s 逐漸盛行，將架構從分散式改成微服務的方式進行，在讓開發團隊使用上可以更靈活且容易。\n雖然使用 K8s 可以讓我們的服務更靈活方便，但也會將服務切的越來越細，這時會讓開發變的十分複雜，我們在架構上從一開始的單體式架構，變成分散式架構，再到最後的微服務，讓開發人員需要處理的事情會越來越多。服務要如何連線？Log 要如何記錄？以及當一個請求會經過多個服務時，相對的延遲也會增加，這時要怎麼去處理等。在監控上，因為服務切分得很細，當線上有一個服務有問題時，要如何快速的找到問題點也是一大挑戰。\n當我們使用分散式系統或是微服務時發生故障時，會很難快速的恢復服務，因為每個服務都互相依賴，在以往都是透過經驗以及對系統的了解來得以解決。那有什麼其他的方式，能夠讓我們更快掌握每個服務呢？我們先來了解一個名詞：可觀測性(Observability)","opentelemetry#Opentelemetry":"那我們這次要介紹的可觀測性(Observability)工具就是 Opentelemetry，縮寫 OTel，它是由 CNCF (Cloud Native Computing Foundation) 組織孵化的開源專案，在 2021 年 5 月由 OpenTracing 與 OpenCensus 兩個框架合併，結合兩項分散式追蹤框架最重要的特性成為下一代收集遙測數據的新標準。\ntelemetry 又叫做遙測，是指能夠跨越不同系統來收集資料 (包含 LOG、Metric、trace) 的能力。\n我們可以看一下官網的說明：\nOpentelemetry 是雲原生的可觀測性(Observability)框架，提供標準化的 API、SDK 與協議自動檢測、蒐集、導出遙測數據資料 (Metrics、Log、Trace)，並支援 W3C 定義的 Http trace-context 規範，降低開發者在搜集遙測數據上的困難度，以及方便進行後續分析以及性能的優化。\nOpentelemetry\n在 OpenTelemetry 核心元件如下：\nAPI：開發人員可以透過 OpenTelemetry API 自動生成、蒐集應用程式(Application)的遙測數據資料(Metrics, Log, Trace)，每個程式語言都需實作 OpenTelemetry 規範所定義的 API 方法簽章。\nSDK：是 OpenTelemetry API 的實現。\nOTLP：規範定義了遙測數據的編碼與客戶端及服務器之間如何交換的協議 (gRPC、HTTP)。\nCollector：OpenTelemetry 中儲存庫，用於接收、處理、導出遙測數據到各種後端平台。","參考資料#參考資料":"Observability：https://linkedin.github.io/school-of-sre/level101/metrics_and_monitoring/observability/\n[OpenTelemetry] 現代化監控使用 OpenTelemetry 實現 : 可觀測性(Observability)：https://marcus116.blogspot.com/2022/01/modern-monitoring-using-openTelemetry-with-Observability.html\n[OpenTelemetry] 現代化監控使用 OpenTelemetry 實現 : OpenTelemetry 開放遙測：https://marcus116.blogspot.com/2022/01/opentelemetry-opentelemetry.html\n淺談 Observability(下)：https://ithelp.ithome.com.tw/m/articles/10287598\nManage services, spans, and traces in Splunk APM：https://docs.splunk.com/Observability/apm/apm-spans-traces/traces-spans.html","可觀測性observability#可觀測性(Observability)":"可觀測性有三個重要的特性，分別是：\nMetrics 負責監控系統有什麼狀況，當要發生服務故障前可以透過設定閥值搭配告警提早得知。\nLogs 當問題發生時，可以用來查看故障時正在執行哪些服務，以及產生的錯誤資訊。\nTraces （後面詳細介紹）\n可觀測性三大支柱\n我們對於 Metrics 跟 Logs 有基本的了解，所以我這邊會注重在 Traces 的部分：\n當有多個微服務的複雜分散式系統，用戶的請求會由系統中的多個微服務進行處理。Metrics 跟 Logs 可以提供我們有關系統如何去處理這些請求的一些資訊，但沒有辦法提供微服務的詳細訊息以及他是如何影響客戶端的請求。這時候就需要透過 Trace 來協助我們追蹤。\nTrace 可以在連續的時間維度上，透過 Trace 以及 Span 關聯，把空間給排列展示出來，並且有 Trace-Context 規範，能夠直觀的看到請求在分散式系統中經過所有服務的紀錄。\n什麼是 Trace、Span 、Trace-Context 呢？\n我們先說 Span，Span 又可以叫跨度，是系統中最小的單位，可以看下方圖片，SpanA 的資料是來源 SpanB，SpanB 來源是 SpanC 等等，每一個 Span 可以把它想成一個請求後面所有經過服務的工作流程，例如：nginx_module、db、redis 等等。\n請求的整個過程叫做 Trace，那他要怎麼知道 SpanA ~ SpanE 是同一個請求呢？\n就需要透過 TraceID 以及 SpanID 來記錄：\nTraceID：是唯一的 ID，用於識別整個分散式追蹤的一條請求路徑。在下方圖片中，當請求進入時，就會被賦予一個 TraceID，所有有經過的 Span 都會記錄此 TraceID，這樣才可以把不同服務依據 TraceID 關聯成同一個請求。\nSpanID：是一條請求路徑中單個操作唯一的 ID。追蹤路徑是由多個 Span 組成，每個 Span 都代表一個操作或特定的時間段。當請求進入時，每個服務就會產生一個 Span 來代表它處理請求的的時間。這些 Span 使用 TraceID 來連接再一起，形成完整的請求追蹤。\nTrace 示意圖\n那要怎麼查看每個 Span 的紀錄內容呢，就需要 Trace-Context：\n會放置一些用於追蹤和識別請求的上下文信息，例如 Trace ID、Span ID 和其他相關的數據。這些上下文信息可以是一些關鍵的數據，可以幫助我們在整個分佈式系統中追蹤請求的路徑，並將相關請求和操作關聯起來。\nECK Trace 示意圖\n上面的圖片中，可以看到 call /product/XXXX 後，會經過需多的 Span，隨便點擊一個 Span 可以看到它記錄的 Trace-Context，以及都會包含 TraceID 及 SpanID\nECK Trace 示意圖\nTrace 優點可以看到跨維度看到中間的資訊，對於找到問題以及瓶頸十分方便，但缺點就是因為需要在 Span 中產生 ID 以及內容，需要在程式裡面加入一定的套件以及調整程式碼。\n所以我們在可觀測性(Observability)最終的目的是希望可以透過可觀測性工具讓我們知道：\n請求通過哪些服務 每個服務在處理請求時做了些什麼 如果請求很慢，瓶頸在哪邊 如果請求失敗，錯誤點在哪 請求的路徑是什麼 為什麼花這麼長的時間 "},"title":"什麼是 Opentelemetry？可觀測性 (Observability) 又是什麼？"},"/blog/other/":{"data":{"":"先放置還沒有想到的分類 Blog 文章。\nBookstack 開源知識庫筆記平台安裝 (K8s + docker)發布日期：2023-04-07 找出程式碼、開源套件、容器的安全漏洞工具 - Snyk發布日期：2022-09-20 Linux 常用指令發布日期：2022-06-17 清除 Linux 機器上的 Swap (Buff、Cache、Swap 比較)發布日期：2022-06-06 "},"title":"其他(還想不到分類)"},"/blog/other/bookstack/":{"data":{"":"最近剛好有公司同事離職，想要把交接的資料給整理整理，雖然部門之前有架設專用的 wiki 給 RD 使用，但覺得介面沒有到很好用，於是就在網路上尋找，可以多人編輯的筆記系統，一開始有想過用 CodiMD (HackMD)，但考量到需要多層的架構來區分文件，最後選擇 Bookstack 這個開源知識庫筆平台來作為組內的筆記系統，以下會簡單說一下 Bookstack 的特色以及使用 K8s 跟 docker 的安裝教學。","bookstack-介紹#Bookstack 介紹":"介紹部分主要參考 Bookstack 簡介，以下列出會選擇它的三個特色：\n簡潔的書本列表模式 書架分類\n書本分類\n頁面章節分類\n最主要是因為有以上幾個不同的分層架構，在資料整理上會更方便、更好彙整，可以自訂書架以及書本、頁面或是章節的封面以及內容。\n強大的搜尋功能 搜尋功能\n當我們的筆記內容越來越多，雖然有上面提到的分類模式，想要找到內容還是需要花一段時間，但 Bookstack 有強大的搜尋功能，可以針對書架、書本、章節或是書面個別搜尋，可以利用時間、標籤、標題或是內容來快速找到想要的內容。\n畫圖功能 在討論系統或是程式的架構，最好的方式就是用畫圖的來表示。在以往都是使用 Draw.io 來畫圖，當畫完圖後需要匯出畫好的圖以外，如果怕畫圖的原檔消失，還需要再另外下載原檔來保留，很不方便，又怕忘記下載，而 Bookstack 內建可直接編輯的功能，當畫完圖後有問題，可以直接點擊圖片來編輯，而這些功能還會搭配內建的版控，若有問題還可以還原到正確的圖片版本。\n畫圖功能","參考資料#參考資料":"BookStack 簡介：https://docs.ossii.com.tw/books/bookstack/page/bookstack\nBookStack Installation：https://www.bookstackapp.com/docs/admin/installation/","安裝說明#安裝說明":"那簡單說明為什麼會選擇 Bookstack 我們就來安裝它，這邊有使用 K8s + docker 來測試安裝，那我們就一起來看程式碼吧，程式碼放在這 👈：\nK8s namespace.yamlapiVersion: v1 kind: Namespace metadata: name: bookstack 我習慣會將不同的服務切 namespace 來部署，大家可以依照習慣來使用，下方的 yaml 都是建在此 namespace 上。\ndeployment.yamlapiVersion: apps/v1 kind: Deployment metadata: name: bookstack namespace: bookstack labels: app: bookstack spec: replicas: 1 selector: matchLabels: app: bookstack template: metadata: labels: app: bookstack spec: containers: - name: bookstack image: linuxserver/bookstack ports: - name: http containerPort: 80 env: - name: DB_DATABASE value: bookstack - name: DB_HOST value: \u003c\u003c更換此處\u003e\u003e - name: DB_PORT value: \"3306\" - name: DB_PASSWORD value: \u003c\u003c更換此處\u003e\u003e - name: DB_USERNAME value: \u003c\u003c更換此處\u003e\u003e - name: MAIL_USERNAME value: example@test.com - name: MAIL_PASSWORD value: mailpass - name: MAIL_HOST value: smtp.server.com - name: MAIL_PORT value: \"465\" - name: MAIL_ENCRYPTION value: SSL - name: MAIL_DRIVER value: smtp - name: MAIL_FROM value: no-reply@test.com - name: APP_URL value: https://\u003c\u003c更換此處\u003e\u003e - name: APP_LANG value: zh_TW - name: APP_TIMEZONE value: Asia/Taipei resources: limits: cpu: \"0.5\" memory: \"512Mi\" 必要更換的參數有標示 «更換此處»，其餘可以依照各組織來自行配置，Bookstack 會將內容存在 db，圖片等存在 pod 中，需要永久保存請使用 pvc + pv 或是另外掛 nas。\nsvc.yamlapiVersion: v1 kind: Service metadata: name: bookstack namespace: bookstack spec: type: NodePort selector: app: bookstack ports: - name: http protocol: TCP port: 80 targetPort: 80 ingress.yamlapiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: bookstack-ingress namespace: bookstack annotations: kubernetes.io/ingress.class: nginx service.beta.kubernetes.io/do-loadbalancer-enable-proxy-protocol: \"true\" spec: rules: - host: \u003c\u003c更換此處\u003e\u003e http: paths: - path: / pathType: Prefix backend: service: name: bookstack port: number: 80 最後透過 ingress 的 domain 去訪問 svc \u003e pod 上，就完成部署拉～第一次登入要使用預設帳號及密碼 admin@admin.com/password\ndocker docker-compose.yamlversion: \"3.8\" services: bookstack: image: lscr.io/linuxserver/bookstack container_name: bookstack environment: - PUID=\u003c\u003c更換此處\u003e\u003e - PGID=\u003c\u003c更換此處\u003e\u003e - DB_HOST=bookstack_db - DB_PORT=3306 - DB_USER=bookstack - DB_PASS=bookstack - DB_DATABASE=bookstackapp - APP_LANG=zh_TW - APP_TIMEZONE=Asia/Taipei - APP_URL=\u003c\u003c更換此處\u003e\u003e - GOOGLE_APP_ID=\u003c\u003c更換此處\u003e\u003e - GOOGLE_APP_SECRET=\u003c\u003c更換此處\u003e\u003e volumes: - /bookstack/config:/config ports: - 80:80 restart: unless-stopped depends_on: - bookstack_db bookstack_db: image: lscr.io/linuxserver/mariadb container_name: bookstack_db environment: - PUID=\u003c\u003c更換此處\u003e\u003e - PGID=\u003c\u003c更換此處\u003e\u003e - MYSQL_ROOT_PASSWORD=bookstack - TZ=Asia/Taipei - MYSQL_DATABASE=bookstackapp - MYSQL_USER=bookstack - MYSQL_PASSWORD=bookstack volumes: - /bookstack/config:/config restart: unless-stopped docker 的部分就更簡單了，一樣是把 «更換此處» 換成對應的內容即可，env 的部分可以參考官方文件，這邊比較特別的是我們有多使用 Google 來 Oauth 登入，Bookstack 支援多種的 Oauth 登入方式，可以參考 Third Party Authentication。\n架設好 Bookstack 就可以開始寫部門或是組織內的筆記拉～ 如果有開放外網連線，要記得修改預設的管理員帳號，以及用管理員帳號登入，到功能與安全的公開存取給取消，這樣就必須要登入才可以瀏覽筆記了！那就交給大家自己去玩這個好用的筆記工具囉～～ 😍\n功能與安全設定"},"title":"Bookstack 開源知識庫筆記平台安裝 (K8s + docker)"},"/blog/other/linux-clear-swap/":{"data":{"":"今天在工作時，遇到機器的 Swap 超過預警值，需要手動去清除 Swap，那剛好就由這次機會來介紹要如何清除 Linux 機器上的 Swap，以及查詢 Linux 記憶體的使用狀況！","buff-跟-cache-以及-swap-的比較#buff 跟 cache 以及 Swap 的比較":" 比較 buff cache Swap 功用 記憶體寫完資料會先暫存起來，等之後再定期將資料存到硬碟上 記憶體讀完資料後暫存起來，可以在下此查詢時快速的顯示 硬碟的交換分區，當 buff/cache 記憶體已經用完後，又有新的讀寫請求時，就會將部份內存的資料存入硬碟，也就是把內存的部分空間當成虛擬的記憶體來做使用 ","free-與-available-的比較#free 與 available 的比較":" free：是真正尚未被使用的實體記憶體數量 available：是應用程式認為可用的記憶體數量，可以理解成 available = free + buff/cache 接下來我們就要進入主題，如何清除 Swap ：","參考資料#參考資料":"Linux 內存、Swap、Cache、Buffer 詳細解析：https://os.51cto.com/article/636622.html\nlinux free 命令下 free/available 區別：https://www.796t.com/content/1545715382.html\n釋放 linux 的 swap 記憶體：https://www.796t.com/article.php?id=207781","如何查詢-linux-記憶體#如何查詢 Linux 記憶體":"在講 Swap 之前，我們先來說一下怎麼查詢 Linux 記憶體，可以使用以下指令來顯示：\nfree 下完後，格式會長這樣：\ntotal used free shared buff/cache available Mem: 32765564 8499252 1825132 1857720 22441180 19693100 Swap: 16776188 0 16776188 但這樣子不是很好觀察，所以我們可以加上 -h 來顯示大小的單位，讓我更清楚的知道每一個的大小：\ntotal used free shared buff/cache available Mem: 31G 8.1G 1.6G 1.8G 21G 18G Swap: 15G 0B 15G 那我們接著先來說說使用 free 查詢後，所有欄位的意思吧！\nfree 第一列 Mem：記憶體的使用資訊 Swap：交換空間的使用資訊 free 第一行 total：系統總共的可用實體記憶體大小 used：已被使用的實體記憶體大小 free：還剩下多少可用的實體記憶體 shared：被共享使用的實體記憶體大小 buff/cache：被 buffer 和 cache 使用的實體記憶體大小 available：可被 應用程式 使用的實體記憶體大小 ","清除-swap#清除 Swap":"首先第一步我們先使用 free 來查看目前的 Swap 使用狀態：\nSwap 使用 (尚未清除)\n可以看到我們的 Swap used 是 797M，我們設定它不能超過 5%，超過就會通知，所以我們要把它手動清除。\n先檢查記憶體 available 為什麼要先檢查 available，是因為一開始會使用到 Swap 的原因就是因為應用程式的可用記憶體空間不足，所以現在要清除 Swap 條件就是：Mem 的 available 必須要大於 Swap 的 used 才可以，否則會導致記憶體爆炸 💥\n將記憶體資料暫存到硬碟 接下來因為我們要清除 Swap ，所以不能讓資料在寫入記憶體中，所以我們先使用下方指令，讓記憶體的資料暫存到硬碟。\nsync 這個指令就是將存於暫存的資料強制寫入到硬碟中，來確保清除時導致資料遺失。\n關閉 Swap，再打開 Swap 沒錯，Swap 的清除就是把他先關掉，再重新打開，他就會自己清除 Swap 的資料了！使用的指令如下：(清除過程需要稍等，讓他進行刪除動作)\nswapoff -a \u0026\u0026 swapon -a 確認都沒問題後，我們就使用 free 來重新查看記憶體狀態：\nSwap 使用 (已清除)\n可以看到我們清除完 Swap 後，Swap 的 used 已經從 797M 變成 0B。\n如果碰到執行 swapoff -a \u0026\u0026 swapon -a 出現 swapoff: Not superuser.，只需要在指令前面加上 sudo 就可以了！ "},"title":"清除 Linux 機器上的 Swap (Buff、Cache、Swap 比較)"},"/blog/other/linux-command/":{"data":{"":"因為最近在管理機器時，常常會使用各式各樣的指令來協助管理，所以把常用的指令依照不同類別整理在底下呦 😘","參考資料#參考資料":"3 種檢查遠端埠號是否開啟的方法：https://www.ltsplus.com/linux/3-ways-check-remote-server-open-port","系統類#系統類":"顯示當前進程狀態 ps ps [參數] -A 列出所有的進程 -w 可以加寬顯示較多的訊息 -au 顯示更詳細的訊息 -aux 顯示所有包含其他用戶的進程 au(x) 輸出的格式：\nUSER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND USER：行程的擁有者 PID：pid %CPU：佔用的 CPU 使用率 %MEM：佔用的記憶體使用率 VSZ：佔用的虛擬記憶體大小 RSS：佔用的記憶體大小 TTY：終端的次要裝置號碼 STAT：該行程的狀態： D：無法中斷的休眠狀態 (通常都是 IO 進程) R：正在執行中 S：靜止狀態 T：暫停執行 Z：不存在但暫時無法消除 W：沒有足夠的記憶體可以分配 \u003c：高優先序的進程 N：低優先序的進程 START：進程開始時間 TIME：執行的時間 COMMAND：所執行的指令 刪除執行中的進程 kill kill [-s \u003c訊息名稱或編號\u003e] [程序] or [-l \u003c訊息編號\u003e] -l \u003c訊息編號\u003e：若不加 \u003c訊息編號\u003e選項，-l 會列出全部的訊息名稱 -s \u003c訊息名稱或編號\u003e：指定要送出的訊息 最常用的訊息是\n1 (HUP)：重新加載進程 9 (KILL)：刪除一個進程 15 (TERM)：正常停止一個進程 顯示系統開機紀錄 who who [參數] -b：查看最後一次(上次)系統啟動時間 -r：查看最後一次(上次)系統啟動時間及運行級別 top up 後面代表系統到目前運行的時間，所以反推就可以知道啟動時間\nuptime uptime 17:44 up 2 days, 8:48, 3 users, load averages: 3.48 3.37 3.56 一樣會顯示到目前已經啟動多久，跟是幾點啟動等資訊","網路類#網路類":"查詢遠端 Port 是否開放 nc (netcat) nc (netcat)，可以讀取經過 TCP 及 UDP 的網路連線資料，是一套很實用的網路除錯工具。\n安裝指令：\nyum install nc 檢查 Port 是否有開放，可以用以下指令來查詢：\nnc -zvw3 \u003cIP:Port\u003e -z 只進行掃描，不進行任何的資料傳輸\n-v 顯示掃描訊息\n-w3 等待 3 秒\n如果 Port 有開放，會回傳以下內容：\nNcat: Version 7.50 ( https://nmap.org/ncat ) Ncat: Connected to \u003cIP:Port\u003e. Ncat: 0 bytes sent, 0 bytes received in 0.01 seconds. 如果沒有開放，會回傳以下內容：\nNcat: No route to host. nmap nmap (Network Mapper) 是另一個可以檢查 Port 的工具，安裝語法是這樣：\nyum install nmap 檢查 Port 是否有開放，可以用以下指令來查詢：\nnmap \u003cIP\u003e -p \u003cPort\u003e 如果 Port 有開放，會回傳以下內容：\nStarting Nmap 7.92 ( https://nmap.org ) at 2022-06-17 16:06 CST Nmap scan report for\u003cDomain\u003e (\u003cIP\u003e) Host is up (0.0039s latency). PORT STATE SERVICE 80/tcp open http Nmap done: 1 IP address (1 host up) scanned in 1.09 seconds 如果沒有開放，會回傳以下內容：\nStarting Nmap 7.92 ( https://nmap.org ) at 2022-06-17 16:06 CST Nmap scan report for\u003cDomain\u003e (\u003cIP\u003e) Host is up (0.0039s latency). PORT STATE SERVICE 80/tcp filtered http Nmap done: 1 IP address (1 host up) scanned in 1.09 seconds Telnet Telnet 也是一個可以檢查 Port 的工具，安裝語法是這樣：\nyum install telnet 檢查 Port 是否有開放，可以用以下指令來查詢：\ntelnet \u003cIP\u003e \u003cPort\u003e 如果 Port 有開放，會回傳以下內容：\nTrying \u003cIP\u003e… Connected to \u003cIP\u003e. Escape character is ‘^]’. ^CConnection closed by foreign host. 如果沒有開放，會回傳以下內容：\nTrying \u003cIP\u003e… telnet: Unable to connect to remote host: Connection refused "},"title":"Linux 常用指令"},"/blog/other/snyk/":{"data":{"":"前幾天有機會可以去參加 DevOpsDays Taipei 2022 活動，聽到了一門有關於如何將 Security 加入 DevOps - GitLab CI/CD with Snyk 的講座，覺得蠻有趣的，想說順便紀錄一下這幾天去聽的內容，文章底下有附上本次大會的共筆以及該講者於自己部落格所發的文章，歡迎大家先行查看歐 😆\nDevOps 圖片 (醫院 DevOps 如何落地(1) ─ DevOps 與醫院)\n我們平常熟知的 DevOps 文化應該是像上面這張圖一樣，可以透過自動化「軟體交付」和「架構變更」的流程，來使得構建、測試、發布軟體能夠更加地快捷、頻繁和可靠。\n通常在 Development 跟 Operations 之外還有一個專門負責檢查程式有無漏洞或是安全性問題的單位，那我們試想一下，如果在 Dev 或是 Ops 中間先加上了檢查安全的步驟，是不是可以讓整體流程效率更加提升呢！？ 可以免去程式寫完才發現有安全性問題的漏洞，需要重新修改，而是在部署前，或是程式開發中就先被檢查出來。\n對於資訊安全方面，開發人員進行開發時，必須關注目前的寫法是不是安全的，除了開發人員的程式碼外，還要注意使用的開源程式或是套件是不是已經有被掃出漏洞，加上現在越來越多人使用容器化，對於 Container Images 裡面所包含的套件安全也是一大問題。\n所以單純的自動化整合與部署已經不能滿足需求，就像下圖一樣，開始有了 DevSecOps 這個詞產生，在 DevOps 的 CI/CD 流程中加入資安解決方案，藉由資安解決方案來減少開發人員去檢查程式漏洞的時間，也讓 DevOps 快速迭代更加的完整。\nDevSecOps 圖片 (Apps Built Better: Why DevSecOps is Your Security Team’s Silver Bullet)","snyk#Snyk":"那我們這次要使用的工具是 - Snyk，我先簡單介紹一下這個公司以及工具，它是位於波士頓的網絡安全公司，專門從事雲計算，提供增輕鬆集成，可以將安全掃描結合到 IDE、Repository、CI/CD 等，連續掃描以及一鍵修復等功能。他本身是一個 SaaS 的服務，所以需要到官網註冊一個帳號才能使用，也支援不同的 SSO 登入，詳細可以參考註冊畫面。\n這個工具主要提供了 4 種掃描的功能， 分別是：\nCode Open Source Container IaC 那我們會依照這４種功能依序介紹，詳細的可以參考 Snyk User Documentation # Explore Snyk products。\nCode 能夠直接掃描目前開發 Project 的程式碼有沒有安全性的問題，可以大幅減少開發人員去檢查程式漏洞的時間，目前支援了以下三種掃描方式：\nIDE Plugins Web CLI IDE Plugins IDE 我們這邊以 VS Code 為例來說明，如果有在用 VS Code 的大家應該知道 VS Code 有一個 Marketplace，可以安裝很多不同的套件 Plugins，之後有時間也會寫一篇好用的 Plugins 文章，大家可以在持續關注我歐 😍\n首先先開啟左側的 Marketplace，搜尋 Snyk，選擇下載量最高的準沒錯，選擇 Snyk Security - Code and Open Source Dependencies。 VS Code Marketplace 安裝 Snyk\n安裝完後，在左側欄位應該就會看到 Snyk 的狗狗 Logo，如果沒有登入過，會先要求你登入 Snyk 並授權給 Plugins 使用。最後當你開啟專案時，會開始掃描程式碼，並且會依照開源套件安全性、程式碼安全性來分類，會顯示套件遇到什麼漏洞需要升級以及程式碼哪一段會有安全性的問題，能夠讓開發人員在第一時間就修復安全問題。 IDE Plugins 掃描\nWeb 登入 Snyk 網站，直接加入對應的 Project，會直接於網頁上掃描漏洞，並提示錯誤內容。\n選擇加入 project\n掃描漏洞並顯示錯誤內容\nCLI 要使用 CLI 需要先安裝 Snyk CLI，我們一樣使用 Homebrew 來安裝：\nbrew tap snyk/tap brew install snyk 當然除了 Homebrew 以外還有其他的安裝方式，例如：curl 下載執行檔、npm、yarn、docker 執行等等，詳細可以參考以下安裝連結。\n可以直接使用以下指令才進行測試 sync test，還有其他 CLI 指令，詳細可以參考 Snyk CLI：\nCLI 掃描 (官網)\nOpen Source 能夠掃描目前開發的 Project 中有沒有使用到有漏洞的 library，目前支援以下圖片語言，詳細可以參考 Open Source - Supported languages and package managers：\nOpen Source library 掃描 (官網)\nContainer 在 Container image 中會存放著不同的 base image ，在 base image 中又會使用不同的 library。所以這些 container 的安全性也是需要被關注的。\n可以直接使用 web 來選擇相對的 Container 倉庫去做掃描，如果有安裝上面說的 Snyk CLI，也可以使用該指令執行 snyk container test \u003ccontainer image\u003e\nContainer 掃描\nIac Snyk 的基礎設施即代碼 (IaC) 可幫助開發人員編寫安全的基礎設施配置，預防錯誤的設定產生，並且支援多種格式：\nK8s YAML HashiCorp Terraform AWS CloudFormation Azure Resource Manager (ARM) ","參考資料#參考資料":"DevOpsDays Taipei 2022 共同筆記 - 在你的 DevOps 中加入一點 Security — GitLab CI/CD with Snyk - 鄭荃樺 (Barry.Cheng) ：https://hackmd.io/@DevOpsDay/2022/%2F%40DevOpsDay%2FHkm1iY6xi\n身為 DevOps 工程師，使用 Snyk 掃描漏洞也是很正常的：https://barry-cheng.medium.com/%E8%BA%AB%E7%82%BAdevops%E5%B7%A5%E7%A8%8B%E5%B8%AB-%E4%BD%BF%E7%94%A8snyk%E6%8E%83%E6%8F%8F%E6%BC%8F%E6%B4%9E%E4%B9%9F%E6%98%AF%E5%BE%88%E6%AD%A3%E5%B8%B8%E7%9A%84-d7d8f2ad2304\n在你的 DevOps 中加入一點 Security — GitLab CI/CD with Snyk (講師簡報)：https://s.itho.me/ccms_slides/2022/9/26/3a2ac1ef-4143-4328-9fef-066782ac7dc6.pdf"},"title":"找出程式碼、開源套件、容器的安全漏洞工具 - Snyk"},"/blog/php-laravel/":{"data":{"":"此分類包含 PHP Laravel 相關的文章。\nLaravel 進階 (內建會員系統、驗證 RESTful API 是否登入、使用 Repository 設計模式)發布日期：2022-03-08 Laravel 介紹 (使用 Laravel 從零到有開發出一個留言板功能並搭配 RESTful API 來實現 CRUD)發布日期：2022-03-02 如何在 Nginx 下實作第一個 PHP 留言板 RESTful API發布日期：2022-02-24 PHP 介紹發布日期：2022-02-22 "},"title":"PHP Laravel"},"/blog/php-laravel/laravel-advanced/":{"data":{"":"本篇是 Laravel 介紹 的進階篇，由於有些說明介紹會沿用上一篇的內容，建議要先瀏覽過上一篇呦～ (可以從這裡下載最後程式碼！)","laravel-內建會員系統#Laravel 內建會員系統":"Laravel 這個框架，很方便的地方就是，他可以將我們常用的帳號登入註冊等功能內建在 Laravel 框架內，那我們來看一下要怎麼使用吧。(本篇 Laravel Framework：5.4.36)\n它的配置文件在 config/auth.php ，其中包含了用於調整認證服務行為或是標注選項配置等。\n要怎麼開始呢!? 先使用 artisan 指令產生我們要的檔案以及路由吧！\n$ php artisan make:auth 主要會產生\nnew file: app/Http/Controllers/HomeController.php new file: resources/views/auth/login.blade.php new file: resources/views/auth/passwords/email.blade.php new file: resources/views/auth/passwords/reset.blade.php new file: resources/views/auth/register.blade.php new file: resources/views/home.blade.php new file: resources/views/layouts/app.blade.php modified: routes/web.php 其中 routes/web.php 多了：\nAuth::routes(); Route::get('/home', 'HomeController@index')-\u003ename('home'); views 底下放的就是顯示的畫面，所以現在可以瀏覽\n登入頁面：http://127.0.0.1:8000/login\n註冊頁面：http://127.0.0.1:8000/register\n其他像是 controller 或是 migration 都已經內建在裡面了，稍後實作會講到！\n實作 Migration 我們照上一篇的流程，由於 route 已經設定好，所以我們先來設定 migration ， 檔案就放在 database/migrations 底下的 {日期時間}_create_users_table.php ，我們來看一下預設的資料表有哪些欄位吧!\npublic function up() { Schema::create('users', function (Blueprint $table) { $table-\u003eincrements('id'); $table-\u003estring('name'); $table-\u003estring('email')-\u003eunique(); $table-\u003estring('password'); $table-\u003erememberToken(); $table-\u003etimestamps(); }); } /** * Reverse the migrations. * * @return void */ public function down() { Schema::dropIfExists('users'); } 簡單看一下，他會建立一個名叫 users 的資料表，欄位分別有自動累加(id)、字串(name)、唯一字串(email)、字串(密碼)、rememberToken()、timestamps()。\n那我們這次想要做的事，不使用 email 做登入，改成使用 username來當作登入驗證，所以我們把 name、email的欄位改成\npublic function up() { Schema::create('users', function (Blueprint $table) { $table-\u003eincrements('id'); $table-\u003estring('username')-\u003eunique(); $table-\u003estring('password'); $table-\u003erememberToken(); $table-\u003etimestamps(); }); } /** * Reverse the migrations. * * @return void */ public function down() { Schema::dropIfExists('users'); } 設定 username 欄位為唯一值，移除掉 email ，再加入一個 api_token 用於後續的 RESTful API 驗證。\n使用 php artisan migrate 將 users 這個 table 給建起來。\n因為我們修改 name、email 改成 username 這個欄位，所以我們也要修改一下 views 顯示的畫面，由於是簡單的 HTML 這邊就不再多描述，直接放上修改後的程式碼。\nView Login.blade \u003cdiv class=\"form-group{{ $errors-\u003ehas('username') ? ' has-error' : '' }}\"\u003e \u003clabel for=\"username\" class=\"col-md-4 control-label\"\u003eUsername\u003c/label\u003e \u003cdiv class=\"col-md-6\"\u003e \u003cinput id=\"username\" type=\"text\" class=\"form-control\" name=\"username\" value=\"{{ old('username') }}\" required autofocus\u003e @if ($errors-\u003ehas('username')) \u003cspan class=\"help-block\"\u003e \u003cstrong\u003e{{ $errors-\u003efirst('username') }}\u003c/strong\u003e \u003c/span\u003e @endif \u003c/div\u003e \u003c/div\u003e Register.blade \u003cdiv class=\"form-group{{ $errors-\u003ehas('username') ? ' has-error' : '' }}\"\u003e \u003clabel for=\"username\" class=\"col-md-4 control-label\"\u003eUsername\u003c/label\u003e \u003cdiv class=\"col-md-6\"\u003e \u003cinput id=\"username\" type=\"text\" class=\"form-control\" name=\"username\" value=\"{{ old('username') }}\" required\u003e @if ($errors-\u003ehas('username')) \u003cspan class=\"help-block\"\u003e \u003cstrong\u003e{{ $errors-\u003efirst('username') }}\u003c/strong\u003e \u003c/span\u003e @endif \u003c/div\u003e \u003c/div\u003e Model 接下來到 app 底下找到 User.php 檔案，由於筆者習慣將 model 放到專屬的資料夾，不要讓他在 app 裡面流浪，所以會建立一個 Models 的資料夾，來存放所有的 models ，那移動原本的 model 有些有使用到它的路徑都要做修改，這邊以 User 檔案為示範。(因為後續會說到怎麼驗證登入 API ，所以上一篇的 Message model 也要記得修改歐！)\n因為我們移動後，原本的路徑是 App 要改成 App\\Models，會影響到的程式有以下幾個 (附上片段程式碼)\nconfig/auth.php 約在70行左右 'providers' =\u003e [ 'users' =\u003e [ 'driver' =\u003e 'eloquent', 'model' =\u003e App\\Models\\User::class, //修改片段 ], config/services.php 約在33行左右 'stripe' =\u003e [ 'model' =\u003e App\\Models\\User::class, //修改片段 'key' =\u003e env('STRIPE_KEY'), 'secret' =\u003e env('STRIPE_SECRET'), ], database/factories/UserFactory.php 約在15行左右 $factory-\u003edefine(App\\Models\\User::class, function (Faker\\Generator $faker) { //修改片段 static $password; return [ 'name' =\u003e $faker-\u003ename, 'email' =\u003e $faker-\u003eunique()-\u003esafeEmail, 'password' =\u003e $password ?: $password = bcrypt('secret'), 'remember_token' =\u003e str_random(10), ]; }); app/Http/Controllers/Auth/RegisterController.php 約在5行左右 use App\\Models\\User; 都修改好了，我們就繼續來修改 User 這個 model ，將原本的 name、email ，修改成以下\n\u003c?php namespace App\\Models; use Illuminate\\Notifications\\Notifiable; use Illuminate\\Foundation\\Auth\\User as Authenticatable; class User extends Authenticatable { use Notifiable; /** * The attributes that are mass assignable. * * @var array */ protected $fillable = [ 'username', 'password', ]; /** * The attributes that should be hidden for arrays. * * @var array */ protected $hidden = [ 'password', 'remember_token', ]; } 裡面有一個是 fillable 跟 hiddem ，順便解釋一下這兩個是在做什麼\nfillable：當使用者輸入這些 attribute 以外的參數(資料表的欄位)，就會發生錯誤。 hidden：想要限制能出現在陣列或是JSON 格式的屬性資料，比如密碼欄位等等，不想要顯示，只需要把該欄位加入 hidden。 再加碼一個\nguarded：這個屬性與 fillable 相反，當使用者輸入該參數的值，就會被擋掉。以下這個例子是允許任何的 input 資料，但十分建議不要這樣。 protected $guarded = []; Controller 接下來是要修改我們的 controller，主要需要修改的有兩個，檔案在 app\\Http\\Auth 底下的 registerController 、 loginController 兩個檔案，我們先來修改 registerController 註冊功能，看一下原本的程式碼在做什麼事情。\nRegisterController 原本程式碼 protected function validator(array $data) { return Validator::make($data, [ 'name' =\u003e 'required|string|max:255', 'email' =\u003e 'required|string|email|max:255|unique:users', 'password' =\u003e 'required|string|min:6|confirmed', ]); } /** * Create a new user instance after a valid registration. * * @param array $data * @return \\App\\User */ protected function create(array $data) { return User::create([ 'name' =\u003e $data['name'], 'email' =\u003e $data['email'], 'password' =\u003e bcrypt($data['password']), ]); } 可以看到在 validator() 就是在驗證註冊時的欄位，以 email 來說明，他需要符合必填(required)、字串(string)、email格式(email)、最大長度(max:255)、在 users 資料表中唯一(unique:users) 才可以註冊，另外兩個也是如此。\n那 create() 就是會建立註冊的資料到資料表中，就是這麼簡單XD，詳細可以參考 Laravel 官網 Form Request Validation\n那還記得我們把 name、email 給刪掉，改成 username 來做登入嗎，我們來看一下我們要修改哪些東西！\nprotected function validator(array $data) { return Validator::make($data, [ 'username' =\u003e 'required|string|max:255|unique:users', 'password' =\u003e 'required|string|min:6|confirmed', ]); } /** * Create a new user instance after a valid registration. * * @param array $data * @return \\App\\Models\\User */ protected function create(array $data) { return User::create([ 'username' =\u003e $data['username'], 'password' =\u003e bcrypt($data['password']), ]); } 由於我們刪除 email ，所以要把 unique:users 設定在 username。\nLoginController 註冊頁面都已經修改好了，理論上連線到 http://127.0.0.1:8000/register 頁面，輸入 username 以及 密碼，會可以註冊成功囉，那接下來我們來修改登入的 controller。\nuse AuthenticatesUsers; /** * Where to redirect users after login. * * @var string */ protected $redirectTo = '/home'; /** * Create a new controller instance. * * @return void */ public function __construct() { $this-\u003emiddleware('guest')-\u003eexcept('logout'); } 因為 Laravel 框架內建了驗證登入功能，所以只需要使用 use AuthenticatesUsers; 就可以達到驗證效果，登入成功就會使用 redirectTo 導向到 /home，那問題來了，因為 AuthenticatesUsers 預設是使用 email 來做登入驗證，但我們改成 username，我們要怎麼修改呢！？ 一起來看看吧\n//由於判斷是用 AuthenticatesUsers 這個內建的 trait 來實作，其中 username() 這個方法就是來指定登入要用的欄位 public function username() { return 'username'; } 上面有說因為使用了 AuthenticatesUsers 來做驗證，我們盡量不去修改框架內的檔案，當然 Laravel 也有配套措施，只要使用 username() 這個方法就可以指定登入要驗證的欄位，因為筆者在學習這段時也找了很久，這邊附上 Laravel API 可以去查看一下。\n我們這時就可以使用 http://127.0.0.1:8000/login 來登入囉！試試看吧～\n驗證 RESTful API 是否登入 本篇會使用到上一篇的 RESTful API 留言板來進行修改，所以有興趣的朋友，可以先去看玩上一篇 Laravel 介紹 歐～\n接下來也會同時使用 Repository 設計模式來修改程式碼，那 Repository 設計模式是什麼呢，讓我先來介紹。\nRepository 設計模式 還記得我們上一次，把所有的邏輯以及資料庫的處理的都放在 Controller 裡面嗎！如果只是單一個小專案，還可以這樣做沒關係，但專案越來越大，會使用的功能也越來越多，會造成 Controller 檔案肥大且難以維護，基於 SOLID 原則，我們應該要使用 Repository 設計模式來補助 Controller，將相關的資料庫邏輯放在不同的 Repository，方便中大型的專案做維護。\n小知識\nSOLID：簡單來說就是物件導向設定上為了讓軟體維護、開發變得更容易的五個準則的縮寫。\nSingle Responsibility Principle (SRP) 單一職責原則 Open-Closed Principle (OCP) 開放封閉原則 Liskov Substitution Principle (LSP) 里氏替換原則 Interface Segregation Principle (ISP) 介面隔離原則 Dependency Inversion Principle(DIP) 依賴反轉原則 SOLID目的也就是讓你程式碼達成低耦合、高內聚、降低程式碼壞味道，透過分離與clean code來提高可讀性會讓你的程式碼等同於設計文件，所以在修改或新增過程中降低產生Bug的機率，也可以較快的找到與解決出問題的地方，可以有效的減少技術債。 (資料來源：我該學會SOLID嗎?)\n不太清楚沒關係，我們會慢慢介紹到！那我們先來看看這次要修改什麼呢！？ 我想要在使用 API 時，去檢查有沒有登入，才會進行動作，舉個例子，我們希望在新增留言、修改留言以及刪除留言都需要登入，且是由本人操作才算成功。\nMigration 在此之前，我們先來修改一下上次的 migration {日期時間}_create_message_table.php 檔案吧\npublic function up() { Schema::create('message', function (Blueprint $table) { $table-\u003eincrements('id'); //留言板編號 $table-\u003einteger('user_id')-\u003eunsigned(); //留言者ID $table-\u003eforeign('user_id')-\u003ereferences('id')-\u003eon('users'); $table-\u003estring('content', 20); //留言板內容 $table-\u003einteger('version')-\u003edefault(0); $table-\u003etimestamps(); //留言板建立以及編輯的時間 $table-\u003esoftDeletes(); //軟刪除時間 }); } 可以看到我們將資料庫的名稱從 messsages 改為 message ，後續程式部分也都會修改，大家要在注意一下 ～\n我們這次加入了留言者 ID (使用外鍵連接 users 的 id)、按讚者 ID (使用外鍵連接 users 的 id)、留言板樂觀鎖、softDeletes軟刪除的欄位(軟刪除後續會提到)，並且因為我們同樣的資料不要重複儲存，所以刪除 name 要查詢就使用 join 來做查詢。\n我們還希望可以多一個來存放是誰按讚的的資料表。所以一樣使用 migration 新增一個 {日期時間}_create_like_table.php 檔案\npublic function up() { Schema::create('like', function (Blueprint $table) { $table-\u003ebigIncrements('id'); //按讚紀錄編號 $table-\u003einteger('message_id')-\u003eunsigned()-\u003enullable(); //文章編號 $table-\u003eforeign('message_id')-\u003ereferences('id')-\u003eon('message'); $table-\u003einteger('user_id')-\u003eunsigned()-\u003enullable(); //帳號編號 $table-\u003eforeign('user_id')-\u003ereferences('id')-\u003eon('users'); $table-\u003edateTime('created_at'); //按讚紀錄建立時間 $table-\u003esoftDeletes(); //軟刪除時間 }); } 會存放文章的編號並且使用外鍵連接 message 的 id，以及按讚者的 ID 也使用外鍵連接 users 的 id。\n列出本次會使用的功能以及對應的方法、是否需要登入、登入後其他人是否可以操作 功能 方法 是否需要登入 登入後其他人是否可以操作 查詢全部留言 getAllMessage 否 不需登入 查詢{id}留言 getMessage 否 不需登入 新增留言 createMessage 是 否 修改{id}留言 updateMessage 是 否 按讚{id}留言 likeMessage 是 可以 刪除{id}留言 deleteMessage 是 否 登入 API 我們上面介紹有使用到 Laravel 內建的登入 LoginController 來進行登入，但通常我們在使用時，都會另外再多一個登入用的 API ，那我們來看一下要怎麼設計吧！\n我們先使用 php artisan make:controller LoginController 新增一個登入的 API，他會產生在 app/Http/Controllers/ 目錄下\nnamespace App\\Http\\Controllers; use Illuminate\\Http\\Request; use Illuminate\\Support\\Facades\\Validator; use Illuminate\\Support\\Facades\\Auth; class LoginController extends Controller { public function login(Request $request) { $rules = [ 'username' =\u003e 'required', 'password' =\u003e 'required' ]; $validator = Validator::make($request-\u003eall(), $rules); if ($validator-\u003efails()) { return response()-\u003ejson([\"message\" =\u003e \"格式錯誤\"], 400); } if (!Auth::attempt([ 'username' =\u003e $request-\u003eusername, 'password' =\u003e $request-\u003epassword ])) { return response()-\u003ejson([\"message\" =\u003e \"登入失敗\"], 401); } return response()-\u003ejson([\"message\" =\u003e \"登入成功\"], 200); } public function logout() { Auth::logout(); return response()-\u003ejson([\"message\" =\u003e \"登出成功\"], 200); } } 這邊的 Login 會先驗證格式是否正確，在使用 Auth:attempt 來檢查是否有註冊過，並且回傳相對應的訊息， Logout 就使用 Auth::logout 即可。\n好了後我們先到 routes/api.php 新增登入跟登出 API 的路徑\nRoute::post('login', 'LoginController@login'); Route::post('logout', 'LoginController@logout'); 我們接下來設定 Middleware ，什麼是 Middleware 呢！？\nMiddleware Middleware 中文翻譯是中介軟體，是指從發出請求 (Request)之後，到接收回應(Response)這段來回的途徑上，\n用來處理特定用途的程式，比較常用的 Middleware 有身份認證 (Identity) 、路由(Routing) 等，再舉個例子\n某天早上你去圖書館看書， 下午去公園畫畫， 晚上去KTV 唱歌， 等到要準備回家的時候發現學生證不見了， 你會去哪裡找? (假設學生證就掉在這3個地方) 對於記憶不好的人來說，會按照 KTV \u003e 公園 \u003e 圖書館的路線去尋找。\n假設在公園找到學生證，就不會再去圖書館了，由於這條路是死巷，所以只能返回走去KTV的路，這個就是 Middleware 的運作原理。\n所以我們需要再請求時，先檢查是否有登入，才可以去執行需要權限的功能。\n我們可以使用內建的 Auth::check 來檢查是否有登入，我們接著看要怎麼做吧！\n先下指令生成一個放置登入驗證權限的 Middleware ，我把它取名為 ApiAuth\n$ php artisan make:middleware ApiAuth Middleware created successfully. 接著要把剛剛生成的 ApiAuth 檔案放置在 app/Http/Kernel.php 檔案中\nprotected $routeMiddleware = [ 'auth' =\u003e \\Illuminate\\Auth\\Middleware\\Authenticate::class, 'auth.basic' =\u003e \\Illuminate\\Auth\\Middleware\\AuthenticateWithBasicAuth::class, 'bindings' =\u003e \\Illuminate\\Routing\\Middleware\\SubstituteBindings::class, 'can' =\u003e \\Illuminate\\Auth\\Middleware\\Authorize::class, 'guest' =\u003e \\App\\Http\\Middleware\\RedirectIfAuthenticated::class, 'throttle' =\u003e \\Illuminate\\Routing\\Middleware\\ThrottleRequests::class, 'api.auth' =\u003e \\App\\Http\\Middleware\\ApiAuth::class // 這邊 ]; 接下來我們就可以開始撰寫 ApiAuth 檔案的內容了\npublic function handle($request, Closure $next) { if (!Auth::check()) { return response(['message' =\u003e '用戶需要認證'], 401); } return $next($request); } 這邊的意思是指，在 request 的時候，我們使用內建的 Auth::check 來檢查，如果登入就可以繼續使用，如果沒有登入會回傳用戶需要認證以及 401 的 status code。\nRoute 接下來，我們要把我們設定好的 ApiAuth 設定在 route\\api.php 的路由中，\nRoute::get('message', 'MessageController@getAll'); Route::get('message/{id}', 'MessageController@get'); Route::post('message', 'MessageController@create')-\u003emiddleware('api.auth'); Route::put('message/{id}', 'MessageController@update')-\u003emiddleware('api.auth'); Route::patch('message/{id}', 'MessageController@like')-\u003emiddleware('api.auth'); Route::delete('message/{id}', 'MessageController@delete')-\u003emiddleware('api.auth'); 可以看到跟我們上一篇的 route 設定的差不多，只是將 MessageController 後面的方法做了一點變化，簡化名稱(這與我們後面講到的 Repository 設計模式有關)，以及加上 like 這個方法來當作我們的按讚功能，後面將我們需要登入驗證才可以使用的功能，加入 middleware('api.auth');\nModel 因為我們在取得資料時，不希望顯示 deleted_at 給使用者，所以在 app\\Models\\Message.php 這個 model 裡面用 hidden 來做設定。\nmessage\nnamespace App\\Models; use Illuminate\\Database\\Eloquent\\Model; use Illuminate\\Database\\Eloquent\\SoftDeletes; class Message extends Model { use SoftDeletes; protected $table = 'Message'; protected $fillable = [ 'user_id', 'name', 'content' ]; protected $hidden = [ 'deleted_at' ]; } 可以看到我們還多引用 SoftDeletes ，它叫軟刪除，一般來說我們在設計資料庫時，不會真正意義上的把資料刪掉，還記得我們再新增 message 跟 like 資料表時，有多了一個 softDeletes() 欄位嗎，這個欄位就是當我們刪除時，他會記錄刪除時間，但在查詢時就不會顯示這筆資料，讓使用者覺得資料已經真正刪除了，但實際上資料還是存在在資料庫中。\nlike\nuse Illuminate\\Database\\Eloquent\\Model; use Illuminate\\Database\\Eloquent\\SoftDeletes; class Like extends Model { use SoftDeletes; protected $table = 'like'; protected $fillable = [ 'message_id', 'user_id', 'created_at' ]; public $timestamps = false; } 可以看到我們還多引用 SoftDeletes ，它叫軟刪除，一般來說我們在設計資料庫時，不會真正意義上的把資料刪掉，還記得我們再新增 message 跟 like 資料表時，有多了一個 softDeletes() 欄位嗎，這個欄位就是當我們刪除時，他會記錄刪除時間，但在查詢時就不會顯示這筆資料，讓使用者覺得資料已經真正刪除了，但實際上資料還是存在在資料庫中。\nRepository 還記得我們上次把 RESTful API 要處理的邏輯都寫在 Controller 裡面嗎，我們光是一個小功能就讓整個 Controller 變得肥大，在後續維護時或是新增功能時，會導致十分不便利，因此我們要將 Repository 設計模式 給導入，那要怎麼實作呢～\n我們要先在 app 底下新增一個 Repositories 目錄，在目錄底下再新增一個 MessageRepository.php 檔案來專門放我們與資料庫拿資料的程式，讓 Controller 單純處理商業邏輯我們整個檔案分成幾段來看\n先新增這個檔案的命名空間，並將我們原先放在 MessageController 的引用給拿進來，我們 Repositories 單純處理與資料庫的交握，或是引用 Message 跟 like 的 Models。\nnamespace App\\Repositories; use App\\Models\\Message; use App\\Models\\Like; 查詢留言資料讀取\npublic static function getAllMessage() { return Message::select( 'message.id', 'message.user_id', \"users.username as name\", 'message.version', 'message.created_at', 'message.updated_at' ) -\u003eleftjoin('like', 'message.id', '=', 'like.message_id') -\u003eleftjoin('users', 'message.user_id', '=', 'users.id') -\u003eselectRaw('count(like.id) as like_count') -\u003egroupBy('id') -\u003eget() -\u003etoArray(); } public static function getMessage($id) { return Message::select( 'message.id', 'message.user_id', \"users.username as name\", 'message.version', 'message.created_at', 'message.updated_at' ) -\u003eleftjoin('like', 'message.id', '=', 'like.message_id') -\u003eleftjoin('users', 'message.user_id', '=', 'users.id') -\u003eselectRaw('count(like.id) as like_count') -\u003egroupBy('id') -\u003eget() -\u003ewhere('id', $id) -\u003efirst(); } 回傳全部的留言資料 getAllMessage()，由於我們想要顯示留言者 id，只能一個一個把我們想要的 select 出來，不能透過 model 來顯示，使用 leftjoin 來查詢，最後多一個來顯示各個文章的總數。\n回傳{id}留言資料 getMessage($id)，一樣跟回傳全部的留言資料一樣，只是多一個 where 來顯示輸出的 id 留言資料。\n新增留言資料讀取\npublic static function createMessage($id, $content) { Message::create([ 'user_id' =\u003e $id, 'content' =\u003e $content ]); } 使用 create 來新增資料，將 user_id 帶入傳值進來的 $id，content 帶入 $content。\n修改留言資料讀取\npublic static function updateMessage($id, $user_id, $content, $version) { return Message::where('version', $version) -\u003ewhere('id', $id) -\u003ewhere('user_id', $user_id) -\u003eupdate([ 'content' =\u003e $content, 'version' =\u003e $version + 1 ]); } 先使用 where 來檢查樂觀鎖 version ，在查詢此 id 是否存在，以及編輯者是否為發文者，最後用 update 來更新資料表，分別更新 user_id、content、version(樂觀鎖每次加1) 等欄位。\n按讚留言資料讀取\npublic static function likeMessage($id, $user_id) { Like::create([ 'message_id' =\u003e $id, 'user_id' =\u003e $user_id, 'created_at' =\u003e \\Carbon\\Carbon::now() ]); } 按讚我們會在 like 的資料表中來新增紀錄，所以也是使用 create 來新增，新增 message_id、user_id、created_at 。\n刪除留言資料讀取\npublic static function deleteMessage($id, $user_id) { return Message::where('id', $id) -\u003ewhere('user_id', $user_id) -\u003edelete(); } 刪除功能因為我們使用軟刪除，所以不用顧慮 FK 外鍵，所以可以直接刪除 message。\nController 到這裡我們講完 MessageRepository.php 的內容了，那原本的 Controller 剩下什麼呢 !?\nnamespace App\\Http\\Controllers; use App\\Repositories\\MessageRepository; use Illuminate\\Http\\Request; use Illuminate\\Support\\Facades\\Auth; use Illuminate\\Support\\Facades\\Validator; 我們在商業邏輯上會處理 Request 內容，以及使用 MessageRepository 來對資料庫做存取、Auth 取的登入者的資訊等等功能，所以要記得先把他引用進來歐。\n查詢留言功能\n// 查詢全部的留言 public function getAll() { return MessageRepository::getAllMessage(); } // 查詢id留言 public function get($id) { if (!$message = MessageRepository::getMessage($id)) { return response()-\u003ejson([\"message\" =\u003e \"找不到留言\"], 404); } return $message; } 由於我們 MessageRepository 都只有單純與資料庫進行交握，所有的判斷以及回傳都會在 controller 來做處理，getAll() 會使用到 MessageRepository::getAllMessage() 的查詢並回傳顯示查詢的資料。\nget(id) 會先用 MessageRepository::getMessage($id) 來檢查 id 是否存在，如果不存在就會回傳找不到留言 404 的 status code，如果存在就回傳存在變數 message 的 MessageRepository::getMessage($id) 資料。\n新增留言功能\n// 新增留言 public function create(Request $request) { $user = Auth::user(); $rules = ['content' =\u003e 'required|max:20']; $validator = Validator::make($request-\u003eall(), $rules); if ($validator-\u003efails()) { return response()-\u003ejson([\"message\" =\u003e \"沒有輸入內容或長度超過20個字元\"], 400); } MessageRepository::createMessage($user-\u003eid, $request-\u003econtent); return response()-\u003ejson([\"message\" =\u003e \"新增紀錄成功\"], 201); } 我們先使用 Auth::user() 將登入的使用者資料存在 $user 中，在檢查輸入的 request 內容是否有超過 20 的字元，如果有就回傳內容長度超過20個字元 400。接著就用 MessageRepository::createMessage 將要新增的資料帶入，最後回傳新增紀錄成功 201 。\n修改留言功能\n// 更新id留言 public function update(Request $request, $id) { $user = Auth::user(); if (!$message = MessageRepository::getMessage($id)) { return response()-\u003ejson([\"message\" =\u003e \"找不到留言\"], 404); } $rules = ['content' =\u003e 'required|max:20']; $validator = Validator::make($request-\u003eall(), $rules); if ($validator-\u003efails()) { return response()-\u003ejson([\"message\" =\u003e \"沒有輸入內容或長度超過20個字元\"], 400); } if (!MessageRepository::updateMessage($id, $user-\u003eid, $request-\u003econtent, $message['version'])) { return response()-\u003ejson([\"message\" =\u003e \"更新留言失敗\"], 400); } return response()-\u003ejson([\"message\" =\u003e \"修改成功\"], 200); } 一樣先把登入的使用者存入 $user，檢查是否有這個 id ，沒有就回傳找不到留言 404，接下來檢查輸入的內容長度，如果超過，就回傳內容長度超過20個字元 400，再檢查要修改留言的與留言者是不是同一個使用者，如果不是就回傳權限不正確 403，最後就將資料透過 MessageRepository::updateMessage 來做更新，並回傳修改成功 200。\n按讚留言功能\n// 按讚id留言 public function like($id) { $user = Auth::user(); if (!MessageRepository::getMessage($id)) { return response()-\u003ejson([\"message\" =\u003e \"找不到留言\"], 404); } MessageRepository::likeMessage($id, $user-\u003eid); return response()-\u003ejson([\"message\" =\u003e \"按讚成功\"], 200); } 一樣先把登入的使用者存入 $user，先檢查是否有這個留言，沒有就回傳找不到留言 404，接著就使用 MessageRepository::likeMessage 來記錄按讚留言，並回傳按讚成功 404。\n刪除留言功能\n// 刪除id留言 public function delete($id) { $user = Auth::user(); if (!MessageRepository::deleteMessage($id, $user-\u003eid)) { return response()-\u003ejson([\"message\" =\u003e \"找不到留言\"], 404); } return response()-\u003ejson([\"message\" =\u003e \"刪除成功,沒有返回任何內容\"], 204); } 一樣先把登入的使用者存入 $user，先檢查是否有這個留言，沒有就回傳找不到留言 404，在檢查文章權限，錯誤就回傳權限不正確 403，最後檢查是否有被按讚，有的話要先刪除 like 裡面的按讚紀錄，最後再刪除文章(所有的刪除，因為我們在 model 裡面有使用 softdelete 軟刪除，所以不會真的刪除，而是在欄位的 deleted_at 加入刪除時間，來讓查詢時以為他被刪除了！)\nPostman 測試 那我們一樣來看一下 Postman 的測試，這邊只顯示需要登入才能使用的 API。\n登入 新增留言 成功\n我們把帳號密碼放到 Body 來傳送，如果帳號密碼正確，就會顯示登入成功，並且在 Cookie 裡面的 laravel_session，可以用來判斷是否登入，以及登入的人是誰。\n新增留言 - 成功 新增留言 成功\n新增留言成功，因為 有登入，所以可以從 Cookie 裡面的 laravel_session 來驗證是否登入 會顯示新增紀錄成功以及回應 201 Created\n新增留言 - 失敗 新增留言 失敗\n新增留言失敗，因為 沒有登入，無法從 Cookie 裡面的 laravel_session 來驗證是否登入 ，所以會顯示用戶需要認證以及回應 401 Unauthorized\n修改留言 - 成功 修改留言 成功\n修改留言成功，因為 有登入，所以可以從 Cookie 裡面的 laravel_session 來驗證是否登入 ，會顯示修改成功以及回應 200 OK\n修改留言 - 失敗 - 沒有登入 修改留言 失敗\n修改留言失敗，因為 沒有登入，無法從 Cookie 裡面的 laravel_session 來驗證是否登入 ，會顯示用戶需要認證以及回應 401 Unauthorized\n修改留言 - 失敗 - 權限不足 修改留言 失敗\n修改留言失敗，雖然 有登入，但存在 Cookie 裡面的 laravel_session 不是當初的留言者 ，會顯示權限不正確以及回應 403 Forbidden\n按讚留言 - 成功 修改留言 成功\n按讚留言成功，因為 有登入，所以可以從 Cookie 裡面的 laravel_session 來驗證是否登入 ，會顯示按讚成功以及回應 200 OK\n按讚留言 - 失敗 - 沒有登入 修改留言 失敗\n按讚留言失敗，因為 沒有登入，無法從 Cookie 裡面的 laravel_session 來驗證是否登入 ，會顯示用戶需要認證以及回應 401 Unauthorized\n刪除留言 - 成功 刪除留言 成功\n刪除留言成功，因為 有登入，所以可以從 Cookie 裡面的 laravel_session 來驗證是否登入 ，不會顯示訊息但會回應 204 No Content\n刪除留言 - 失敗 - 沒有登入 刪除留言 失敗\n刪除留言失敗，因為 沒有登入，無法從 Cookie 裡面的 laravel_session 來驗證是否登入 ，會顯示用戶需要認證以及回應 401 Unauthorized\n刪除留言 - 失敗 - 權限不足 刪除留言 失敗\n刪除留言失敗，雖然 有輸入正確的 token ，但不是當初的留言者 ，會顯示權限不正確以及回應 403 Forbidden","參考資料#參考資料":"Laravel Auth 自定義user 模型目錄結構：https://www.itread01.com/content/1545012913.html\nuser ( Model )：https://ithelp.ithome.com.tw/articles/10220381\nLaravel Form Request Validation：https://laravel.com/docs/5.4/validation#form-request-validation\nlaravel Validation 驗證格式：https://ithelp.ithome.com.tw/articles/10250237"},"title":"Laravel 進階 (內建會員系統、驗證 RESTful API 是否登入、使用 Repository 設計模式)"},"/blog/php-laravel/laravel/":{"data":{"":"本文章會介紹什麼是 Laravel ，以及它有什麼特別之處，可以讓它在2015年被評為最受歡迎的 PHP 框架第一名，並說明為什麼要使用框架，最後實作一個留言板功能搭配 RESTful API 來實現 CRUD。\n就像是 Laravel 官網的大標題，『 The PHP Framework for Web Artisans 為網頁藝術家創造的 PHP 框架 』，那再了解 Laravel 之前，先來介紹一下框架是什麼：","artisan-相關指令#Artisan 相關指令":"Controller php artisan make:controller {檔案名稱}Controller //建立一個 Controller Migration php artisan make:seeder //產生seeder檔案 php artisan make:factory //產生factory 檔案 php artisan make:migration //產生migration 檔案 php artisan migrate //將這次的migration讀入資料庫建立架構 php artisan migrate:rollback //推回上一次的migration php artisan migrate:reset //推回全數的migration php artisan migrate:refresh //推回所有遷移並且再執行一次 php artisan migrate --seed //將這次的migration讀入資料庫建立架構並且也跑seeder php artisan migrate:refresh --seed //推回所有遷移並且再執行一次以及seeder php artisan db:seed //單純跑資料的seeder 填充資料 Test php artisan make:test {檔案名稱} //測試程式 想必大家到這步驟已經對 Laravel 不陌生了，已經熟悉上面的教學介紹後，可以再去學習下一篇 Laravel 進階，會有如何使用內建會員系統，以及如何判斷 RESTful API 是否登入等進階實作ㄡ～","laravel-框架#Laravel 框架":"Laravel 是基於上面所說的 MVC 架構打造的框架，並且設計出許多可以讓開發者更有效率的工具。\nArtisan：提供許多指令，讓你可以使用這些指令，來快速完成許多的任務 (有列出部分指令於 Artisan 相關指令)。 Routing 路由：管理網址與頁面的路徑指定，辨識傳入的 request 傳送至對應的 Controller，回傳指定的 View。 Blade 模板引擎 (View)：模板解析工具， 是 Laravel 所使用的模板引擎，Blade 會將 PHP 及 HTML 完整分離的工具。 Auth 認證：透過 Laravel 預設提供的認證讓 Controller 快速解決經常會用到的驗證需求，像是：使用者註冊、使用者認證、重置密碼的 e-mail 連結、重置密碼邏輯等。 Eloquent ORM 物件關聯對映 (Model)：Laravel 預設的 ORM，將資料庫的欄位映射成物件 (Ｍodel 模型)，只需要用 PHP 的語法，不需要撰寫 SQL 指令就可以用物件的方式讀取欄位資訊以及與資料庫的互動。 Migration 資料庫遷移：聽起來很像是更換資料庫，但他其實算是一種資料庫的版本控制，可以讓團隊在修改資料庫結構的同時，保持彼此的進度ㄧ致，通常會結合Schema 結構生成器一起使用，可以簡單管理資料庫結構。 小知識\nORM：英文叫 Object Relational Mapping，翻譯成中文為物件關聯對映。在網站開發結構中，是在資料庫和Model 資料容器兩者之間，簡單來說，它是可以讓開發者更簡單、安全的方式從資料庫讀取資料，因為 ORM 的特性是可以透過物件導向程式語言去操作資料庫。\n優點 安全性：可以避免 SQL injection，遇到奇怪的值，會自動擋掉。 簡化性：可以將原本 SQL Select * From users 等指令透過物件導向程式語言去操作資料庫。 通用性：因為 ORM 是程式語言和資料庫之間的關係，就算有要轉換資料庫，也比較不會遇到要修改程式的狀況。 缺點 效能：為了要達成方便性，通常都會犧牲到效能的問題，因為等於多了『把程式語言轉譯成SQL語言』這項工作。 學習曲線高：對於初學者來說，ORM 需要融合 SQL 語言以及程式語言兩種不同的概念以及語法，會比單純學 SQL 語法還較複雜。 複雜查詢維護性低：有些 SQL 語法 ORM 沒有支援，所以導致有些情況還是需要導入原生 SQL 寫法。 (資料來源：資料庫設計概念 - ORM)","php-常見框架對比#PHP 常見框架對比":"雖然本次介紹的主題是 Laravel 但我們也簡單介紹一下其他的框架優點，以及附上表格讓大家可以更清楚。\n差別 Laravel Symfony CodeIgniter CakePHP Zend 2 許可 License MIT MIT BSD BSD MIT 人氣排名 Popularity\n(2022-03 Google 搜尋熱門程度) 第一名 第二名 第三名 第四名 第五名 表現 慢 慢 快 慢 慢 模板 Blade Twig PHP 內建 PHP 資料庫 MySQL PostgreSQL SQLite SQL Server MySQL PostgreSQL SQLite SQL Server Oracle MySQL PostgreSQL SQLite SQL Server Oracle MySQL PostgreSQL SQLite SQL Server Oracle MySQL PostgreSQL SQLite SQL Server Oracle 物件關係對映 有 Doctrine 沒有內建 有 Doctrine 測試 PHPUnit PHPUnit 內建 PHPUnit PHPUnit Laravel 該框架可能是Web 開發人員中最受歡迎的框架。Laravel 是一個免費的開源PHP 框架，適用於Web 應用程序開發，且適用於移動應用程序場。(上面有介紹過，這邊就簡化)\nLaravel 框架優點\n易於學習 無縫數據遷移 在 PHP 社群中很受歡迎 支持 MVC 架構 大量學習素材 (文件、圖檔、影片教程) 模板引擎 簡單的單元測試 Symfony 該框架是一個廣泛的PHP MVC 框架，目前Symfony 已經成為一個可靠和成熟的平台框架。\nSymfony 非常穩定、文檔齊全、性能卓越。這些特點使Symfony 成為開發大型企業項目的完美選擇。\n使 Symfony 成為 PHP 框架中獨一無二的特性之一是它的可重用 PHP 組件。使用可重用組件，開發時間減少了許多模塊，如表單創建、對象配置、模板等。\n可以直接從舊組件構建，節約了大量成本。Symfony 易於在大多數平台上安裝和配置，並且可以獨立於數據庫引擎。它具有高度的靈活性，可以與 Drupal 等大型項目集成\nSymfony 框架優點\n官方長期技術支持 內置測試功能 豐富的框架內置功能 官方培訓課程和認證 CodeIgniter 該框架可能是最適合開發動態網站的PHP 框架。它是一個非常簡單的輕量級PHP 框架。\n它的大小只有2 MB 左右（包括文檔）。因此，CodeIgniter 本身俱有最小的佔用空間，它允許Web 開發人員添加第三方插件來開發更複雜的功能。\nCodeIgniter 還提供了幾個預構建的模塊，用於為Web 開發創建健壯的、可重用的組件。由於設置過程簡單，這個PHP 框架非常適合初學者。\nCodeIgniter 框架優點\nMVC 架構 Top-Notch 錯誤處理 提供卓越的性能 包中提供了幾種工具 內置安全工具 優秀的文檔 CakePHP 該框架對個人是完全免費，並提供付費的商業用途。它將幫助您開發功能豐富且視覺上令人印象深刻的網站。\nCakePHP 起初是一個簡單而優雅的工具包，在過去的15 年裡它變得更加強大。由於它的CRUD（創建、讀取、更新和刪除）框架，CakePHP 是最容易學習的框架。\n使用CakePHP 部署Web 網站是\"小菜一碟\"，只需要一個Web 服務器和CakePHP 框架的副本。\nCakePHP 框架優點\n插件和組件的簡易擴展 適當的類繼承 零配置 現代框架 支持AJAX 快速構建 內置驗證 Zend 2 該框架是一個完整的面向對象的PHP 框架。這個PHP 框架是可定制的，對於需要添加項目特定功能的開發人員來說，這是一個好處。\nZend 構建於敏捷方法之上，可幫助開發人員為大型客戶創建、高質量的Web 應用程序的框架。它非常適合複雜的企業級項目，Zend 主要關注安全性、性能和可擴展性。\nZend 框架主要受大型IT 企業和銀行等金融機構的青睞。\nZend 2 框架優點\nMVC 組件 卓越的前端技術支持工具 大型開發者社區 簡單的雲API 支持第三方組件 數據加密 支持AJAX 會話管理 ","參考資料#參考資料":"PHP與Laravel簡介：https://ithelp.ithome.com.tw/articles/10237537\n比較 PHP 網頁框架：https://opensourcedoc.com/blog/comparing-php-web-frameworks/\nLaravel 從入門到放棄 (一) – 透過 composer 架設 laravel 網站：https://christmasq.wordpress.com/2018/12/04/php-laravel-%E5%BE%9E%E5%85%A5%E9%96%80%E5%88%B0%E6%94%BE%E6%A3%84-%E4%B8%80-%E9%80%8F%E9%81%8E-composer-%E6%9E%B6%E8%A8%AD-laravel-%E7%B6%B2%E7%AB%99/\nPHP Laravel 安裝教學https://angela52799.medium.com/php-laravel-%E5%AE%89%E8%A3%9D%E6%95%99%E5%AD%B8-16fc4c2271ee\n資料庫設計概念 - ORM：https://ithelp.ithome.com.tw/articles/10207752\nLaravel 6.0 初體驗！怎麼用最新的 laravel 架網站！：https://ithelp.ithome.com.tw/articles/10213294\nLaravel Route(路由：]https://ithelp.ithome.com.tw/articles/10217731","實作#實作":" Laravel 官網\n詳細可以參考 Laravel 官網 的文件，裡面有更詳細的欄位型態說明！\n環境設定 要安裝 Laravel 框架有一些系統的要求，要先安裝 PHP ，請參考 PHP 官網的安裝教學\n安裝完後可以使用來檢查是否安裝成功\n$ php --version PHP 7.1.33 (cli) (built: Jan 20 2022 04:04:37) ( NTS ) Copyright (c) 1997-2018 The PHP Group 再安裝 Composer ，請參考 Composer 官網的安裝教學\n$ composer --version Composer version 2.2.7 都安裝好後，我們就要來安裝 Laravel 框架囉，根據 Laravel 官網安裝步驟，先用 Composer 來安裝 $ composer global require \"laravel/installer\" 安裝後一樣先來檢查是否安裝成功 $ laravel --version zsh: command not found: laravel 發現 zsh 找不到 laravel 這個命令，那我們看一下官網怎麼說 ( zsh 是我所使用的 bash ，這邊會依照安裝環境而有所不同)\nMake sure to place the $HOME/.composer/vendor/bin directory (or the equivalent directory for your OS) in your $PATH so the laravel executable can be located by your system.\n代表我們要將 Laravel 的目錄，放到 $PATH ，系統才可以使用 laravel 來執行檔案。 以 macOS 來說， Laravel installer 的目錄都會放在 $HOME/.composer/vendor/bin ，我們在根目錄建立一個 .bash_profile 的檔案來放置我們的 $PATH。\nexport PATH=\"$HOME/.composer/vendor/bin:$PATH\" 輸入上面的 PATH ，記得儲存離開後要用這個指令來更新一下檔案 $ source .bash_profile 我們再次檢查是否安裝好 Laravel\n$ laravel --version Laravel Installer 2.3.0 因為我們後續會使用到資料庫的功能，所以也可以先把他安裝好 ＭySQL 官網的安裝教學\n$ mysql -V mysql Ver 8.0.28 for macos11.6 on x86_64 (Homebrew) 本次實作の版本配置 macOS 11.6 PHP 7.1.33 MySQL 8.0.28 Composer 2.2.7 Laravel Installer 2.3.0 Laravel Framework 5.4.36 新增專案 安裝成功後，就可以開始來使用 Laravel 框架自動生成檔案囉！要怎麼做呢？就接著看下去吧\n可以使用 Laravel 來新增專案\n$ laravel new 專案名稱 也可以透過 composer 指令 (可以指定框架版本，那我們這次使用的是5.4版本)\n$ composer create-project --prefer-dist laravel/laravel 專案名稱 \"5.4.*\" 完成後就進入該專案目錄底下，接著可以執行該指令來檢查框架版本\n$ php artisan -V Laravel Framework 5.4.36 小提醒\nLaravel 框架可使用的的版本，取決於你的 PHP 版本，所以要找相對應的歐！使用 Laravel 5.4 跟 PHP 8.1 ，他會說 PHP 版本太新，無法安裝。\nLaravel 資料夾與檔案介紹 我們可以看到這個專案目錄下，Laravel 幫我們生成了許多資料夾以及檔案，接著來簡單說明每一個資料夾與檔案的功能與用途吧\napp：主要放置 controller 的地方，提供網站的應用處理流程，包括處理用戶的行為和資料 Model 上的改變等事件之類別方法，提供給 view 來呼叫。透過 Laravel 之架構也可透過 controller 輕易建構 RESTful API。 bootstrap：內含之 app.php 為將此框架初始化及建構起來之程式。 config：內含此專案網站之環境設定、資料庫設定等設定程式。 database：放置資料庫設定 (Model)。 public：放置網站入口，透過 index.php 導入 route 之設定首頁 resources：放置 view 的資源，用以呈現網站頁面 routes：放置所有專案 route 的設定，其中較常用之 web.php 做為主要頁面導向及與 controller 的溝通，而 api.php 則作為專案提供 api 的設定。 storage：主要放置些程式生成的檔案如頁面樣板 (Blade templates)、系統記錄 (logs)、file caches、file based sessions 等資料。 tests：放置測試用的案例 (Test Case)。 vendor：放置透過 composer 下載管理的套件。 artisan：可於專案中透過 php artisan 執行 Laravel 設計好的基本操作行為，如建立 controller 等動作。 composer.json：管理專案使用套件，詳細用法見 composer 官方說明。 composer.lock：鎖定專案使用套件版號。 package.json：管理 npm 套件，主要提供給前端使用。 phpunit.xml：phpunit 測試設定檔，可規範執行單元測試之範圍，做批量測試。 webpack.mix.js：build 設定檔，幫助我們將 resources 中之 js 及 sass 等前端設定檔 compile 成 js、css 等檔案並 deploy 到 public 中供頁面使用。 建立第一個 Laravel 網頁 文章參考來源\n下面教學流程是參考ReccaChao Laravel 6.0 初體驗！怎麼用最新的 laravel 架網站！ 的文章下去學習以及介紹，在學習過程中再加上自己的理解以及不同範例的筆記來分享給大家，大家也可以先去瀏覽大大的文章，再回來呦XD\n我們經過千辛萬苦，將 Laravel 給安裝好，也設定好它所需的環境參數，打開 IED 準備開始學習如何建立第一個 Laravel 網頁吧！\n在開始說明前，還要先下一個指令。在介紹 Laravel 專案下的資料夾以及檔案中，其中一個 “Artisan”，是 Laravel 專用的指令工具，可以幫我們處理很多事情。\n像是我們現在要啟動內建的伺服器來瀏覽我們的網站，這時我們要下\n$ poser global requirephp artisan serve Laravel development server started: \u003chttp://127.0.0.1:8000\u003e [Thu Mar 3 15:13:07 2022] 127.0.0.1:52345 [200]: /favicon.ico 這個伺服器會在這個終端機下被建立，屬於這個專案的伺服器，如果想要關閉，可以使用 Ctrl + C 來關閉。 小提醒\n由於此伺服器是專屬於該專案的伺服器，所以下指令時，也要在該專案的目錄下歐！\n如果想要使用特定的 Port ，可以在後面加入此參數，就可以依照設定的 Port 來當作我們伺服器的位子囉！(預設是8000 Port)\n$ php artisan serve --port=7777 Laravel development server started: \u003chttp://127.0.0.1:7777\u003e [Thu Mar 3 15:45:23 2022] 127.0.0.1:54206 [200]: /favicon.ico 維護模式 當我們想要修改系統或是資料庫的欄位時，就可以將該網站設定成維修模式。Laravel 讓這個工作變得很容易，只需要用以下兩個指令來控制\n啟動維護模式\n$ php artisan down --message=\"Upgrading Database\" --retry=60 還可以設定一些參數， --message 是用來顯示或紀錄客製化訊息， --retry 用來當作HTTP head 的 Retry-After 的值 Laravel 維護模式顯示頁面\n關閉維護模式\n$ php artisan up 維護模式顯示模板\n維護模式顯示的預設模板放置在 resources/views/errors/503.blade.php。你可以自由的根據需求修改。\n#### Routes 路由 我們要來建立我的第一個網頁，網頁的其中一個關鍵就是路徑。那我們網站的路徑，在 Laravel 裡面，都放置在 routes/ 裡面，打開後可以看到4個檔案有\nweb.php：這個和我們現在要說明的路徑有關，我們在瀏覽器打的網址，前面是 domain name，那在 domain 之後的字串，會在我們這個檔案裡面來定義哪些字串要導向哪一個流程或是檔案，範例 url:{domain}/hello-world。 api.php：我們在做前後端分離的專案時會使用這個檔案，與 web.php 功能差不多，預設用來管理 API 的路徑。 channels.php：和 Broadcast 有關係，這個是 Laravel 的廣播功能，不太常使用，所以先跳過，之後有碰到再來說明。 console.php：這個和我們指令有關，像是我們上面介紹的 php artisan ，這個檔案就是和這個部分有關。 web.php 那我們就先來針對 web.php 來做介紹，我們剛剛使用指令開啟的伺服器網址是 http://127.0.0.1:8000，使用瀏覽器進入後，可以看到下面這個 Laravel 預設的頁面(此為 Laravel 5.4版本，新版的好像不太一樣)\nLaravel 預設首頁\n那我們開啟 routes/web.php 檔案中，可以看到下面這一段預設的路由\nRoute::get('/', function () { return view('welcome'); }); 這段的意思就是代表，如果我們網址沒有輸入其他的目錄，就會顯示 welcome 這個網頁，那 welcome 這個顯示的畫面，就放在 resources/view 底下名為 welcome.blade.php 的檔案中，打開來看就是簡單的 HTML。\n那現在我們了解路由的含義，現在換我們來實作一個看看，首先我們希望網址列輸入 https://127.0.0.1:8000/hello-world 可以出現 hello world 的字樣，那我們可以到 web.php 中，新增下面這個路由\nRoute::get('/hello-world', function () { return view('hello-world'); }); 再到到 resources/view 新增 hello-world.blade.php 檔案，簡單輸入 HTML 格式，如下\n\u003c!DOCTYPE html\u003e \u003chtml lang=\"en\"\u003e \u003chead\u003e \u003cmeta charset=\"UTF-8\"\u003e \u003cmeta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\"\u003e \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"\u003e \u003ctitle\u003eDocument\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003ch1\u003e hello ~ pinyi \u003c/h1\u003e \u003c/body\u003e \u003c/html\u003e 使用瀏覽器在網址列輸入 http://127.0.0.1/hello-world ，就可以看到顯示 hello ~ pinyi 的字樣囉～\n除了回傳網頁來做顯示以外，也可以單獨回傳字串，並帶入參數歐，我們來試試看要怎麼做。\n首先我們先將路由修改成下方，讓我們在 hello-world 後面可以輸入我們的名稱，並且顯示在畫面上\nRoute::get('/hello-world/{name}', function ($name) { return 'hello ~ ' . $name; }); 我們在網址列輸入 http://127.0.0.1/hello-world/pinyi 就會顯示 hello ~ pinyi ，很神奇吧，只要在括號中 {} 輸入你要的參數，並且後面的 function 也要帶入參數，就可以將資料給帶進去了 !\napi.php 我們可以看到 api.php 檔案也是使用 Route 來運作，如下\nRoute::middleware('auth:api')-\u003eget('/user', function (Request $request) { return $request-\u003euser(); }); middleware 就是一種過濾或是防火牆的概念，這個我們後續會再說明。\n我們也在此新增一個跟上面一樣的 hello-world，如下\nRoute::get('/hello-world', function () { return 'Hello World ~'; }); 在 api.php 比較特別的部分是需要在字串前面加入 api ，因為 api.php 就是預設讓我們來放 api 的地方，如範例 url:{domain}/api/hello-world，那就會回傳 hello world ~\nView Blade 模板引擎 我們已經知道要怎麼透過路由，讓使用者輸入網址後，根據網址指向想要呈現的畫面，那我們的畫面要怎麼呈現呢 !?\n上面有使用簡單的 HTML 來做說明，那本章節就來介紹怎麼修改我們的前端輸出吧。\nLayout 我們在設計網頁時，通常都會使用樣板，讓網頁相同的地方只做一次就可以顯示，不需要每一個頁面都重複寫相同的程式碼，只需要在想要顯示的地方來顯示各自的內容就好\n如果聽不太懂，我以我的 Blog 來舉例，紅色框框，在每一頁都會顯示，我們可以把他寫成一個檔案，就不需要每頁都重複去寫，藍色框框，會依照每一頁的內容來做顯示。\nPinYi 小天地\n那我們可以透過 Laravel 的 Blade Engine 輕鬆地達到這件事情，首先，到 resources/views/ 目錄下建立一個叫 layouts 的資料夾。這個資料夾之後會讓我們來放不同的模板\n在 layouts 下建立一個 header.blade.php 檔案，內容如下\n\u003c!DOCTYPE html\u003e \u003chtml lang=\"en\"\u003e \u003chead\u003e \u003cmeta charset=\"UTF-8\"\u003e \u003cmeta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\"\u003e \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"\u003e \u003ctitle\u003eDocument\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003ch1\u003e測試網站，我是標題\u003c/h1\u003e \u003cdiv class=\"container\"\u003e @yield('content') \u003c/div\u003e \u003c/body\u003e \u003c/html\u003e 可以看到我們把 “測試網站，我是標題”，看成是一個 header，然後在 container 裡面使用了 @yield() 的語法，這個語法代表我們可以在其他的網頁 call 這個 header 來做顯示，則 container 就會依照該網頁想要呈現的內容來做顯示，我們再繼續看下去\n我們在 resources/views/ 下，新增一個 test.blade.php 的檔案，內容如下\n@extends('layouts.header') @section('content') \u003cp\u003e今天天氣很好歐！\u003c/p\u003e @endsection 這邊簡單說明一下這個檔案裡面的意思，\n@extends('layouts.header') 是指繼承 layouts 這個資料夾底下的 header.blade.php 檔案內容，\n@section('content') 是指我們剛剛在 header.blade.php 檔案裡面所下的 @yield(content) 的內容。\n我們再到 route/web.php 來設定路由，加入這一段程式碼\nRoute::get('/test', function () { return view('test'); }); 我們在去瀏覽 http://127.0.0.1:8000/test ，就可以看到成品囉！這代表我們成功將 layouts/header.blade.php 的模板放到 test.blade.php 裡面囉。\nPinYi 小天地\n當然，我們也可以透過 route 將後端的資料傳到前端，我們先到 routes/web.php 底下修改成\nRoute::get('/test/{name}', function ($name) { return view('test', ['name' =\u003e $name]); }); 再到， resources/views/test.blade.php 修改成以下\n@extends('layouts.header') @section('content') \u003cp\u003e今天天氣很好歐！ {{$name}}\u003c/p\u003e @endsection 就可以看到我們可以輸入 http://127.0.0.1:8000/test/pinyi ，網頁前端就會顯示\nPinYi 小天地\n我們就成功將後端的資料送至前端！ 要注意 resources/views/test.blade.php 底下需要用兩個 {{}} 來括住參數呦，只有一個 {} 他會當成一般的字串來顯示呦～\nController 控制器 除了剛剛在 routes 檔案中定義所有請求以及處理外，我們在使用 Laravel 時，通常會使用 Controller 來處理邏輯的請求，我們來看一下實際案例吧 !\n題目：我們有一個冷知識產生器，依照我們輸入的 id 不同，會顯示不同的冷知識，那我們要怎麼做呢 !?\n首先我們先到 routes 設定好我們的路由 http://127.0.0.1/trivia/(輸入的id)\nRoute::get('/trivia/{id}', 'TriviaController@trivia'); 前面是我們的路由，後面則是我們要對應的 Controller 位置\n接下來使用 artisan 指令來建立這個 Controller\n$ php artisan make:controller TriviaController Controller created successfully. Laravel 會自動產生一個 TriviaController.php 的檔案，位置在 app/Http/Controllers 底下\nController 產生檔案位置\n接著我們來修改 TriviaController.php ，加入我們在 routes 宣告的函式 trivia，並把剛剛在 routes 所帶的參數 id 給帶入\n\u003c?php namespace App\\Http\\Controllers; use Illuminate\\Http\\Request; class TriviaController extends Controller { public function trivia($id){ return $id; } } 使用瀏覽器輸入 http://127.0.0.1/trivia/{任意數字} 來檢查是否有將 routes 跟 controller 串再一起。\n正常的話，會看到畫面顯示你所輸入的 {任意數字} (當然因為沒有限制輸入，所以輸入任何字元都可以 XD)\n接下來，依照我們的題目，我們輸入不同的 id ，要顯示不同的冷知識，那我們要在哪去處理這個邏輯呢 !?\n這個問題，網路上大家看法都不一樣，我針對我學習的以及 ReccaChao Laravel 6.0 初體驗！怎麼用最新的 laravel 架網站！ 的看法，將邏輯都寫在 Services 裡面在由 controller 來做控制，這樣可以把每一個元件的職責都拆分出來。\n那我們就要先在 app 底下建立一個 Services 的資料夾，來存放我們的服務。再建立一個這次題目的邏輯存放的位置 TriviaService.php\n\u003c?php namespace App\\Services; /** * Class TriviaService */ class TriviaService { /** * @return string */ public function trivia($id) { $arr = [ '李小龍的動作非常快，快到看不清，所以拍電影時只好放慢膠片的速度。', '流沙一般都不深，根本不像電影里的那樣，所以不用擔心，除非使面部被流沙掩埋，不然根本不會有事。', '最早被打上條形碼的產品是箭牌口香糖。', '在菲律賓溜溜球曾被作為武器。', '打噴嚏時無法睜着眼睛。', ]; return $arr[$id]; } } 以及修改 controller.php 檔案內容\n\u003c?php namespace App\\Http\\Controllers; use Illuminate\\Http\\Request; use App\\Services\\TriviaService; class TriviaController extends Controller { public function trivia($id){ return (new TriviaService())-\u003etrivia($id); } } 記得要 use App\\Services\\TriviaService; ，下面才可以 Call TriviaService() 。\n這時候我們再去測試一下，就可以發現，網頁會依照我們所輸入的 id，而顯示不同的冷知識。 這時候如果輸入非數字或是超過陣列的值，就會出現錯誤！\n依照輸入不同的 id 顯示的冷知識\nDatabase 資料庫 當資料量越來越大，我們不會將要顯示的內容都寫在程式碼裡面，所以我們需要建立資料庫，來存放我們的資料，在 PHP 常見框架對比 部分有提到 Laravel 支援 MySQL、PostgreSQL、SQLite 、SQL Server 資料庫，本次介紹使用 MySQL 來做說明。\n首先我們要先確認資料庫是否與 PHP 有所連結，我們先打開 .env 這個隱藏檔來看，大約在第8行有一段關於資料庫的敘述\nDB_CONNECTION=mysql DB_HOST=127.0.0.1 DB_PORT=3306 DB_DATABASE=homestead DB_USERNAME=homestead DB_PASSWORD=secret 由於我們本次是以 Mysql 來做說明，所以 DB_CONNECTION 不需要改，DB_HOST 以及 DB_PORT 輸入你要連線的資料庫主機以及 Port 號，DB_DATABASE 輸入想要連線的資料庫名稱，以及你的 DB_USERNAME、DB_PASSWORD 帳號及密碼，都完成後，我們來看一下 database 資料夾下有哪些東西吧\nmigrations：Laravel 用程式碼來代替 SQL，讓我們資料庫有版本控制的功能 factories：他控制整個資料被填充的過程，例如：user 資料應該如何被產生？之後我們在 seeds 可以使用 factory()這個方法 seeds：裡面放置產生假資料的檔案。 小提醒\n在學習時，有看到大神文章底下有人留言說 DB_USERNAME、DB_PASSWORD 不可以有 # ，會導致無法運作。(目前沒有測試過，大家可以再多加留意)\n先大概了解這三個東西所存放的東西，後續會一一介紹到！\nMigration 資料庫遷移 在過去開發時，會有一個很頭痛的一件事情，就是要怎麼樣同步各地方的資料庫格式。如果有一個新進工程師要開始開發，只能請別的工程師匯出自己在開發中的資料庫，十分的麻煩，而且如果要修改格式，還需要告知每一個在開發的工程師。\n為了解決這個問題，Laravel 框架提供了一個新的功能：資料庫遷移(Migration)。他是利用程式碼來修改資料庫格式，所以資料庫格式和其他的程式碼都使用版本控制內。這麼一來，每次有新的工程師要建構開發環境時，只需要提醒他在取得程式碼的時後，記得跑一下 migration 就可以囉～\n我們來實作看看吧！我們要下 php artisan migrate 指令來把這次的 migration 讀入資料庫並建立架構，詳細的指令介紹，放在 Artisan 相關指令 \u003e Migration。\n執行後發現 …. 竟然有錯誤\nMySQL 8.x 版本導致錯誤\n在網路上找了一下資料，好像是Mysql 8.x 版本才會出現的問題，發現需要在我們的 config\\database.php mysql 裡面多加一個標籤 modes ，就可以正常運作了！\nLaravel Mysql 8.x 錯誤文章\nLaravel mysql migrate error\n'mysql' =\u003e [ 'driver' =\u003e 'mysql', 'host' =\u003e env('DB_HOST', '127.0.0.1'), 'port' =\u003e env('DB_PORT', '3306'), 'database' =\u003e env('DB_DATABASE', 'forge'), 'username' =\u003e env('DB_USERNAME', 'forge'), 'password' =\u003e env('DB_PASSWORD', ''), 'unix_socket' =\u003e env('DB_SOCKET', ''), 'charset' =\u003e 'utf8mb4', 'collation' =\u003e 'utf8mb4_unicode_ci', 'prefix' =\u003e '', 'strict' =\u003e true, 'engine' =\u003e null, 'modes' =\u003e [ 'ONLY_FULL_GROUP_BY', 'STRICT_TRANS_TABLES', 'NO_ZERO_IN_DATE', 'NO_ZERO_DATE', 'ERROR_FOR_DIVISION_BY_ZERO', 'NO_ENGINE_SUBSTITUTION', ], ], 加入 modes 修復成功\n使用 migration 後的資料庫\n建立屬於自己的 migration 後續實作題目\n我們會建立一個留言板的資料表，並且透過 RESTful API 的方式去查詢、新增、修改、刪除。\n安裝好之後，我們來建立屬於我們的 migration ，來改變現有的資料庫格式。老樣子，一樣使用 artisan 來產生一個 migration create_messages_table 。\n$ php artisan make:migration create_messages_table Created Migration: 2022_03_07_054858_create_messages_table 這樣子就建立好了，那我們來看一下這個檔案裡面寫了什麼吧\n\u003c?php use Illuminate\\Support\\Facades\\Schema; use Illuminate\\Database\\Schema\\Blueprint; use Illuminate\\Database\\Migrations\\Migration; class CreateMessagesTable extends Migration { /** * Run the migrations. * * @return void */ public function up() { // } /** * Reverse the migrations. * * @return void */ public function down() { // } } 可以看到裡面有一個 up 跟 down 的函式，這兩個代表什麼意思呢？\n我們執行 migrate 的時候，會呼叫到 up() 函式的內容，如果使用到 migrate:rollback 回溯時，就會呼叫 down() 函式的內容來還原 migrate 所做的事情。\n所以每一個 migration 裡面的 down() 都必須要能復原 up() 所做的事情\n因為我們還沒有新增 migrate ，所以回溯機制等等再說明。migrate 可以把他理解新增資料表，那當我們修要修改資料表，就可以使用 migrate:rollback 來回溯歐！\nmigrate：可以把他理解成將程式語言轉成 SQL 的功能 我先來實際操作看看，我們先在剛剛的 {日期時間省略}_create_messages_table ，裡面改成下方\n\u003c?php use Illuminate\\Support\\Facades\\Schema; use Illuminate\\Database\\Schema\\Blueprint; use Illuminate\\Database\\Migrations\\Migration; class CreateMessagesTable extends Migration { public function up() { Schema::create('messages', function (Blueprint $table) { $table-\u003ebigIncrements('id'); //留言板編號 $table-\u003estring('name', 20); //留言板姓名 $table-\u003estring('content'); //留言板內容 $table-\u003etimestamps(); //留言板建立以及編輯的時間 }); } public function down() { Schema::dropIfExists('messages'); } } up() 裡面的是透過 Schema 結構生成器，產生資料表。(Schema 後續有比較詳細的說明)\ndown() 是指如果進行 migrate:rollback 回溯時會刪除 messages 這個資料表 。\n新增資料表 我們下 migrate 指令來將 {日期時間省略}_create_messages_table.php 執行 up() 的部分將程式轉成新增資料表，並下指令來檢查一下，是否與我們設定的 Schema 相同。\n$ php artisan migrate Migrating: 2022_03_07_054858_create_messages_table Migrated: 2022_03_07_054858_create_messages_table messages 欄位\n很神奇吧，透過程式就可以直接生成資料表的欄位，這也就是為什麼會說 migration 可以透過版本控制來讓新工程師建構環境的原因呦，除了生成以外，也可以透過 migrate:rollback 回溯來刪除資料表，那接著我們來看看要怎麼去刪除資料表吧！\n阿對了，也可以使用 $ php artisan migrate:status 來查詢目前的檔案的執行狀態，檢查一下是否運作中。\nMigration 是否運作中\n修改資料表 我們下 migrate:rollback 指令來將 {日期時間省略}_create_messages_table.php 執行 down() 的部分將程式轉成刪除資料表，並下指令來檢查一下，是否與我們設定的 Schema 相同。\n$ php artisan migrate:rollback Rolling back: 2022_03_07_054858_create_messages_table Rolled back: 2022_03_07_054858_create_messages_table 這邊就是回溯來刪除資料表，那接著我們來看一下目前資料表的狀態吧！\nMigration 是否運作中\n這時我們想要修改一下資料表的欄位，想要將 message 型態從 varchar(255) 改成 varchar(20) ，以及在 message 後面加入一個 version int(1) 且預設值為 0 的欄位，這時我們只需要修改 {日期時間省略}_create_messages_table.php ，在執行 php artisan migrate 就可以囉！\n\u003c?php use Illuminate\\Support\\Facades\\Schema; use Illuminate\\Database\\Schema\\Blueprint; use Illuminate\\Database\\Migrations\\Migration; class CreateMessagesTable extends Migration { public function up() { Schema::create('messages', function (Blueprint $table) { $table-\u003ebigIncrements('id'); //留言板編號 $table-\u003estring('name', 20); //留言板姓名 $table-\u003estring('contect',20); //留言板內容 $table-\u003einteger('version')-\u003edefault(0); //留言板版本 $table-\u003etimestamps(); //留言板建立以及編輯的時間 }); } public function down() { Schema::dropIfExists('messages'); } } $ php artisan migrate Migrating: 2022_03_07_054858_create_messages_table Migrated: 2022_03_07_054858_create_messages_table 成功後，我們一樣來檢查一下，是否與我們設定的 Schema 相同。\n修改後 message_board 欄位\nSchema 結構生成器 前面我們使用 Schema 結構生成器，來產生能轉成 SQL 讀懂的程式語言，像是 $table-\u003estring('name', 20) 或是 $table-\u003etimestamps() ，如果對於 SQL 有基本的了解，大致上也可以猜的出來他在做什麼動作，那我也將常用到的資料型態使用表格列出來，讓大家可以有對照。\nLaravel 官網\n詳細可以參考 Laravel 官方網站\u003e資料庫\u003e資料表\u003e創建欄位 的文件，裡面有更詳細的欄位型態說明！\n加入欄位 更新現有的資料表，可使用 Schma::table 方法：\nSchema::table('users', function($table) { $table-\u003estring('email'); }); 指令 功能描述 $table-\u003ebigIncrements('id'); ID 自動累加 $table-\u003eboolean('confirmed'); 相當於 Boolean 型態 $table-\u003echar('name', 4); 相當於 Char 型態，長度為 4 $table-\u003edateTime('created_at');\t相當於 datetime 型態 $table-\u003efloat('amount');\t相當於 flaot 型態 $table-\u003einteger('votes'); 相當於 Integer 型態 $table-\u003estring('name', 100);\t相當於 string 型態，長度為 100 $table-\u003etimestamps(); 加入 created_at 和 updated_at 欄位 -\u003enullable() 允許此欄位為 NULL -\u003edefault($value) 宣告此欄位的預設值 修改欄位名稱 要修改欄位名稱，可在結構生成器內使用 renameColumn 方法，請確認在修改前 composer.json 檔案內已經加入 doctrine/dbal。\nSchema::table('users', function($table) { $table-\u003erenameColumn('from', 'to'); }); 移除欄位 要移除欄位，可在結構生成器內使用 dropColumn 方法，請確認在移除前 composer.json 檔案內已經加入 doctrine/dbal。\nSchema::table('users', function($table) { $table-\u003e dropColumn('from'); }); 檢查是否存在 您可以輕鬆的檢查資料表或欄位是否存在，使用 hasTable 和 hasColumn 方法：\n檢查資料表是否存在\nif (Schema::hasTable('users')) { // } 檢查欄位是否存在\nif (Schema::hasColumn('users', 'email')) { // } 加入索引 結構生成器支援多種索引類型，有兩種方法可以加入，可以在定義欄位時直接加上去，或者是分開另外加入：\n$table-\u003estring('email')-\u003eunique();\n或是分開另外加入：\n指令 功能描述 $table-\u003eprimary('id'); 加入主鍵 $table-\u003eprimary(array('first', 'last')); 加入複合鍵 $table-\u003eunique('email'); 加入唯一索引 $table-\u003eindex('state'); 加入基本索引 Eloquent ORM 存取資料庫內容 我們在前面，已經學會了如何建立資料庫以及修改資料表欄位，還記得我們題目是要寫一個 RESTful API 來實現 CRUD ，那我們現在就要針對如何去存取資料庫內容來做介紹！\n在 Laravel 中其中一個核心的物件：Eloquent Model ，他是負責存取資料庫的一組類別，實做了 Active Record 的架構。\n那我們沿用先前的資料表來進行存取資料庫的實作，建立一個 Message 物件。\n$ php artisan make:model Message Model created successfully. 他會放在 app/ 底下，我們查看 Message.php\n\u003c?php namespace App; use Illuminate\\Database\\Eloquent\\Model; class Message extends Model { // } 我們創建這個 model 的目的是要讓 Laravel 知道 messages 資料表中哪些欄位可以使用，所以我們可以參考官方預設 User 物件，加入 $fillabel 這個變數來存放可用欄位。\n\u003c?php namespace App; use Illuminate\\Database\\Eloquent\\Model; class Message extends Model { protected $fillable = [ 'name','content','created_at','updated_at' ]; } 我們讓 Laravel 可以使用 name、contect、created_at、updated_at 的四個欄位。\n我們先隨意新增一筆資料來做顯示的測試資料。\nINSERT INTO `messages` (`id`, `name`, `content`, `version`, `created_at`, `updated_at`) VALUES (NULL, 'test', 'hello world', '0', '2022-03-07 14:15:22', '2022-03-07 14:15:22'); 這次我們先看看是否能正確的讀取資料庫，我們就先不透過 Controller，直接在 route 來做設定。\nRoute::get('/messages', function(){ return App\\Message::all(); }); 當你瀏覽 http://127.0.0.1:8000/api/messages ， 應該會出現以下資訊（資料依據你輸入的會不一樣）\n[ { \"id\": 1, \"name\": \"test\", \"content\": \"hello world\", \"version\": 0, \"created_at\": \"2022-03-07 14:15:22\", \"updated_at\": \"2022-03-07 14:15:22\" } ] 到這裡，代表我們已經可以成功讀到資料庫內的資料了！\nSeeder 我們在開發時，會常常需要進行測試，那在 Laravel 中，有一個功能是 seeder ，他可以幫我們產生資料，讓我們在後續的開發和測試更順利。\n建立一個 seeder 我們先來建立一個 Message 的 seeder\n$ php artisan make:seeder MessageTableSeeder Seeder created successfully. 他會被放到 seeds 目錄下，照慣例我們打開來看一下內容是什麼\n\u003c?php use Illuminate\\Database\\Seeder; class MessageTableSeeder extends Seeder { /** * Run the database seeds. * * @return void */ public function run() { // } } 我們在 run() 裡面，加入可以新增資料的程式碼\n\u003c?php use Illuminate\\Database\\Seeder; use App\\Message; class MessageTableSeeder extends Seeder { /** * Run the database seeds. * * @return void */ public function run() { $Message = new Message; $Message-\u003ename = 'test'; $Message-\u003econtent = 'hello world2'; $Message-\u003ecreated_at = date(\"Y-m-d h:i:s\"); $Message-\u003esave(); } } 記得要 use App\\Message; ，加入後，使用 php artisan db:seed --class=MessageTableSeeder 指令，檢查是否自動新增成功\nmessages 資料\nRESTful API 實現 CRUD 接下來，就來到我們本次的重點，要使用 Laravel 來開發出留言板功能，搭配使用 RESTful API 來實現 CRUD\n老樣子，我們先來設定我們的 route 路由，這次實作的是留言板，所以我們希望網址是 http://127.0.0.1:8000/api/messages，因為這次是 API，先到 routes/api.php 底下來設定 \u003c?php use Illuminate\\Http\\Request; /* |-------------------------------------------------------------------------- | API Routes |-------------------------------------------------------------------------- | | Here is where you can register API routes for your application. These | routes are loaded by the RouteServiceProvider within a group which | is assigned the \"api\" middleware group. Enjoy building your API! | */ Route::get('messages', 'MessageController@getAllMessages'); // 查詢全部留言 Route::get('messages/{id}', 'MessageController@getMessages'); // 查詢 {id} 留言 Route::post('messages', 'MessageController@createMessages'); // 新增留言 Route::put('messages/{id}', 'MessageController@updateMessages'); // 修改 {id} 留言 Route::delete('messages/{id}', 'MessageController@deleteMessages'); //刪除 {id} 留言 已經忘記的，可以先回到 Controller 控制器 複習一下上面的路由是什麼意思。\n接下來，我們使用 建立屬於自己的 migration 來生成我們本次要使用的資料表，再來新增一個與 Eloquent Model 相同的物件，選擇我們要的資料表\n\u003c?php namespace App; use Illuminate\\Database\\Eloquent\\Model; class Message extends Model { protected $table = 'Messages'; protected $fillable = [ 'name','content','created_at','updated_at' ]; } 都好了我們來撰寫我們的 controller，我會依照每一個功能(CRUD)來做說明，要記得加入剛剛的 model use App\\Message;。\n查詢全部留言 class MessageController extends Controller { // 查詢全部留言 public function getAllMessages() { $messages = Message::get(['id','name','content','created_at','updated_at'])-\u003etoJson(JSON_PRETTY_PRINT); return response($messages, 200); } } 會依照 model 選擇的資料表來做查詢，只顯示 id、name、content 、created_at 、updated_at 五個欄位，並把他用 JSON 表示，最後用 response() 來回應 status code。\n查詢{id}留言 class MessageController extends Controller { // 查詢{id}留言 public function getMessages($id) { if (Message::where('id',$id)-\u003eexists()) { $messages = Message::where('id',$id)-\u003eget(['id','name','content','created_at','updated_at'])-\u003etoJson(JSON_PRETTY_PRINT); return response($messages, 200); } else { return response()-\u003ejson([\"messages\" =\u003e \"找不到訊息\"], 404); } } } 先判斷輸入的 id 是否存在，如果存在才顯示 id、name、content 、created_at 、updated_at 五個欄位，並把他用 JSON 表示，最後用 response() 來回應 status code，如果不存在，直接用 JSON 表示 response() 404錯誤訊息。\n新增留言 class MessageController extends Controller { // 查詢{id}留言 public function createMessages(Request $request) { if (strlen($request-\u003econtent)\u003c20){ $messages = new Message; $messages-\u003ename = $request-\u003ename; $messages-\u003econtent = $request-\u003econtent; $messages-\u003eupdated_at = NULL; $messages-\u003esave(); return response()-\u003ejson([\"messages\" =\u003e \"新增紀錄成功\"], 201); } if (strlen($request-\u003econtent)\u003e20){ return response()-\u003ejson([\"messages\" =\u003e \"內容長度超過20個字元\"], 400); } } } 我們希望輸入的內容不要超過20的字，所以我們先用 strlen() 來判斷輸入長度，如果小於20，就將 request 輸入的名字及內容輸入，用 save() 來儲存，並把他用 JSON 表示，最後用 response() 來回應 status code，如果小於20，直接用 JSON 表示 response() 400錯誤訊息。\n修改留言 class MessageController extends Controller { // 修改{id}留言 public function updateMessages(Request $request,$id) { if (Message::where('id',$id)-\u003eexists() \u0026\u0026 Message::where('version',0)-\u003elockForUpdate() ) { if (strlen($request-\u003econtent)\u003c20){ $messages = Message::find($id); $messages-\u003ename = is_null($request-\u003ename) ? $messages-\u003ename : $request-\u003ename; $messages-\u003econtent = is_null($request-\u003econtent) ? $messages-\u003econtent : $request-\u003econtent; $messages-\u003eversion = '1'; $messages-\u003esave(); return response()-\u003ejson([\"message\" =\u003e \"修改成功\"],200); } if (strlen($request-\u003econtent)\u003e20){ return response()-\u003ejson([\"messages\" =\u003e \"內容長度超過20個字元\"], 400); } } else { return response()-\u003ejson([\"message\" =\u003e \"找不到訊息\"],404); } } 先判斷輸入的 id 是否存在以及是否有被上鎖，在判斷輸入的長度，如果小於20，再判斷是否有輸入，有就將 request 輸入的值輸入，用 save() 來儲存，並把他用 JSON 表示，最後用 response() 來回應 status code，如果小於20，直接用 JSON 表示 response() 400錯誤訊息。\n刪除留言 class MessageController extends Controller { // 刪除{id}留言 public function deleteMessages($id) { if (Message::where('id',$id)-\u003eexists()) { $messages = Message::find($id); $messages-\u003edelete(); return response()-\u003ejson([\"message\" =\u003e \"刪除成功,沒有返回任何內容\"],204); } else { return response()-\u003ejson([\"message\" =\u003e \"找不到訊息\"],404); } } } 先判斷輸入的 id 是否存在，如果存在，就用 delete() 來刪除，最後用 response() 來回應 status code，如果不存在，直接用 JSON 表示 response() 404錯誤訊息。\nPostman 測試 查詢全部留言 - 無資料 查詢全部留言 無資料\n查詢全部留言，因為目前沒有資料，所以會顯示空陣列以及回應 200 OK\n查詢全部留言 - 成功 查詢全部留言 成功\n查詢全部留言成功，會顯示全部資料以及回應 200 OK\n查詢{id}留言 - 成功 查詢{id}留言 成功\n查詢{id}留言成功，會顯示{id}資料以及回應 200 OK\n新增留言 - 成功 新增留言 成功\n新增留言成功，會顯示新增紀錄成功以及回應 201 Created\n新增留言 - 失敗 新增留言 失敗\n新增留言失敗，因為超出20個字元，所以會顯示內容長度超過20個字元以及回應 400 Bad Request\n修改留言 - 成功 修改留言 成功\n修改留言成功，會顯示修改成功以及回應 200 OK\n修改留言 - 失敗 修改留言 失敗\n修改留言失敗，會顯示找不到訊息以及回應 404 Not Found\n刪除留言 - 成功 刪除留言 成功\n刪除留言成功，不會顯示訊息但會回應 204 No Content\n刪除留言 - 失敗 刪除留言 失敗\n刪除留言失敗，會顯示找不到訊息以及回應 404 Not Found","框架是什麼#框架是什麼":" 框架 (Framework) 是一個被設計用來完成特定任務的規範，程式開發人員必須遵從這個規則來開發。 目前大多數的框架都是參考 MVC 架構的概念來做設計，原因是在早期開發網頁時，都是直接將 HTML 以及 PHP 混合再一起的方式來編寫，雖然開發上很方便，但在後面維護或是新增功能上，會十分不便利，例如：單純要修改網站畫面的元件時，還需要從混雜的程式碼中找到要修改的元件，也很容易不小心修改到其他的功能。 MVC 於是有人想到把這些各自的任務區分開來，MVC 架構分別代表的是 M (Model) : 屬於資料的部分，可能是商用邏輯或是資料庫的存取。 V (View)：屬於顯示的部分，像是 HTML、CSS 等 C (Controllor)：會針對請求做出回應或是處理，例如從 Model 取出資料，並顯示在 View 上面 MVC 架構 一起走向MVC(上)","測試#測試":"寫完程式後，除了自己一個一個去測試，也可以寫成程式來測試是否有錯誤。會在此談到測試，是因為自動測試變得簡單也是 Laravel 這個框架的一個重要特性。\n那我先來說明一下什麼是自動測試\n簡單來說，常見的測試，就是我們在完成新功能後，會一個一個實際去測試我們寫的功能，是不是跟我們預想的那樣子運行。 一開始網站還小，功能不多時，還可以應付。但當網站越來越龐大，功能越來越多後，或是要使用不同權限帳號來檢查所有功能，會很辛苦的。 幸好，我們可以用自動測試的程式來解決這些瑣事！\n如果我們每次為每個功能撰寫程式時，都寫另一段小程式來測試這個功能。 隨著時間過去，需求的改變，雖然功能越來越多，但是相對應的，測試這些功能的程式也會越來越多，並且不管這個功能是多久之前開發的，每個功能都有相對應的測試。 如何運行自動測試 在 PHP 的世界，自動測試通常使用 PHPUnit 這個工具。然後 Laravel 在安裝時，也自動幫我們安裝這個工具了～ Laravel 幫我們把 PHPUnit 安裝在 vendor/ 裡面，我們來看看要怎麼使用它吧！使用的指令如下\n$ vendor/phpunit/phpunit/phpunit PHPUnit 5.7.27 by Sebastian Bergmann and contributors. .. 2 / 2 (100%) Time: 146 ms, Memory: 10.00MB OK (2 tests, 2 assertions) 奇怪，我們什麼都還沒做，為什麼會有兩個程式已經通過測試了，是因為 Laravel 預設兩個檔案來當測試的案例，分別位於 tests/Feature/ExampleTest.php 以及 tests/Unit/ExampleTest.php (單元測試)，我這邊就簡單說明 Feature/ExampleTest.php 這個測試程式做哪些事情\n\u003c?php namespace Tests\\Feature; use Tests\\TestCase; use Illuminate\\Foundation\\Testing\\WithoutMiddleware; use Illuminate\\Foundation\\Testing\\DatabaseMigrations; use Illuminate\\Foundation\\Testing\\DatabaseTransactions; class ExampleTest extends TestCase { /** * A basic test example. * * @return void */ public function testBasicTest() { $response = $this-\u003eget('/'); $response-\u003eassertStatus(200); } } 這個 Feature/ExampleTest.php 的 testBasicTest() 會檢查如果連線到 / ，HTTP Status 有沒有回傳 200 (成功連線)，這個測試功能就是在檢查首頁是否存在 ～\n實作測試程式 大概了解原理後，一樣我們來實作看看，那我們先來輸入指令，產生一個測試程式\n$ php artisan make:test HelloWorldTest Test created successfully. Laravel 會自動幫我們在 tests/Feature 底下新增了 HelloWorldTest.php 檔案，內容如下(框架版本5.4)\n\u003c?php namespace Tests\\Feature; use Tests\\TestCase; use Illuminate\\Foundation\\Testing\\WithoutMiddleware; use Illuminate\\Foundation\\Testing\\DatabaseMigrations; use Illuminate\\Foundation\\Testing\\DatabaseTransactions; class HelloWorldTest extends TestCase { /** * A basic test example. * * @return void */ public function testExample() { $this-\u003eassertTrue(true); } } 我們測試的項目是想要連線到 hello-world/ 目錄，HTTP Status 會回傳 200 ，以及網站內要看到『 hello world ~ 』的文字，我們將他預設的 testExample() 的內容改成\npublic function testExample() { $response = $this-\u003eget('/hello-world'); // 如果連線 hello-world/，HTTP Status 應該要顯示 200 $response-\u003eassertStatus(200); //連線到網頁內，應該要可以看到「『 hello world ~』文字 $response-\u003eassertSee('hello world ~'); } 來運作一下吧，一樣使用同樣的指令來做測試，順便讓大家看一下測試錯誤的錯誤訊息吧！\n$ vendor/phpunit/phpunit/phpunit PHPUnit 5.7.27 by Sebastian Bergmann and contributors. .F. 3 / 3 (100%) Time: 200 ms, Memory: 10.00MB There was 1 failure: 1) Tests\\Feature\\HelloWorldTest::testExample Expected status code 200 but received 404. Failed asserting that false is true. FAILURES! Tests: 3, Assertions: 3, Failures: 1. 我故意在 routes 下沒有放 hello-world/ 路徑，所以返回顯示 404 ，而不是 200 ，因此，就可以知道要怎麼去修改程式囉 !"},"title":"Laravel 介紹 (使用 Laravel 從零到有開發出一個留言板功能並搭配 RESTful API 來實現 CRUD)"},"/blog/php-laravel/nginx-php-restful-api/":{"data":{"":"由於網路上文章幾乎都是使用 Apache 來實作第一個 PHP RESTful API，也比較沒有中文的介紹，剛好這次也還在學習，所以把學習的紀錄分享給大家，這次會搭配實作留言板來說明，那本次教學內容採用的是\nmacOS 11.6 Nginx 1.21.6 PHP 8.1.2 無框架 無物件導向 來做教學以及示範，那我們先來簡單了解一下什麼是RESTful API：","什麼是restful-api#什麼是RESTful API":"定義 REST ，指得是一組架構約束條件和原則，符合 REST 設計風格的Web API 稱為 RESTful API，主要以下面三點為定義\n直觀簡單的資源網址 URL 比如：http://example.com/resources 對資源的操作：Web 服務在該資源上所支持的請求方法，比如：POST、GET、PUT、PATCH、DELETE 傳輸的資源：Web 服務接受與返回的類型，比如：JSON、XML 分別說明每一項定義\n直觀簡單的資源網址 URL RESTful API URL 結構 (Best Practices for RESTful API Design)\n可以看到這張圖後面的 Endpoint，分別代表應用服務 (Application context) 、 版本 (Version)、 資源(Resource)、 參數(Parameter) 。\n應用服務：可以取名 api 或是 restful-api 。 版本：可以對 API 進行版本控制，可以有升級的服務，也不會對現有的 API 造成影響。 資源：要使用名詞而非動詞來命名，且建議使用複數 壞的命名(以我們要實作的留言板查詢、新增、修改、刪除示範) 查詢 /selectmessage 新增 /createmessage 修改 /updatemessage 刪除 /deletemessage 好的命名 查詢 GET =\u003e /messages (回傳所有留言) 新增 POST =\u003e /messages (新增留言) 修改 PUT =\u003e /messages/1 (修改單筆留言全部資料) 修改 PATCH =\u003e /messages/1 (修改單筆留言有更動資料) 刪除 DELETE =\u003e /messages/1 (刪除單筆留言) 對資源的操作 RESTful API 傳送時，會依照我們所定的 HTTP Request Method 請求方法，那主要有以下 5 種的 Method\nGET : 此方法只能向指定的資源要求取得資料，並不會更動內部的資料 POST：向指定的資源要求新增資料 PUT：向指定的資源要求修改資料內容 PATCH：向指定的資源要求修改部分資料內容 DELETE：向指定的資源要求刪除資料內容 傳輸的資源 剛剛是客戶端對伺服器的請求，那我們也要針對他的請求給予對應的回應 Response ，那回應的格式有 JSON 、XML ，但都以 JSON 較為普遍，所以我們後續實作也會以 JSON 作為我們的 Response 。\n回應的格式內容會依照文件所自訂，但基本上都會回傳狀態碼 http status code ，下面就用表格的方式來說明常用的狀態碼、以及狀態碼對應的意思跟我們在 RESTful API 使用的場景。\n狀態碼 名稱 說明 RESRful API 使用場景 200 OK 請求成功 GET、PUT 方法，取得資料 201 Created 新的資源已建立 POST 方法，新增資料 204 No Content 沒有返回任何內容 DELETE 方法，刪除資料 400 Bad Request 請求不正確 401 Unauthorized 用戶需要認證 403 Forbidden 禁止訪問，與401 不同的是，用戶已經認證，但沒有權限 404 Not Found 沒有找到指定的資源 GET、PUT、DELETE 方法，該資料不存在 500 Internal Server Error 伺服器發生錯誤 ( 參考RESTful Web API 設計指南 )\n為什麼要使用 RESTful API REST 使用所有標準 HTTP 協議方法 - GET、POST、PUT、DELETE、PATCH ，以及更具體的 URL 。 REST 將客戶端與伺服器之間的操作分開 - 他允許開發人員使用它們希望使用的任何前端技術，包含像是 AngularJS、Bootstrap、VUE、Ember、ReactJS、CycleJS、ExtJS、PHP、.NET 等等，極大提高了可移植性。 REST 針對 web 進行了優化 - 因為它依賴 HTTP 協議。此外，由於它的主要數據格式是 JSON，它基本上兼容所有互聯網瀏覽器。 RESTful API 安全性 我們在設計 API 時，要確保 RESTful API 的安全性，像是 PUT 、 DELETE 更新刪除這類型的操作並不安全，沒有權限認證下任何人都可以使用來操作，總不可能讓陌生人去更動我們的資料吧？因此安全性沒有做足夠容易成為駭客攻擊的對象。\n要怎麼做比較安全？\n例如我們的留言板：登入後(帳號密碼) \u003e 伺服器驗證成功並取的一組的 API token \u003e 就可以使用這組 token 來訪問 API 資源，也就知道是誰去對這筆資料進行操作","參考資料#參考資料":"PHP實現RESTful風格的API實例\n[XAMPP][PHP] 製作Restful API Best Practices for RESTful API Design RESTful API與MVC名詞介紹","實作開始#實作開始":"1. 修改 Nginx.conf 我們這邊就直接對架設好的 Nginx 設定檔來修改，如果不清楚要怎麼架設的，這方面網路上蠻多文章，如果還是不清楚，可以留言告訴我，我在另外寫一篇文章來介紹環境安裝。\n為什麼要先修改 conf 呢？修改的目的就是要讓原本網址\nhttp://localhost/api/index.php?messages=all 變成\nhttp://localhost/api/messages 來實現我們在介紹時所說的直觀簡單的資源網址，我說明一下上面網址的關係，\nhttp://localhost 是因為我們本地端運行，如果已經上線的，那就是你自己的網址，\n/api 是我來放這支 api 的目錄，他位於網頁的根目錄下方(詳細的配置下方會附上)，\nindex.php 是我來放這支 api 的網頁，後面的 messages 是我們這次的資源，後面可以接我想要處理的參數\n那要怎麼達成讓網址變成我們想要呈現的直觀簡單的網址呢，這時候我們就要使用 Nginx.conf 來做設定，網路上的文章比較多的是 apache 的 htaccess，我一開始還以為 Nginx 也可以使用 htaccess ，試了半天才知道，在 Nginx 要改用 Nginx.conf 來設定，那我下方會附上我原本的 .htaccess 以及轉換後的 conf，那接下來我一步一步來介紹\n線上轉換工具\n網路上蠻多工具可以線上轉換，我推薦可以用 Apache htaccess to Nginx converter Apache .htaccess 檔案 RewriteRule ^/api/messages$ /api/index.php?messages=all [nc,qsa] RewriteRule ^/api/messages/(\\d+)$ /api/index.php?messages=$1 [nc,qsa] 這是我會修改部分的 Nginx.conf ，可以看到最後兩條 location，這邊就是我們轉換過來的設定\nNginx.conf 檔案(片段) server { listen 80; server_name localhost; #charset koi8-r; #access_log logs/host.access.log main; location / { root html; index index.html index.htm index.php; } location = /api/messages { rewrite ^(.*)$ /api/index.php?messages=all; } location /api { rewrite ^/api/messages/(\\d+)$ /api/index.php?messages=$1; } 他的意思代表的我們網址路徑是在 /api/messages ，我們讓原本 /api/index.php?messages 這段變成 /api/messages/ 的設定，設定好記得先下 sh sudo nginx -t 指令來檢查一下Nginx.conf檔案是不是都正確，再用 sh sudo nginx -s reload 指令重新啟動 Nginx (是否成功，後面會帶到如何做測試，就繼續一起往下看吧！)\nCREATE TABLE `messages` ( `id` int NOT NULL, `name` varchar(20) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci NOT NULL, `message` text CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci NOT NULL, `sendtime` datetime NOT NULL, `version` int DEFAULT NULL ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci; "},"title":"如何在 Nginx 下實作第一個 PHP 留言板 RESTful API"},"/blog/php-laravel/php-introduce/":{"data":{"":"","1-php-是什麼#1. PHP 是什麼":"PHP 全名是超文本前處理器(Hypertext Preprocessor)，是一種開源的通用電腦手稿語言\n根據W3Techs的報告，截至2021年9月，有78.9%網站都是使用PHP，像是著名的 FaceBook、Tesla、Slack、WordPress\nstackshare 是分享開發工具的網站，可以看到不少公司也是使用PHP來進行開發的\n其優點是\n是開源、免費的 是跨平台的開發語言，Linux、Windows、macOS也可以使用 能開發：動態網站、爬蟲程式、WordPress 外掛佈景主題 也可以結合多種的資料庫，例如：Mysql、Mariadb、Oracle 過去使用 PHP 開發網頁程式，就是有什麼寫什麼，程式邏輯和網頁顯示混在一起，雖然開發方便但會造成高維護成本，和難以擴充功能\n物件導向程式設計 函數式編程 事件驅動程式開發 PHP 有各式各樣的 Web Framework：\nLaravel - 最受歡迎的PHP Web Framewrok Codelgniter - 最新版的Codelgniter4 只有 1.4 MB，小而強 php-cli vs php-fpm CGI CGI 是一種協定，為了保障 web server 傳過來的資料是標準格式\n如果請求 index.html ，web server 會先去找這個文件，在丟給瀏覽器，但這個僅限靜態文件而已\n如果請求 index.php ，就需要去找 php 的解析器來處理，那處理中一定會傳遞一些資料，像是 post 或是 url 還有 http header等，CGI 就是規定要傳哪些資料、以及怎麼樣的格式\nFastCGI 是用來提高 CGI 處理性能用的 PHP-cli vs PHP-fpm PHP-cli 可以直接在命令列使用php命令來處理或顯示 php檔案，因為他內建了一個 HTTP 伺服器，可以提供 HTTP 服務\nPHP-fpm 是一個多進程架構的 FastCGI 服務，內建了 PHP 的解析器，配合Nginx來使用","2-php-基本語法#2. PHP 基本語法":"標籤 PHP 程式可以放置在檔案中的任何位置，其檔案副檔名是.php，\nPHP的標籤是開頭 \u003c ?php 以及 ? \u003e 結尾 \u003c?php [ 程式碼 ] ?\u003e PHP 不分大小寫，例如 if、else、while、echo，函數等，但變數有區分大小寫 $color = 'blue'; echo \"My car is \" . $color . \"\u003cbr\u003e\"; ECHo \"My car is \" . $cOLOr . \"\u003cbr\u003e\"; eCHo \"My car is \" . $COLOR . \"\u003cbr\u003e\"; My car is blue My pen is My dog is 註解 PHP 程式中可以使用註解功能，註解後該段程式不會被執行\nPHP 單行註解 // 註解的用意是可以告訴自己或是閱讀該程式的人該行程式碼的用意或想法 PHP 多行註解 /* 註解的用意是可以告訴自己或是閱讀該程式的人該行程式碼的用意或想法 echo \"My car is \" . $color . \"\u003cbr\u003e\"; ECHo \"My car is \" . $cOLOr . \"\u003cbr\u003e\"; eCHo \"My car is \" . $COLOR . \"\u003cbr\u003e\"; */ 變數 PHP 程式中可以使用變數功能，變數是儲存訊息的容器，變數以\" $ “符號開頭，後面是變數名稱\nPHP 變數規則\n變數名必須以字母或是下底線開頭，不可以用數字開頭 變數名只包含字母數字或是下底線(A-z、0-9、_) 變數名區分大小寫 ($age 跟 $AGE 是兩個不同的變數) $color = 'blue'; $items = 'Toy'; $num1 = '13'; $num2 = '18'; echo \"I have \" . $num1+$num2 . \" \" . $color . \" \" . $items; I have 31 blue Toy 變數作用區 PHP 變數有不同的作用區(local、global、static)\nlocal 函式中的變數具有local scope，只能用於該函式中使用\n\u003c?php function myTest() { $x = 5; // local scope echo \"\u003cp\u003eVariable x inside function is: $x\u003c/p\u003e\"; } myTest(); // using x outside the function will generate an error echo \"\u003cp\u003eVariable x outside function is: $x\u003c/p\u003e\"; ?\u003e Variable x inside function is: 5 Variable x outside function is: global 函式外部的變數具有global scope，只能用於該函式外部使用\n\u003c?php $x = 5; // global scope function myTest() { // using x inside this function will generate an error echo \"\u003cp\u003eVariable x inside function is: $x\u003c/p\u003e\"; } myTest(); echo \"\u003cp\u003eVariable x outside function is: $x\u003c/p\u003e\"; ?\u003e Variable x inside function is: Variable x outside function is: 5 想要在函式中，用外部的變數，就在函式內用global 來定義變數\n\u003c?php $x = 5; $y = 10; function myTest() { global $x, $y; $y = $x + $y; } myTest(); echo $y; // outputs 15 ?\u003e 15 static 正常函式執行完，變數值都會被刪除，想要保留值可以加 static\n\u003c?php function myTest() { static $x = 0; echo $x; $x++; } myTest(); myTest(); myTest(); ?\u003e 012 顯示 PHP 程式中可以用 echo 以及 print 來顯示資訊\nPHP echo / print 顯示 \u003c?php $txt1 = \"學習PHP\"; $txt2 = \"Ian_Zhuang\"; $x = 5; $y = 4; echo \"\u003ch2\u003e\" . $txt1 . \"\u003c/h2\u003e\"; echo $txt2 . \" 在 \" . $txt1 . \"\u003cbr\u003e\"; echo $x + $y; print \"\u003ch2\u003e\" . $txt1 . \"\u003c/h2\u003e\"; print $txt2 . \" 在 \" . $txt1 . \"\u003cbr\u003e\"; print $x + $y; ?\u003e 學習PHP Ian_Zhuang 在 學習PHP 9 學習PHP Ian_Zhuang 在 學習PHP 9 資料型態 變數可以儲存不同型態的資料，以下是 PHP 支持的資料型態\n小提示\n可以用var_dump() 函數來查看資料的型態以及值\n字串 (String) 整數 (Integer) 浮點數 (Float) 布林 (Boolean) 陣列 (Array) 物件 (Object) 空值 (NULL) 字串 (String) 字串可以是引號內的任何文本，可以使用單引號或雙引號來表示\n\u003c?php $x = \"Hello world!\"; $y = 'Hello world!'; echo $x; echo \"\u003cbr\u003e\"; echo $y; ?\u003e Hello world! Hello world! 整數 (Integer) 整數有幾點規則：\n必須至少有一個數字 不能有小數點 可以是正數也可以是負數 可以指定不同的進制表示法 其範圍介於 -2,147,483,648 和 2,147,483,647 之間的非十進制數 \u003c?php $x = 5985; var_dump($x); ?\u003e int 5985 浮點數 (Float) 浮點數是帶小數點的數字或指數形式的數字\n\u003c?php $x = 10.365; var_dump($x); ?\u003e float 10.365 布林 (Boolean) 布爾值表示兩種可能的狀態：TRUE 或 FALSE\n\u003c?php $x = true; $y = false; var_dump($x); ?\u003e boolean true 陣列 (Array) 陣列可以將多個值存在一個變數中\n\u003c?php $x = 10.365; var_dump($x); ?\u003e array (size=3) 0 =\u003e string 'Apple' (length=5) 1 =\u003e string '小米' (length=6) 2 =\u003e string 'Sony' (length=4) 物件 (Object) 在討論物件前，要先知道類別(Class)與物件(Object)的關係\n類別 (Class) ：可以比喻為房屋的設計藍圖，是為了讓大家了解房屋的結構與形狀。 物件 (Object) ：可以比喻為真的房屋，物件是類別的實體化 為了要蓋好一棟房子有房子的設計藍圖(類別)還不夠，還需要蓋房子的材料，而材料就是所謂的資料(data)。\n類別定義結構和行為用來產生物件，當多個物件是由同一 個類別產生出來時，每個物件都是一個獨立個體。\n建立類別語法很簡單，只需要使用 Class 來定義一個類別 \u003c?php class MyClass { // 在大括號裡面宣告類別的屬性與方法 } $obj = new MyClass; //使用 new 來實體化類別並將他存入變數中 var_dump($obj); //查看類別內容 ?\u003e object(MyClass)[1] 我們在替 MyClass 加入屬性，用 public 決定屬性的可視性 \u003c?php class MyClass { public $prop = \"I'm a class Property\"; } $obj = new MyClass; echo $obj-\u003eprop; ?\u003e I'm a class Property 因為有很多物件實例化都來自同一個類別，如果沒有指定被實體化的物件，程式碼會無法判斷，所以要用 -\u003e 在 PHP 的物件中，來存取物件的屬性和方法。\n定義類別(Class)的方法(Methods)，方法(Methods)是類別裡面的函式(Functions)，物件可以藉由執行這些方法來更動每個物件的行為。 \u003c?php class MyClass { public $prop = \"I'm a class Property!\"; public function set($newval) { $this-\u003eprop = $newval; } public function get() { return $this-\u003eprop . \"\u003cbr /\u003e\"; } } $obj = new MyClass; echo $obj-\u003eget(); //得到屬性的值。 $obj-\u003eset(\"I'm a new Property value!\"); //設定新的屬性的值。 echo $obj-\u003eget(); ?\u003e I'm a class Property! I'm a new Property value! 物件導向允許物件透過$this來參考自己。物件使用$this就如同直接使用物件名稱來指定物件，同等於MyClass-\u003eprop1。\n空值 (NULL) Null 是一種特殊的數據類型，它只能有一個值：NULL\n如果創建的變量沒有值，則會自動為其分配 NULL\n\u003c?php $x = \"Hello world!\"; $x = null; var_dump($x); ?\u003e null 運算符號 PHP 將運算符號分為以下幾組\n算術運算符 賦值運算符 比較運算符 邏輯運算符 字符串運算符 算術運算符 \u003c?php $x = 3; $y = 6; echo $x + $y; // 輸出 x + y ，3 + 6 ，顯示 9 echo $x - $y; // 輸出 x - y ，3 - 6 ，顯示 -3 echo $x * $y; // 輸出 x * y ，3 * 6 ，顯示 18 echo $x / $y; // 輸出 x / y ，3 / 6 ，顯示 0.5 echo $x % $y; // 輸出 x % y ，3 % 6 ，顯示 3 echo $x ** $y; // 輸出 x ^ y ，3 ^ 6 ，顯示 729 ?\u003e 賦值運算符 \u003c?php $x = 3; $y = 6; echo $x += $y; //輸出 x = x + y ， 9 = 3 + 6 ，輸出 9 $x = 3; $y = 6; echo $x -= $y; //輸出 x = x - y ， -3 = 3 - 6 ，輸出 -3 $x = 3; $y = 6; echo $x *= $y; //輸出 x = x * y ， 18 = 3 * 6 ，輸出 18 $x = 3; $y = 6; echo $x /= $y; //輸出 x = x / y ， 0.5 = 3 / 6 ，輸出 0.5 $x = 3; $y = 6; echo $x %= $y; //輸出 x = x % y ， 3 = 3 % 6 ，輸出 3 ?\u003e 比較運算符 \u003c?php $x = 3; $y = 6; $z = \"6\"; var_dump($x == $y); // 判斷 x 等於 y (值)，3 不等於6 ，回傳 false var_dump($x === $z); // 判斷 x 等於 z (型態and值)，數字不等於字串 ，回傳 false var_dump($x != $y); // 判斷 x 不等於 y (值)，3不等於6 ，回傳 true var_dump($x \u003c\u003e $y); // 判斷 x 不等於 y (型態)，數字不等於字串 ，回傳 true var_dump($x \u003e $y); // 判斷 x 大於 y ，3沒有大於6 ，回傳 false var_dump($x \u003c $y); // 判斷 x 小於 y ，3小於6 ，回傳 true var_dump($x \u003e= $y); // 判斷 x 大於等於 y ，3沒有大於等於6 ，回傳 false var_dump($x \u003c= $y); // 判斷 x 小於等於 y ，3小於等於6 ，回傳 true var_dump($x \u003c=\u003e $y); // 判斷 x y 的關係 ，如果 x 大於 y ，回傳 1 ; 如果 x 小於 y ，就回傳 -1 ; 如果 x 等於 y ，就回傳 0 ?\u003e 邏輯運算符 \u003c?php $x = 3; $y = 6; if ($x == 3 and $y == 6){ echo \"And\"; } // 判斷 x 等於 3 且 y 等於 6 (兩者都要true) if ($x == 3 or $y == 9){ echo \"Or\"; } // 判斷 x 等於 3 或 y 等於 9 (擇一true) if ($x == 3 \u0026\u0026 $y == 6){ echo \"And\"; } // 判斷 x 等於 3 且 y 等於 6 (兩者都要true) if ($x == 3 || $y == 9){ echo \"Or\"; } // 判斷 x 等於 3 或 y 等於 9 (擇一true) if ($x == 3 xor $y == 9){ echo \"Xor\"; } // 判斷 x 等於 3 或 y 等於 9 (任一者為true，但只能一者為true) if ($x != 4){ echo \"Not\"; } // 判斷 x 不等於 4 ?\u003e 條件判斷 條件語句用於根據不同的條件下的操作\nif if…else if…elseif…else switch if 如果條件為真，就執行代碼\n案例：如果現在時間(hour)小於20，就輸出 Have a good day ! \u003c?php $t = date(\"H\"); if ($t \u003c \"20\") { echo \"Have a good day!\"; }s ?\u003e 輸出 Have a good day! if-else 如果條件為真，就執行該程式碼，如果條件為假，就執行另一個程式碼\n案例：如果現在時間(hour)小於20，就輸出 Have a good day! ，超過時間，就輸出 Have a good night! \u003c?php $t = date(\"H\"); if ($t \u003c \"20\") { echo \"Have a good day!\"; } else { echo \"Have a good night!\"; } ?\u003e if-elseif-else 針對兩個以上的條件判斷，如果條件為真，就執行該程式碼，如果條件為假，就執行另一個程式碼\n案例：如果現在時間(hour)小於10，就輸出 Have a good morning! ，如果時間小於20，輸出Have a good day!，否則，就輸出 Have a good night! \u003c?php $t = date(\"H\"); if ($t \u003c \"10\") { echo \"Have a good morning!\"; } elseif ($t \u003c 20) { echo \"Have a good day!\"; } else { echo \"Have a good night!\"; } ?\u003e switch 根據不同的條件來執行不同的操作\n\u003c?php $weather = '下雨天'; switch($weather) { case '晴天': echo \"今天天氣是『 $weather 』\"; break; case '陰天': echo \"今天天氣是『 $weather 』\"; break; case '下雨天': echo \"今天天氣是『 $weather 』\"; break; default: echo \"世界末日\"; break; } ?\u003e 迴圈 想要相同程式碼反覆執行一定的次數\nwhile do…while for foreach while 只要指定條件為真，就會循環通過該程式碼\n\u003c?php $x = 1; while($x \u003c= 3) { echo \"The number is: $x \u003cbr\u003e\"; $x++; } ?\u003e 輸出 The number is: 1 The number is: 2 The number is: 3 do…while 先執行一次do，再檢查指定條件是否為真，是的話就會循環通過該程式碼\n\u003c?php $x = 1; do { echo \"The number is: $x \u003cbr\u003e\"; $x++; } while ($x \u003c= 5); ?\u003e 輸出 The number is: 6 因為do…while的條件會在執行循環程式碼後才檢查，所以do…while 至少會循環一次該程式碼。\nfor 假如預先知道需要循環幾次，就可以使用for，可以設定程式碼在循環中的次數。\n\u003c?php for ($x = 0; $x \u003c= 10; $x++) { echo \"The number is: $x \u003cbr\u003e\"; } ?\u003e 輸出 The number is: 0 The number is: 1 The number is: 2 The number is: 3 The number is: 4 The number is: 5 The number is: 6 The number is: 7 The number is: 8 The number is: 9 The number is: 10 foreach 此循環適用於陣列，可以用來顯示陣列的key跟value\n\u003c?php $age = array(\"Peter\"=\u003e\"35\", \"Ben\"=\u003e\"37\", \"Joe\"=\u003e\"43\"); foreach($age as $x =\u003e $val) { echo \"$x = $val\u003cbr\u003e\"; } ?\u003e 輸出 Peter = 35 Ben = 37 Joe = 43 break 想要在迴圈執行中跳出，可以使用break 指令，來跳出循環。\n\u003c?php for ($x = 0; $x \u003c 10; $x++) { if ($x == 4) { break; } echo \"The number is: $x \u003cbr\u003e\"; } ?\u003e 輸出 The number is: 0 The number is: 1 The number is: 2 The number is: 3 continue continue 與 break 是相對的指令。break 中斷目前執行的迴圈，continue 則是回到迴圈的開頭，執行「下一次」迴圈。\n\u003c?php for ($x = 0; $x \u003c 5; $x++) { if ($x == 2) { continue; } echo \"The number is: $x \u003cbr\u003e\"; } ?\u003e 輸出 The number is: 0 The number is: 1 The number is: 3 The number is: 4 函式 PHP除了內建的函式外，還可以創建自己的函式 注意：函式名稱必須以字母或下底線開頭。不區分大小寫。\n\u003c?php function writeMsg() { echo \"Hello world!\"; } writeMsg(); // call the function ?\u003e 輸出 Hello world! 也可以在函式裡面放入任意數量的參數，只需要用逗號分開\n\u003c?php function familyName($fname, $year) { echo \"$fname Born in $year \u003cbr\u003e\"; } familyName(\"Hege\", \"1975\"); familyName(\"Stale\", \"1978\"); familyName(\"Kai Jim\", \"1983\"); ?\u003e 輸出 Hege Born in 1975 Stale Born in 1978 Kai Jim Born in 1983 ","3-php-常用函式#3. PHP 常用函式":"日期和時間 可以透過輸入參數，顯示想要的日期\nd - 代表月份中的某天（01 到 31） m - 代表一個月（01 到 12） Y - 代表年份（四位數） l（小寫“L”）— 代表星期幾 \u003c?php echo \"Today is \" . date(\"Y/m/d\") . \"\u003cbr\u003e\"; echo \"Today is \" . date(\"Y.m.d\") . \"\u003cbr\u003e\"; echo \"Today is \" . date(\"Y-m-d\") . \"\u003cbr\u003e\"; echo \"Today is \" . date(\"l\"); ?\u003e 輸出 Today is 2022/02/22 Today is 2022.02.22 Today is 2022-02-22 Today is Tuesday 可以透過輸入參數，顯示想要的時間\nH - 一個小時的 24 小時格式（00 到 23） h - 小時的 12 小時格式，前面補零（01 到 12） i - 前面補零的分鐘（00 到 59） s - 前面補零的秒數（00 到 59） a - 小寫的 Ante meridiem 和 Post meridiem（am 或 pm） \u003c?php date_default_timezone_set(\"Asia/Taipei\"); echo \"The time is \" . date(\"h:i:sa\"); ?\u003e 輸出 The time is 12:12:10am 可以使用 strtotime()，顯示想要的日期\n\u003c?php $d=strtotime(\"tomorrow\"); echo date(\"Y-m-d h:i:sa\", $d) . \"\u003cbr\u003e\"; $d=strtotime(\"next Saturday\"); echo date(\"Y-m-d h:i:sa\", $d) . \"\u003cbr\u003e\"; $d=strtotime(\"+3 Months\"); echo date(\"Y-m-d h:i:sa\", $d) . \"\u003cbr\u003e\"; ?\u003e 輸出 2022-02-23 12:00:00am 2022-02-26 12:00:00am 2022-05-22 04:21:41am json 可以使用 json_encode()，將陣列存為 JSON 格式，也可以用 json_decode()，將JSON格式變成陣列\n\u003c?php $age = array(\"Peter\"=\u003e'35', \"Ben\"=\u003e'45', \"Ian\"=\u003e'22'); echo $enjson = json_encode($age); var_dump(json_decode($enjson,true)); ?\u003e 輸出 {\"Peter\":\"35\",\"Ben\":\"45\",\"Ian\":\"22\"} array (size=3) 'Peter' =\u003e string '35' (length=2) 'Ben' =\u003e string '45' (length=2) 'Ian' =\u003e string '22' (length=2) min/max 可以使用 min()，來找到參數列表裡面的最小值，也可以 max()，來找到參數列表裡面的最大值\n\u003c?php echo(min(0, 150, 30, 20, -8, -200)); echo '\u003cbr\u003e'; echo(max(0, 150, 30, 20, -8, -200)); ?\u003e 輸出 -200 150 abs 可以使用 abs()，將參數值變成絕對值\n\u003c?php echo(abs(-6.7)); ?\u003e 輸出 6.7 round 可以使用 round()，將浮點數四捨五入到最接近的整數\n\u003c?php echo(round(0.60)); echo '\u003cbr\u003e'; echo(round(0.49)); ?\u003e 輸出 1 0 rand 可以使用 rand()，會隨機產生一個亂數，也可以設定最大值與最小值的範圍來隨機產生亂數\n\u003c?php echo(round(0,100)); ?\u003e 輸出 88 ","4-php-表單#4. PHP 表單":"GET vs POST 舉個例子，如果 HTTP 代表現在我們現實生活中寄信的機制，那麼信封的撰寫格式就是 HTTP。我們可以將信封外的內容稱為 http-header，信封內的書信稱為 message-body。\n假設 GET 表示信封內不得裝信件的寄送方式，像是明信片一樣，你可以把要傳遞的資訊寫在信封(http-header)上。而 POST 就是信封內有裝信件的寄送方式，不但信封可以寫東西，信封內 (message-body) 還可以放你想要寄送的資料或檔案。\n因為使用 GET 的方法來傳送表單訊息對每個人都是可見的(所有的變數名稱與值都會顯示在 URL)，所以絕對不要使用 GET 來傳送密碼或是一些敏感訊息。\nPHP 可以用 $_GET 和 $_POST 來收集表單數據\nGET 我們先模擬 $_GET ，先建立一個 index.html 的檔案程式碼如下\nHTML 表單 (GET) \u003chtml\u003e \u003cbody\u003e \u003cform action=\"welcome_get.php\" method=\"get\"\u003e Name: \u003cinput type=\"text\" name=\"name\"\u003e\u003cbr\u003e E-mail: \u003cinput type=\"text\" name=\"email\"\u003e\u003cbr\u003e \u003cinput type=\"submit\"\u003e \u003c/form\u003e \u003c/body\u003e \u003c/html\u003e 再建立一個 welcome.php 的檔案程式碼如下\nPHP ($_GET) \u003chtml\u003e \u003cbody\u003e Welcome \u003c?php echo $_GET[\"name\"]; ?\u003e\u003cbr\u003e Your email address is: \u003c?php echo $_GET[\"email\"]; ?\u003e \u003c/body\u003e \u003c/html\u003e POST 我們先模擬 $_POST ，先建立一個 index.html 的檔案程式碼如下\nHTML 表單 (POST) \u003chtml\u003e \u003cbody\u003e \u003cform action=\"welcome_get.php\" method=\"post\"\u003e Name: \u003cinput type=\"text\" name=\"name\"\u003e\u003cbr\u003e E-mail: \u003cinput type=\"text\" name=\"email\"\u003e\u003cbr\u003e \u003cinput type=\"submit\"\u003e \u003c/form\u003e \u003c/body\u003e \u003c/html\u003e 再建立一個 welcome.php 的檔案程式碼如下\nPHP ($_POST) \u003chtml\u003e \u003cbody\u003e Welcome \u003c?php echo $_POST[\"name\"]; ?\u003e\u003cbr\u003e Your email address is: \u003c?php echo $_POST[\"email\"]; ?\u003e \u003c/body\u003e \u003c/html\u003e 資料驗證 密碼：需為8碼至20碼，且並包含特殊符號、大小寫英文字母、數字至少各1碼 \u003c?php $password = '@aaaa5aI'; if(preg_match('/(?=.*[@#$%^\u0026+=])(?=.*[a-z])(?=.*[A-Z])(?=.*[0-9])[@#$%^\u0026+=a-zA-Z0-9]{8,20}$/', $password)){ echo '密碼『',$password,'』正確。'; } else{ echo '密碼『',$password,'』不正確(需符合8至20碼，且包含特殊符號、大小寫英文、數字各一碼)。'; } ?\u003e 輸出 密碼『@aaaa5ai』不正確(需符合8至20碼，且包含特殊符號、大小寫英文、數字各一碼)。 密碼『@aaaa5aI』正確。 手機電話：需為10碼，且開頭為09接後8碼的數字 \u003c?php $phone = '0912345678'; if(preg_match('/^09[0-9]{8}$/', $password)){ echo '手機電話『',$phone,'』正確。'; } else{ echo '手機電話『',$phone,'』不正確(需為10碼，且符合開頭為09後接8碼的數字)。'; } ?\u003e 輸出 手機電話『0212345678』不正確(需為10碼，且符合開頭為09後接8碼的數字)。 手機電話『0912345678』正確。 信箱 \u003c?php $email = '123@gmail.com'; if(filter_var(\"$email\", FILTER_VALIDATE_EMAIL)) { echo '信箱『',$email,'』正確。'; } else{ echo '信箱『',$email,'』不正確。'; } ?\u003e 輸出 信箱『123@gmailcom』不正確。 信箱『123@gmail.com』正確。 ","5-php-restful#5. PHP RESTful":"REST ，指得是一組架構約束條件和原則，符合 REST 設計風格的Web API 稱為 RESTful API，主要以下面三點為定義\n直觀簡單的資源網址URL 比如：http://example.com/resources\n傳輸的資源：Web 服務接受與返回的類型，比如：JSON、XML\n對資源的操作：Web 服務在該資源上所支持的請求方法，比如：POST、GET、PUT、PATCH、DELETE\n加油 還有一篇可以一起學習\n詳細可以參考另一篇文章 如何在 Nginx 下實作第一個 PHP 留言板 RESTful API 裡面有更詳細的介紹","6-資料來源#6. 資料來源":"PHP 是什麼，架設網站最適合的程式語言：https://kamadiam.com/what-is-php/\nPHP 新手指南：3分鐘快速認識PHP：https://www.happycoding.today/posts/23\nPHP 教程：https://www.w3schools.com/php/\nPHP基礎語法(一)：Hello world與基本資料型態：https://ithelp.ithome.com.tw/articles/10218595\nPHP-物件導向(OOP)介紹：https://ithelp.ithome.com.tw/articles/10206973\n秒懂PHP的FastCGI跟PHP-FPM有什麼關係：https://www.astralweb.com.tw/what-is-differences-between-fastcgi-php-fpm/"},"title":"PHP 介紹"},"/blog/rd/":{"data":{"":"此分類包含 RD 使用的相關工具文章。\nFluentd-Server 出現 Fluent::Plugin::Elasticsearch Error 400 - Rejected by Elasticsearch 錯誤解決發布日期：2022-12-12 Kibana 新增 index 索引時一直轉圈圈以及顯示 HTTP 403 Forbidden發布日期：2022-12-08 用 EFK 收集容器日誌 (HAProxy、Redis Sentinel、Docker-compose)發布日期：2022-04-22 "},"title":"RD 使用的相關工具"},"/blog/rd/fluentd-server-show-fluent-plugin-elasticsearch-error400/":{"data":{"":"先說結論 … EFK 真的好多雷 😆 我們今天又再度踩雷拉，這次又是炸的分身碎骨，花了 4 個多小時才找到原因，也有可能是小弟我跟 EFK 不是很熟 XD。就如標題所說，我在抽 Log 的時候發現有部分的 Log 送不到 Kibana，檢查 Fluentd-Server 的 Log 發現裡面有 Fluent::Plugin::Elasticsearch Error 400 - Rejected by Elasticsearch 的錯誤訊息，到底這個 Error 400 是什麼咧，就跟著我一起重溫找問題的苦難吧 XD","efk-結構#EFK 結構":"在開始之前我先簡單說一下我現在的 EFK 結構：\nEFK 結構\n現在的 EFK 結構如上圖，我在要抽 Log 的 Pod 中多塞一個 fluentd-bit container，將 fluentd-bit 與服務(例如 nginx) 的 log 掛相同路徑，讓 fluentd-bit 可以抽到服務 log，接著就透過 fluentd-server-svc 打到 fluentd-server pod，後面就是常見的 EFK 結構，就不多做說明。","參考資料#參考資料":"Elasticsearch 400 error #467：https://github.com/uken/fluent-plugin-elasticsearch/issues/467","問題的出現#問題的出現":"我們就像平常一樣寫好整個 EFK 的 yaml，run 的時候也正常，可以收到 log，但很奇怪的是我們先送 log_false 的 log 可以正常收到，但再接著送 log_true 就收不到，我們也嘗試反過來送，先送 log_true，再送 log_false，發現換收不到 log_false 的 log\nlog_false { \"result\": false, \"message\": \"我是馬賽克\", \"code\": 11223344, \"data\": { \"end_at\": \"2022-12-19 04:09:00\" }, \"response_code\": \"326dgf241geh1596489\", \"job_name\": \"叮叮噹噹聖誕節\", \"method\": \"GET\", \"url\": \"url.com\" } log_true { \"result\": true, \"data\": true, \"response_code\": \"wkCJl1dfr45671607965\" } 接著在 Fluentd-Server Log 中發現 Fluent::Plugin::Elasticsearch Error 400 - Rejected by Elasticsearch 錯誤\nfluentd-server 跳出錯誤 LOG\n我到網路上搜尋，看看有沒有人有遇到跟我一樣的問題，發現在 uken/fluent-plugin-elasticsearch 也有其他人有遇到同樣的問題\nuken/fluent-plugin-elasticsearch issus 詢問\n有人說可能是 Elasticsearch 的 disk 滿了，也有人說是裝在 Fluentd-Server 的套件 fluent-plugin-elasticsearch 版本有問題，或是將 Elasticsearch 跟 Kibana index 刪除就可以，我有砍掉 index 再重新倒入 Log，還是會有問題。\n一開始以為是 log_false 跟 log_true 的欄位不同，所以才會有這個問題發生（你看就知道我跟 EFK 不熟吧ＸＤ），所以有另外倒一些其他服務(欄位也不同)的 log 進去測試，發現是可以正常抽到，且 kibana 那邊會依照 es 提供的欄位自動新增，所以也不是欄位的問題，那最後的問題是什麼呢？","如何解決問題#如何解決問題":"最後在我們仔細檢查後發現，log_false 跟 log_true 的欄位其中一樣存的資料型態不太一樣，發現其中的 data 欄位在 log_false 的資料型態是 array，在 log_true 存的時候是字串，所以導致當其中一個 log 寫入後，另一個 log 就會因為同欄位的資料型態有所不同，而無法寫入 log，且在 Fluentd-Server 會出現標題的 Error 400 原因拉～\n最後重新調整欄位的資料型態，就順利解決此次問題 ✌️✌️✌️"},"title":"Fluentd-Server 出現 Fluent::Plugin::Elasticsearch Error 400 - Rejected by Elasticsearch 錯誤解決"},"/blog/rd/kibana-create-index-forbidden/":{"data":{"":"前幾天在工作使用 Kibana 時，想要新增一個新的索引，發現選擇索引並按下新增的按鈕，會一直轉圈圈，等了一陣子，使用開發工具 F12 查看，跳出了 HTTP 403 Forbidden，到底是什麼原因導致的呢！？我們一起看下去吧，會從問題的出現到問題原因再到如何解決問題，來仔細介紹，希望大家不要像我一樣踩到雷 🤣","參考資料#參考資料":"Kibana 创建索引 POST 403 (forbidden) on create index：https://www.cnblogs.com/caoweixiong/p/10972120.html","問題原因#問題原因":"文章的說明是索引變成只允許讀取的狀態，其原因是因為出現這個 HTTP 403 Forbidden 前，ElasticSearch 的空間滿了，導致 Kibana 會自動的將索引改成只允許讀取的狀態，我們來看一下剛剛的 Index 狀態是不是像他說的一樣變成只允許讀取的狀態呢\n可以到 kibana 的 Dev Tools 下指令來查詢歐，輸入 GET _settings，就會顯示以下圖片的內容囉 索引只允許讀取的設定變成\n發現正如文章所說，是因為 read_only_allow_delete 的狀態變成了 true，所以才沒辦法新增索引～","問題的出現#問題的出現":"那天是個變冷的 12 月，要幫 RD 同仁新增 Kibana 的索引時，照往常一樣輸入 Index pattern，按下一步，選擇 Configure settings ：\n輸入 Index pattern (圖片為範例，正常都是用 -* )\n按下新增索引，他就開始無限的轉圈圈，有去查看 ElasticSearch 的 log，發現也沒有特別的錯誤訊息\n新增索引後，一直轉圈圈\n接著想說打開開發者工具 F12 來看看，是卡在哪一個點，卻發現有幾個紅字寫著 HTTP 403 Forbidden\n開發者工具網頁內容顯示 HTTP 403 Forbidden\n想說為什麼會有 HTTP 403 Forbidden，之前也沒有看過類似的錯誤訊息，於是就開始在網路上亂晃，最後在同事的幫助下找到了一個跟我們情況很相似的文章 Kibana 创建索引 POST 403 (forbidden) on create index","如何解決問題#如何解決問題":"我們查看文章的解決辦法，有兩種辦法，一個是擴大 ElasticSearch 的空間，以及使用 kibana 的 Dev Tools 下指令修改，那我們兩種都有做，這邊就直接介紹下指令需要輸入什麼～\n需要再 Dev Tools 輸入以下指令，來修改 Index 的狀態：\nPUT _settings { \"index\": { \"blocks\": { \"read_only_allow_delete\": \"false\" } } } Dev Tools 修改 Index 的狀態\n最後我們用 GET _settings 檢查一下索引狀態是不是已經變回原本的了：\n索引只允許讀取的設定變成 false\n最後就可以順利新增索引拉 👍👍👍\n順利新增索引"},"title":"Kibana 新增 index 索引時一直轉圈圈以及顯示 HTTP 403 Forbidden"},"/blog/rd/redis-sentinel-docker-compose-haproxy-efk/":{"data":{"":"前情提要：本篇是 用 HAProxy 對 Redis 做負載平衡 (Redis Sentinel、Docker-compose) 以及 Redis 哨兵模式 (Sentinel) 搭配 Docker-compose 實作 的後續文章，主要會優化原本的程式碼，並使用 EFK 來收集 LOG！","什麼是-efk-#什麼是 EFK ?":"隨著現在各種程式系統複雜度越來越高，特別是現在都往雲上開始作部署，當我們想要查看 log 的時候，不可能一個一個去登入各節點去查看 log，不僅效率低，也會有安全性的問題，所以不可能讓工程師直接去訪問每一個節點。\n而且現在大規模的系統基本上都採用叢集的部署方式，意味著對每個 service，會啟動多個完全一樣的 POD 對外提供服務，每個 container 都會產生自己的 log，從產生 log 來看，你根本不知道是由哪個 POD 產生的，這樣對查看分佈式的日誌更加困難。\n所以在雲時代，需要一個收集並分析 log 的解決方案。首先需要將分佈在各個角落的 log 統一收集到一個集中的地方，方便查看。收集之後，還可以進行各種統計以及分析，甚至用流行的大數據或機器學習的方法來進行分析。\n所以誕生了 ELK 或是 EFK 的解決方式：\nELK 是由 Elasticsearch、Logstash、Kibana 所組成， EFK 是由 Elasticsearch、(Filebeats or Fluentd)、Kibana 所組成，兩者的差異在中間使用的開源程式，我會分別介紹一下每一個程式主要的用途：\nElasticsearch：它是一個集中儲存 log 的地方，更重要的是它是一個全文檢索以及分析的引擎，它能讓用戶以近乎實時的方式來查看、分析海量的數據。 Logstash、Filebeats、Fluentd：它們主要是收集分佈在各處的 log 並進行處理(Filebeats 僅收集)。 Kibana：它是為 Elasticsearch 開發的前端 GUI，可以讓用戶很方便的以圖形化進行查詢 Elasticsearch 中儲存的數據，同時也提供各式各樣的模組可以使用。 Logstash、Filebeats、Fluentd 關係：\nFilebeats 是一個輕量級收集本地 log 數據的方案，它僅能收集本地的 log，不能對 log 做處理，所以 Filebeats 通常會將 log 送到 Logstash 做進一步的處理。\n那為什麼不直接使用 Logstash 來收集收集並處理 log 呢？\n因為 Logstash 會消耗許多的記憶體，所以才會先透過 Filebeats 收集資料再傳給 Logstash 做處理。\n另外 Filebeats、Logstash、Elasticsearch 和 Kibana 都是屬於同一家公司的開源項目：https://www.elastic.co/guide/\n而 Fluentd 則是另一家公司的開源項目：https://docs.fluentd.org/\n那我們在這邊就使用 Fluentd 來做我們的 EFK 示範：\nEFK 示意圖 EFK Stack: Elasticsearch, Fluentd and Kibana on Docker\n上面這張圖代表我們會有很多 Docker 容器的 log (也就是之前的 redis 以及其他 nginx 等的容器)，會先透過 Fluentd 收集並處理各容器的 log 在傳送到 Elasticsearch 集中儲存，再使用 Kibana 圖形化介面來查詢或檢索儲在 Elasticsearch 的 log。","參考資料#參考資料":"elastic 官網：https://www.elastic.co/\nfluentd 官網：https://www.fluentd.org/\nBuild the EFK system used for simulating logging server on Docker：https://stackoverflow.com/questions/71155142/build-the-efk-system-used-for-simulating-logging-server-on-docker\n開源日誌管理方案ELK 和EFK 的區別：https://wsgzao.github.io/post/efk/","實作#實作":"那接下來會使用 Docker-compose 實作 EFK 的 LOG 分析，範例程式連結 點我 😘\n版本資訊\nmacOS：11.6 Docker：Docker version 20.10.12, build e91ed57 Nginx：1.20 PHP：7.4-fpm Redis：6.2.6 HAProxy：HAProxy version 2.5.5-384c5c5 2022/03/14 - https://haproxy.org/ Elasticsearch：8.1.3 Fluentd：v1.14 Kibana：8.1.3 檔案結構 . ├── docker-volume │ ├── fluentd │ │ └── fluent.conf │ ├── haproxy │ │ └── haproxy.cfg │ ├── nginx │ │ └── nginx.conf │ ├── php │ │ ├── info.php │ │ ├── r.php │ │ └── rw.php │ └── redis │ ├── redis.conf │ ├── redis1 │ ├── redis2 │ └── redis3 ├── docker.sh ├── efk │ └── Docker-compose.yaml ├── fluentd │ └── Dockerfile ├── haproxy_sentinel │ ├── Docker-compose.yaml │ ├── sentinel1 │ │ └── sentinel.conf │ ├── sentinel2 │ │ └── sentinel.conf │ └── sentinel3 │ └── sentinel.conf ├── nginx_php_redis │ └── Docker-compose.yaml └── php └── Dockerfile 這是主要的結構，簡單說明一下：(檔案越來越多了XD，這次把每一個都有分類，所以結構會與之前不太相同)\ndocker-volume/fluentd/fluent.conf：fluentd 的設定檔。 docker-volume/haproxy/haproxy.cfg：haproxy 的設定檔。 docker-volume/nginx/nginx.conf：nginx 的設定檔。 docker-volume/php/(r.php、rw.php)：測試用檔案。 docker-volume/redis/redis.conf：redis 的設定檔。 docker-volume/redis/(redis1、redis2、redis3)：放 redis 的資料。 docker.sh：是我另外多寫的腳本，可以查看相對應的角色。 efk/Docker-compose.yaml：會放置要產生的 elasticsearch、kibana、fluentd 的容器設定檔。 fluentd/Dockerfile：因為 fluentd 需要另外安裝 fluent-plugin-elasticsearch 才能使用，所以用 Dockerfile 另外寫 fluent 的映像檔。 haproxy_sentinel/Docker-compose.yaml：會放置要產生的 haproxy、sentinel1、sentinel2、sentinel3 的容器設定檔。 haproxy_sentinel/(sentinel1、sentinel2、sentinel3)/.conf：哨兵的設定檔。 nginx_php_redis/Docker-compose.yaml：會放置要產生的 nginx、php、redis1、redis2、redis3 的容器設定檔。 php/Dokcerfile：因為在 php 要使用 redis 需要多安裝一些設定，所以用 Dockerfile 另外寫 PHP 的映像檔。 那我們就依照安裝的設定開始說明：(這邊只說明與上一篇不同地方，重複的就請大家回去看之前的文章) docker-volume/fluentd/fluent.conf \u003csource\u003e @type forward bind 0.0.0.0 port 24224 \u003c/source\u003e \u003cmatch *.**\u003e @type copy \u003cstore\u003e @type elasticsearch host elasticsearch port 9200 logstash_format true logstash_prefix fluentd logstash_dateformat %Y%m%d include_tag_key true type_name access_log tag_key @log_name flush_interval 1s \u003c/store\u003e \u003cstore\u003e @type stdout \u003c/store\u003e \u003c/match\u003e 這是 fluent 的設定檔，可以在這邊做自訂的設定，例如 host、port、預設日期、tag_key 等等。\nfluentd/Dockerfile FROM fluent/fluentd:v1.14 USER root RUN [\"gem\", \"install\", \"fluent-plugin-elasticsearch\"] 因為我們需要先安裝 fluent-plugin-elasticsearch 才可以讓 fluentd 來使用 elasticsearch，所以多寫一個 Dockerfile 來設定。\nefk/Docker-compose.yaml version: '3.8' services: elasticsearch: image: elasticsearch:8.1.3 container_name: elasticsearch environment: - discovery.type=single-node - xpack.security.enabled=false # elasticsearch 8.x版本後會自動開啟SSL networks: efk_network: ipv4_address: 172.20.0.3 ports: - 9200:9200 kibana: image: kibana:8.1.3 container_name: kibana environment: - ELASTICSEARCH_HOSTS=http://elasticsearch:9200 # - I18N_LOCALE=zh-CN networks: efk_network: ipv4_address: 172.20.0.4 ports: - 5601:5601 depends_on: - elasticsearch fluentd: build: ../fluentd container_name: fluentd volumes: - ../docker-volume/fluentd:/fluentd/etc depends_on: - elasticsearch ports: - \"24224:24224\" - \"24224:24224/udp\" networks: efk_network: ipv4_address: 172.20.0.5 networks: efk_network: driver: bridge name: efk_network ipam: config: - subnet: 172.20.0.0/16 gateway: 172.20.0.1 這邊是設定 EFK 三個容器的檔案，比較特別的是 elasticsearch 在 8.x 版本後會自動開啟 SSL 連線，所以沒有使用的或是在測試中的要先把它關掉，不然會連不上！，kibana 它支援 I18N 多語系，但目前沒有繁體中文，所以想要看中文的可以切成 zh-CH 的簡體中文來使用，其他就是基本的設定 Posts 以及我們全部設定好 IP，在後續測試會比較方便～\nnginx_php_redis/Docker-compose.yaml version: \"3.8\" services: nginx: image: nginx container_name: nginx networks: efk_network: ports: - 8080:80 volumes: - ../docker-volume/nginx/:/etc/nginx/conf.d/ environment: - TZ=Asia/Taipei logging: driver: \"fluentd\" options: fluentd-address: 172.20.0.5:24224 tag: nginx php: build: ../php container_name: php networks: efk_network: expose: - 9000 volumes: - ../docker-volume/php/:/var/www/html environment: - TZ=Asia/Taipei logging: driver: \"fluentd\" options: fluentd-address: 172.20.0.5:24224 tag: php redis1: image: redis container_name: redis1 command: redis-server /usr/local/etc/redis/redis.conf --appendonly yes volumes: - ../docker-volume/redis/redis1/:/data - ../docker-volume/redis/:/usr/local/etc/redis/ environment: - TZ=Asia/Taipei networks: efk_network: ipv4_address: 172.20.0.11 ports: - 6379:6379 logging: driver: \"fluentd\" options: fluentd-address: 172.20.0.5:24224 tag: redis1 redis2: image: redis container_name: redis2 command: redis-server /usr/local/etc/redis/redis.conf --slaveof redis1 6379 --appendonly yes volumes: - ../docker-volume/redis/redis2/:/data - ../docker-volume/redis/:/usr/local/etc/redis/ environment: - TZ=Asia/Taipei networks: efk_network: ipv4_address: 172.20.0.12 ports: - 6380:6379 depends_on: - redis1 logging: driver: \"fluentd\" options: fluentd-address: 172.20.0.5:24224 tag: redis2 redis3: image: redis container_name: redis3 command: redis-server /usr/local/etc/redis/redis.conf --slaveof redis1 6379 --appendonly yes volumes: - ../docker-volume/redis/redis3/:/data - ../docker-volume/redis/:/usr/local/etc/redis/ environment: - TZ=Asia/Taipei networks: efk_network: ipv4_address: 172.20.0.13 ports: - 6381:6379 depends_on: - redis1 - redis2 logging: driver: \"fluentd\" options: fluentd-address: 172.20.0.5:24224 tag: redis3 networks: efk_network: external: name: efk_network 那這邊基本上都與上一篇一樣，沒有修改特別的地方，只有修改網路名稱以及：\nlogging: driver: \"fluentd\" options: fluentd-address: 172.20.0.5:24224 tag: nginx 我們要把每一個容器的 log 進行收集與處理，所以我們使用 logging 然後 driver 選擇 fluentd，並且要設定 fluentd 的 IP 位置以及每一個容器可設定不同的 tag 方便我們查詢。\nhaproxy_sentinel/Docker-compose.yaml version: '3.8' services: haproxy: image: haproxy container_name: haproxy volumes: - ../docker-volume/haproxy/:/usr/local/etc/haproxy environment: - TZ=Asia/Taipei networks: efk_network: ipv4_address: 172.20.0.20 ports: - 16379:6379 - 8404:8404 logging: driver: \"fluentd\" options: fluentd-address: 172.20.0.5:24224 tag: haproxy sentinel1: image: redis container_name: redis-sentinel-1 networks: efk_network: ports: - 26379:26379 command: redis-server /usr/local/etc/redis/sentinel.conf --sentinel volumes: - ./sentinel1:/usr/local/etc/redis/ environment: - TZ=Asia/Taipei logging: driver: \"fluentd\" options: fluentd-address: 172.20.0.5:24224 tag: sentinel1 sentinel2: image: redis container_name: redis-sentinel-2 networks: efk_network: ports: - 26380:26379 command: redis-server /usr/local/etc/redis/sentinel.conf --sentinel volumes: - ./sentinel2:/usr/local/etc/redis/ environment: - TZ=Asia/Taipei logging: driver: \"fluentd\" options: fluentd-address: 172.20.0.5:24224 tag: sentinel2 sentinel3: image: redis container_name: redis-sentinel-3 networks: efk_network: ports: - 26381:26379 command: redis-server /usr/local/etc/redis/sentinel.conf --sentinel volumes: - ./sentinel3:/usr/local/etc/redis/ environment: - TZ=Asia/Taipei logging: driver: \"fluentd\" options: fluentd-address: 172.20.0.5:24224 tag: sentinel3 networks: efk_network: external: name: efk_network 與前面一樣，多了一個 logging 來設定 fluentd 位置以及 tag。","測試#測試":"我們先用 docker-compose up 來啟動 efk/Docker-compose.yaml，接著再啟動 nginx_php_redis/Docker-compose.yaml，最後啟動 haproxy_sentinel/Docker-compose.yaml：\n啟動 efk/Docker-compose.yaml\n啟動 nginx_php_redis/Docker-compose.yaml\n啟動 efk/Docker-compose.yaml\n(啟動 efk 時要等他跑完，因為他需要啟動一陣子，如果還沒等他啟動完畢就啟動下一個，會導致 fluentd 的 fluentd-address 尚未設定好，導致啟動錯誤)\n這時候可以使用瀏覽器搜尋以下網址：\ntest.com:5601：Kibana GUI 頁面。 kibana 設定\n如果連線成功進來，就代表我們有安裝好 Elasticsearch 以及 kibana，那我們來做一些設定，讓我們可以在 kibana 上面看到 log。\n先點選左邊的欄位，點選 “Stack Management” ，可以看到目前的 kibana 的版本，再點選左邊的 “Kibana \u003e Date Views”：\nkibana 設定\n然後會跳出一個視窗點選 “Create data view”，在 name 欄位輸入 fluentd* (右側有 fluentd 加時間，代表我們接 fluentd 有成功)，按下 “Create data view”\nkibana 設定\n接著點選左側的欄位，點選 “Analytics \u003e Discover” ，就可以看到我們目前所有的 log 囉！\nkibana 設定\nkibana Analytics \u003e Discover"},"title":"用 EFK 收集容器日誌 (HAProxy、Redis Sentinel、Docker-compose)"},"/blog/redis/":{"data":{"":"此分類包含 Redis 相關的文章。\n用 HAProxy 對 Redis 做負載平衡 (Redis Sentinel、Docker-compose)發布日期：2022-04-22 Redis 哨兵模式 (Sentinel) 搭配 Docker-compose 實作發布日期：2022-04-19 Redis 介紹發布日期：2022-03-08 "},"title":"Redis"},"/blog/redis/redis-introduce/":{"data":{"":"","什麼是-redis-#什麼是 Redis ?":"Redis 全名是 Remote Dictionary Server ，是快速的開源記憶體鍵值資料庫 (keys-value database)。\n由於 Redis 的回應時間極短，低於一毫秒，可以讓遊戲、金融服務、醫療保健等即時應用服務每秒處理幾百萬個請求。\nRedis 的優勢 效能 所有的 Redis 資料都是存放在記憶體中，進而實現低延遲和高傳輸量的資料存取。\n彈性的資料結構 一般的鍵值資料庫提供的資料結構有限，而 Redis 提供多樣化的資料結構來滿足服務的需求，包含字串(Strings)、哈希(Hashes)、列表(Lists)、集合(Sets)、有序集合(Zset)。(後續會有詳細介紹)\n簡單易用 Redis 可以用更少、更精簡的指令來取代傳統複雜的程式碼，可以存取應用程式的資料。並支援 Java、Python、PHP、C/C++、C#、JavaScript、Node.js、Ruby、R、GO。\n複寫和持久性 Redis 採主要-複本架構，支援非同步複寫，可以將資料複寫到多個複本伺服器。不但可以提升讀取效能(因為請求可分割到多部伺服器)，還可以再主服務器發生故障時快速恢復。至於持久性，Redis 支援時間點備份，會將資料複製到磁碟中。","參考資料#參考資料":"Redis 官網：https://redis.io\nRedis：https://aws.amazon.com/tw/redis/\nRedis 基本資料形態：https://blog.judysocute.com/2020/10/04/redis-%E5%9F%BA%E6%9C%AC%E8%B3%87%E6%96%99%E5%BD%A2%E6%85%8B/\nRedis 發布訂閱：https://www.runoob.com/redis/redis-pub-sub.html","實際操作#實際操作":"安裝 Redis 使用 Homebrew 來安裝 Redis (Mac OS：11.6)\n$ brew install redis 安裝好用 -v 檢查版本\n$ redis-server -v Redis server v=6.2.6 sha=00000000:0 malloc=libc bits=64 build=c6f3693d1aced7d9 $ redis-cli -v redis-cli 6.2.6 一個是 Server、另一個是 Cli，所以在稍後測試時，需要開啟兩個 Terminal 來執行歐！\n執行 Redis Server $ redis-server 39403:C 08 Mar 2022 11:13:17.500 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo 39403:C 08 Mar 2022 11:13:17.500 # Redis version=6.2.6, bits=64, commit=00000000, modified=0, pid=39403, just started 39403:C 08 Mar 2022 11:13:17.500 39403:M 08 Mar 2022 11:13:17.501 * Increased maximum number of open files to 10032 (it was originally set to 256). 39403:M 08 Mar 2022 11:13:17.501 * monotonic clock: POSIX clock_gettime _._ _.-``__ ''-._ _.-`` `. `_. ''-._ Redis 6.2.6 (00000000/0) 64 bit .-`` .-```. ```\\/ _.,_ ''-._ ( ' , .-` | `, ) Running in standalone mode |`-._`-...-` __...-.``-._|'` _.-'| Port: 6379 | `-._ `._ / _.-' | PID: 39403 `-._ `-._ `-./ _.-' _.-' |`-._`-._ `-.__.-' _.-'_.-'| | `-._`-._ _.-'_.-' | https://redis.io `-._ `-._`-.__.-'_.-' _.-' |`-._`-._ `-.__.-' _.-'_.-'| | `-._`-._ _.-'_.-' | `-._ `-._`-.__.-'_.-' _.-' `-._ `-.__.-' _.-' `-._ _.-' `-.__.-' 39403:M 08 Mar 2022 11:13:17.503 # Server initialized 39403:M 08 Mar 2022 11:13:17.503 * Ready to accept connections 如果出現上面符號，就代表 Server 已經啟動，接下來再開另一個 Terminal 來執行 Cli。\nCli 開啟後，在下一個 ping 指令，該指令用於檢測 redis 服務是否啟動，正常會顯示 pong。\n$ redis-cli 127.0.0.1:6379\u003e ping PONG 接下來，我們都會在 Cli 視窗做測試，會詳細介紹每一個資料型態以及其適合情境。\n後續會使用 Cli 畫面來做示範，但也有不錯的圖形化工具可以用於 redis 上 AnotherRedisDesktopManager 大家可以去試用看看~\n資料型態 字串 (Strings) set、get 這是最基本的型態，可以存放 binary, string, integer, float資料，一個 Strings 的欄位，最高可儲存 512 Megabytes，這裡會用到的指令是 set、get ，分別用來儲存以及讀取字串，我們來看一下範例吧。\n127.0.0.1:6379\u003e set string hello-world OK 127.0.0.1:6379\u003e get string \"hello-world\" 我們先用 set 將 hello-world 字串存到 string 這個 key，再用 get 顯示 string 裡面的 value。\nincr、decr Redis 還有一些方便的指令，如果存入的 value 是 integer 型態，就可以使用 incr 、decr ，來累加與累減。分別代表累加，像是我們的 ++ ，以及累減，像是我們的 - -。\n127.0.0.1:6379\u003e set num 10 OK 127.0.0.1:6379\u003e incr num (integer) 11 127.0.0.1:6379\u003e incr num (integer) 12 127.0.0.1:6379\u003e decr num (integer) 11 127.0.0.1:6379\u003e decr num (integer) 10 append 如果 key 已經存在並且它是字串，可以使用 append 指令，會從字串最後面附加進去，如果不存在，則會直接建立一個，並把值存進去。\n127.0.0.1:6379\u003e exists str (integer) 0 127.0.0.1:6379\u003e append str \"Hello\" (integer) 5 127.0.0.1:6379\u003e append str \" World~\" (integer) 12 127.0.0.1:6379\u003e get str \"Hello World~\" getrange 可以輸入字串的開始位元與結束位元，會依照你輸入的去顯示字串。我把它理解成是陣列的 key 與 value 的關係。\n127.0.0.1:6379\u003e set a \"This is a string\" OK 127.0.0.1:6379\u003e getrange a 0 3 \"This\" 127.0.0.1:6379\u003e getrange a -3 -1 \"ing\" 127.0.0.1:6379\u003e getrange a 0 -1 \"This is a string\" 127.0.0.1:6379\u003e getrange a 10 100 \"string\" mset 我們也可以設定的時候，把要設定的值都一起設定，只需要使用 mset 就可以達成。\n127.0.0.1:6379\u003e mset 1 1 2 2 OK 127.0.0.1:6379\u003e get 1 \"1\" 127.0.0.1:6379\u003e get 2 \"2\" 127.0.0.1:6379\u003e get 3 (nil) 如果用 get 顯示資料，若沒有對應的 key ，會顯示 (nil)。\n字串型態適合場景 字串(strings) 型態適合用於圖片快取 （使用binary）、累計次數、觀看累計次數\nString 適用場景圖\n哈希 (Hashes) 可以把他想像成二維陣列，應該會比較好理解，我網路上找了一張圖，應該會比較清楚！\nHashes 示意圖 (Redis 基本資料形態)\nHashes 是用來存放一組相同性質的資料，這些資料 Hashes 或是物件的某一屬性，與 String 較為不同的是他可以取回單一個欄位資料，但 String 必須取回所有資料，單一個 Key 可以存放2^32 - 1的資料欄位，\n他的資料型態有像是，一個 user001 裡面是一個 Hashes，Hashes 裡面又會存放 name、phone、gender ，我們來實際操作看看。\nhset、hget 127.0.0.1:6379\u003e hset student name ian phone 0980123456 gender M (integer) 3 127.0.0.1:6379\u003e hget student name \"ian\" 127.0.0.1:6379\u003e hget student phone \"0980123456\" 127.0.0.1:6379\u003e hget student gender \"M\" Hashes 的話要使用 hset、hget 來對 Hashes 做儲存以及讀取。\nhgetall 想要一次顯示 Hashes 裡面的 key 跟 value ，可以使用 hgetall 來顯示全部資料。\n127.0.0.1:6379\u003e hset student name ian phone 0980123456 gender M (integer) 3 127.0.0.1:6379\u003e hgetall student 1) \"name\" 2) \"ian\" 3) \"phone\" 4) \"0980123456\" 5) \"gender\" 6) \"M\" hkeys 想要單獨顯示 Hashes 裡面的 key ，可以使用 hkeys 來顯示。\n127.0.0.1:6379\u003e hset student name ian phone 0980123456 gender M (integer) 3 127.0.0.1:6379\u003e hkeys student 1) \"name\" 2) \"phone\" 3) \"gender\" hvals 想要單獨顯示 Hashes 裡面的 value ，可以使用 hvals 來顯示。\n127.0.0.1:6379\u003e hset student name ian phone 0980123456 gender M (integer) 3 127.0.0.1:6379\u003e hvals student 1) \"ian\" 2) \"0980123456\" 3) \"M\" hlen 想要顯示 Hashes 裡面的 key 長度，可以使用 hlen 來顯示。\n127.0.0.1:6379\u003e hgetall student 1) \"name\" 2) \"ian\" 3) \"gender\" 4) \"M\" 127.0.0.1:6379\u003e hlen student (integer) 2 hincrby 想要增加 Hashes 裡面的 value 整數，可以使用 hincrby 來新增。\n127.0.0.1:6379\u003e hset test a 10 b 20 (integer) 2 127.0.0.1:6379\u003e hincrby test a 2 (integer) 12 hdel 想要刪除 Hashes 裡面的 key，可以使用 hdel 來刪除。\n127.0.0.1:6379\u003e hset student name ian phone 0980123456 gender M (integer) 3 127.0.0.1:6379\u003e hdel student phone (integer) 1 127.0.0.1:6379\u003e hgetall student 1) \"name\" 2) \"ian\" 3) \"gender\" 4) \"M\" 哈希型態適合場景 哈希(Hashes) 型態適合用於每次只需要取用一部分的資料\nHashes 適用場景圖\n列表 (Lists) lpush、lrange Lists 資料型態可以想像成程式語言中的Array物件。Lists 單一個Key可以存放2^32 - 1，這邊會使用到 lpush、lrange 來對 Lists 做儲存以及讀取。\n127.0.0.1:6379\u003e lpush list2 a a b b c d e (integer) 7 127.0.0.1:6379\u003e lrange list2 0 10 1) \"e\" 2) \"d\" 3) \"c\" 4) \"b\" 5) \"b\" 6) \"a\" 7) \"a\" rpush 除了從隊伍頭放入資料，也可以用 rpush 從隊伍尾放入資料，如果使用 lrange 來顯示方向也會相反歐。\n127.0.0.1:6379\u003e rpush list3 a a b b c d e (integer) 7 127.0.0.1:6379\u003e lrange list3 0 10 1) \"a\" 2) \"a\" 3) \"b\" 4) \"b\" 5) \"c\" 6) \"d\" 7) \"e\" lpop、rpop 也可以使用 lpop、rpop 分別從隊伍頭或尾彈出一筆資料。\n127.0.0.1:6379\u003e rpush list3 a a b b c d e (integer) 7 127.0.0.1:6379\u003e lpop list3 \"a\" 127.0.0.1:6379\u003e rpop list3 \"e\" lset 可以使用 lset 來設定指定位置的資料。\n127.0.0.1:6379\u003e lrange list3 0 2 1) \"a\" 2) \"b\" 3) \"c\" 127.0.0.1:6379\u003e lset list3 1 w OK 127.0.0.1:6379\u003e lrange list3 0 2 1) \"a\" 2) \"w\" 3) \"c\" 列表型態適合場景 列表(Lists) 型態適合用於文章列表或者資料分頁展示的應用\nLists 適用場景圖\n集合 (Sets) 其實跟 Lists 一樣，就是資料的集合，只是 Sets 多了一層限制，就是集合中的值不能重複，這邊會使用到 sadd、smembers 來對 Sets 做儲存以及讀取。\n127.0.0.1:6379\u003e sadd set1 a b c d a a b b (integer) 4 127.0.0.1:6379\u003e smembers set1 1) \"c\" 2) \"a\" 3) \"b\" 4) \"d\" 可以看到他實際寫入的只有 4 筆資料。\nspop 從 set 集合中隨機跳出一定數量的資料。\n127.0.0.1:6379\u003e sadd set1 a a b b c d (integer) 4 127.0.0.1:6379\u003e spop set1 2 1) \"c\" 2) \"d\" 127.0.0.1:6379\u003e spop set1 2 1) \"a\" 2) \"b\" sismember 可以使用 sismember 來檢查輸入值是否為 Set 集合的成員。\n127.0.0.1:6379\u003e sadd set a b b c d (integer) 4 127.0.0.1:6379\u003e sismember set d (integer) 1 127.0.0.1:6379\u003e sismember set f (integer) 0 srem 可以使用 srem 來刪除 Set 集合中的成員。\n127.0.0.1:6379\u003e sadd set a b b c d (integer) 4 127.0.0.1:6379\u003e srem set a d (integer) 2 127.0.0.1:6379\u003e smembers set 1) \"b\" 2) \"c\" sdiff 會顯示第一個集合與其他集合不同的值。\n127.0.0.1:6379\u003e sadd set1 a b c d (integer) 4 127.0.0.1:6379\u003e sadd set2 a b (integer) 2 127.0.0.1:6379\u003e sadd set3 b (integer) 1 127.0.0.1:6379\u003e sdiff set1 set2 set3 1) \"c\" 2) \"d\" 集合型態適合場景 集合(Sets) 型態適合用於文章中的Tag標籤、或是要排除相同資料\nSets 適用場景圖\n有序集合 (Zset) 故名思義，有序集合就是有排序的集合，Sorted Sets 結構會 多一個數字值去為排序的權重 來決定先後順序，這邊會使用到 zadd 、 zrangebyscore 來對 Zset 做儲存以及讀取。\n127.0.0.1:6379\u003e zadd sortset 10 '10' (integer) 1 127.0.0.1:6379\u003e zadd sortset 6 '6' (integer) 1 127.0.0.1:6379\u003e zadd sortset 2 '2' (integer) 1 127.0.0.1:6379\u003e zadd sortset 99 '99' (integer) 1 127.0.0.1:6379\u003e zrangebyscore sortset 0 100 1) \"2\" 2) \"6\" 3) \"10\" 4) \"99\" 就可以看出在顯示的時候並不是依照寫入順序，而是依照我們所設定的權重去排序的。\nwithscores 如果想要顯示 value 與 score ，可以使用 withscores 來顯示。\n127.0.0.1:6379\u003e zadd money 2000 tom (integer) 1 127.0.0.1:6379\u003e zadd money 3500 peter (integer) 1 127.0.0.1:6379\u003e zadd money 5000 jack (integer) 1 127.0.0.1:6379\u003e zrange money 0 -1 withscores 1) \"tom\" 2) \"2000\" 3) \"peter\" 4) \"3500\" 5) \"jack\" 6) \"5000\" zremrangebyscore 移除有序集合中，指定分數內區間的所有成員。\n127.0.0.1:6379\u003e zadd money 2000 tom (integer) 1 127.0.0.1:6379\u003e zadd money 3500 peter (integer) 1 127.0.0.1:6379\u003e zadd money 5000 jack (integer) 1 127.0.0.1:6379\u003e zremrangebyscore money 1500 3500 (integer) 2 127.0.0.1:6379\u003e zrange money 0 -1 withscores 1) \"jack\" 2) \"5000\" zcard 可以使用 zcard 來計算集合中元素的數量。\n127.0.0.1:6379\u003e zadd sort 10 10 (integer) 1 127.0.0.1:6379\u003e zadd sort 3 3 (integer) 1 127.0.0.1:6379\u003e zadd sort 66 66 (integer) 1 127.0.0.1:6379\u003e zadd sort 33 33 (integer) 1 127.0.0.1:6379\u003e zcard sort (integer) 4 有序集合型態適合場景 有序集合(Zset) 型態適合用於需要有序排列的資料\nZset 適用場景圖\n發布訂閱 (PUB/SUB) Redis 發布訂閱 (pub/sub) 是一種消息通信模式，發送者 (pub) 發送消息，訂閱者 (sub) 接收消息。\nRedis 客戶端可以使用 subscribe 來訂閱任意數量的頻道。\n下面這張圖是頻道1，以及訂閱這個頻道的三個用戶端分別是客戶端2、客戶端7、客戶端5\nSubscribe 示意圖\n當我們有新消息通過 publish 指令發送給頻道1 ，這個消息就會被發送給有訂閱頻道1的三個客戶端。\nPublish 示意圖\n我們來模擬一下吧 ! 先開兩個 Terminal 來執行 redis-cli 一個當作發送(pub)，另一個當作接收(sub)。\n我們先用第一個 Terminal 訂閱一個頻道 channel_1\n127.0.0.1:6379\u003e subscribe channel_1 Reading messages... (press Ctrl-C to quit) 1) \"subscribe\" 2) \"channel_1\" 3) (integer) 1 開啟另一個 Terminal 發送訊息到 channel_1\n127.0.0.1:6379\u003e publish channel_1 \"Hello World~\" (integer) 1 127.0.0.1:6379\u003e publish channel_1 \"ian~\" (integer) 1 127.0.0.1:6379\u003e publish channel_1 \"test~\" (integer) 1 這時候再切換回來第一個 Terminal ，就可以看到他接收到我們傳送的訊息 127.0.0.1:6379\u003e subscribe channel_1 Reading messages... (press Ctrl-C to quit) 1) \"subscribe\" 2) \"channel_1\" 3) (integer) 1 1) \"message\" 2) \"channel_1\" 3) \"Hello World~\" 1) \"message\" 2) \"channel_1\" 3) \"ian~\" 1) \"message\" 2) \"channel_1\" 3) \"test~\" "},"title":"Redis 介紹"},"/blog/redis/redis-sentinel-docker-compose-haproxy/":{"data":{"":"前情提要：本篇是 Redis 哨兵模式 (Sentinel) 搭配 Docker-compose 實作 的後續文章，主要會優化原本的程式碼，並加上 HAProxy 來做負載平衡！","什麼是-haproxy-以及負載均衡-#什麼是 HAProxy 以及負載均衡 ?":"HAProxy 是一個使用 C 語言編寫的自由及開放原始碼軟體，其提供高可用性、負載均衡，以及基於 TCP 和 HTTP 的應用程式代理。\n負載平衡 (Load Balance)：\n現在很多網路服務都需要服務大量使用者，以前可以砸錢擴充機器硬體設施，但隨著網路服務的用量暴增，增加伺服器硬體設備已經無法解決問題。\n為了可以擴充服務，負載平衡成為主流的技術，這幾年雖然雲端與分散式儲存運算技術火紅，除非有特別的使用需求，不然在技術上負載均衡算是比較容易達成與掌握的技術。\n負載平衡除了分流能力之外，有另一個很大的好處就是可以提供 High Availability，也就是傳說中的 HA 架構，好讓你一台機器掛了其他伺服器可以繼續服務，降低斷線率。\nHAProxy 與 Reids Sentinel 示意圖 selcukusta/redis-sentinel-with-haproxy","參考資料#參考資料":"HAProxy 首頁：http://www.haproxy.org/\nHAproxy的安裝設定及範例：https://tw511.com/a/01/6959.html\nredis sentinel集群配置及haproxy配置：https://www.cnblogs.com/tzm7614/p/5691912.html\n富人用 L4 Switch，窮人用 Linux HAProxy！：https://blog.toright.com/posts/3967/%E5%AF%8C%E4%BA%BA%E7%94%A8-l4-switch%EF%BC%8C%E7%AA%AE%E4%BA%BA%E7%94%A8-linux-haproxy%EF%BC%81.html\nselcukusta/redis-sentinel-with-haproxy：https://github.com/selcukusta/redis-sentinel-with-haproxy\nHow to Enable Health Checks in HAProxy：https://www.haproxy.com/blog/how-to-enable-health-checks-in-haproxy/","實作#實作":"那接下來會使用 Docker-compose 實作 Redis 哨兵模式 + HAProxy，範例程式連結 點我 😘\n版本資訊\nmacOS：11.6 Docker：Docker version 20.10.12, build e91ed57 Nginx：1.20 PHP：7.4-fpm Redis：6.2.6 HAProxy：HAProxy version 2.5.5-384c5c5 2022/03/14 - https://haproxy.org/ 檔案結構 . ├── Docker-compose.yaml ├── docker-volume │ ├── haproxy │ │ └── haproxy.cfg │ ├── nginx │ │ └── nginx.conf │ ├── php │ │ ├── info.php │ │ ├── r.php │ │ └── rw.php │ └── redis │ ├── redis.conf │ ├── redis1 │ ├── redis2 │ └── redis3 ├── php │ └── Dockerfile ├── redis.sh └── sentinel ├── Docker-compose.yaml ├── sentinel1 │ └── sentinel.conf ├── sentinel2 │ └── sentinel.conf └── sentinel3 └── sentinel.conf 這是主要的結構，簡單說明一下：\nDocker-compose.yaml：會放置要產生的 Nginx、PHP、redis1、redis2、redis3 容器設定檔。 docker-volume/haproxy/haproxy.cfg：haproxy 的設定檔。 docker-volume/nginx/nginx.conf：nginx 的設定檔。 docker-volume/php/(r.php、rw.php)：測試用檔案。 docker-volume/redis/redis.conf：redis 的設定檔。 docker-volume/redis/(redis1、redis2、redis3)：放 redis 的資料。 php/Dokcerfile：因為在 php 要使用 redis 需要多安裝一些設定，所以用 Dockerfile 另外寫 PHP 的映像檔。 redis.sh：是我另外多寫的腳本，可以查看相對應的角色。 sentinel/Docker-compose.yaml：會放置要產生的 haproxy、sentinel1、sentinel2、sentinel3 的容器設定檔。 sentinel/(sentinel1、sentinel2、sentinel3)/.conf：哨兵的設定檔。 那我們就依照安裝的設定開始說明：\nDocker-compose.yaml version: '3.8' services: nginx: image: nginx:1.20 container_name: nginx networks: HAProxy_Redis: ports: - \"8888:80\" volumes: - ./docker-volume/nginx/:/etc/nginx/conf.d/ - ./log/nginx/:/var/log/nginx/ environment: - TZ=Asia/Taipei php: build: ./php container_name: php networks: HAProxy_Redis: expose: - 9000 volumes: - ./docker-volume/php/:/var/www/html redis1: image: redis container_name: redis1 command: redis-server /usr/local/etc/redis/redis.conf --appendonly yes volumes: - ./docker-volume/redis/redis1/:/data - ./docker-volume/redis/:/usr/local/etc/redis/ - ./log/redis1:/var/log/redis/ environment: - TZ=Asia/Taipei networks: HAProxy_Redis: ipv4_address: 172.20.0.11 ports: - 6379:6379 redis2: image: redis container_name: redis2 command: redis-server /usr/local/etc/redis/redis.conf --slaveof redis1 6379 --appendonly yes volumes: - ./docker-volume/redis/redis2/:/data - ./docker-volume/redis/:/usr/local/etc/redis/ - ./log/redis2:/var/log/redis/ environment: - TZ=Asia/Taipei networks: HAProxy_Redis: ipv4_address: 172.20.0.12 ports: - 6380:6379 depends_on: - redis1 redis3: image: redis container_name: redis3 command: redis-server /usr/local/etc/redis/redis.conf --slaveof redis1 6379 --appendonly yes volumes: - ./docker-volume/redis/redis3/:/data - ./docker-volume/redis/:/usr/local/etc/redis/ - ./log/redis3:/var/log/redis/ environment: - TZ=Asia/Taipei networks: HAProxy_Redis: ipv4_address: 172.20.0.13 ports: - 6381:6379 depends_on: - redis1 - redis2 networks: HAProxy_Redis: driver: bridge name: HAProxy_Redis ipam: config: - subnet: 172.20.0.0/16 gateway: 172.20.0.1 一樣詳細的 Docker 設定說明，可以參考 Docker 介紹 內有詳細設定說明。其他比較特別的地方是：\n幫每一個容器都設定好 IP ，方便後續測試使用。 有掛載 log 目錄，可以將我們設定好的 log 做收集。 呈上，有加入 environment 時區，這樣在看 log 的時候才知道正確時間。 docker-volume/haproxy/haproxy.cfg global log stdout format raw local0 info defaults mode http # 默認模式 { tcp | http | health }，tcp 是4層，http 是7層，health 只會返回 OK timeout client 10s # 客戶端超時 timeout connect 5s # 連接超時 timeout server 10s # 伺服器超時 timeout http-request 10s log global listen admin_status bind 0.0.0.0:8404 mode http stats enable stats uri /redis stats realm Global\\ statistics stats refresh 1s listen rw-redis # 判斷是否為 master 並可讀可寫 bind 0.0.0.0:16379 mode tcp balance roundrobin option tcp-check # redis 健康检查，確保是 master tcp-check connect tcp-check send PING\\r\\n tcp-check expect string +PONG tcp-check send info\\ replication\\r\\n tcp-check expect string role:master tcp-check send QUIT\\r\\n tcp-check expect string +OK server redis1 redis1:6379 check inter 2000 server redis2 redis2:6379 check inter 2000 server redis3 redis3:6379 check inter 2000 listen r-redis # 判斷是否為 master、slave 並可讀 bind 0.0.0.0:16380 mode tcp balance roundrobin server redis1 redis1:6379 check inter 2000 server redis2 redis2:6379 check inter 2000 server redis3 redis3:6379 check inter 2000 這裡是本章的重點，我們會在這邊設定好 haproxy，詳細說明請看：\ndefaults： 一些初始值，像是 mode 我們預設 http，它主要有三種模式 { tcp | http | health }，tcp 是4層，http 是7層，health 只會返回 OK，以及客戶端、連接、伺服器、http 請求超時時間設定。\nlisten admin_status：\nbind：我們要開啟 HAProxy 監控平台的 port。 mode：模式，我們使用 http 模式。 stats ：是否要啟動平台。 stats uri：平台網址，我們使用 redis。 stats refresh：平台自動更新時間，我們設定 1 秒。 listen rw-redis：\nbind ：rw 使用 16379 Port 來當輸出。 balance：使用負載平衡。 option tcp-check # redis 健康检查，確保是 master tcp-check connect tcp-check send PING\\r\\n tcp-check expect string +PONG tcp-check send info\\ replication\\r\\n tcp-check expect string role:master tcp-check send QUIT\\r\\n tcp-check expect string +OK 上面這些是用來判斷角色是不是 master。\n最後放我們 3 個 redis 服務：\nserver redis1 redis1:6379 check inter 2000 server redis2 redis2:6379 check inter 2000 server redis3 redis3:6379 check inter 2000 check：開啟健康偵測。 inter：參數更改檢查間隔，預設是 2 秒。 docker-volume/nginx/nginx.conf server { listen 80; server_name default_server; return 404; } server { listen 80; server_name test.com; index index.php index.html; error_log /var/log/nginx/error.log warn; access_log /var/log/nginx/access.log; root /var/www/html; location / { try_files $uri $uri/ /index.php?$query_string; } location ~ \\.php$ { fastcgi_pass php:9000; fastcgi_index index.php; include fastcgi_params; fastcgi_param SCRIPT_FILENAME /var/www/html$fastcgi_script_name; } } Nginx 設定檔案。\ndocker-volume/php/rw.php \u003c?php $redis = new Redis(); $redis-\u003econnect('172.20.0.20', 16379); $r = $redis-\u003einfo(); echo $r['run_id'] . '\u003cbr\u003e' . $r['role'] . '\u003cbr\u003e\u003cbr\u003e'; echo '\u003cpre\u003e', print_r($r), '\u003c/pre\u003e'; 跟 r.php 比較不同的是，使用 16379 Port，我們在 haproxy.cfg 有設定 rw-redis，來判斷是不是 master 並且是可讀可寫。\ndocker-volume/php/r.php \u003c?php $redis = new Redis(); $redis-\u003econnect('172.20.0.20', 16380); $r = $redis-\u003einfo(); echo $r['run_id'] . '\u003cbr\u003e' . $r['role'] . '\u003cbr\u003e\u003cbr\u003e'; echo '\u003cpre\u003e', print_r($r), '\u003c/pre\u003e'; 使用 16380 Port，在 haproxy.cfg 有設定 r-redis，來顯示是不是 master、slave 且可讀。\nphp/Dockerfile FROM php:7.4-fpm RUN pecl install -o -f redis \\ \u0026\u0026 rm -rf /tmp/pear \\ \u0026\u0026 echo \"extension=redis.so\" \u003e /usr/local/etc/php/conf.d/redis.ini \\ \u0026\u0026 echo \"session.save_handler = redis\" \u003e\u003e /usr/local/etc/php/conf.d/redis.ini \\ \u0026\u0026 echo \"session.save_path = tcp://redis:6379\" \u003e\u003e /usr/local/etc/php/conf.d/redis.ini 因為 PHP 要使用 Redis，會需要安裝一些套件，所以我們將 PHP 分開來，使用 Dockerfile 來設定映像檔。\nredis.sh #!/bin/bash green=\"\\033[1;32m\";white=\"\\033[1;0m\";red=\"\\033[1;31m\"; echo \"redis1 IPAddress:\" redis1_ip=`docker inspect redis1 | grep \"IPv4\" | egrep -o \"[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\"` echo $redis1_ip; echo \"------------------------------\" echo \"redis2 IPAddress:\" redis2_ip=`docker inspect redis2 | grep \"IPv4\" | egrep -o \"[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\"` echo $redis2_ip; echo \"------------------------------\" echo \"redis3 IPAddress:\" redis3_ip=`docker inspect redis3 | grep \"IPv4\" | egrep -o \"[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\"` echo $redis3_ip; echo \"------------------------------\" echo \"haproxy IPAddress:\" haproxy_ip=`docker inspect haproxy | grep \"IPv4\" | egrep -o \"[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\"` echo $haproxy_ip; echo \"------------------------------\" echo \"redis1:\" docker exec -it redis1 redis-cli info Replication | grep role echo \"redis2:\" docker exec -it redis2 redis-cli info Replication | grep role echo \"redis3:\" docker exec -it redis3 redis-cli info Replication | grep role 這個是我自己所寫的腳本，可以詳細知道目前服務的角色轉移狀況。\nsentinel/Docker-compose.yaml version: '3.8' services: haproxy: image: haproxy container_name: haproxy volumes: - ../docker-volume/haproxy/:/usr/local/etc/haproxy environment: - TZ=Asia/Taipei networks: HAProxy_Redis: ipv4_address: 172.20.0.20 ports: - 16379:6379 - 8404:8404 sentinel1: image: redis container_name: redis-sentinel-1 networks: HAProxy_Redis: ports: - 26379:26379 command: redis-server /usr/local/etc/redis/sentinel.conf --sentinel volumes: - ./sentinel1:/usr/local/etc/redis/ - ../log/sentinel1:/var/log/redis/ environment: - TZ=Asia/Taipei sentinel2: image: redis container_name: redis-sentinel-2 networks: HAProxy_Redis: ports: - 26380:26379 command: redis-server /usr/local/etc/redis/sentinel.conf --sentinel volumes: - ./sentinel2:/usr/local/etc/redis/ - ../log/sentinel2:/var/log/redis/ environment: - TZ=Asia/Taipei sentinel3: image: redis container_name: redis-sentinel-3 networks: HAProxy_Redis: ports: - 26381:26379 command: redis-server /usr/local/etc/redis/sentinel.conf --sentinel volumes: - ./sentinel3:/usr/local/etc/redis/ - ../log/sentinel3:/var/log/redis/ environment: - TZ=Asia/Taipei networks: HAProxy_Redis: external: name: HAProxy_Redis sentinel/sentine.conf 因為 sentine 內容都基本上相同，所以舉一個來說明：\nport 26379 logfile \"/var/log/redis/redis-sentinel.log\" protected-mode no #設定要監控的 Master，最後的 2 代表判定客觀下線所需的哨兵數 sentinel monitor mymaster 172.20.0.11 6379 2 #哨兵 Ping 不到 Master 超過此毫秒數會認定主觀下線 sentinel down-after-milliseconds mymaster 5000 要設定指定的 Port sentine1 是 26379、sentine2 是 26380、sentine3 是 26381。接下來要設定要監控的 Master，最後的數字代表我們前面有提到客觀下線需要達到的哨兵數。以及主觀下線的時間跟 failover 超過的時間。","測試#測試":"我們先用 docker-compose up 來啟動 Docker-compose.yaml，接著再啟動 sentinel/Docker-compose.yaml：\n啟動 Docker-compose.yaml\n啟動 sentinel/Docker-compose.yaml\n由於為了測試，有先將所有容器設定好 IP，就不會像上一篇文章一樣要去抓 IP ，才能啟動 Sentinel。\n這時候可以使用瀏覽器搜尋以下網址：\ntest.com:8404/redis：HAProxy 監看平台(只取片段)。 HAProxy 監控平台\ntest.com:8888/rw.php：只會顯示 master，並且可讀可寫。 master (redis1)\ntest.com:8888/r.php：會顯示 master、slave，且可讀。 master (redis1)\nslave (redis2)\nslave (redis3)\n接下來可以執行：\n$ sh redis.sh redis1 IPAddress: 172.20.0.11 ------------------------------ redis2 IPAddress: 172.20.0.12 ------------------------------ redis3 IPAddress: 172.20.0.13 ------------------------------ haproxy IPAddress: 172.20.0.20 ------------------------------ redis1: role:master redis2: role:slave redis3: role:slave 就會顯示三個 redis 的 IP 以及 haproxy 的 IP，這些都是已經寫在 Docker-compose.yaml 檔案內的，如果忘記的可以再往前看 ↑\n接下來我們可以先一直 F5 test.com:8888/r.php，來模擬大量的讀取請求，如果發現網站內容一直在更換，就代表我們成功透過 HAProxy 做到負載平衡了，可以將讀取的需求分給三個服務做處理！那因為 test.com:8888/rw.php 他只會抓 master，所以刷新還是同一個 master。\n還記得我們上次用 Redis 的哨兵模式嗎？那我們用它來搭配 Haproxy 會有什麼結果呢？\n我們先使用 docker stop 來模擬服務中斷：\n$ docker stop redis1 redis1 可以看到 test.com:8404/redis 原本綠色的 redis1 開始變成黃色，最後變成紅色：\n模擬中斷服務 HAProxy 監控平台\n最後可以看到 Redis Sentinel 作動，將 master 轉移到 redis3：\n模擬中斷服務 HAProxy 監控平台\n這時候我們再去看 test.com:8888/rw.php ，就會發現與剛剛的 master 不太一樣囉，因為已經變成 redis2 了！\nmaster (redis2)\n代表我們 HAProxy 也有成功將 master 給顯示出來！\n我們再去看 test.com:8888/r.php ，就可以發現剩下 redis2 以及 redis 3 了，因為 redis1 被我們給暫停服務了，而且 redis2 變成 master！\nmaster (redis2)\nslave (redis3)"},"title":"用 HAProxy 對 Redis 做負載平衡 (Redis Sentinel、Docker-compose)"},"/blog/redis/redis-sentinel-docker-compose/":{"data":{"":"","什麼是-redis-哨兵模式-sentinel-#什麼是 Redis 哨兵模式 (Sentinel) ?":"有關 Redis 之前有寫一篇 Redis 的介紹文，有興趣可以去看看！\nRedis 提供非常實用的功能來讓我們實現多機的 in-memory 資料庫：\n主從複製模式 (Master-Slave Replication) 哨兵模式 (Sentinel) 叢集模式 (Cluster) 我們這邊主要介紹哨兵模式 (Sentinel)，但主要也是由主從複製模式 (Master-Slave Replication) 修改而來：\n哨兵模式就是用來監視 Redis 系統，哨兵會監控 Master 是否正常運作。如果遇到 Master 出現故障或是離線時，哨兵之間會開始判斷，直到我們所設定需達到的判斷數量後，哨兵會將其所屬的 Slave 變成 Master，並再將其他的 Slave 指向新的 Master。\n哨兵模式 (Sentinel)\n監控 哨兵會和要監控的 Master 建立兩條連接，Cmd 和 Pub/Sub：\nCmd 是哨兵用來定期向 Master 發送 Info 命令以取得 Master 的訊息，訊息中會包含 Master 有哪些 Slave。當與 Master 獲得 Slave 訊息後，哨兵也會和 Slave 建立連接。 哨兵也會定期透過 Cmd 向 Master、Slave 和其他哨兵發送 Ping 指令來檢查是否存在，確認節點狀態等。 Pub/Sub 讓哨兵可以透過訂閱 Master 和 Slave 的 __Sentinel__:hello 這個頻道來和其他哨兵定期的進行資訊交換。 主觀下線 (SDOWN) 主觀下線是指單個哨兵認為 Master 已經停止服務了，有可能是網路不通或是接收不到訂閱等，而哨兵的判斷是依據傳送 Ping 指令之後一定時間內是否收到回覆或是錯誤訊息，如果有哨兵就會主觀認為這個 Master 已經下線停止服務了。\n客觀下線 (ODOWN) 客觀下線是指由多個哨兵對同一個 Master 各自進行主觀下線的判斷後，再綜合所有哨兵的判斷。若是認為主觀下線的哨兵到達我們所配置的數量後，即為客觀下線。\n故障轉移 (Failover) 當 Master 已經被標記為客觀下線時，起初發現 Master 下線的哨兵會發起一個選舉 (採用的是 Raft 演算法)，並要求其他哨兵選他做為領頭哨兵，領頭哨兵會負責進行故障的恢復。當選的標準是要有超過一半的哨兵同意，所以哨兵的數量建議設定成奇數個。\n此時若有多個哨兵同時參選領頭哨兵，則有可能會發生一輪後沒有產生勝選者，則所有的哨兵會再等隨機一個時間再次發起參選的請求，進行下一輪的選舉，一直到選出領頭為止。所以若哨兵數量為偶數就很有可能一直無法選出領頭哨兵。\n選出領頭哨兵後，領頭哨兵會開始從下線的 Master 所屬 Slave 中跳選出一個來變成新的 Master，挑選的依據如下：\n所有在線的 Slave 擁有最高優先權的，優先權可以透過 slave-priority 來做設定。 如果有多個同為最高優先權的 Slave，則會選擇複製最完整的。 若還是有多個 Slave 皆符合上述條件，則選擇 id 最小的。 接著領頭哨兵會將舊的 Master 更新成新的 Master 的 Slave ，讓其恢復服務後以 Slave 的身份繼續運作。","參考資料#參考資料":"Redis (六) - 主從複製、哨兵與叢集模式：https://blog.tienyulin.com/redis-master-slave-replication-sentinel-cluster/","測試#測試":"我們先用 docker-compose up 來啟動 Docker-compose.yaml ，接著下 sh redis.sh 指令來查看各服務的 IP 位置以及目前角色：\n啟動 Docker-compose.yaml\n使用 redis.sh 檢查目前 IP 及角色\n接下來先修改 docker-volume/php/index.php\n$sentinel = array( array( 'host' =\u003e '192.168.208.4', 'port' =\u003e 6379, 'role' =\u003e 'master', ), array( 'host' =\u003e '192.168.208.5', 'port' =\u003e 6379, 'role' =\u003e 'slave1', ), array( 'host' =\u003e '192.168.208.6', 'port' =\u003e 6379, 'role' =\u003e 'slave2', ), ); 將各自的 IP 帶入測試的網站中。\n可以瀏覽 test.com:8888 是否有正常抓到 redis 的 master\ntest.com:8888 測試網站\n再來修改 sentinel 內的 sentinel(1、2、3).conf 檔案\nport 26379 #設定要監控的 Master，最後的 2 代表判定客觀下線所需的哨兵數 sentinel monitor mymaster 192.168.208.4 6379 2 #哨兵 Ping 不到 Master 超過此毫秒數會認定主觀下線 sentinel down-after-milliseconds mymaster 5000 #failover 超過次毫秒數即代表 failover 失敗 sentinel failover-timeout mymaster 180000 將要監控的 Master IP 帶入 sentinel monitor mymaster。\n再啟動 sentinel/Docker-compose.yaml\n啟動 Docker-compose.yaml\n接下來我們來模擬假設 Master 服務中斷後，sentinel 會發生什麼事情：\ndocker stop redis-master 下完指令後，再使用 sh redis.sh 來看看目前的 role 狀態：\n使用 redis.sh 檢查目前 IP 及角色\n發現已經抓不到 master IP 以及他的角色。\n等待一下子後，重新下 sh redis.sh 來看目前的 role 狀態：\n使用 redis.sh 檢查目前 IP 及角色\n就會發現已經將 master 轉移到原 slave1。\n那我們來看一下 sentinel 在背後做了哪些事情：\n判斷是否主觀下線及客觀下線，並發起投票\n可以看到三個哨兵都認為 master 為 主觀下線 (sdown)，這時 sentinel-2 就認定為 客觀下線 (odown)，並發起投票要求成為領頭哨兵。\n進行透票，確認誰當選\n我們可以看到 Sentinel2 和 Sentinel3 都投給 Sentinel2，所以最後 Sentinel2 當選。\n領頭哨兵選定新 master\n接著 sentinel2 選出 redis-slave1 (192.168.208.5:6379) 作為 Master ，並且使用 failover-state-send-slaveof-noone 來將 redis-slave1 解除 Slave 狀態變成獨立的 master，隨後將 redis-slave1 升成 master。\n設定 master 並修改原 master 變成 slave\n設定完新的 Master 後，Sentinel2 讓原本的 Master 轉為 Slave，並且讓 redis-slave2(192.168.208.6:6379) 指向新的 Master。最後 Sentinel1 和 Sentinel3 開始從 Sentinel2 取得設定然後更新自己的設定，至此整個故障轉移就完成了。\n最後我們來看一下我們用 PHP 連線的測試：\n連線 master\n就會發現，已經 slave1 變成現在的 master。\n那我們最後把原本的 master 恢復，看看會發生什麼事情：\n連線 master\n會發現因為該啟動 master，所以他還認為他是 master，但過一下下，在查看就正常顯示 slave1 為 master，舊的 master 就變成 slave。\nRedis 常見錯誤問題 在學習的時候，有發現啟動 sentinel 的時候，會跳出 WARNING: Sentinel was not able to save the new configuration on disk!!!: Permission denied 錯誤訊息，後來再去翻 redis github 的 issus 才發現作者也有發現這個問題，且已經修復了，主要是權限的問題，以及要掛載目錄而非 conf 檔案。\nredis WARNING 錯誤訊息 github issus","用-docker-實作-redis-哨兵模式#用 Docker 實作 Redis 哨兵模式":"那接下來會使用 Docker 來實作 Redis 的哨兵模式，範例程式連結 點我 😘\n版本資訊\nmacOS：11.6 Docker：Docker version 20.10.12, build e91ed57 Nginx：1.20 PHP：7.4-fpm Redis：latest 檔案結構 . ├── Docker-compose.yaml ├── docker-volume │ ├── log │ │ └── nginx │ │ ├── access.log │ │ └── error.log │ ├── nginx │ │ └── nginx.conf │ ├── php │ │ └── index.php │ ├── redis-master │ ├── redis-slave1 │ └── redis-slave2 ├── redis.sh ├── php │ └── Dockerfile └── sentinel ├── Docker-compose.yaml ├── sentinel1.conf ├── sentinel2.conf └── sentinel3.conf 這是主要的結構，簡單說明一下：\nDocker-compose.yaml：會放置要產生的 Nginx、PHP、redis-master、redis-slave1、redis-slave2 容器設定檔。 docker-volume：是我們要掛載進去到容器內的檔案，包含像是 nginx.conf 或是 log/nginx 以及 redis 記憶體儲存內容。 redis.sh：是我另外多寫的腳本，可以在最後方面我們測試 Redis sentinel 是否成功。 php/Dokcerfile：因為在 php 要使用 redis 需要多安裝一些設定，所以用 Dockerfile 另外寫 PHP 的映像檔。 sentinel/Docker-compose.yaml：會放置要產生的 sentinel1、sentinel2、sentinel3 的容器設定檔。 sentinel(1、2、3).conf：哨兵的設定檔。 那我們就依照安裝的設定開始說明：\nDocker-compose.yaml version: '3.8' services: nginx: image: nginx:1.20 container_name: nginx ports: - \"8888:80\" volumes: - ./docker-volume/nginx/:/etc/nginx/conf.d/ - ./docker-volume/log/nginx/:/var/log/nginx/ php: build: ./php container_name: php expose: - 9000 volumes: - ./docker-volume/php/:/var/www/html redis-master: image: redis container_name: redis-master volumes: - ./docker-volume/redis-master/:/data ports: - 6379:6379 command: redis-server --appendonly yes redis-slave1: image: redis container_name: redis-slave1 volumes: - ./docker-volume/redis-slave1/:/data ports: - 6380:6379 command: redis-server --slaveof redis-master 6379 --appendonly yes depends_on: - redis-master redis-slave2: image: redis container_name: redis-slave2 volumes: - ./docker-volume/redis-slave2/:/data ports: - 6381:6379 command: redis-server --slaveof redis-master 6379 --appendonly yes depends_on: - redis-master - redis-slave1 詳細的 Docker 設定說明，可以參考 Docker 介紹 內有詳細設定說明。比較特別的地方是：\nredis-(master、slave1、slave2)\nvolumes：將 redis 的資料掛載到 docker-volume/redis-(master、slave1、slave2)。 command：使用 redis-server 啟動，並且將該服務器轉變成指定服務器的從屬服務器 (slave server)。(如果想要保存 redis 的資料，要記得在 後面加上 –appendonly yes) docker-volume/nginx/nginx.conf server { listen 80; server_name default_server; return 404; } server { listen 80; server_name test.com; index index.php index.html; error_log /var/log/nginx/error.log warn; access_log /var/log/nginx/access.log; root /var/www/html; location / { try_files $uri $uri/ /index.php?$query_string; } location ~ \\.php$ { fastcgi_pass php:9000; fastcgi_index index.php; include fastcgi_params; fastcgi_param SCRIPT_FILENAME /var/www/html$fastcgi_script_name; } } Nginx 設定檔案。\nphp/Dockerfile FROM php:7.4-fpm RUN pecl install -o -f redis \\ \u0026\u0026 rm -rf /tmp/pear \\ \u0026\u0026 echo \"extension=redis.so\" \u003e /usr/local/etc/php/conf.d/redis.ini \\ \u0026\u0026 echo \"session.save_handler = redis\" \u003e\u003e /usr/local/etc/php/conf.d/redis.ini \\ \u0026\u0026 echo \"session.save_path = tcp://redis:6379\" \u003e\u003e /usr/local/etc/php/conf.d/redis.ini 因為 PHP 要使用 Redis，會需要安裝一些套件，所以我們將 PHP 分開來，使用 Dockerfile 來設定映像檔。\nsentinel/sentine.conf 因為 sentine 內容都基本上相同，所以舉一個來說明：\nport 26379 #設定要監控的 Master，最後的 2 代表判定客觀下線所需的哨兵數 sentinel monitor mymaster 192.168.176.4 6379 2 #哨兵 Ping 不到 Master 超過此毫秒數會認定主觀下線 sentinel down-after-milliseconds mymaster 5000 #failover 超過次毫秒數即代表 failover 失敗 sentinel failover-timeout mymaster 180000 要設定指定的 Port sentine1 是 26379、sentine2 是 26380、sentine3 是 26381。接下來要設定要監控的 Master，最後的數字代表我們前面有提到客觀下線需要達到的哨兵數。以及主觀下線的時間跟 failover 超過的時間。\nsentinel/Docker-compose.yaml version: '3.8' services: sentinel1: image: redis container_name: redis-sentinel-1 ports: - 26379:26379 command: redis-sentinel /usr/local/etc/redis/sentinel.conf volumes: - ./sentinel1.conf:/usr/local/etc/redis/sentinel.conf sentinel2: image: redis container_name: redis-sentinel-2 ports: - 26380:26379 command: redis-sentinel /usr/local/etc/redis/sentinel.conf volumes: - ./sentinel2.conf:/usr/local/etc/redis/sentinel.conf sentinel3: image: redis container_name: redis-sentinel-3 ports: - 26381:26379 command: redis-sentinel /usr/local/etc/redis/sentinel.conf volumes: - ./sentinel3.conf:/usr/local/etc/redis/sentinel.conf networks: default: external: name: redis_default docker-volume/php/index.php \u003c?php $redis = new Redis(); $sentinel = array( array( 'host' =\u003e '192.168.176.4', 'port' =\u003e 6379, 'role' =\u003e 'master', ), array( 'host' =\u003e '192.168.176.5', 'port' =\u003e 6379, 'role' =\u003e 'slave1', ), array( 'host' =\u003e '192.168.176.6', 'port' =\u003e 6379, 'role' =\u003e 'slave2', ), ); foreach ($sentinel as $value) { try { $redis-\u003econnect($value['host'], $value['port']); $redis-\u003eset('foo', 'bar'); echo \"連線成功 \" . $value['host'] . \"\u003cbr\u003e目前 master：\" . $value['role'] . \"\u003cbr\u003e\"; } catch (\\Exception $e) { continue; } } 為了要讓 PHP 可以知道目前的 Master 是哪一個服務器，所以寫了一個 try…catch 來做判斷，並且把3個服務內容都放到陣列中，後續再測試中會再說明。\nredis.sh #!/bin/bash green=\"\\033[1;32m\";white=\"\\033[1;0m\";red=\"\\033[1;31m\"; echo \"master IPAddress:\" master_ip=`docker inspect redis-master | grep \"IP\" | egrep -o \"[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\"` echo $master_ip; echo \"------------------------------\" echo \"slave1 IPAddress:\" slave1_ip=`docker inspect redis-slave1 | grep \"IP\" | egrep -o \"[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\"` echo $slave1_ip; echo \"------------------------------\" echo \"slave2 IPAddress:\" slave2_ip=`docker inspect redis-slave2 | grep \"IP\" | egrep -o \"[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\"` echo $slave2_ip; echo \"------------------------------\" echo \"master:\" docker exec -it redis-master redis-cli info Replication | grep role echo \"slave1:\" docker exec -it redis-slave1 redis-cli info Replication | grep role echo \"slave2:\" docker exec -it redis-slave2 redis-cli info Replication | grep role 這個是我自己所寫的腳本，再等等測試時，可以詳細知道目前服務的角色轉移狀況。"},"title":"Redis 哨兵模式 (Sentinel) 搭配 Docker-compose 實作"},"/blog/terraform/":{"data":{"":"此分類包含 Terraform 相關的文章。\n如何導入 Terragrunt，Terragrunt 好處是什麼？發布日期：2023-06-21 如何將 Terraform 改寫成 module ?發布日期：2023-06-16 Terraform 如何多人共同開發 (將 tfstate 存在後端)發布日期：2023-05-29 使用 Terraform 建立 Google Kubernetes Engine發布日期：2022-11-29 使用 Terraform 建立 Google Compute Engine發布日期：2022-11-28 什麼是 IaC ? Terraform 又是什麼？發布日期：2022-09-26 "},"title":"Terraform"},"/blog/terraform/terraform-gce/":{"data":{"":"嗨嗨大家好，距離上一篇筆記又隔了 3 個月，最近公司有專案在忙，沒時間把上次提到的 Terraform 應用筆記寫完，現在他來拉～～～ 😂 我們這次的主題是使用 Terraform 來建立 Google Compute Engine 的機器，想知道要怎麼用一段程式碼就可以建立、修改、刪除 Google Compute Engine 的機器一定要來看這一篇～我們開始囉 🧑‍💻","修改-google-compute-engine#修改 Google Compute Engine":"當我們發現我們建立的 Google Compute Engine 參數有錯，想要修改時，我們只需要修改程式碼部分，並重新下一次 terraform apply 來修改 Google Compute Engine，就會看到以下畫面 (有些設定檔是不能修改的，若修改他會重新創建一個新的機器，像是 name 之類的，使用時要小心一點 😉)\n我們拿剛剛提到的 nat_ip，我們先把它註解掉，再下 terraform apply 看看機器有什麼變化～\nterraform changed\n可以看到他會提示說，他會改變 network_interface，也移除 access_config 的設定，執行後的 Resources 也會從剛剛的 added 變成 changed，我們看一下 GCP 有沒有改變：\nterraform changed\n可以看到原本的外部 IP 位置被改掉了～ 最後提醒：如果有使用 terraform 來修改資源設定，不能動到特定的項目，不然他的流程是先把原本的給刪掉，再重新建立一個新的，原本的機器沒有備份，東西就會不見歐～","刪除-google-compute-engine#刪除 Google Compute Engine":"最後假如我們要刪除 Google Compute Engine，也可以使用 terraform 的指令來刪除，我們順便來測試一下上面有設定的 deletion_protection 刪除保護機制是不是正常～\n目前 deletion_protection 還是 true，我們直接下 terraform destroy，看看是否可以刪除 Google Compute Engine\nterraform destroy\n可以看到他會提醒你說 Deletion Protection is enabled，必須先把他改成 false terraform apply 後才可以刪除～\n改成 false 在下 terraform apply\n現在 deletion_protection 是 false，我們就可以下 terraform destroy 來刪除 Google Compute Engine 🔪\nterraform destroy\nGoogle Compute Engine 刪除中…\n以上就是簡單的使用 Terraform 建立 Google Compute Engine 介紹囉～歡迎大家留言指教，明天的文章是介紹如何使用 Terraform 建立 GKE 💕","參考資料#參考資料":"registry.terraform.io/providers (google_compute_instance)：https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/compute_instance","建立-google-compute-engine#建立 Google Compute Engine":"當我們寫好 Google Compute Engine tf 檔案後，我們接著把他建立，建立前要先使用 terraform init 來做初始化\nterraform init\n接著可以先使用 terraform plan 來查看我們的設定是否是我們想要的，或是直接用 terraform apply 來建立 Google Compute Engine\nterraform apply\n可以看到成功建立我們的 test Google Compute Engine（也可以看到因為我們有開 nat_ip 所以有外部 IP）\nGCP Google Compute Engine","撰寫-google-compute-engine-tf-檔案#撰寫 Google Compute Engine tf 檔案":"相信大家有先看完上一篇 什麼是 IaC ? Terraform 又是什麼？ 才來看這一篇的對吧 😎，對於 Terraform 的程式架構及指令，我們這邊就不多做介紹，我們直接來看程式要怎麼寫～(程式碼主要是參考官方文件，加上一些其他的設定來做介紹，程式碼也會同步到 Github ，需要的也可以去 Clone 來使用歐！ Github 程式碼連結 )\n小提醒：由於程式碼較長，我將他拆開來說明 💖\n選擇供應者以及對應的專案 provider \"google\" { project = \"gcp-20210526-001\" } 由於我們要建立的 Google Compute Engine 是由 Google 所提供的 api 來建立的，所以一開始要先設定好提供者的名稱 google 以及我們要在哪一個 GCP 的專案 ID\nresource 設定 接下來的設定都會放在以下的 google_compute_instance resource 內，為了方便介紹，就不會標明 google_compute_instance，詳細完整程式碼請參考 GitLab Github 程式碼連結\nresource \"google_compute_instance\" \"default\" { } 基本設定 name = \"test\" description = \"我是 test 機器\" machine_type = \"n2-standard-8\" zone = \"asia-east1-b\" tags = [\"test\"] labels = { env = \"test\" } deletion_protection = \"true\" name：GCE 要求資源的唯一名稱。如果有更改此項會直接強制創建的新資源 (必填) description：對此資源的簡單說明 (選填) machine_type：要創建的機器類型 (必填) zone：創建機器的所在區域，若沒有填寫，則會使用提供者的區域 (選填) tags：附加到實體的網路標籤列表 (選填) labels：一組分配給 disk 的 key/value 標籤 (選填) deletion_protection：刪除保護，預設是 false，當我們使用 terraform destroy 刪除 GCE 時，必須先改成 false，才可以刪除，否則會無法刪除且 Terraform 運行也會失敗，算是一個保護機制，後面再刪除 Google Compute Engine 時會測試畫面 (選填) 啟動 disk 設定 boot_disk { initialize_params { image = \"debian-cloud/debian-10-buster-v20210512\" type = \"pd-balanced\" size = \"50\" } } image：初始化此 disk 的 image (選填) type：GCE disk 類型 (選填) size：image 大小，已 GB 為單位，如果未指定，將會繼承其初始化 disk 的 image 大小 (選填) 網路設定 network_interface { network = \"projects/rd-gateway/global/networks/rd-common\" subnetwork = \"projects/rd-gateway/regions/asia-east1/subnetworks/rd-common-asia-east1-pid-cicd\" access_config { nat_ip = \"\" } } network：設定附加到的網路名稱或是 self_link (選填) subnetwork：設定附加到的子網路名稱或是 self_link (選填) nat_ip：如果想要有外網的 ip，必須加上此參數，才會產生一組外網 ip (選填) 權限設定 service_account { email = \"676962704505-compute@developer.gserviceaccount.com\" scopes = [\"storage-rw\", \"logging-write\", \"monitoring-write\", \"service-control\", \"service-management\", \"trace\"] } email：服務帳戶電子郵件地址。如果未提供，則使用預設的 Google Compute Engine 服務帳戶 (選填) scopes：服務範圍列表，可以點我查看範圍的完整列表 (必填) 以上只是我在建立 Google Compute Engine 最簡單的設定，當然還有很多其他的設定，可以參考 registry.terraform.io/providers (google_compute_instance) 裡面有更多的 resource 設定，有需要的就自己來看看吧 🧐"},"title":"使用 Terraform 建立 Google Compute Engine"},"/blog/terraform/terraform-gke/":{"data":{"":"我們接續昨天的建立 Google Kubernetes Engine 文章，今天要來介紹的是如何用 Terraform 建立 Google Kubernetes Engine，由於使用 terraform 去建立、修改、刪除的指令大家應該都清楚了，那我今天的文章就不在多說，直接來介紹一下要怎麼撰寫 Google Kubernetes Engine tf 檔案 😏","參考資料#參考資料":"registry.terraform.io/providers (google_container_cluster)：https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/container_cluster\nregistry.terraform.io/providers (google_container_node_pool)：https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/container_node_pool","撰寫-google-kubernetes-engine-tf-檔案#撰寫 Google Kubernetes Engine tf 檔案":"程式碼會同步到 Github ，需要的也可以去 Clone 來使用歐！ Github 程式碼連結，小提醒：由於程式碼較長，我將他拆開來說明 💖\n由於等等程式碼較長，所以我在前面這邊先做說明，GKE 的結構是 叢集(cluster) \u003e 節點池(node_pool) \u003e 節點(node)，本次的介紹範例，會有一個叢集裡面有一個節點池，節點池裡面有 6 個節點數量，範例裡面會加上我比較常用到的一些設定，以及一些文件裡面的用法，大家可以依照自己的需求來使用參數：\n限制使用的版本 在上一篇 使用 Terraform 建立 Google Compute Engine，我們知道 Terraform 其實就是對應的提供商，提供對應的 api 來讓我們可以用 terraform 去建置很多 IaC，但供應商提供的 api 會隨著版本而有所更動，可能換了一個版本，原本可以使用的 resource 參數就會有所不同，所以我們可以在一開始，先設定好這隻 tf 要使用的供應商及對應的版本，可以參考以下程式碼：\nterraform { required_providers { google = { source = \"hashicorp/google\" version = \"~\u003e 4.38.0\" } } } 可以看到我們把 google 這個供應商裡面設定好他的 source 以及 version，這樣就算之後 goolge 有更新 terraform 的 api，我們也不需要去更換參數就可以使用了～\n選擇供應者以及對應的專案 provider \"google\" { project = \"project\" } 除了可以使用專案 ID 以外，當然也可以使用專案的名稱拉 🥳\nresource 設定 google_container_cluster resource \"google_container_cluster\" \"cluster\" { name = \"test\" location = \"asia-southeast1-b\" min_master_version = \"1.22.12-gke.300\" network = \"projects/gcp-202011216-001/global/networks/XXXX\" subnetwork = \"projects/gcp-202011216-001/regions/asia-southeast1/subnetworks/XXXX\" default_max_pods_per_node = 64 remove_default_node_pool = true initial_node_count = 1 enable_intranode_visibility = false ip_allocation_policy { cluster_secondary_range_name = \"gke-pods\" services_secondary_range_name = \"gke-service\" } resource_labels = { \"env\" = \"test\" } addons_config { http_load_balancing { disabled = false } horizontal_pod_autoscaling { disabled = false } network_policy_config { disabled = false } } master_auth { client_certificate_config { issue_client_certificate = false } } private_cluster_config { enable_private_endpoint = false enable_private_nodes = true master_ipv4_cidr_block = \"172.16.0.0/28\" } logging_config { enable_components = [\"SYSTEM_COMPONENTS\", \"WORKLOADS\"] } monitoring_config { enable_components = [\"SYSTEM_COMPONENTS\"] } node_config { machine_type = \"e2-medium\" disk_size_gb = 100 disk_type = \"pd-standard\" image_type = \"COS_CONTAINERD\" oauth_scopes = [ \"https://www.googleapis.com/auth/devstorage.read_only\", \"https://www.googleapis.com/auth/logging.write\", \"https://www.googleapis.com/auth/monitoring\", \"https://www.googleapis.com/auth/servicecontrol\", \"https://www.googleapis.com/auth/service.management.readonly\", \"https://www.googleapis.com/auth/trace.append\" ] metadata = { disable-legacy-endpoints = \"true\" } } } name：叢集的名稱，在這個專案及區域唯一名稱 (必填) location：要將此叢集建立在哪一個區域 (選填) min_master_version：master 的最低版本 (選填) network：叢集連接到的 Google Compute Engine 網絡的名稱或 self_link (選填) subnetwork：啟動叢集的 Google Compute Engine 子網的名稱或 self_link (選填) default_max_pods_per_node：此叢集中每個節點的預設最大 pod 數 (選填) remove_default_node_pool：如果設定為 true，則在創建叢集時會幫我們刪除預設的節點池。會使用到這個的原因是因為 terraform 沒辦法修改預設節點池的名稱，所以我的做法是，會新增要的節點池，在使用這個參數把預設的給刪掉(選填) initial_node_count：要在此叢集的預設節點池中創建的節點數 (選填) enable_intranode_visibility：是否為此叢集啟用了節點內可見性 (選填) ip_allocation_policy：為 VPC 原生叢集分配叢集 IP (選填) resource_labels：應用於叢集的 GCE 資源標籤 key/value (選填) addons_config http_load_balancing：是否要啟用 HTTP (L7) 的負載平衡 (選填) horizontal_pod_autoscaling：是否要啟用 HPA 水平 Pod 自動擴展 (選填) network_policy_config：是否要啟用網路策略 (選填) master_auth client_certificate_config：是否要啟用該叢集客戶端證書授權 (選填) private_cluster_config enable_private_endpoint：是否要啟用叢集專用的私有端點，禁止公共端點的訪問 (選填) enable_private_nodes：是否要啟用私有叢集功能，在叢集創建私有端點 (選填) master_ipv4_cidr_block：私有端點 IP 範圍 (選填) logging_config：叢集的日誌記錄配置 enable_components (公開日誌的 GKE 組件) 設定，包含：SYSTEM_COMPONENTS、APISERVER、CONTROLLER_MANAGER、SCHEDULER、WORKLOADS (必填) monitoring_config：叢集的監控配置 enable_components (GKE 組件公開指標) 設定，包含：SYSTEM_COMPONENTS、APISERVER、CONTROLLER_MANAGER、SCHEDULER (選填) node_config：創建預設節點池參數 machine_type：Google Compute Engine 機器類型，預設為 e2-medium (選填) disk_size_gb：每個節點的 disk 大小，以 GB 為單位。允許最小為 10 GB，預設為 100 GB (選填) disk_type：連接到每個節點的 disk 類型，有 pd-standard、pd-balanced 或 pd-ssd，預設為 pd-standard (選填) image_type：創建新節點池後 NAP 使用的預設 image 類型。該值必須是 [COS_CONTAINERD、COS、UBUNTU_CONTAINERD、UBUNTU] 之一。COS 和 UBUNTU 已於 GKE 1.24 棄用 (選填) oauth_scopes：在預設服務帳戶下的所有節點虛擬機上可用的一組 Google API 範圍。 (選填) metadata：分配給叢集中實例的 key/value (選填) google_container_node_pool resource \"google_container_node_pool\" \"aaa\" { name = \"aaa\" project = \"project\" location = google_container_cluster.cluster.location cluster = google_container_cluster.cluster.name node_count = 6 node_locations = [ google_container_cluster.cluster.location ] node_config { # 省略 ... 與上面的 google_container_cluster 相同 } management { auto_repair = true auto_upgrade = false } upgrade_settings { max_surge = 1 max_unavailable = 0 } } name：節點池名稱 (選填) project：創建節點池的項目 ID (選填) location：叢集所在位置，可以使用資源名稱 (google_container_node_pool) +命名 (cluster) + 參數 (location) 來代表 (選填) cluster：叢集名稱，可以使用資源名稱 (google_container_node_pool) +命名 (cluster) + 參數 (name) 來代表 (選填) node_count：節點數量 (選填) node_locations：節點區域 (選填) management：節點管理配置 auto_repair：是否要自動修復 (選填) auto_upgrade：是否要自動升級 (選填) upgrade_settings：指定節點升級設定及方式 max_surge：升級期間可以添加到節點池的額外節點數 (選填) max_unavailable：升級期間可以同時不可用的節點數 (選填) 可以看到有很多設定都是選填的，所以不需要像我範例一樣，把所有的都打出來，可以參考官方文件，將自己想要的設定寫出來，並注意其他參數的預設值是多少，就可以打造屬於自己的 Terraform 建立 Google Kubernetes Engine IaC 程式囉～"},"title":"使用 Terraform 建立 Google Kubernetes Engine"},"/blog/terraform/terraform-module/":{"data":{"":"當我們要管理的資源越來越多後，會產生很多的 tf 檔案，假設我們現在有三個 gce 服務，會在以下三個不同環境上面運作，每個環境都會有我們之前學會的基本 tf 檔案(包含 provider.tf 、main.tf、backend.tf)，且其中的 main.tf 檔案內有些設定會不太一樣，如下：\ndev backend.tf main.tf provider.tf prod backend.tf main.tf provider.tf qa backend.tf main.tf provider.tf dev/main.tfresource \"google_compute_instance\" \"instance\" { project = \"馬賽克\" name = \"test-dev\" machine_type = \"e2-small\" zone = \"asia-east1-b\" boot_disk { initialize_params { image = \"debian-cloud/debian-10\" size = 50 } } .... 其他省略不寫 .... } qa/main.tf (多了 tags)resource \"google_compute_instance\" \"instance\" { project = \"馬賽克\" name = \"test-qa\" machine_type = \"e2-small\" zone = \"asia-east1-b\" tags = [\"for-qa\"] boot_disk { initialize_params { image = \"debian-cloud/debian-10\" size = 50 } } .... 其他省略不寫 .... } prod/main.tf (多了 labels)resource \"google_compute_instance\" \"instance\" { project = \"馬賽克\" name = \"test-prod\" machine_type = \"e2-small\" zone = \"asia-east1-b\" labels = { aaa = \"test1\" bbb = \"test2\" ccc = \"test3\" } boot_disk { initialize_params { image = \"debian-cloud/debian-10\" size = 50 } } .... 其他省略不寫 .... } 可以看到三個 main.tf 檔案除了 name 以外，在 qa 還多了 tags、prod 多了 labels 等設定，等於我們會依照每個不同環境不同服務去客製化他的 tf 資源設定，雖然非常直覺，但往後的維護以及調整卻非常不方便 ( 假設我們現在要全部都加上 labels，就必須一個一個檢查並調整 )。\n為了方便我們維護以及重複使用，因此有了 module，可以先將全部會使用到的設定寫成模板，透過參數的方式帶入即可，module 有以下幾個優點：\n重複使用性： module 讓程式碼更易於重複使用。當我們需要在多個項目中使用相同的基礎架構或配置時，可以先將其封裝為一個 module。這樣，我們只需要在不同的項目中引用並調整模組的參數，而不需要重新寫整個 tf 檔。\n抽象化：將 Terraform 代碼轉換為 module 可以將詳細的實現細節抽象化，僅寫必要的參數。這樣做可以提高程式碼的可讀性和可維護性，並降低使用者學習和使用的門檻。\n參數化配置：module 可以使用輸入參數來接收不同的配置值。這意味著您可以根據需要動態更改模組的行為，而不需要直接修改模組的內部程式。這使得配置更靈活並支持不同環境的部署。\nmodule 版本控制：將 Terraform 程式封裝為 module 後，可以使用 git 對其進行版本控制。可以更輕鬆地協作和共享 module (可以將 module 與 Terraform 分別存放，並使用對應 tag or 分支來做開發 )。","參考資料#參考資料":"Types and Values：https://developer.hashicorp.com/terraform/language/expressions/types","實作#實作":"當我們完成上面的架構後，我們進入 projects/prod/main.tf 路徑下，開始用 module 的方式建立資源，建立資源的流程與原本的相同，一樣是 init \u003e plan \u003e apply 這三個步驟，那我們一個一個來看，與原本的建立方式有哪些不同之處吧～\ninit 我們使用 terraform init 來看看原本 init 與使用模組 init 後差在哪裡：\n原先 terraform init 結果\n使用 module init 結果\n可以看到有使用 module 在初始化的時候，會連同 module 也一併初始化，接著我們進到 .terraform 資料夾內，可以看到有 moduels 資料夾。\n.terraform 檔案差異\n在進去看會看有一個 modules.json 檔案，會紀錄 module 使用的路徑，因此當我們使用的 module 有改變時，要記得重新 init 才可以確保使用的 module 是正確的。\n使用 module 會多一個 modules.json 檔案\nplan 我們一樣下 terraform plan 指令，來看看兩者顯示的差異：\n原先 terraform plan 結果\n使用 module plan 結果\n可以看到使用 module 在 plan 時，預覽創建的資源格式不同，也就代表他存在 tfstate 檔案的格式也會不同 (這個後面會在提到，與 import 也有關係)\napply 使用 terraform apply 來看建立資源後的結果有什麼不同：\n原先 terraform apply 結果\n使用 module apply 結果\napply 看到的與 plan 顯示的一樣，使用 module 建立的資料格式會不太一樣，所以我們來看看兩者 tfstate 檔案的差異：\n原先 terraform 建立的 tfstate 檔案\n使用 module 建立的 tfstate 檔案\nimport import 的功用是可以從雲上服務轉成 tf，在之前原本的 terraform 是要先建立一個空的 resource：\nresource \"google_compute_instance\" \"instance\" { } 再使用 terraform import google_compute_instance.instance 專案ID/機器地區/機器名稱 來匯入雲上服務的狀態到後端存到 tfstate 的位子。\n原先 terraform import 線上服務\n那我們現在改成 module，會比較麻煩一點，因為我們有在 variables.tf 設定我們的變數，若是沒有設定預設值，就必須一定要輸入，所以我們在建立時，要先把變數的空值也補上，如下：\nmodule \"ian-test\" { source = \"../../module/google_compute_instance\" project_id = \"\" instance_name = \"\" machine_type = \"\" instance_zone = \"\" instance_tags = [] instance_labels = {} boot_disk_image_name = \"\" boot_disk_size = 50 attached_disk_enabled = false network_name = \"\" subnetwork_name = \"\" nat_ip_enabled = false metadata = {} resource_policies = [] service_account_email = \"\" service_account_scopes = [] } \" \" 是 string 格式的空值，[ ] 是 list 格式的空值，{ } 是 map 格式的空值，其他的 bool 我預設會給他 false，number 我會隨便給他一個數字 xD。這邊帶入的內容不是很重要，主要是讓他可以去抓到他的架構，我們也可以在 variables.tf 設定時都補上預設值。\n再使用 terraform import module.ian-test.google_compute_instance.instance 專案ID/機器地區/機器名稱 來匯入狀態檔案。(這邊要記得依照你 module 設定的名稱帶入)\n使用 module import 線上服務","檔案說明#檔案說明":"\n首先我們要先定義我們的 module，我們先建立以下資料夾結構以及對應檔案：(同步到 GitHub 需要程式碼的可以前往查看)\n(再次提醒，會區分檔案名稱是因為方便調整跟維護，也可以把它全部寫在同一個 tf 檔案內歐)\nmodule google_compute_instance main.tf outputs.tf variables.tf projects dev backend.tf main.tf provider.tf prod backend.tf main.tf provider.tf qa backend.tf main.tf provider.tf module 資料夾：放我們 module 設定 (這邊範例是放 gce)\nprojects 資料夾：放我們不同服務、不同環境設定 (這邊為了簡化，範例只以不同環境為例)\nmodule/google_compute_instance/main.tfprovider \"google\" { project = var.project_id zone = var.instance_zone } resource \"google_compute_instance\" \"instance\" { name = var.instance_name machine_type = var.machine_type zone = var.instance_zone tags = var.instance_tags labels = var.instance_labels boot_disk { auto_delete = var.boot_disk_auto_delete initialize_params { image = var.boot_disk_image_name size = var.boot_disk_size } } dynamic \"attached_disk\" { for_each = var.attached_disk_enabled ? [1] : [] content { device_name = var.attached_disk_name mode = var.attached_disk_mode source = var.attached_disk_source } } network_interface { network = var.network_name subnetwork = var.subnetwork_name dynamic \"access_config\" { for_each = var.nat_ip_enabled ? [1] : [] content { } } } metadata = var.metadata enable_display = var.enable_display resource_policies = var.resource_policies service_account { email = var.service_account_email scopes = var.service_account_scopes } timeouts {} deletion_protection = var.deletion_protection allow_stopping_for_update = var.allow_stopping_for_update } 我們需要把所有設定的值都挖洞，使用 var 的方式來帶入參數，這邊要注意的是等號前面的值或是 block 名稱都是不能修改的，他是 google 定義的 api 變數，但 var 後的參數名稱我們可以自訂 (後面 variable.tf 會在詳細說明)，那這邊比較特別的用法是 dynamic，以下說明：\ndynamic \"attached_disk\" { for_each = var.attached_disk_enabled ? [1] : [] content { device_name = var.attached_disk_name mode = var.attached_disk_mode source = var.attached_disk_source } } 我們有些 block 只有在特定服務時才需使用，例如上面的 attached_disk 他是 gce 另外掛載其他磁碟的設定，如果有需要我們才會多設定這個 block，沒有則不需要加，因此須使用 dynamic 來動態產生 block，這邊的設定是我們要在參數要帶入 attached_disk_enabled 用 for_each 來判斷是否需要這個 block，如果是 true，就會產生 attached_disk block，且需要輸入 attached_disk_name、attached_disk_mode、attached_disk_source。\nmodule/google_compute_instance/variables.tfvariable \"project_id\" { type = string description = \"GCP 專案 ID\" } variable \"instance_name\" { type = string description = \"GCE 名稱\" } variable \"machine_type\" { type = string description = \"GCE 類型\" } variable \"instance_zone\" { type = string description = \"GCE 所在區域\" } variable \"instance_tags\" { type = list(string) description = \"GCE 網路標記\" } variable \"instance_labels\" { type = map(string) description = \"GCE 標籤\" } variable \"boot_disk_auto_delete\" { type = bool description = \"是否刪除 instance 時，自動刪除開機磁碟\" default = true } variable \"boot_disk_image_name\" { type = string description = \"GCE 映像檔名稱\" } variable \"boot_disk_size\" { type = number description = \"GCE 開機磁碟大小 (單位: GB)\" } variable \"attached_disk_enabled\" { type = bool description = \"是否啟用附加磁碟\" default = false } variable \"attached_disk_name\" { type = string description = \"GCE 附加磁碟名稱\" default = \"\" } variable \"attached_disk_mode\" { type = string description = \"GCE 附加磁碟模式\" default = \"READ_ONLY\" validation { condition = contains([\"READ_WRITE\", \"READ_ONLY\"], var.attached_disk_mode) error_message = \"不符合附加磁碟模式的值，請輸入 READ_WRITE 或 READ_ONLY\" } } variable \"attached_disk_source\" { type = string description = \"GCE 附加磁碟來源\" default = \"\" } variable \"network_name\" { type = string description = \"GCE 網路名稱\" } variable \"subnetwork_name\" { type = string description = \"GCE 子網路名稱\" } variable \"nat_ip_enabled\" { type = bool description = \"是否啟用 NAT IP\" default = false } variable \"metadata\" { type = map(string) description = \"GCE 中繼資料\" } variable \"enable_display\" { type = bool description = \"是否啟用虛擬顯示\" default = false } variable \"resource_policies\" { type = list(string) description = \"GCE 資源原則\" } variable \"service_account_email\" { type = string description = \"GCE 服務帳戶電子郵件\" } variable \"service_account_scopes\" { type = list(string) description = \"GCE 服務帳戶範圍\" } variable \"deletion_protection\" { type = bool description = \"是否啟用刪除保護\" default = false } variable \"allow_stopping_for_update\" { type = bool description = \"是否允許自動停止後更新\" default = false } 這個檔案會定義每個變數的名稱以及資料型態，也可以寫說明以及預設的值，這邊比較特別的是 validation ，他可以驗證帶入的參數是否符合 condition 內容，也可以自定義錯誤的訊息，如下：\nvariable \"attached_disk_mode\" { type = string description = \"GCE 附加磁碟模式\" default = \"READ_ONLY\" validation { condition = contains([\"READ_WRITE\", \"READ_ONLY\"], var.attached_disk_mode) error_message = \"不符合附加磁碟模式的值，請輸入 READ_WRITE 或 READ_ONLY\" } } 這邊限制 attached_disk_mode 輸入必須符合 READ_WRITE or READ_ONLY 的值，如果輸入其他不符合的會顯示 error_message 內容。\n另外 variable 這邊有幾個資料型態可以選擇，如下：\nstring：字串，不知道要選什麼就選他沒錯 xD\nbool：布林值，只有 true、false 兩種選項，適用於判斷的內容，例如剛剛上面說的 attached_disk_enabled 就是使用 bool\nnumber：數字，只能輸入數字\nlist (tuple)：清單，內容可以放置類似 [\"us-west-1a\", \"us-west-1c\"] 的資料\nmap (object)： key value 存放模式，例如：\n{ \"aaa\": \"test1\", \"bbb\": \"test2\", \"ccc\": \"test3\" } module/google_compute_instance/outputs.tfoutput \"instance_id\" { value = google_compute_instance.instance.instance_id } 這邊主要放置要輸出的內容，像我們這邊就會把 instance_id 給顯示出來。\nprojects 我這邊只示範 prod 的部分\nprojects/prod/main.tfmodule \"ian-test\" { source = \"../../module/google_compute_instance\" project_id = \"馬賽克\" instance_name = \"test-prod\" machine_type = \"e2-small\" instance_zone = \"asia-east1-b\" instance_tags = [] instance_labels = { \"aaa\" = \"test1\" \"bbb\" = \"test2\" \"ccc\" = \"test3\" } boot_disk_image_name = \"debian-cloud/debian-10\" boot_disk_size = \"50\" attached_disk_enabled = false network_name = \"馬賽克\" subnetwork_name = \"馬賽克\" nat_ip_enabled = false metadata = {} resource_policies = [] service_account_email = \"馬賽克\" service_account_scopes = [\"storage-ro\", \"logging-write\", \"monitoring-write\", \"service-control\", \"service-management\", \"trace\"] } 這邊我們可以定義要使用 module 的叫什麼，這邊我就取名 google_compute_instance，然後他會去 source \"../../module/ian-test\"，也就是我們剛剛在上面先挖洞的模板，底下就開始帶入我們在 variables.tf 有設定的參數。這邊比較要注意的是，在 main.tf、variables.tf 有使用的變數設定，都必須要寫在個別資源 tf 的檔案裡面，沒有的就帶入對應資料型態的空值，例如 instance_tags、metadata、resource_policies 等等。"},"title":"如何將 Terraform 改寫成 module ?"},"/blog/terraform/terraform-tfstate/":{"data":{"":"此篇是接續上一篇 什麼是 IaC ? Terraform 又是什麼？的 Terraform 文章，我們在上一篇有提到 terraform apply 完後，會多一個檔案 *.tfstate，這個檔案是用來存放服務狀態的檔案，它包含基礎架構的狀態和資源的詳細信息。假設大家都在自己的本地去 apply 同一個服務，會導致每個人的 tfstate 檔案內容不同，有可能去覆蓋掉其他人已經調整的內容，因此我們必須將此 tfstate 檔案存放在一個地方，讓大家都去使用同一份檔案來調整資源。\n我們常用的儲存方式會將 tfstate 存在 gitlab 或 gcs (gcp 架構為例)，以下會簡單說明要如何把 tfstate 存到後端以及各頁面的功能：","gcs#gcs":"gcs 儲存比較簡單一點，因為他就是一個 bucket，所以頁面就跟一般的 gcs 一樣，會顯示檔案名稱、大小、類型等，如果需要查看 tfstate，可以點擊最後的下載按鈕來查看\n那我們接著使用上面 gitlab 的範例檔案，只是要將 backend.tf 內容改為以下：\nbackend.tfterraform { backend \"gcs\" { bucket = \"pid-terraform-state\" prefix = \"/aaa\" } } 上面的設定是指，我們將 backend 後端設定改成 gcs，並且選擇名為 pid-terraform-state 的 bucket，此 bucket 需要先手動建立(因為 bucket 名稱是全域不重複，所以不需要特別設定其 project_id，只要有權限正確都可以跨專案使用)，以及我們要將此 tfstate 存在 aaa 資料夾內。\n接著我們重新刪除剛剛 gitlab 已產生的 .terraform/ 跟 .terraform.lock.hcl 檔案，重新下 init，就可以到 gcs 對應資料夾下，新增了 defaulte.tfstate 檔案。\n產生 defaulte.tfstate\nLock 我們一樣來看一下 gcs 的 lock 會長什麼樣子，gcs lock 會產生一個 default.tflock 檔案，由他去判斷現在是否是 Lock 狀態\ngcs Lock 會出現 .tflock 檔案\n當有其他人也執行 plan or apply 後，就會顯示以下：\ngcs Lock 其他人不能操作","gitlab#gitlab":"那我們要怎麼把 tfstate 存到 gitlab 呢？首先跟之前一樣，先新增 provider.tf 來放供應商的來源以及版本，以及 main.tf 來放 gce 相關設定，最後還要多一個 backend.tf 來放我們要儲存 tfstate 的位置設定，如下：(同步到 GitHub 需要程式碼的可以前往查看)\n(這次範例會使用 gce，此項會需要 gitlab 先啟用 Infrastructure 功能以及建立自己的 gitlab token)\nprovider.tfterraform { required_providers { google = { source = \"hashicorp/google\" version = \"~\u003e 4.48.0\" } } } main.tf provider \"google\" { project = \"XXXXX\" zone = \"asia-east1-b\" } resource \"google_compute_instance\" \"instance\" { name = \"test\" machine_type = \"e2-small\" zone = \"asia-east1-b\" labels = { env = \"11\" } boot_disk { initialize_params { image = \"debian-cloud/debian-10\" } } network_interface { network = \"projects/XXXX/global/networks/test\" subnetwork = \"projects/XXXX/regions/asia-east1/subnetworks/testtest\" } } backend.tf « 新的 專案 ID 要寫我們想要放 terraform state 的 GitLab Project ID，服務名稱是指顯示在 GitLab terraform state 的名稱\ngitlab 個人 token 是指個人存取權杖，大家再依照自己的來做設定\nbackend.tfterraform { backend \"http\" { address = \"[Gitlab 網址]/api/v4/projects/[專案ID]/terraform/state/[服務名稱]\" lock_address = \"[Gitlab 網址]/api/v4/projects/[專案ID]/terraform/state/[服務名稱]/lock\" unlock_address = \"[Gitlab 網址]/api/v4/projects/[專案ID]/terraform/state/[服務名稱]/lock\" username = \"[Gitlab 帳號]\" password = \"[Gitlab 個人 token]\" lock_method = \"POST\" unlock_method = \"DELETE\" retry_wait_min = 5 } } 當我們新增好後，就跟之前步驟一樣，先 init \u003e plan \u003e apply 來做測試，在 init 時會發現，與之前不太一樣的是，在 Initializing the backend 的下方有多了綠色的成功設定後端字樣，代表他也會將後端的相關資訊存進 .terraform 資料夾中，所以有變更後端儲存位置，要記得重新 init 歐\ninit 初始化後，會將後端資訊也存到 .terraform 資料夾\n當我們 plan \u003e apply 完成後，可以觀察一下，發現原本會產生的 terraform.tfstate 檔案沒有出現在該目錄下：\napply 完，沒有在本地產生 .tfstate 檔案\n這時候，我們可以到剛剛在上面設定的專案 ID 內的有一個 Infrastructure / Terraform，裡面就會存放 Terraform state 檔案，如下：\ngitlab/Infrastructure/Terraform\n會顯示狀態名稱、更新資訊、以及 Actions 等欄位：\ngitlab terraform tfstate 網頁\n功能部分可以看後面的 Actions 欄位底下有三個點點，可以下載對應的 tfstate 檔案、Lock 讓其他人不能對此進行 apply，或是刪除此 tfstate 檔案等\ngitlab terraform tfstate 功能說明\n這樣我們就可以透過同一份的 tfstate 檔案來做管理，但有個前提是，之後對該資源的變更都只能使用 tf，如果還有用 WEB UI 去調整，就會遇到線上服務與 tfstate 儲存狀態不同的問題。\nLock 那當我們已經有了共同儲存的地方，也溝通好，不會使用 WEB UI 去調整，但如果有兩個人同時去下 apply 的話，第一個人的 apply 還在執行，後面那個人的 apply 是不是就會蓋掉前一個人的設定呢？\n所以 Terraform 在 0.14 版本推出了 Lock 功能，當有人在 plan or apply 的時候，我們去查看 gitlab Terraform state，會看到我們的 tfstate 檔案會被 Lock 起來\nGitLab Lock 鎖住\n此時除了第一個操作者，其他人再去 plan or apply 就會出現錯誤，可以看到是誰正在使用，以及操作的動作是 plan or apply\n在 Lock 下，其他人沒辦法去 plan or apply\n在 CI 時，需要在 plan 時就將它給 lock，避免第一個人 plan 完，沒有及時的去執行 apply，後來有其他人比第一個人先調整了資源，第一個人再來執行 apply，就會導致第一個人 apply 的內容與自己原先看 plan 的內容會不同，也有可能會將上一個人調整的設定給覆蓋，當第一個操作者結束動作後，該 Lock 才會被解鎖。\n其他 gitlab terraform state 詳細內容可以參考：https://docs.gitlab.com/ee/user/infrastructure/iac/terraform_state.html"},"title":"Terraform 如何多人共同開發 (將 tfstate 存在後端)"},"/blog/terraform/terraform/":{"data":{"":"跟上一篇 Snyk 一樣，本系列也是去參加 DevOpsDay Taipei 2022 活動聽到各位產業大佬目前在使用的名詞以及技術，想說回家也充實一下自己，了解一下在 DevOpsDay 最常出現的 IaC 是什麼？以及聽說很方便的 Terraform 又是什麼，將學習的過程打成此篇筆記，歡迎大家多交流，那我們就開始囉。\n目前打算寫本篇介紹 IaC 以及 Terraform 以外，之後還想寫另外兩篇說明 Terraform 如何共同維護開發(將 .tfstate 檔案存在 gitlab or gcs 上)、怎麼把 Terraform 轉成 module，最後導入 Terragrunt 達到 DRY 等等，也會有用 Terraform 建立 GCE 以及 GKE。大家可以持續關注此篇文章，最後會在文章後附上連結 😍","terraform-又是什麼#Terraform 又是什麼？":"IaC 的工具有很多種，接著我們就來介紹其中一個工具 - Terraform，Terraform 是什麼呢？根據官網的說明可以知道，Terraform 是 HashiCorp 所開發的基礎設施即代碼工具。它可以使用人類方便讀的配置文件來定義資源和基礎設施，以下有使用 Terraform 幾個優點：\nTerraform 可以管理多個雲平台上的基礎架構 使用人類可讀的配置語言來幫助我們快速編寫基礎設施代碼 可以將配置提交給版本控制，安全地在基礎架構上進行協作 管理任何基礎設施 Terraform 提供插件讓 Terraform 可以通過其 API 與雲平台和其他服務進行交互。HashiCorp 和 Terraform 社區編寫了 3193 多個提供商來管理像 AWS、Azure、GCP、Kubernetes、Helm、GitHub 等資源，可以到 Terraform Registry 查看更多平台或服務的提供者，當然如果沒有找到想要的提供者，也可以自己編寫自己的套件。\n標準化部署工作流程 提供商會將基礎設施的每個單元 (例如建立 VM 或是 VPC) 定義為資源。你可以將來自不同提供者的資源組合，變成模組，讓我們可以用一致的語言和工作流程去管理他們。\nTerraform 什麼是 Terraform 的基礎設施即代碼？","什麼是-iac-#什麼是 IaC ?":"IaC 全名是 Infrastructure as Code (基礎設施即代碼)，從字面意思就可以略知一二，也就是把基礎設施變成程式碼，在還沒有這些 IaC 工具之前，大家都是開啟 WEB UI 畫面來進行建置或設定，雖然使用 UI 點一點就建好了，但這些步驟都沒有被紀錄下來 (git)，也沒有辦法透過其他人一起 Review 的方式來避免人為操作錯誤。因此有了 IaC 這些工具，可以將實際的操作流程，轉換成程式碼或是其他像是 JSON、Yaml 的方式給紀錄下來，以下是導入 IaC 帶來的好處：\n建置 CI/CD 自動化 (不需要再仰賴 UI 進行操作) 版本控制 (大家可以透過 MR 規定 code review，避免出現人為錯誤) 重複使用 (可以將常用的變成參數代入，減少建置時間) 環境一致性 (以上 IaC 說明來自 小惡魔 - 初探 Infrastructure as Code 工具 Terraform vs Pulumi 文章，寫得真的很好，推推)\nInfrastructure as Code 初心企服行研07：认识「基础设施即代码」(Infrastructure as Code) — 初心内参","參考資料#參考資料":"初探 Infrastructure as Code 工具 Terraform vs Pulumi：https://blog.wu-boy.com/2021/02/introduction-to-infrastructure-as-code-terraform-vs-pulumi/\n今晚我想認識 Terraform：https://ithelp.ithome.com.tw/articles/10233759","安裝-terraform#安裝 Terraform":"安裝 Terraform 的方式有很多種，我就以我在使用的 Mac OS 為例，其他可以參考 Install Terraform：\n安裝步驟 先安裝 HashiCorp tap，這是 HashiCorp 在 Homebrew 的儲存庫： brew tap hashicorp/tap 使用 hashicorp/tap/terraform brew install hashicorp/tap/terraform 如何驗證是否安裝成功 打開一個新的 Terminal，使用 terraform -help 檢查是否有安裝成功，也可以在 -help 後面加入參數來查看該參數的功能與更多訊息\n驗證 Terraform 安裝成功 Install Terraform\n自動補全指令 可以啟動終端機上的 Tab 自動補全功能，執行以下指令，再重開終端機，就會出現了：\nterraform -install-autocomplete 想要解除自動補全 (雖然應該不會拉)，執行以下指令：\nterraform -uninstall-autocomplete 放上成果圖片\n快速入門 當我們安裝好，想要最快的了解 Terraform ，當然是自己動手做一次，我們依照官網的教學，可以在一分鐘內使用 Docker 配置好 NGINX 伺服器，那我們開始囉！\n首先，我們必須要先安裝好 Docker，下載 Mac 版 Docker 桌面 建立一個資料夾，並進入該資料夾內 將以下 Terraform 配置文件貼到檔案中，並取名 main.tf：(同步到 GitHub 需要程式碼的可以前往查看) provider \"docker\" {} resource \"docker_image\" \"nginx\" { name = \"nginx:1.23\" keep_locally = false } resource \"docker_container\" \"nginx\" { image = docker_image.nginx.name name = \"nginx\" ports { internal = 80 external = 8000 } } 再開一個檔案取名為 provider.tf，將以下配置文件貼到檔案中： terraform { required_providers { docker = { source = \"kreuzwerker/docker\" version = \"~\u003e 2.13.0\" } } } (以上程式碼來自官網 安裝 Terraform#快速入門 加上小修改)\n先來簡單說明一下 Terraform 程式碼格式，Terraform 的檔案副檔名是 *.tf，採用名為 HCL (HashiCorp Configuration Language) 的組態語言來描述基礎架構。\n(補充說明：只要是同一個目錄下有 .tf 檔案結尾的，Terraform 都會去執行，所以檔案名稱可以自己取名，但為了方便管理都會使用 main、provider、backend、output 的檔案來放置對應的內容)\nHCL 是一種宣告式的語言，讓你直接寫下期望的基礎架構，而不是寫下過程的每一個步驟。\n檔案介紹 我們先看 provider.tf 檔案，檔案內會先寫好需要的供應商來源以及版本 (版本有點像是對應供應商提供的 api 版本)\nterraform { required_providers { 供應商名稱 = { source = \"供應商來源\" version = \"~\u003e 所使用的版本\" } } } main.tf 檔案內的 provider 區塊會寫供應商的相關設定，假設我們使用 google 就會在裡面先設定好 project id 等。\nresource 區塊會需要寫雲端資源名稱以及自定義的名稱，雲端資源名稱這項是不可以更改的，假設我們要使用 docker 的 container 服務，這邊就需要填寫 docker_container。自定義的名稱可以是你想要為使用這個雲端資源去定義的名稱。\nprovider \"供應商名稱\" {} resource \"雲端資源名稱\" \"自定義的名稱\" { 屬性 = 值 } 所以我們已上面的 Docker 配置好 NGINX 伺服器為例，provider 我們這次使用的是 docker， resource 我們可以拆開來寫，\n像是第一個 resource docker_image 我們幫他取叫 nginx，裡面就是放有關 image 的設定，詳細的 image 設定可以參考 Resource (docker_image)，\n第二個 resource docker_container 一樣叫 nginx，裡面用的 image 是拿前面的 docker_image resource name 來使用，一樣詳細可以參考 Resource (docker_container)。\n小提醒，如果不知道要怎麼寫 provider 供應商設定，可以打開 terraform 官網找到該供應商，點選右邊的 USE PROVIDER\n官方教學\n可以看到官方教學要怎麼使用這個供應商。\n指令說明 接著有幾個指令要帶大家認識：\nterraform init：初始化項目，下載 tf 檔案中所需要的外掛套件 terraform plan：會產生一份執行計劃。上面會寫著它將會做哪些事，你可以驗證是否符合你預期的設計 terraform apply：實際運作，把基礎架構建置完成。在完成之後，會把目前的狀態儲存到一份檔案中 (*.tfstate) terraform destroy：會銷毀用 Terraform 起的服務 terraform fmt：幫你整理好 tf 文件 terraform validate：靜態檢查 tf 文件 附上懶人指令\nalias ti='terraform init' alias ta='terraform apply' alias tp='terraform plan' alias td='terraform destroy' 由於在 apply 的時候會跳出詢問視窗，如果是要寫成腳本，可以把指令改成 terraform apply -auto-approve 就不需要輸入 yes 了！\n實際操作 有上面的指令後，我們來實際操作看看：\n首先到該 main.tf 檔案目錄下，先使用 terraform apply 來測試看看： 無法直接執行 apply\n會發現沒有辦法直接用 terraform apply 指令來建置服務，我們看一下他提示的說明，他說他找不到 lock file，需要先進行初始化才可以執行，所以我們的建置流程是先 init –\u003e apply\nterraform init 我們先執行 terraform init，可以看到他會下載 tf 檔案中所需要的外掛套件 (docker) terraform init\n當我們初始化後，資料夾會多一個檔案 (.terraform.lock.hcl) 以及資料夾 (.terraform)\ninit 前後檔案差異\n.terraform.lock.hcl：是 Terraform 中用於鎖定和管理外部提供者（providers）版本的檔案。它的主要功能是確保在不同的環境中使用相同的外部提供者版本，以避免在團隊合作或不同環境中引入不一致性和問題。\n.terraform/：資料夾主要用於存儲初始化和管理基礎架構相關的臨時文件。\nterraform plan 接著我們使用 terraform plan 來查看我們的計劃，可以看到他會列出我們所寫的 tf 裡面有用到的 resource，除了我們有設定的屬性，其他的屬性也會顯示出來，可以更方便地讓我們知道這個 resource 有哪些屬性可以設定 terraform plan\nterraform apply 最後我們檢查都沒有問題，就可以使用 terraform apply 來建置，apply 其實跟 plan 一樣都會先讓我們看一下計劃，但會跳出詢問是否要執行，除非你輸入 yes，否則就跟 plan 單純顯示計劃內容，最後我們就可以看到他成功在 docker 上面建立 nginx 服務 terraform apply\n查看 docker nginx 以及檢查其服務\n當我們 apply 完，服務也建立後，查看一下資料夾後會發現，又多了一個檔案 terraform.tfstate：\n多了一個檔案 terraform.tfstate\nterraform.tfstate： 是 Terraform 的狀態檔案，它包含了基礎架構的狀態和資源的詳細信息。預設情況下，這個檔案是本地的並且只存在於 Terraform 初始化和操作的目錄中。(但要如何實現共同維護同一個 IaC 呢，敬待後續分享 🤣)\nterraform destory 另外，當你想要移除服務時，可以使用 terraform destroy 來將服務給移除 terraform destroy\nterraform import 最後還有一個蠻重要的，就是我們已經有很多服務都是使用 WEB UI 方式建立的服務，那我們要怎麼把它變成 tf 檔案呢？ 跟我們剛剛說的 terraform.tfstate 檔案有關，他會儲存我們 IaC 的狀態，所以我們才可以透過他知道現在是對資源做新增、修改、刪除哪個操作\n那當這個檔案不見時，如果再重新下 terraform apply，他會認為你是新增狀態，但實際上 docker 服務還是啟動的狀態，所以就會錯誤，會跟你說他已經存在。\n測試沒有 terraform.tfstate 直接下 terraform apply\n所以這時候我們要把線上服務的資源轉成 tf ，第一步要先把 resource 的框架給寫出來，其他可以先留空白，如下 main.tf：\nprovider \"docker\" {} resource \"docker_image\" \"nginx\" { name = \"nginx:1.23\" keep_locally = false } resource \"docker_container\" \"nginx\" { } 接著使用 terraform import 來將線上服務的資源套用到我們 main.tf 裡面的 resource，所以會長得像：\nterraform import docker_container.nginx 7f363ea3f6a64b5432ae3627f490b3e297abf80f196bce9c028ec2eb82706f12 import 後面會加上 main.tf resource 名稱 docker_container，以及我們取名的 nginx，最後帶 container id (docker ps 查詢)，就可以匯出 terraform.tfstate 檔案囉，詳細的 import 可以參考每個供應商資源的網頁(這邊以 docker 為例)\nterraform import 匯出 terraform.tfstate\nterraform show 當我們匯出後，可以看一下 terraform.tfstate，它會是一個 json 格式，如果要轉成 tf 格式，還需要使用 terraform show 來將 terraform.tfstate 檔案轉成 tf 格式，如下：\nterraform show 將 terraform.tfstate 轉成 tf\n如果透過上述的方式來轉成 tf，會發現在重新 apply 時，會出現錯誤，這邊以 gce 的當範例，轉完的 tf 設定，有些是 tf 不支援的參數，只會顯示在 tfstate，所以還需要手動刪除。\n轉換後還需將不用的設定給移除"},"title":"什麼是 IaC ? Terraform 又是什麼？"},"/blog/terraform/terragrunt/":{"data":{"":"我們接續上一篇的 如何將 Terraform 改寫成 module ? ，我們已經將 Terraform 改成 module 的方式來進行管理，但當我們要管理的資源越來越多，且有分不同的專案時，整個服務架構會長的像以下：\nmodules google_compute_instance main.tf outputs.tf variables.tf projects gcp-1234 aaa backend.tf main.tf provider.tf bbb backend.tf main.tf provider.tf ccc backend.tf main.tf provider.tf gcp-2345 aaa backend.tf main.tf provider.tf bbb backend.tf main.tf provider.tf ccc backend.tf main.tf provider.tf gcp-3456 aaa backend.tf main.tf provider.tf bbb backend.tf main.tf provider.tf ccc backend.tf main.tf provider.tf 這邊的範例是以不同專案來分，再分區不同的服務，每個服務裡面都會有 backend.tf、main.tf、provider.tf 檔案所組成，我們可以來比較一下 gcp-1234 的 aaa 服務以及 gcp-2345 的 aaa 服務差異：\n檔案差異\n可以看到在 backend.tf 除了 prefix 路徑以外，其他設定也都一樣，但因為 Terraform 本身沒辦法透過帶入參數的方式來設定 backend.tf 後端部分，所以必須要先寫好每個服務所存放的後端位置，十分的不方便。\nbackend.tfterraform { backend \"gcs\" { bucket = \"terragrunt-tfstate\" prefix = \"/gcp-1234-aaa\" } } 為了減少上述這些需要一直重複寫差不多檔案的工作內容，因此有了 Terragrunt 這個工具，Terragrunt 是 Terraform 的包裝器，可以彌補 Terraform 上的一些缺陷，並且讓我們的 IaC 更貼近 DRY 原則。\n這邊說明一下 DRY 原則\nDRY 全名是 Don’t repeat yourself，也就是不要做重複的事情，能夠一次做完的就不要重複的去做。","terragrunt-好處#Terragrunt 好處":"\n接著介紹一下 Terragrunt 的好處：\n方便管理後端狀態設定\n將後端存儲桶納入管理\n使用 generate 自動生成檔案\n使用 include 檔案來達到 DRY 原則\n管理 Module 之間的依賴性\n產生依賴關聯圖\n方便管理後端狀態設定 首先第一個方便管理後端狀態設定，也就是我們上面提到的 backend.tf 設定。在 Terraform 原生為了區別不同專案不同服務的狀態檔，就必須先寫好每個儲存的路徑，但使用 Terragrunt，可以先在該目錄下，也就是 gcp-3456 目錄下先寫一個設定檔案 (我們以 gcp-3456 專案為例)，讓底下的 aaa、bbb、ccc 服務可以去 include 它，我們就不需要每個服務都寫幾乎差不多的設定檔，接著我們在 gcp-3456 資料夾下新增 terragrunt.hcl 檔案來說明：\nterragrunt.hclremote_state { backend = \"gcs\" generate = { path = \"backend.tf\" if_exists = \"overwrite\" } config = { bucket = \"terragrunt-tfstate\" prefix = \"${path_relative_to_include()}\" } } 這邊的設定其實跟之前的 backend.tf 差不多，只是後端現在儲存的 block 改叫做 remote_state，可以看到 backend 設定我們一樣是存在 gcs 上，generate 這個 block 它會判斷 backend.tf 檔案是否存在，如果沒有它就會幫我們建立，其中設定檔案內容是將狀態檔案存在 terragrunt-tfstate 這個 bucket，並透過${path_relative_to_include()} 這個變數來自動帶入有 include 這份檔案的路徑，並在相對路徑產生 backend.tf 檔案。\n有點抽象，所以我畫一個比較簡單的架構圖來做說明一下，假設現在有三個服務，如下：\naaa terragrunt.hcl bbb terragrunt.hcl ccc terragrunt.hcl terragrunt.hcl 我們剛剛的後端設定是寫在此根目錄的 terragrunt.hcl 檔案(第 8 行)，然後 ccc 這個服務去 include 根目錄的 terragrunt.hcl 檔案， Terragrunt 就會自動幫你產生以下的 backend.tf 檔案：\nbackend.tfterraform { backend \"gcs\" { bucket = \"terragrunt-tfstate\" prefix = \"/ccc\" } } 這樣就可以省下我們要重複寫 backend.tf 的時間，在維護上也會更加的方便。\n將後端存儲桶納入管理 接著，大家有沒有想過，我們都已經使用 Terraform 來管理 IaC ，並把狀態檔案放到 gcs 上面來保存，但一開始還沒有用 Terraform 管理 gcs 的資源，我們還需要在設定 backend.tf 前，先手動去新增一個 gcs ，才能來存放 tfstate 狀態檔案呢？\n因此在 Terragrunt remote_state 的 config 時，可以多設定 gcs 的 project id 以及 location，Terragrunt 在初始化後端時，會檢查是否有該 gcs bucket，如果沒有就會自動建立，設定檔如下：\nterragrunt.hclremote_state { backend = \"gcs\" generate = { path = \"backend.tf\" if_exists = \"overwrite\" } config = { project = \"gcp-xxxxxx\" location = \"asia\" bucket = \"terragrunt-tfstate\" prefix = \"${path_relative_to_include()}\" } } 使用 generate 自動生成檔案 在剛剛我們可以使用 generate 來自動 backend.tf 檔案，那代表我們也可以把每個 provider.tf 的內容，也透過 generate 來生成，如下：\nterragrunt.hclgenerate \"provider\" { path = \"provider.tf\" if_exists = \"overwrite\" contents = \u003c\u003cEOF terraform { required_providers { google = { source = \"hashicorp/google\" version = \"~\u003e 4.48.0\" } } } EOF } 這樣子每個 include 這份設定檔的服務除了 backend.tf 檔案以外，還會自動產生 provider.tf 檔案。\n使用 include 檔案來達到 DRY 原則 我們搞定了 backend.tf 跟 provider.tf 後，還剩下 main.tf，所以我們也將它改成 Terragrunt 的格式如下：\nterragrunt.hclterraform { source = \"${get_path_to_repo_root()}/modules/google_compute_instance\" } include \"root\" { path = find_in_parent_folders() } inputs = { instance_name = \"gcp-3456-ccc\" machine_type = \"e2-small\" instance_zone = \"asia-east1-b\" instance_tags = [] instance_labels = {} boot_disk_auto_delete = true boot_disk_image_name = \"debian-cloud/debian-10\" ... 設定部分省略 ... } 這邊可以看到 terraform source 它就是我們使用 module 的路徑，也可以用 ${get_path_to_repo_root()} 這個變數他會自動抓該專案的根目錄，我們就不需要去特別設定。\n此外也可以將 module 獨立成一個專案，或是使用其他人寫好的 module，在 source 的時候可以使用 git::https://[gitlab-網址]/sre/terraform/module.git//google_compute_address 的方式來取得 module，可以設定要使用哪個分支或是 tag，只需要在網址後面加上，?ref=[分之 or tag 名稱] 即可，這樣可以讓開發中的 module 不會影響到線上其他正在使用中的 module 設定。\n(module.git 後面的 // 是 Terraform module source 的規則，如果不加會跳警告訊息)\n接著我們可以看到 include \"root\" {} 這段，裡面有使用 find_in_parent_folders 這邊變數，他就是上面的提到會自動去抓放在父資料夾的 remote_state 跟 generate terragrunt.hcl 檔案。\n後面的 input 就跟使用 module 時一樣，將 module 的參數帶入即可。\n補充：所以我們也可以把一些通用的設定寫在根目錄的 terragrunt.hcl 檔案，例如專案的 id，可以寫以下內容來讓 include 它的檔案吃到同一個參數設定：\nterragrunt.hclinputs = { project_id = \"gcp-3456\" } 管理 Module 之間的依賴性 由於 Terragrunt 是把每個服務拆分成最小化，沒辦法把使用不同 module 的資源放在一起(單純使用 module 的話，可以一次 source 多了 module，並把他放在同一個 tf 檔案中)，那像是我們建立 k8s 會使用到 google_container_cluster、google_container_node_pool 兩種不同的 module 該怎麼辦呢？\n首先我們先在 modules 資料夾放上 google_container_cluster、google_container_node_pool 兩個 module 的設定檔案，詳細程式可以點我前往\nmodules google_container_cluster main.tf outputs.tf variables.tf google_container_node_pool main.tf variables.tf 在 projects 底下新增 gke 資料夾，新增 terragrunt.hcl 來放 remote_state provider 的設定，並區分兩個資料夾，分別是 cluster 資料夾來存放 cluster 資訊，以及 test 資料夾來存放 test node-pool 資訊：\nprojects gke cluster terragrunt.hcl terragrunt.hcl test terragrunt.hcl cluster 的 terragrunt.hcl 檔案如下：\nterragrunt.hclterraform { source = \"${get_path_to_repo_root()}/modules/google_container_cluster\" } include { path = find_in_parent_folders() } inputs = { cluster_name = \"tf-test\" cluster_location = \"asia-east1-b\" node_locations = [] cluster_version = \"1.24.12-gke.500\" network_name = \"projects/gcp-202011216-001/global/networks/bbin-testdev\" subnetwork_name = \"projects/gcp-202011216-001/regions/asia-east1/subnetworks/bbin-testdev-dev-platform\" node_max_pods = 64 remove_default_node_pool = true initial_node_count = 1 enable_shielded_nodes = false resource_labels = { \"dept\" : \"pid\", \"env\" : \"dev\", \"product\" : \"bbin\" } dns_enabled = false cluster_dns = \"PROVIDER_UNSPECIFIED\" cluster_dns_scope = \"DNS_SCOPE_UNSPECIFIED\" private_cluster_ipv4_cidr = \"172.16.0.176/28\" binary_authorization_enabled = true binary_authorization = \"DISABLED\" } test node-pool 檔案如下：\nterragrunt.hclterraform { source = \"${get_path_to_repo_root()}/modules/google_container_node_pool\" } include { path = find_in_parent_folders() } dependency \"cluster\" { config_path = \"../cluster\" } inputs = { cluster_name = dependency.cluster.outputs.cluster_name cluster_location = dependency.cluster.outputs.cluster_location cluster_version = dependency.cluster.outputs.cluster_version node_pool_name = \"test\" node_count = 1 node_machine_type = \"e2-small\" node_disk_size = 100 node_disk_type = \"pd-standard\" node_image_type = \"COS_CONTAINERD\" node_oauth_scopes = [ \"https://www.googleapis.com/auth/devstorage.read_only\", \"https://www.googleapis.com/auth/logging.write\", \"https://www.googleapis.com/auth/monitoring\", \"https://www.googleapis.com/auth/service.management.readonly\", \"https://www.googleapis.com/auth/servicecontrol\", \"https://www.googleapis.com/auth/trace.append\" ] node_tags = [] node_taint_enabled = false node_taint_key = \"\" node_taint_value = \"\" node_taint_effect = \"\" auto_repair = true auto_upgrade = true upgrade_max_surge = 1 upgrade_max_unavailable = 0 upgrade_strategy = \"SURGE\" autoscaling_enabled = true autoscaling_max_node_count = 2 autoscaling_min_node_count = 1 autoscaling_total_max_node_count = 0 autoscaling_total_min_node_count = 0 } 上面兩個檔案分別是 cluster 的設定，以及 test node-pool 的設定，裡面的設定，上面基本都有提過，這邊要提的是 dependency，dependency 他是 Terragrunt 提供讓我們可以方便地去管理 IaC 之間的相依性，像是我們這邊，需要先建立好 cluster 才能建立 node_pool，此時就可以依靠 dependency block 來完成需求。\n( 靠 dependency 來取得 cluster 的資訊，並帶入 node_pool 中 )\n此時的執行指令是在 gke 目錄下，使用 terragrunt run-all [參數] 來跑整個相依的 module，我們這邊就建立一個名為 tf-test 的 cluster，並且有一個名為 test 的 node_pool ，其他設定請參考上面程式：\n測試 terragrunt run-all\n(黃色的 WARN 是因為 gcs 上還沒有存過該狀態檔案，所以會跳出提示)\n等到 cluster 建立完成後，會將 cluster 的資訊帶入 node_pool，才開始建立 node_pool 的資源：\n測試 terragrunt run-all\n產生依賴關聯圖 當我們服務使用到很多依賴關係，想要釐清是誰依賴誰，如果單純看程式會比較麻煩，在 Terragrunt 還有一個好用的指令，可以使用以下指令，產生對應的依賴關係圖，在檢視時可以更清楚知道關係：\nterragrunt graph-dependencies | dot -Tpng \u003e graph.png ( 這個 dot 指令是另外的套件，會將關係圖程式碼轉成圖檔，請先安裝 brew install graphviz )\n關聯圖","terragrunt-安裝方式#Terragrunt 安裝方式":"那要怎麼使用 Terragrunt 呢！？\n第一步當然是安裝它囉，我們系統是 macOS，所以我們安裝方式是 Homebrew 來進行安裝：\nbrew install terragrunt 接著我們的指令會從 terraform XXX 變成以下：\nterragrunt plan terragrunt apply terragrunt output terragrunt destroy Terragrunt 會將所有命令、參數和選項直接轉發到 Terraform。(所以我們也需要下載 Terraform)\nTerragrunt 的預設檔案名稱是 terragrunt.hcl ，Terragrunt 的設定檔案基本上與 Module 差不多，只是有更多更方便的變數可以使用。","參考資料#參考資料":"事半功倍 — 使用 Terragrunt 搭配 Terraform 管理基礎設置：https://medium.com/act-as-a-software-engineer/%E4%BA%8B%E5%8D%8A%E5%8A%9F%E5%80%8D-%E4%BD%BF%E7%94%A8-terragrunt-%E6%90%AD%E9%85%8D-terraform-%E7%AE%A1%E7%90%86%E5%9F%BA%E7%A4%8E%E8%A8%AD%E6%96%BD-f70c30166639"},"title":"如何導入 Terragrunt，Terragrunt 好處是什麼？"},"/projects/":{"data":{"":" 快速將圖片轉成 webp 格式 (Shell Script)使用 Shell Script 快速將當前目錄下的所有圖片轉成 webp 格式，並且會自動移除原始圖片。 臺中市北區國民運動中心 (API)用爬蟲抓取取得游泳池、健身房人數 API。 臺灣行事曆 (API)臺灣行事曆，資料來源中華民國政府行政機關辦公日曆表，可以透過 API 取得資料，並新增其他欄位，例如：民國年、星期英文、星期縮寫等等，方便大家使用。 京緯工程有限公司管理後台 (Web)協助京緯工程有限公司建立公司後台管理系統，自動計算薪資、工程管理、發票管理、材料管理、員工管理等功能。 京緯工程有限公司官網 (Web)協助京緯工程有限公司建立公司官方網頁。 巔峰極速 兌換虛寶網站 (Web)該遊戲有大量虛寶可以兌換，但官方提供的網頁，需要重複輸入 ID 以及驗證碼還有序號，因此寫了一個小網頁，可以只輸入一次 ID 以及驗證碼，就兌換完所有序號。 活動申請系統 (Web)幫學校開發的大型校務系統，方便系統社團使用電子填單方式申請活動，也讓之後的學弟妹，可以更快速的查看到歷屆辦理的活動。 "},"title":"專案成就"}}