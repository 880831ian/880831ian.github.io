{"/about/":{"data":{"":" 哈囉大家好，我叫莊品毅 (CHUANG,PIN-YI)，也可以叫我 Ian，目前是一位 System Architect (SA) 工程師，負責規劃以及測試公司的系統架構，並且協助 RD 團隊解決系統相關的問題。 最近配合公司多雲端的需求，有開始接觸 Amazon Web Services (AWS) 相關服務，並且協助導入 Datadog ，來整合公司內分散的監控系統。\n除此之後，也熟悉使用 Google Cloud Platform (GCP) 雲端相關服務，喜歡使用 Terraform + Terragrunt 來管理雲端大量的 IaC 資源 (GCP Module：12、AWS Module：17)，當然也會使用 Grafana、Prometheus、EFK 等監控 Log 收集工具來確保服務的穩定性以及可追蹤性。並協助 RD 建立 CICD 部署流程。\n在下班空閒時間，我喜歡閱讀技術相關的文件、寫部落格，也會參加一些線下技術社群的活動，曾參加過：Google Next 2025、DevOpsDay 2023|2024、Cloud Summit 2024，希望能夠透過這些活動來學習更多的知識，並且與更多的技術人員交流。\n歡迎大家使用下方 giscus 留言系統留言交流 d(`･∀･)b。","工作經驗#工作經驗":" 凡谷興業有限公司 - SA 架構師 (2024/10/01 - 現在)\nAmazon Web Services (AWS) 服務架構設計、導入與權限管理\n設計公司多雲架構，規劃從 GCP 災難備援至 AWS，確保基礎設施具備高擴展性與靈活性。 開發 17 組 Terraform 模組，實現 IaC（基礎設施即程式碼），確保環境一致性並加速團隊上手時間與維運成本。 撰寫 40+ 篇技術文件，涵蓋 AWS IaC 指南、AWS 與 GCP 服務比較、Graviton 遷移效能及優化成本分析等、各項測試報告。 負責多個專案的服務建置、CICD 自動化部署，確保系統能在 AWS 穩定且高效運行。 協助 RD 研究與部署 AWS macOS GitLab Runner 進行 iOS APP 打包，負責資源建立、問題排查與最佳化，確保 CI/CD 流程順暢運行。 與 AWS 代理商合作，導入 AWS Organizations 及 IAM Identity Center，統一帳戶與權限管理，簡化資源治理並確保安全與合規。 雲端服務架構重構與內網識別優化\n重新設計整體服務架構，導入 Internal DNS，實現服務去 IP 化，提升內部 API 識別性與一致性，同時考量 GCP、AWS 跨雲通用性，並為後續導入 Service Mesh 奠定基礎。 優化網路架構，將部分需經地端處理的流量調整為透過雲端服務處理，成功減少地端設備負載與專線依賴，提升系統穩定與維運效率，並大幅降低設備與頻寬相關成本。 將服務動靜態分離，搭配加速 CDN，強化前端靜態資源加載效率與用戶體驗。 Datadog 監控系統導入與實作\n主導技術處導入 Datadog，整合原本分散於各單位的監控系統，提升監控資料集中性與可視化，查詢效率提升達 90%，大幅加速跨單位除錯與協調流程。 與 Datadog 官方技術團隊深入合作，確保公司需求符合最佳實作方案。 撰寫相關技術文件，協助團隊快速導入，確保監控系統長期穩定運維。 技術趨勢追蹤與問題解決\n主動追蹤 GCP GKE 非註冊發布通道的強制升級公告，撰寫應對方案與檢查腳本，協助團隊避免升級造成服務中斷，並防止提前切換至 Extended Channel 而產生約 $3,000 美金開支。\n關注 Docker Image Pull Rate Limit 政策變更，提出解決方案並驗證其可行性，確保 CI/CD 流程穩定運作不受限制影響。\n將最新技術公告與因應措施整理後，於公開會議中向全技術處同仁分享，有效提升團隊對技術變動的掌握與應對效率。\n凡谷興業有限公司 - SRE 工程師 (2022/02/07 - 2024/10/01)\n導入與優化 IaC 工具鏈，建構一致且自動化的基礎設施部署流程\n導入 Helmfile 管理 50+ 微服務的 Helm Chart，實現 Kubernetes 應用部署流程的一致性與可重複性，提升系統可維護性與部署標準化。 重構 Terragrunt 架構，統一 Terraform 模組與目錄結構，強化 IaC 管理可讀性，明顯提升維運效率與團隊協作品質。 開發 12 組 Terraform 模組，推動基礎設施模組化與標準化，取代 UI 操作以避免設定偏差，並導入版本控制機制，大幅降低人為錯誤與維運風險。 建立涵蓋 Helmfile 與 IaC 的 CI/CD 自動化流程，加速服務部署與基礎設施環境重建，縮短交付時間。 落實最小權限原則，系統性清查移除冗餘 IAM 權限，強化基礎設施安全性並符合內部合規要求。 GCP 成本優化實踐\n優化 Prometheus 監控架構，調整 Samples Ingested 設定，針對指標來源進行分析與過濾，在不影響監控品質的前提下，大幅降低儲存與計算資源，每日節省約 $200 美金。 與 RD 協作優化 Log 結構，剔除無效與冗餘訊息，減少不必要的儲存成本，顯著降低日誌佔用空間與長期儲存費用。 熟悉 GCP 生態系統與 SRE / DevOps 實務操作\n熟悉 GCP 各項核心服務，包含 GKE、GCE、GCS、Cloud Load Balancing、Cloud SQL、Memorystore、VPC、IAM 等，具備全善的建置與維運經驗。 熟練使用 GitLab、GitHub、Jenkins、Ansible 等工具，規劃並實作 CI/CD 自動化部署流程，提升開發與交付效率。 建置完整監控與日誌系統，使用 Fluentd、Elasticsearch、Kibana 搭配 Google Managed Prometheus，實現效能監控與問題追蹤。 ","聯絡方式#聯絡方式":" Email：880831ian@gmail.com LinkedIn：https://www.linkedin.com/in/pinyi/ GitHub：https://github.com/880831ian Telegram：https://t.me/pinyichuchu ","證照#證照":" Google Cloud Professional Cloud Architect\n證照編號: ed519d0fcb984d428460841eb83419c8 / 發照日期： Feb 21, 2025 / 到期日：Feb 21, 2027\nRED HAT CERTIFIED ENGINEER (RHCE)\n證照編號: 190-008-011 / 發照日期： July 5, 2019 / 到期日：July 5, 2021\nRED HAT CERTIFIED SYSTEM ADMINISTRATOR (RHCSA)\n證照編號: 190-008-011 / 發照日期： Jan 11, 2019 / 到期日：Jan 11, 2021"},"title":"關於我"},"/blog/":{"data":{"":"","介紹#介紹":"👋 你好、妳好、大家好，歡迎來到我的秘密花園，這邊主要會記錄我研究一個新的技術或工具、以及處理一些 SA、SRE 遇到的靈異事件問題的小天地。\n會開始寫 Blog 的初衷主要是我的小腦袋瓜，如果不寫下來，過陣子很容易就忘記 (´_ゝ`)，當然也希望可以幫助到有相同問題的人 (可以使用搜尋功能來快速找到相關文件喔)，如果有任何問題或建議，歡迎在下方留言。","聲明#聲明":"由於在學習新的技術或工具時，會參考網路上許多的文件和照片，雖然會加上自己的見解與實作內容改寫而成，且會於文章後附上相關資料來源，如有侵犯到您的權益，請於下方告知，我會立即刪除相關內容，謝謝 (๑•́ ₃ •̀๑)。"},"title":"Blog"},"/blog/agile/":{"data":{"":"此分類包含 敏捷開發 (Agile) 相關的文章。\n串接 Jira 工單自動化通知到 Google Chat "},"title":"敏捷開發 (Agile)"},"/blog/agile/jira-to-google-chat/":{"data":{"":"","介紹#介紹":"我們使用 Jira 的 Ticket 來當作工單，其他單位如果需要請 SRE 協助，都需要填寫此工單，我們的流程是以下：\n待審核 -\u003e SRE 已簽核 -\u003e 進行中 -\u003e (阻塞) -\u003e 完成 工單看板\n我們會由資深工程師來審核，狀態會從 待審核 變成 SRE 已簽核，這時候我們希望能夠收到通知，才由當週值班的工程師來進行處理。","使用-google-chat-card#使用 Google Chat Card":"上面的教學已經完成簡單的通知，但訊息顯示的方式不夠直覺，因此我們可以使用 Google Chat Card 來顯示更多資訊。\nGoogle Chat Cards v2 提供了更多的選項，例如：header、sections、widgets、buttons 等等，可以讓訊息更加豐富。\n官方 Card 範例\n目前 Card 只能使用 v2 版本，它實際上的格式就是一個 JSON，我們可以使用官方提供的 UI Kit Builder 來幫助我們建立 Card。\n可以從左側，選擇想要的元件來自訂 card，右側會預覽顯示結果，最後直接複製 JSON 內容，再貼到 Jira 的自訂資料裡面。\nUI Kit Builder 工具\n最後要注意的是，使用 UI Kit Builder 所產生的 JSON 格式，不是 Google Chat Card 的最終格式，需要自行修改 (很神奇吧 ┐(´д`)┌)。\n需要再多以下的架構，才會正常運作：\n{ \"cardsV2\": [ { \"cardId\": \"unique-card-id\", \"card\": { \u003c\u003c這邊才放入 UI Kit Builder 產生的 JSON 內容\u003e\u003e } } ] } 最終成果如下：\n最終成果\n有太多內容要馬賽克 (´・ω・`)","參考資料#參考資料":"Google Chat Cards v2：https://developers.google.com/workspace/chat/api/reference/rest/v1/cards\nUI Kit Builder：https://addons.gsuite.google.com/uikit/builder?hl=zh-tw\nGoogle Icon：https://fonts.google.com/icons","如何送通知到-google-chat#如何送通知到 Google Chat":"由於公司目前使用 Google Chat 來當作通訊工具，所以我希望能夠在 Jira 工單的狀態變更時，能夠自動送通知到 Google Chat。\n那 Google Chat 有提供幾種方式可以串接，我們這邊用 Webhook 來串接。\n建立聊天室 我們先建立一個聊天室，名稱跟詳細設定就請自行設定\n建立聊天室\n選擇應用程式與整合 進入聊天室，點選聊天室名稱 (例如：jira 串接 google chat 測試)，選擇應用程式與整合\n點選應用程式與整合\n新增 Webhook 選擇 Webhook，並且點擊新增 Webhook，輸入名稱以及自定義的圖片，再點擊儲存\n建立 Webhook\n複製 Webhook 網址 建立完成後，就會在底下出現剛剛建立的 Webhook，點選複製 Webhook 網址\n複製 Webhook 網址\n測試 Webhook 我們可以使用 curl 來測試一下 Webhook 是否正常，可以參考以下指令：\ncurl -X POST \"\u003c\u003c請帶入剛剛所複製的 Google Chat Webhook 連結\u003e\u003e\" \\ -H \"Content-Type: application/json; charset=UTF-8\" \\ -d '{\"text\": \"Hello World\"}' 在剛剛建立的聊天室裡面，有出現 Hello World 就代表成功了\n測試 Webhook 成功","設定-jira#設定 Jira":"接下來我們要設定 Jira，當工單狀態變更時，就會送通知到 Google Chat，我們會使用到 Jira 的工單專案的自動化功能。\n自動化可以自訂觸發條件，也可以寫判斷，針對不同的情境來做不同的動作。\n下面是目前我們的設定，當工單狀態變更時，從 To Do 變成 Approved by SRE，就會觸發此自動化。\n範例自動化設定\n由於我們 SRE 內部有拆組別，所以需要判斷這個工單申請的 RD 單位，是哪個小組負責的，我們這邊會使用 if 條件，來判斷摘要是否包含對應小組的關鍵字。\n最後 Then 如果符合條件，就會使用 『傳送網路要求』 這個動作來送通知到 Google Chat。\n動作選項\n打開傳送網路要求，在 Web 要求 URL，就是貼上剛剛在 Google Chat 建立的 Webhook 連結，HTTP 方法選擇 Post，下面就可以再自訂資料裡面設定想要傳送的內容。\n如果要傳送的內容是變數，例如工單緊急程度、工單號碼等等，就需要使用 {{}} 來包住變數，例如：{{issue.key}}、{{issue.fields.summary}}、{{issue.fields.status.name}} 等等。\n不知道有那些變數，可以點擊圖片箭頭的 {} 圖示，裡面會告訴你哪些變數可以使用：\n變數參考\n如果是使用套件，例如：Template，變數就不會出現在 {} 圖示裡面，可以直接帶入 {{issue.template}}。"},"title":"串接 Jira 工單自動化通知到 Google Chat"},"/blog/aws/":{"data":{"":"此分類包含 Amazon Web Services 相關的文章。\nAWS EKS Pod 出現 aws-cni failed to assign an IP address to container 錯誤 Amazon Virtual Private Cloud (VPC) 介紹 (VPC) 介紹 Elastic Kubernetes Service (EKS) 介紹 Identity and Access Management (IAM) 介紹 如何使用 MFA Token 驗證 AWS CLI 設定 AWS CLI 以及 AWS CLI 指令說明 "},"title":"Amazon Web Services"},"/blog/aws/aws-cli/":{"data":{"":"","介紹#介紹":"AWS CLI 是用於使用 AWS 服務的命令列工具。它也用於對 IAM 使用者或角色進行身份驗證，以便從本機電腦存取 Amazon EKS 叢集和其他 AWS 資源。若要從命令列在 AWS 中設定資源，您需要取得要在命令列中使用的 AWS 存取金鑰 ID 和金鑰。","參考資料#參考資料":"探索 AWS：從菜鳥到熟練的完全指南(四)IAM 登入方式：https://hackmd.io/@gdw7l5sPTOyNv76kZ_twjA/SkTPl-xP3\n設定 AWS CLI：https://docs.aws.amazon.com/eks/latest/userguide/install-awscli.html\nAWS CLI 指令參考：https://docs.aws.amazon.com/zh_tw/cli/latest/userguide/cli_code_examples_categorized.html","如何安裝-aws-cli#如何安裝 AWS CLI":"Mac 安裝指令：\nbrew install awscli 自動補齊指令：\ncomplete -C '/usr/local/bin/aws_completer' aws ","如何設定-aws-cli#如何設定 AWS CLI":"AWS 跟 GCP 比較不一樣的是 CLI 在 Google (gcloud) 可以使用 Web 登入的方式來驗證 gcloud auto login，但是在 AWS 需要使用 Access Key ID 和 Secret Access Key 來驗證。\n因此我們在開始使用前，需要先透過 UI 產生自己帳號的 Access Key ID 和 Secret Access Key，才能夠有權限下指令。\n建立存取密鑰 需要先確認是否有以下的 IAM 權限：\n\"Action\": [ \"iam:CreateAccessKey\", \"iam:DeleteAccessKey\", \"iam:GetAccessKeyLastUsed\", \"iam:ListAccessKeys\", \"iam:UpdateAccessKey\", \"iam:TagUser\" ] TagUser：是用來幫金鑰加上標籤值，方便管理。 沒有權限會噴錯\n先登入 AWS 的管理控制台。 點擊右上角的 AWS 使用者名稱，並點選「安全憑證」。 在 AWS 的管理控制台，選擇安全憑證\n找到存取金鑰，並點選「建立存取金鑰」，如果有建立過，也會顯示在這邊。 建立存取金鑰\n點選「命令列介面 (CLI)」，下方要勾選一個確認建議的選項，最後按下一步。 選擇命令列介面 (CLI)案例\n可以選擇是否要建立標籤，如果要建立，需要前面提到的 iam:TagUser IAM 權限。 設定描述標籤\n就可以拿到存取金鑰 (Access Key ID) 以及私密存取金鑰 (Secret Access Key)，要記住私密存取金鑰，只有這個畫面可以拿到，如果沒有儲存，就需要重新建立一個新的金鑰。 產生完金鑰\n配置 AWS CLI 接下來我們要在本機把剛剛產生的 Access Key ID 和 Secret Access Key 設定到 AWS CLI，我們可以下這個指令。 aws configure 下完後，就會顯示以下設定畫面，分別輸入 Access Key ID、Secret Access Key、預設區域以及輸出格式。 AWS Access Key ID [None]: AAKIA2HCM5WKG7R647HTV AWS Secret Access Key [None]: ****************************** Default region name [None]: region-code (例如：亞太地區東京 ap-northeast-1) Default output format [None]: json 就代表成功設定完成囉 ლ(́◕◞౪◟◕‵ლ)\n另外，設定的內容會寫到 ~/.aws/credentials 和 ~/.aws/config 這兩個檔案中，如果想要修改設定，也可以直接修改這兩個檔案。\n~/.aws/credentials[default] aws_access_key_id = AAKIA2HCM5WKG7R647HTV aws_secret_access_key = t5CMmR4Y7cuFC/RgxJS1XXXXXXXXXXXXXXXXXXXX ~/.aws/config[default] region = ap-northeast-1 ","常用-的-aws-cli-指令說明#常用 的 AWS CLI 指令說明":" 指令 描述 aws sts get-caller-identity 驗證使用者身份 aws eks list-clusters --region {REGION} 列出區域的 EKS aws iam list-mfa-devices --user-name {USER_NAME} 列出使用者的 MFA 裝置 aws sts get-session-token --duration-seconds 129600 --serial-number ${MFA_DEVICES_NUMBER} --token-code ${MFA_CODE} 取得 MFA Token "},"title":"設定 AWS CLI 以及 AWS CLI 指令說明"},"/blog/aws/aws-cni-failed-to-assign-ip/":{"data":{"":"此文章是我在公司擔任架構師，在負責稽核資源使用時，有發現有同仁的 AWS EKS node_group 資源開太大 (開發環境使用 2xlarge 機器規格)，所以先手動從 UI 去調整 Node 的機器規格 (RD 未提供建立時的 Terraform 版控)，從 m7g.2xlarge 先改成 m7g.large。\n但由於同仁是用 Terraform 來建置整座 EKS + node_group，有 Launch Templates 以及 Auto Scaling Groups 等元件，能調整機器規格的只能從 Launch Templates 以及 Auto Scaling Groups，但 Launch Templates 沒辦法編輯 (只能新增、刪除)，當時為了想要先快速的減少花費，因此先直接調整 Auto Scaling Groups 的 Node 機器規格。\nAuto Scaling Groups 設定","參考資料#參考資料":"Maximum IP addresses per network interface：https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AvailableIpPerENI.html","尋找問題#尋找問題":"排除 Subnet IP 用完的情況 一開始看錯誤訊息 aws-cni failed to assign an IP address to container 以為是 subnet 的 IP 已經被 Pod 用完了，所以先使用以下指令來查看 Subnet 的可用 IP 數量：\naws ec2 describe-subnets \\ --query \"Subnets[*].{SubnetId:SubnetId, AvailableIps:AvailableIpAddressCount, CidrBlock:CidrBlock}\" \\ --output table （輸出範例，非當下 Subnet）\n也可以從 UI 去看每一個 Subnet 的可用 IP 是多少：\nAWS UI 查看 Subnet 可用 IP\n[!NOTE] AWS Subnet 計算方式： 假設是 /24 計算是： 32-24=8，2 的 8 次方 = 256\nAWS 會保留 5 個 IP，如下：\nXX.XX.XX.0：網路位置\nXX.XX.XX.1：VPC 路由器位址\nXX.XX.XX.2：Amazon DNS 位址\nXX.XX.XX.3：保留給未來用途\nXX.XX.XX.255：廣播位址\n所以最多可用的 IP 數量是 256-5=251\n排除 Pod 數量超過 Node 的限制 排除了 Subnet 用完的情況，後來搜尋了一下文件，發現 AWS 的機器規格，會限制一個 Node 上面，最多能長有幾個 Pod，這個跟 Amazon VPC CNI 插件有關，下面附上一些相關的問題連結：\nNon fatal but persistent warning: “Failed to create pod sandbox … failed to assign an IP address to container.” #3066\nAWS VPC CNI PLUGIN - Error: container runtime network not ready due to NetworkPluginNotReady - How to Resolve\n那要怎麼計算什麼機器規格，Pod 的最大限制是多少，可以用這個公式來計算\nENI * (# of IPv4 per ENI - 1) + 2 可以先用這個指令來看 ENI 以及 IPv4 per ENI 各是多少：\naws ec2 describe-instance-types \\ --filters \"Name=instance-type,Values=c5.*\" \\ --query \"InstanceTypes[].{ \\ Type: InstanceType, \\ MaxENI: NetworkInfo.MaximumNetworkInterfaces, \\ IPv4addr: NetworkInfo.Ipv4AddressesPerInterface}\" \\ --output table Maximum IP addresses per network interface\n每個機器規格對應的 IP 以及 MaxENI\n已 c5.4xlarge 來說，就是 8*(30-1)+2 = 234，一個 Node 最多 234 個 Pod，c5.xlarge 來說，就是 4*(15-1)+2 = 58，一個 Node 最多 58 個 Pod。\n但如果每次都要自己計算，會有點麻煩，所以這邊再提供幾個方式：\n官方已經計算好的對應表：https://github.com/awslabs/amazon-eks-ami/blob/main/templates/shared/runtime/eni-max-pods.txt 每個 Amazon EC2 執行個體類型建議的最大 Pod 數腳本：Amazon EKS recommended maximum Pods for each Amazon EC2 instance type (但這個計算最高只會顯示 110，詳細可以看文件) 使用腳本計算不同 CNI version 以及機器的可用 Pod 數量\n但是，如果是超過 Node 的 Pod 最高限制，應該也會是顯示以下的無法調度錯誤，而不是 aws-cni failed to assign an IP address to container 錯誤：\n無法調度的範例錯誤","找到問題#找到問題":"最後發現，因為我們最一開始只調整了 Auto Scaling Groups 的機器規格，調整完後，資源的確是降低了（從 8 vCPUs / 32.0 GiB → 2 vCPUs / 8.0 GiB），但 Node 的 Allocatable pod 卻還是原本的數量限制，原因是 Controller plane 不知道我們有調整 (Auto Scaling Groups 只會調整 node_group)。\n因此，當服務開上去，Node 能長的 Pod 上限還是舊的 m7g.2xlarge 58 個，但對於 aws-node 來說，cni 最多只能 m7g.large 29 個，所以超過 29 的都會出現這個錯誤，把它趕到其他 Node (還沒有滿 29 上限) 就正常。\n可以用這個指令看到目前 Node 的 HOSTNAME、instance-type、allocatable.pods： kubectl get nodes -o custom-columns=\"HOSTNAME:.metadata.name,INSTANCE-TYPE:.metadata.labels.node\\\\.kubernetes\\\\.io/instance-type,ALLOCATABLE-PODS:.status.allocatable.pods\" 有問題的 Node 有問題的 Node Allocatable-Pods\n正常的 Node 正常的 Node Allocatable-Pods","解決問題#解決問題":"所以我們要避免這個問題有兩個解法：\n更換機器規格需要重建 Launch Templates，用先建後拆，不能只更新 Auto Scaling Groups。 用 Terraform 做完整的管理。 ","遇到問題#遇到問題":"調整完後，再使用 Auto Scaling Groups 的 instance refresh 去更新機器規格，這時候會發現，新 Node 上的 Pod 會偶發出現以下錯誤訊息：\nFailed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"XXXXXXXXX\": plugin type=\"aws-cni\" name=\"aws-cni\" failed (add): add cmd: failed to assign an IP address to container "},"title":"AWS EKS Pod 出現 aws-cni failed to assign an IP address to container 錯誤"},"/blog/aws/cli-mfa/":{"data":{"":"會有這一篇文章也是跟 Identity and Access Management (IAM) 介紹 一樣，在測試 eksctl 建立 EKS 時，發現一直噴錯，說我沒有權限，但我去看我的 IAM Policy 也有加上對應 action 以及 resource，但還是不行，後來發現 cloudformation:* 這個 action 所在的 Policy 有加上 Condition MFA 驗證的條件，所以就來寫一篇如何使用 MFA Token 驗證 AWS CLI 的文章。\n沒有權限噴錯QQ","前情提要#前情提要":"我目前有 3 個 IAM Policy，分別是：\ndefault-policy：放一些個人基本的 IAM 權限 default-policy\npoc-general-policy：放一些我在測試的 PoC 的 IAM 權限，但是仔細看這邊有 Condition MFA 驗證的條件。 poc-general-policy\nReadOnlyAccess：預設的 ReadOnlyAccess 權限。 ReadOnlyAccess\n當你設定好 aws configure 後，你會拿到一個永久性的 IAM 憑證 (也就是 Access Key ID 和 Secret Access Key，設定可以看配置 AWS CLI)，但是如果你的 Policy 有加上 MFA 驗證的條件，那你就需要使用 MFA Token 來驗證，那要怎麼做呢？","參考資料#參考資料":"How do I use an MFA token to authenticate access to my AWS resources through the AWS CLI? ：https://repost.aws/knowledge-center/authenticate-mfa-cli","寫入設定檔#寫入設定檔":"當你拿到 Token 以外，你不可能每次下指令都帶 Token，所以我們要把它寫到 ~/.aws/credentials 檔案中，這樣就可以自動帶入 Token 了。\n我們在 aws configure 的時候，會有一個 profile 的選項，這個選項就是用來設定不同的憑證，預設是 default，我們可以自己設定一個新的憑證，如下：\n[mfa] aws_access_key_id = ASIA2HCXXXXXXXXXXXXX aws_secret_access_key = 25CBN6CjRNLPukXXXXXXXXXXXXX aws_session_token = FwoGZXIvYXdzEXXXXXXXXXXXXX 所以當你要使用 MFA Token 的時候，只要在指令後面加上 --profile mfa 就可以了。\n就可以用 poc-general-policy 的 action 囉～\n當然，如果每次都需要下指令去取得 Token 也是很麻煩的，所以我寫了一個 Shell Script 來幫我們取得 Token 並寫入 ~/.aws/credentials 檔案中，這樣就可以省去每次都要下指令的麻煩了。程式也會同步到 Github，大家可以自行下載使用。\n#! /bin/bash USER_NAME=$1 MFA_CODE=$2 # 顏色設定 RED=\"\\033[1;31m\" GREEN=\"\\033[1;32m\" YELLOW=\"\\033[1;33m\" BLUE=\"\\033[1;34m\" WHITE=\"\\033[0m\" if [ -z $USER_NAME ] || [ -z $MFA_CODE ]; then echo -e \"\\n${YELLOW}提醒說明：請輸入使用者名稱 與 MFA 驗證碼\\n格式如下：./aws_mfa.sh ian_zhuang 235821 \u003c\u003c (請依照手機上的 MFA 號碼)。${WHITE}\\n\" exit 1 fi if [[ ! \"$MFA_CODE\" =~ ^[0-9]{6}$ ]]; then echo -e \"\\n${YELLOW}提醒說明：MFA 驗證碼只支援 6 碼數字。${WHITE}\\n\" exit 1 fi MFA_DEVICES_NUMBER=$(aws iam list-mfa-devices --user-name ${USER_NAME} | jq -r '.MFADevices[0].SerialNumber') if [ -z $MFA_DEVICES_NUMBER ]; then echo -e \"\\n${RED}錯誤說明：MFA 裝置不存在，請確認使用者名稱是否輸入正確${WHITE}\\n\" exit 1 fi MFA_INFO=$(aws sts get-session-token --duration-seconds 129600 --serial-number ${MFA_DEVICES_NUMBER} --token-code ${MFA_CODE}) if [ $? -eq 0 ]; then MFA_ACCESS_KEY_ID=$(echo $MFA_INFO | jq -r '.Credentials.AccessKeyId') MFA_SECRET_ACCESS_KEY=$(echo $MFA_INFO | jq -r '.Credentials.SecretAccessKey') MFA_SESSION_TOKEN=$(echo $MFA_INFO | jq -r '.Credentials.SessionToken') MFA_EXPIRATION=$(echo $MFA_INFO | jq -r '.Credentials.Expiration') # 計算 MFA 到期時間(轉換時區) MFA_0_TIME=$(date -j -f \"%Y-%m-%dT%H:%M:%S\" \"$(echo \"$MFA_EXPIRATION\" | sed 's/+00:00//')\" \"+%Y-%m-%d %H:%M:%S\") MFA_8_TIME=$(date -j -v+8H -f \"%Y-%m-%d %H:%M:%S\" \"$MFA_0_TIME\" \"+%Y-%m-%d %H:%M:%S\") MFA_CREDENTIALS=\"[mfa]\\naws_access_key_id = $MFA_ACCESS_KEY_ID\\naws_secret_access_key = $MFA_SECRET_ACCESS_KEY\\naws_session_token = $MFA_SESSION_TOKEN\" # 檢查是否已經有 [mfa] 區塊 if grep -q \"\\[mfa\\]\" ~/.aws/credentials; then # 更新 [mfa] 區塊 sed -i '' '/\\[mfa\\]/,/^$/d' ~/.aws/credentials echo -e \"$MFA_CREDENTIALS\" \u003e\u003e~/.aws/credentials else # 新增 [mfa] 區塊 echo -e \"\\n\\n$MFA_CREDENTIALS\" \u003e\u003e~/.aws/credentials fi echo -e \"\\n${GREEN}MFA 驗證成功，將 Key 跟 Token 加入or修改到 ~/.aws/credentials 檔案中${WHITE}\" echo -e \"${BLUE}到期時間：${MFA_8_TIME}${WHITE}\\n\" else echo -e \"\\n${RED}MFA 驗證失敗，請確認 MFA 驗證碼是否正確${WHITE}\\n\" exit 1 fi 只需要執行 ./aws_mfa.sh {USER_NAME} {MFA_CODE} 就可以了，程式會自動幫你取得 Token 並寫入 ~/.aws/credentials 檔案中。\n執行腳本","查詢目前有沒有設定-mfa#查詢目前有沒有設定 MFA":"可以先查詢目前有沒有設定 MFA，可以透過以下指令：\naws iam list-mfa-devices --user-name {USER_NAME} 將 {USER_NAME} 換成自己的使用者名稱，如果有設定 MFA，就會顯示出來，如下：\n顯示 mfa 裝置","產生-mfa-token#產生 MFA Token":"我們確定我們有 MFA 設定後，就可以透過已經設定好的 MFA 以及剛剛的 SerialNumber 來產生 MFA Token，指令如下：\naws sts get-session-token --duration-seconds 129600 \\ --serial-number {MFA_DEVICES_NUMBER} \\ --token-code {MFA_CODE} 首先 --duration-seconds 參數是 MFA Token 的有效時間，單位是秒，預設是 12 小時，我們可以調整 900 秒 (15 分鐘) 到 129600 秒 (36 小時)，root 使用者憑證，範圍為 900 秒 (15 分鐘）到 3600 秒 (1 小時)，大家可以自行調整。\n{MFA_DEVICES_NUMBER} 換成剛剛查詢到的 MFA 裝置的 SerialNumber，最後將 {MFA_CODE} 換成你的 MFA 的 Code。\n一起來看一下輸出結果：\n{ \"Credentials\": { \"AccessKeyId\": \"ASIA2HCXXXXXXXXXXXXX\", \"SecretAccessKey\": \"25CBN6CjRNLPukXXXXXXXXXXXXX\", \"SessionToken\": \"FwoGZXIvYXdzEXXXXXXXXXXXXX\", \"Expiration\": \"2024-08-02T19:42:36+00:00\" } } AccessKeyId、SecretAccessKey 就跟 aws configure 拿到的 Access Key ID 和 Secret Access Key 是一樣的類型，只是還多一個 SessionToken，這個 SessionToken 是有經過 MFA 驗證過，所以可以用來執行需要 MFA 驗證條件的 Policy。"},"title":"如何使用 MFA Token 驗證 AWS CLI"},"/blog/aws/eks-introduce/":{"data":{"":"由於公司政策調整，除了原先的 Google Cloud Platform (GCP) 之外，我們也開始使用 AWS 服務，其中包括了 EKS (Elastic Kubernetes Service)。這篇文章將會介紹 EKS 的基本概念以及一些使用上的注意事項。\n後面也會有幾篇 AWS 文章，歡迎大家持續關注。","eks-定價#EKS 定價":"","什麼是-eks#什麼是 EKS？":"那熟悉 Kubernetes 的朋友應該對 EKS 並不陌生，EKS 是 AWS 提供的 Kubernetes 服務，讓使用者可以在 AWS 上快速建立、擴展和管理 Kubernetes 集群。就跟 GCP 的 GKE 一樣。\n流程也跟 GKE 差不多，如圖：\n建立 Cluster：可以透過 eksctl、AWS Console、AWS CLI、Terraform 等等來建立。\neksctl：是一個官方專門給 EKS 的 CLI 工具，可以快速建立、更新和刪除 EKS 集群，詳細可以看：eksctl、Amazon EKS 入門。 Mac 安裝指令：\nbrew tap weaveworks/tap brew install weaveworks/tap/eksctl AWS CLI：是官方的 CLI 工具，除了建立 EKS 集群，還可以管理其他 AWS 服務，詳細可以看：我的另一篇筆記 設定 AWS CLI 以及 AWS CLI 指令說明。 選擇運算資源的方式：可以分成 Fargate、Karpenter、託管節點群組和自管理節點。那我們使用主要是使用託管節點群組。其他的方式會在後面的文章介紹。\n設定：設定必要的控制器、驅動程式或是服務等等。\n部署工作負載：自訂 Kubernetes 物件，例如 Pod、Service、Deployment 等等。\n管理：監督工作負載，整合 AWS 服務以簡化維運並提高工作負載效能。\n在雲端中執行 Amazon EKS 的基本流程\n其他比較詳細的架構以及部署選項可以參考：Amazon EKS 架構、部署選項，這邊就不再贅述。","參考資料#參考資料":"What is Amazon EKS?：https://docs.aws.amazon.com/eks/latest/userguide/what-is-eks.html\nAmazon EKS 入門：eksctl：https://docs.aws.amazon.com/zh_tw/eks/latest/userguide/getting-started-eksctl.html","建立第一個-ekseksctl#建立第一個 EKS：eksctl":"詳細請參考：Amazon EKS 入門：eksctl\n在建立 EKS 前，我們要先確認我們要建立的節點是要使用 Fargate 還是託管節點群組，我們這邊拿 GCP 來說明，託管節點群組就是我們一般建立的 Node_Pool，而 Fargate 就是 Serverless 的概念，不需要自己管理節點，只需要管理 Pod，詳細我後面也會寫一篇文章來介紹，可以點我查看。\n那我們這邊就先以託管節點群組來建立 EKS，建立使用以下指令來建立：\neksctl create cluster --name {my-cluster} --region {region-code} eksctl create cluster --name ian-test --region us-east-1 將 {my-cluster} 替換成自己要的名稱，只能英文數字字元(區分大小寫)和連字號。必須字母數字字元開頭，且長度不可以超過 100 個字元，名稱在該區域中必須是唯一的。\n{region-code} 則是替換成自己要的區域，例如：us-east-1。可以從 Amazon EKS endpoints and quotas 這邊查看 region-code。 如果是 Fargate 的話，只需要在指令後面加上 --fargate\n提醒：使用這個指令建置，會需要有 CloudFormation 的相關權限，如果缺少權限，會顯示以下錯誤：\n( CloudFormation 是將基礎設施視為程式碼的服務，可以對 AWS 和第三方資源進行建模、佈建和管理，詳細一樣請參考後續文章 AWS CloudFormation 介紹。) 使用 eksctl 建立 EKS 缺少 CloudFormation 權限"},"title":"Elastic Kubernetes Service (EKS) 介紹"},"/blog/aws/iam-introduce/":{"data":{"":"先來說一下為什麼會寫這篇文章，主要是在寫跟測試 Elastic Kubernetes Service (EKS) 介紹 文章時，想要用 eksctl 來建立 EKS，它實際上是透過 CloudFormation 來建立的，詳細可以去看 EKS 文章，總之當時遇到權限的問題，找了很久，也看不懂 AWS 的 ARN 是什麼，所以就先來寫一篇 IAM 的文章 (´≖◞౪◟≖)。","iam-policy-範例#IAM Policy 範例":"那了解了 IAM Policy 的每個結構以及功能，我下面就舉例兩個 IAM Policy 來讓大家複習看看。\n範例一 { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"ec2:CreateVolume\", \"Resource\": \"*\" }, { \"Effect\": \"Allow\", \"Action\": \"ec2:CreateTags\", \"Resource\": \"arn:aws:ec2:::volume/*\", \"Condition\": { \"StringLike\": { \"aws:RequestTag/project\": \"*\" } } }, { \"Effect\": \"Deny\", \"Action\": \"ec2:CreateTags\", \"Resource\": \"arn:aws:ec2:::volume/*\", \"Condition\": { \"StringEquals\": { \"aws:ResourceTag/environment\": \"production\" } } } ] } 這個 Policy 有 3 個 Sub-Statement，分別是：\n允許對所有的 EC2 instance 創建 Volume。 允許有 aws:RequestTag/project 的 Tag 對所有的 EC2 Volume 創建 Tag。 拒絕有 aws:ResourceTag/environment 是 production 的 Tag 對所有的 EC2 Volume 創建 Tag。 範例二 { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"ecs:RunTask\", \"ecs:StartTask\" ], \"Resource\": [ \"*\" ], \"Condition\": { \"StringEquals\": { \"aws:RequestTag/environment\": [ \"production\", \"prod-backup\" ] }, \"ArnEquals\": { \"ecs:cluster\": [ \"arn:aws:ecs:us-east-1:111122223333:cluster/default1\", \"arn:aws:ecs:us-east-1:111122223333:cluster/default2\" ] } } } ] } 這個 Policy 有 1 個 Sub-Statement：\n允許在 arn:aws:ecs:us-east-1:111122223333:cluster/default1 和 arn:aws:ecs:us-east-1:111122223333:cluster/default2 這兩個 ECS 集群上，對 aws:RequestTag/environment 標籤為 production 或 prod-backup 的任務執行 RunTask 和 StartTask 操作。\n範例參考：單值內容索引鍵政策範例","policy-type#Policy Type":"Policy Type 有三種：\nAWS 受管\n由 AWS 提供的 Policy，這些 Policy 會包含一些常見的操作，例如：S3 的讀寫、EC2 的啟動、停止等等。如果 AWS 有新增服務，也會同時更新這些 policy。詳細可以看：AWS 受管理政策 AWS 受管 – 職務職能\n這些 Policy 會針對特定的工作角色，例如：系統管理員 (AdministratorAccess)、網路管理員任務角色 (NetworkAdministrator)、資料庫管理員任務角色 (DatabaseAdministrator) 等等，這些 Policy 會包含這些角色常見的操作。詳細可以看：AWS 受管理的工作職能政策\n客戶受管\n使用者自己定義的 Policy，這些 Policy 會針對自己的需求來定義，例如：只能讀取某個 S3 bucket、只能啟動某個 EC2 instance 等等。詳細可以看：客戶受管政策","policy-結構#Policy 結構":"IAM Policy 會是一個 JSON 格式的文件，可以定義了一個或多個 Action，這個 Policy 會告訴 AWS 這個身份可以對哪些資源 (Resource) 進行哪些操作 (Action)。而這個 Policy 會被附加到一個身份上，這個身份可以是一個 IAM User、IAM Group 或 IAM Role。然後一個 User、Group 或 Role 可以有多個 Policy。\n我目前公司帳號的 Policy\n上面的圖片就是一個典型的 IAM 的 Policy 結構，可以看到這個 Policy 會有 Version、Statement、Effect、Action、Resource、Condition (上面沒有 xD) 等等主要元素。\n下面就針對 Version、Statement、Effect、Action、Resource、Condition 這些元素來做一些介紹。\n重點提醒：\n所有的元素都是有區分大小寫的，寫錯大小寫會導致 Policy 無法正確解析。 Version Version：用來告知 AWS 這個 Policy 是使用哪個版本的 IAM Policy 語法，目前最新的版本是 2012-10-17，舊版本是 2008-10-17。詳細請參考：IAM JSON 政策元素：Version\nStatement Statement：是 Policy 的主要元素。此元素為必填。Statement 元素可包含單一陳述式，或是個別陳述式的陣列。每個個別的陳述式區塊都必須用大括號 { } 括起。針對多個陳述式，陣列必須用方括號 [ ] 括起。詳細請參考：IAM JSON 政策元素：Statement\n例如：\n\"Statement\": [{...},{...},{...}] Effect Effect：元素是必填的，指定陳述式是否允許或拒絕。\nEffect 的有效值為 Allow 和 Deny。Effect 值會區分大小寫。 AWS 預設是 Deny，所以如果沒有設定 Effect，則預設是 Deny，需要明確指定 Allow。 Policy 每個 Statement 都必須有一個 Effect 元素。 如果同時有 Allow 和 Deny 套用同一個資源，則 Deny 會優先於 Allow。 詳細請參考：IAM JSON 政策元素：Effect\nAction Action：描述哪一個服務可以做哪些動作。\n每個 AWS 服務都有自己的一組動作，描述您可以使用該服務執行的哪些任務。 使用服務命名做為動作開頭 (iam、ec2、sqs、sns、s3 等) 來指定值，後面則加上對應的動作名稱。 Action 服務和動作名稱不區分大小寫。例如，iam:ListAccessKeys 與 IAM:listaccesskeys 相同 下面舉例幾個 Action：\nAmazon S3 取得物件： \"Action\": \"s3:GetObject\" IAM 修改密碼： \"Action\": \"iam:ChangePassword\" EC2 啟動實例： \"Action\": \"ec2:StartInstances\" 可以從上面得知 s3、iam、ec2 是 AWS 提供的服務，而 GetObject、ChangePassword、StartInstances 是這些服務提供的動作。\n當然，服務很多，再加上每個服務的動作也不一樣，那我們可以依照 武功秘籍 提到的 Actions, resources, and condition keys for AWS services 去查詢對應的動作列表。\n如何方便的撰寫 Action？ 如果，我們需要新增服務很多的 Action，我們只能一個一個輸入嗎？像下面這張圖片，我們可以看到 Elastic Kubernetes Service (EKS) 的 Action 部分清單，假如我們要這些全部的動作，需要把每個都寫出來嗎？\nElastic Kubernetes Service Action 部分清單 Amazon Elastic Kubernetes Service 定義的動作\n當然不用！我們可以使用 * 來代表所有的動作，例如：\n\"Action\": \"eks:*\" 這樣就可以代表所有的 EKS 的動作。\n或是只想要 eks 的 Create 相關動作，可以這樣寫：\n\"Action\": \"eks:Create*\" 這樣就包含了 CreateAccessEntry、CreateAddon、CreateCluster、CreateFargateProfile、CreateNodegroup 等等動作。\n或是我們只想要有 eks 的 cluster 相關動作(不要 Addon、Nodegroup 等等)，可以這樣寫：\n\"Action\": \"eks:*Cluster*\" 這樣就只有 CreateCluster、DeleteCluster、DescribeCluster、ListClusters 等等的動作，沒有 CreateAddon、CreateNodegroup 動作。\n詳細請參考：IAM JSON 政策元素：Action\nResource Resource：描述哪一個服務的哪一個資源。\n這邊以 Amazon S3 為例，我們建立了一個 bucket，它就是一個 Resource，我們上傳了一個檔案到這個 bucket，這個檔案也是一個 Resource。\n但是，我們要如何去定義我們可以對哪些的 AWS Resource 做哪些的操作呢？這時候就可以使用 ARN (Amazon Resource Name) 來定義。\nARN 它的命名會將不同服務、不同 region、不同使用者帳號都考慮進去，產生一個唯一識別 AWS 資源的字串，下面是三種 ARN 格式：\narn:\u003cpartition\u003e:\u003cservice\u003e:\u003cregion\u003e:\u003caccount-id\u003e:\u003cresource-id\u003e arn:\u003cpartition\u003e:\u003cservice\u003e:\u003cregion\u003e:\u003caccount-id\u003e:\u003cresource-type\u003e/\u003cresource-id\u003e arn:\u003cpartition\u003e:\u003cservice\u003e:\u003cregion\u003e:\u003caccount-id\u003e:\u003cresource-type\u003e:\u003cresource-id\u003e 詳細可以參考：ARN 格式\n那我們一樣，將 partition、service、region、account-id、resource-type、resource-id 都個別簡單介紹一下：\npartition (分區) 它是 AWS 的分區，目前有 aws 和 aws-cn 以及 aws-us-gov，分別代表 AWS 區域和中國區域以及 AWS GovCloud (US) 區域，所以基本上我們都會使用 aws。\nservice (服務) 它是 AWS 的服務名稱字首，例如：s3、iam、ec2、eks 等等。不清楚每個服務的字首可以參考 AWS 服務的動作、資源和條件索引鍵，假設我們點進去看到 Amazon S3 的服務名稱字首是 s3。\nAmazon S3 的動作、資源和條件索引鍵\nregion (區域) 它是 AWS 的區域，例如：us-east-1、us-west-2、ap-northeast-1 等等。如果需區域代碼清單，可以查看區域端點\naccount-id (帳號識別碼) 它是擁有資源的 AWS 帳號識別碼 (不含連字號)，例如：123456789012。\nresource-type (資源類型) 它是資源的類型，例如：vpc (虛擬私有雲)，bucket (S3 bucket)，instance (EC2 instance) 等等。\nresource-id (資源識別碼) 它是資源名稱、資源 ID 或資源路徑。某些資源識別碼包括父項資源 (sub-resource-type/父資源/子資源) 或限定元，\n例如：\nIAM User arn:aws:iam::123456789012:user/ian_zhuang 這個在最上面的 Policy 結構中的圖片可以看到我的 User ARN。\nVPC arn:aws:ec2:us-east-1:123456789012:vpc/vpc-0e9801d129EXAMPLE S3 Bucket arn:aws:s3:::* arn:aws:s3:::demo-bucket-a/* arn:aws:s3:::demo-bucket-b/downloads/* 這邊 S3 Bucket 的 ARN 有三個：\n第一個代表的是所有的 S3 bucket \u0026 object。\n第二個代表的 demo-bucket-a 對於此 bucket 的所有 object。\n第三個代表的 demo-bucket-b 的 downloads 目錄下的所有 object。\n另外，還會發現，因為 S3 服務它沒有 \u003cregion\u003e、\u003caccount-id\u003e 所以中間兩個就留空即可。\n詳細可以參考：IAM JSON 政策元素：Resource\nCondition 在上面介紹的 Effect 我們可以設定 Allow 或 Deny 來限制 User、Group、Role 對資源操作的權限，但是有時候我們可能會需要更多的條件來限制這個 Policy，這時候就可以使用 Condition 來設定。\nCondition：它可用於 Policy 生效時指定條件，Condition 元素是選填的。\n這個功能等於是加了一個 if 的判斷，以及 Statement 可以有很多個 Sub-Statement，每個 Sub-Statement 都可以設定 Condition，就可以設定很多種可能。\n它的格式是：\n\"Condition\": { \"\u003ccondition-operator\u003e\": { \"\u003ccondition-key\u003e\": \"\u003ccondition-value\u003e\" } } 會有三個元素組成：分別是 condition-operator、condition-key、condition-value。(condition-value 就是一般要判斷的資料，所以這邊就不多做介紹)\ncondition-operator condition-operator：是一個比較運算子，例如：\n字串的有： 運算子 說明 StringEquals 檢查 condition-key 跟 condition-value 字串是否相等 StringNotEquals 檢查 condition-key 跟 condition-value 字串是否不相等 StringEqualsIgnoreCase 檢查 condition-key 跟 condition-value 字串是否相等，不區分大小寫 StringNotEqualsIgnoreCase 檢查 condition-key 跟 condition-value 字串是否不相等，不區分大小寫 由於有些 condition-key 沒有區分大小寫，所以運算子會有 IgnoreCase 的不區分大小寫的運算子，在設計的時候要注意。\n數值的有： 運算子 說明 NumericEquals 檢查 condition-key 跟 condition-value 數值是否相等 NumericNotEquals 檢查 condition-key 跟 condition-value 數值是否不相等 NumericLessThan 檢查 condition-key 是否小於 condition-value 數值 NumericLessThanEquals 檢查 condition-key 是否小於等於 condition-value 數值 NumericGreaterThan 檢查 condition-key 是否大於 condition-value 數值 還有像是日期條件運算子、布林值條件運算子、二進位條件運算子、IP 地址條件運算子等等，由於運算子蠻多的，可以參考：IAM JSON 原則元素:條件運算子。\ncondition-key 那 condition-key 是什麼呢？它是一個 key，用來指定要檢查的條件，例如：aws:username、aws:MultiFactorAuthAge、s3:authType、eks:namespaces 等等。\n在 condition-key 又分為兩種類型：分別是：\nGlobal Condition Key\n這些是 AWS 提供的全域條件，可以在所有的服務或是 Action 中使用，例如：aws:username、aws:userid 等等。但是有些特定狀況下才可以使用，例如：aws:MultiFactorAuthAge，詳細可以參考：AWS global condition context keys\nService Condition Key\n這些是服務提供的條件，不同服務的 Service 也會有所不同，例如：s3:authType、eks:namespaces 等等。\n想要知道哪些服務有什麼 Condition Key，可以再次開啟 武功秘籍 提到的 Actions, resources, and condition keys for AWS services，我們假設選擇 Amazon Elastic Kubernetes Service (EKS) 服務：\n因為每個 Action 都有對應的 Condition Key，不是任何一個 Action 都與任一個 Condition Key 有搭配，如下圖：\nActions defined by Amazon Elastic Kubernetes Service\n所以需要查看文件裡面的清單，例如像是 AssociateAccessPolicy Action 就會有 eks:policyArn、eks:namespaces、eks:accessScope 的 Condition Key。\n另外，想要知道 Condition Key 對應到哪個資料類型，也可以參考 Condition keys for Amazon Elastic Kubernetes Service，後面會告訴你哪個 Condition Key 的資料類型，方便找到對應的 Condition Operator。\nCondition keys for Amazon Elastic Kubernetes Service\nCondition 規範 Condition 可以在 (condition-operator) 有多個條件 (condition-key)，每個條件都是 and 來相互關聯，例如： { (省略其他元素) \"Condition\": { \"StringEquals\": { \"aws:PrincipalTag/department\": \"finance\", \"aws:PrincipalTag/role\": \"audit\" } } } 代表的是 aws:PrincipalTag/department 跟 aws:PrincipalTag/role 都要符合才會生效。\n如果同一個條件有多個值，每個值都是 or 來相互關聯，例如： { (省略其他元素) \"Condition\": { \"StringEquals\": { \"aws:PrincipalTag/department\": [ \"finance\", \"hr\", \"legal\" ], \"aws:PrincipalTag/role\": [ \"audit\", \"security\" ] } } } 官方的圖片解釋的更清楚：\nCondition 的 condition-operator 不能重複，例如： { (省略其他元素) \"Condition\": { \"StringEquals\": { \"aws:PrincipalTag/department\": \"finance\" }, \"StringEquals\": { \"aws:PrincipalTag/role\": \"audit\" } } } 這樣是錯誤的寫法，因為 StringEquals 出現兩次，要將兩個條件合併成一個條件，例如：\n{ (省略其他元素) \"Condition\": { \"StringEquals\": { \"aws:PrincipalTag/department\": \"finance\", \"aws:PrincipalTag/role\": \"audit\" } } } 詳細可以參考：IAMJSON 政策元素：Condition","什麼是-iam#什麼是 IAM？":"IAM 的全名是 Identity and Access Management，從字面上來看就可以得知，它是用來管理 AWS 資源的身份和存取權限的服務。透過 IAM，您可以控制對 AWS 資源的存取權限，以及對這些資源的操作權限。也就是 Who (身份) 可以 Do (操作) What (資源) 的一個功能。","參考資料#參考資料":"[AWS IAM] 學習重點節錄(2) - IAM Policy：https://godleon.github.io/blog/AWS/learn-AWS-IAM-2-policy/\nActions, resources, and condition keys for AWS services：https://docs.aws.amazon.com/service-authorization/latest/reference/reference_policies_actions-resources-contextkeys.html\n其餘對應的參考資料都直接附在文章中。","武功秘籍#武功秘籍":"文章主要參考：[AWS IAM] 學習重點節錄(2) - IAM Policy，這篇文章寫得很詳細，當然，我會加上一些自己的理解跟測試範例，如果有興趣的話，可以去看看原文。\n下方文章主要都圍繞官方的 Actions, resources, and condition keys for AWS services 文件上，如果後面沒有附連結，基本上就在講這份，請大家可以打開網頁一起看。"},"title":"Identity and Access Management (IAM) 介紹"},"/blog/aws/vpc-introduce/":{"data":{"":"此篇單純是我自己學習 AWS VPC 的筆記，主要是為了熟悉 AWS 的 VPC，了解 VPC 包含哪些元件，以及如何建立 VPC、設定 VPC 等等。","default-vpc#Default VPC":"每個 AWS 帳號都會有一個 Default VPC，這個 VPC 是 AWS 預設給你的，裡面已經設定好一些設定，方便你可以快速啟動資源。\n在 Default VPC 中，會在每個 Availability Zone (AZ) 建立好 Subnet。\n每個 Subnet 都會有一組可以連到 internet 的 Route Table，所以每個 Subnet 都預設具備連網的功能\n每個 Subnet 都會有一個 Internet Gateway 存在以及連接，因此 Default VPC 所有 Subnet 都屬於 Public Subnet。","vpc-ip-範圍#VPC IP 範圍":"目前 AWS 支援的 IP 範圍是根據 RFC 1918 標準。以下是可用的私有 IP 範圍：\n10.0.0.0 - 10.255.255.255（10.0.0.0/8，總共 16,777,216 個 IP 地址） 172.16.0.0 - 172.31.255.255（172.16.0.0/12，總共 1,048,576 個 IP 地址） 192.168.0.0 - 192.168.255.255（192.168.0.0/16，總共 65,536 個 IP 地址） 一個 VPC 最多可以使用 5 個 Ipv4 CIDR 設定，但一般只會設定一個","vpc-peering#VPC Peering":"當不同的 VPC 想要連接時，可以透過 VPC Peering 來連接，VPC Peering 是一種私有的連接，可以在兩個 VPC 之間傳輸流量，使用 Private IP 將兩個 VPC 連接起來。\n不同的 VPC 的 CIDR 不能重疊，否則無法建立 VPC Peering。\nVPC Peering 不限制同一個帳號，可以跟其他帳號的 VPC 連接。\n也可以跨 Region，這種稱為 Inter-Region VPC Peering\nPeering 屬於一對一的連接，如果要連接多個 VPC，則需要建立多個 Peering。例如：A 與 B Peering，B 與 C Peering，A 與 C 之間是無法直接溝通。\n可以設定整個 VPC 進行 Peering，也可以設定特定 Subnet 進行 Peering。\n需要額外設定 Route Table，才可以兩個 VPC 之間可以溝通。\n詳細可以看：What is VPC peering? 什麼是 VPC 對等互連？","vpc-元件#VPC 元件":"從上面可以簡單知道 VPC 架構，那我們來詳細看一下每個組成的元件是什麼，先看細圖：\nVPC Data Flow Virtual Private Cloud\n我們從最大的元件開始看：\nRegion \u0026 VPC 上方淺藍色外框就代表 Region，每個 Region 可以有多個 VPC，範例的 Region 是 eu-west-2。\n深藍色外框就代表 VPC，VPC 是在 Region 層級底下。\nInternet Gateway (IGW) \u0026 Virtual Private Gateway (VGW) 要存取 VPC 的方式有兩種，一種是透過 IGW 可以從 Internet 存取 VPC，另一種是透過 VGW 可以從地端網路存取 VPC。\n不管從 IGW 或 VGW 存取 VPC，都會先到 Route，Route 會根據 traffic 的來源跟目的地，決定要套用哪個 Route Table Rule。\n接著 Route Table 會根據 Rule 決定 traffic 要往哪個方向走，並導向到不同的 Network ACL 來進行流量的控制。\nInternet Gateway (IGW) (比較會用到，另外拉出來說)\nInternet Gateway (IGW) 有自動水平擴展，以及 HA (High Availability) 的特性，會由 AWS 負責管理，我們不需要特別設定。\n沒有對外的頻寬限制。\n每個 default VPC 都會有一個 Internet Gateway (IGW)。\nRoute Table Route Table 是用來指引網路流量要怎麼走，以及去哪裡。Route Table 會有目的地 IP 以及下一站要去哪。\n預設情況下，VPC 內部的 traffic 可以在不同的 subnet 之間自由流動，依靠的就是 local route。\nlocal route 是預設存在，且無法修改的。\nVPC 中可以設定多組的 Route Table，但每個 Subnet 只能指定一組 Route Table，且已經與 Subnet 關聯的 Route Table 不能刪除。\nNetwork ACL (NACL) Network ACL 是網路進到 VPC 的第一道防禦，可以把它想成是一個子網級別的防火牆，用於控制進入和出去子網的流量。\nNetwork ACL 是 stateless，這意味著進入和出去的流量都必須明確允許。如果允許進入流量 (inbound traffic)，出去的流量 (outbound traffic) 必須單獨設定規則允許。\nSecurity Group (SG) 是網路進入的第二道防禦，是虛擬防火牆，用於控制 EC2 實例的進入和出去流量。\n但是比較不一樣的是 Security Group 是 stateful，這意味著如果允許進入流量 (inbound traffic)，相應的出去回應流量 (outbound response traffic) 也自動被允許，反之亦然。\n就算移除 Allow Outbound Traffic 規則，一樣還是會通\nSubnet (Public \u0026 Private) Subnet 是 VPC 的子網路，可以在不同的 Availability Zone (AZ) 建立 Subnet。\nSubnet 又分成 Public Subnet 跟 Private Subnet，Public Subnet 可以連到 Internet，Private Subnet 則不能。\n但如果 Private Subnet 需要連到 Internet (例如 EC2 更新套件版本等)，則需要透過 NAT Gateway 來達成 (只有單向，沒辦法讓外部網路連進來)。\nSubnet 跟 Subnet 之間要溝通，則需要透過 Route Table 來指引路線。","vpc-懶人包#VPC 懶人包":"如果對於上面比較詳細的說明有點複雜，這邊我總結一下 VPC 的重點：\n以網路的角度出發 下面都先用 EC2 來當作 Subnet 底下的後端服務：\nPrivate Subnet 沒辦法對外連網，外面也連不進來。 同一個 Subnet 裡面的 EC2 可以互相溝通，不需要透過其他的元件。 不同的 Subnet 要溝通時，則需要透過 Route Table 元件。 Route Table 功用就是去指引網路流量要怎麼走，以及去哪裡。會有目的地 IP 以及下一站要去哪。會經過 Local 中繼站。 Public Subnet 讓裡面的 EC2 可以連到 Internet，一樣需要 Route Table 去指引路線，目的地是 Internet，下一站則需要到 Internet Gateway (IGW)。 Internet Gateway (IGW) 會放在 VPC 層級上面，是 Public Subnet 對外連線的重要元件。 如果要讓 Private Subnet 能夠連 Internet，Route Table 下一站則需要到 NAT Gateway。 NAT Gateway 是設定在 Public Subnet 上面，透過它才能讓 Private Subnet 連到 Internet。整個流程如下：Private Subnet \u003e Route Table \u003e NAT Gateway \u003e Internet Gateway (IGW) \u003e Internet，這個流程是單向的，外面的網路還是連不進來 Private Subnet。 以安全性的角度出發 外部連線想要進來 Subnet 時，會需要先經過 Network Access Control List (NACL)。 Network Access Control List (NACL) 是隸屬於 Subnet 層級，NACL 會規範，什麼請求可以進來，什麼請求可以出去 Subnet。 進入到 Subnet 後，要在到 EC2 時，會在經過一層保護名為 Security groups (SG)。 Security groups (SG) 是給 EC2 instance 用的，類似防火牆規範。 Network Access Control List (NACL) 跟 Security groups (SG) 差異\n差異 NACL SG 說明 是一個子網級別的防火牆，用於控制進入和出去子網的流量。 是虛擬防火牆，用於控制 EC2 實例的進入和出去流量。 狀態 stateless，這意味著進入和出去的流量都必須明確允許。如果允許進入流量，出去的流量必須單獨設定規則允許。 stateful，這意味著如果允許進入流量（inbound traffic），相應的出去回應流量（outbound response traffic）也自動被允許，反之亦然。 最後如果懶得看文字，建議大家可以去看這部影片：AWS 專家教你：打造高效 VPC 網絡（包含 Subnet、IGW、NAT 等） ，6 分鐘的影片，可以快速了解 VPC 的重點。","什麼是-vpc#什麼是 VPC？":"VPC 的全名是 Virtual Private Cloud，可以在 AWS 雲端內配置邏輯上隔離的虛擬網路。透過建立自己的 VPC，可以完全控制網路環境，包括定義 IP 位址範圍、子網路、路由表和連接選項的能力。\n每個 AWS 帳號包含每個 AWS 區域的預設 VPC。預設 VPC 有先設置好一些設定，方便我們可以快速啟動資源的便捷選項。但是預設 VPC 沒辦法符合長期的網路需求，這時候就需要自己建立 VPC。\n建立額外的 VPC 還有很多優勢，例如：可以按照部門或業務隔離工作負載，可以設定更多的安全性控制 (Network ACL \u0026 Security Group)，可以設定更多詳細的網路設定。\n還可以幫 VPC 建立 VPN 連線，將地端網路與 AWS VPC 連接起來，變成一個 Hybrid Cloud 架構。\nVPC 架構 什麼是 Amazon VPC\n從圖片來看，可以知道：\nVPC 會在 Region 層級底下，每個 Region 可以有多個 VPC。 VPC 是跨 Availability Zone (AZ) 的，可以在不同的 AZ 建立 Subnet。 VPC 會透過 Internet Gateway (IGW) 連接到 Internet。 ","參考資料#參考資料":"什麼是 Amazon VPC：https://docs.aws.amazon.com/zh_tw/vpc/latest/userguide/what-is-amazon-vpc.html\nAWS CSA Associate 學習筆記 - VPC(Virtual Private Cloud) Part 1：https://godleon.github.io/blog/AWS/AWS-CSA-associate-VPC-part1/\nVirtual Private Cloud：https://salmankhalid85.wordpress.com/aws/virtual-private-cloud/\n【技術分享】Internet Gateway 和 NAT Gateway 的區別在哪? 功能分別是什麼?：https://medium.com/@awseducate.cloudambassador/%E6%8A%80%E8%A1%93%E5%88%86%E4%BA%AB-internet-gateway-%E5%92%8C-nat-gateway-%E7%9A%84%E5%8D%80%E5%88%A5%E5%9C%A8%E5%93%AA-%E5%8A%9F%E8%83%BD%E5%88%86%E5%88%A5%E6%98%AF%E4%BB%80%E9%BA%BC-b676f62b1d31","武功秘笈#武功秘笈":"文章主要參考：AWS CSA Associate 學習筆記 - VPC(Virtual Private Cloud) Part 1，這篇文章寫得很詳細，當然，我會加上一些自己的理解跟測試範例，如果有興趣的話，可以去看看原文。"},"title":"Amazon Virtual Private Cloud (VPC) 介紹"},"/blog/conference-or-lecture/":{"data":{"":"此分類包含參加研討會後的心得分享，以及擔任不同講師的講座內容，歡迎參考\n朝陽科技大學 - 安全前瞻：網站防護與 DevOps 技術講座 (2024/05/29) "},"title":"研討會 / 講座"},"/blog/conference-or-lecture/20240529-devops-introduce/":{"data":{"":"","docker-和-kubernetes-概念介紹#Docker 和 Kubernetes 概念介紹":"\nDocker 介紹 Docker 是一種軟體平台，它可以快速建立、測試和部署應用程式。為什麼可以快速建立呢？因為 Docker 會將軟體封裝到名為『容器』的標準單位。其中會包含程式庫、系統工具、程式碼、執行軟體所需的所有項目。 剛剛有提到容器 (Container)，是一種虛擬化技術，它高效率虛擬化及易於遷移和擴展的特性，非常適合現代雲端的開發及佈署。\n那 Container 與傳統的虛擬機 (VM) 有什麼差別呢？我們來看看下面這張圖\nContainer 與 VM 的差異 圖片來源\n可以看到 Container 是以應用程式為單位，而 VM 則是以作業系統為單位。雖然本質來說兩者都是運行在有實體的硬體機器上，但 Container 是一個封裝了相依性資源與應用程式的執行環境。VM 則是一個需要配置好 CPU、RAM 與 Storage 的作業環境，為了更好的做區別，我把 Container、VM 兩個差別用表格來說明：\n比較 容器 (Container) 虛擬機(VM) 單位 應用程式 作業系統 適用服務 多使用於微服務 使用較大型的服務 硬體資源 是以程式為單位，需要的硬體資源很少 VM 會先佔用 CPU、RAM 等等硬體資源，不管有沒有使用都會先佔用 造成衝突 Container 間是彼此隔離的，因此在同一台機器可以執行不同版本的服務 會因為版本不同造成環境衝突 系統支援數量 單機支援上千個容器 一般最多幾十個 優點 Image 較小，通常都幾 MB\n啟動速度快，通常幾秒就可以生成一個 Container\n更新較為容易，只需要利用新的 Image 重新啟動就會更新完了 因為硬體層以上都虛擬化，因此安全性相對較高\n系統選擇較多，在 VM 可以選擇不同的 OS\n不需要降低應用程式內服務的耦合性，不需要將程式內的服務個別拆開來部署 缺點 安全性較 VM 差，因為環境與硬體都與本機共用\n在同一台機器中，每一個 Container 的 OS 都是相同的，無法一個為 Windows、一個為 Linux，還是依賴 Host OS\nContainer 通常會切成微服務的方式作部署，在各元件中的網路連結會比較複雜 Image 的大小通常 GB 以上，比 Container 大很多\n啟動速度通常要花幾分鐘，因此服務重啟速度較慢\n資源使用較多，因為不只程式本身，還要將一部分資源分給 VM 的作業系統 Docker 小總結 更快速的交付和部署：對於開發和維運人員來說，最希望就是一次建立或設定，可以再任意地方正常運行。開發者可以使用一個標準的映像檔來建立一套開發容器，開發完成之後，維運人員可以直接使用這個容器來部署程式。Docker 容器很輕很快！容器的啟動時間都是幾秒中的事情，大量地節約開發、測試、部署的時間。\n更有效率的虛擬化：Docker 容器的執行不需要額外的虛擬化支援，它是核心層級的虛擬化，因此可以實作更高的效能和效率。\n更輕鬆的遷移和擴展：Docker 容器幾乎可以在任意的平台上執行，包括實體機器、虛擬機、公有雲、私有雲、個人電腦、伺服器等。 這種兼容性可以讓使用者把一個服務從一個平台直接遷移到另外一個。\n更簡單的管理：使用 Docker，只需要小小的修改，就可以替代以往大量的更新工作。所有的修改都以增量的方式去更新，從而實作自動化並且有效率的管理。\nDocker 四大元素 Docker 是由這四個東西所組成，了解後就可以知道整個 Docker 的生命週期。\nDockerfile 映像檔 (Image) 容器 (Container) 倉庫 (Repository) Dockerfile 開發人員在使用 Docker 時發現，大多現成的 Docker 映像檔無法滿足他們的需求，因此需要一種能夠生成映像檔的工具。Dockerfile 是一種簡易的文件檔，裡面包含了建立新映像檔所需的指令。\nDockerfile 語法主要由 Command（命令）和 Argument （參數選擇）兩大元素組成。以下是一個簡易的 Dockerfile 示意圖：\n命令式語法＋選擇參數（Command + Argument）\nDockerfile 圖片來源\n映像檔 (Image) Docker 映像檔是一個唯獨的模板。\n例如：一個映像檔可以用一個 Ubuntu Linux 作業系統當作基底，裡面只安裝 Nginx 或使用者想用的套件等。\n我們會使用映像檔來建立 Docker 的容器 (Container) ，Docker 也提供很簡單的機制來建立映像檔或是更新現有的映像檔，也可以去下載別人已經做好的映像檔。\nImage 圖片來源\n容器 (Container) Docker 是利用容器來執行應用程式。\n每一個容器都是由映像檔所建立的的執行程式。它可以被啟動、開始、停止、刪除。且每一個容器都是相互隔離的，不會相互影響。\nContainer 圖片來源\n倉庫 (Repository) 倉庫是集中放置映像檔的所在地，倉庫分為公開倉庫 (Public) 和 私有倉庫 (Private) 兩種形式。最大的倉庫註冊伺服器當然是 Docker hub，存放數量龐大的映像檔供使用者下載，使用者也可以在本地網路內建立一個私有倉庫。可以將倉庫的概念理解成跟 Git 相似。\nRepository 圖片來源\n這樣講還是有點抽象，我們來看一個實際的例子：\n首先我們先看一下 Docker 的 Logo，你們覺得這是什麼動物呢？\nDocekr Logo 圖片來源\n沒錯就是鯨魚，但你們有沒有注意圖片有三個大元素呢？\n海洋 鯨魚 貨櫃 海洋代表的是 Docker 的執行環境，不論是在哪個海洋 (計算環境)，都可以執行，鯨魚代表的是 Docker 這個平台 (或可以指 Image)，而貨櫃代表的是 Docker 的容器，也就是說我們可以有很多個 Container 在同一個 Image 上執行，且互不影響，能夠快速的部署和遷移，並在不同的環境中執行。\n由於時間關係，其他 Docker 詳細介紹以及資料可以參考我之前寫的文章：Docker 介紹 (如何使用 Docker-compose 建置 PHP+MySQl+Nginx 環境)\nDocker 小試身手 為了大家能夠更了解 Docker 的使用，我們來實際操作一下：\n首先請大家先到 GitHub 下載範例程式碼：https://github.com/880831ian/20240529-devops-introduce 接著先執行 docker build -t 0529:latest . 這個指令，這個指令是用來建立一個 Image，這個 Image 是用來執行 Nginx 的程式碼。 接著執行 docker run -d -p 8080:80 0529:latest 這個指令，這個指令是用來執行一個 Container，並且將 Container 的 80 port 對應到本機的 8080 port。 打開瀏覽器，輸入 http://localhost:8080，就可以看到 Nginx 的首頁了。 我們先將關閉 Container，自行修改 index.html 的內容，然後再重新執行 docker build -t 0529:v0.0.1 . 和 docker run -d -p 8080:80 0529:v0.0.1，就可以看到修改後的內容了。 我們下 docker tag 0529:v0.0.1 880831ian/0529:v0.0.1 這個指令，將 Image 改成 Docker Hub 帳號跟 Image 名稱，例如：880831ian/0529:v0.0.1。 最後我們下 docker push 880831ian/0529:v0.0.1 這個指令，這樣就可以將 Image 上傳到 Docker Hub 上，這樣其他人就可以使用你的 Image 來部署服務了。 Docker 運作流程 步驟一、撰寫 Dockerfile\n步驟二、將 Dockerfile 建立為 Image\n步驟三、將 Image 運行為容器。透過這三個簡單的步驟，就能創建自己的 Docker 容器囉！\nDocekr 運作流程 圖片來源\nKubernetes 介紹 Kubernetes 也可以叫 K8s，這個名稱來源希臘語，意思是舵手或是飛行員，所以我們可以看到它的 logo 是一個船舵的標誌，之所以叫 K8s 是因為 Kubernetes 的 k 到 s 中間有 8 的英文字母，為了方便，大家常以這個名稱來稱呼它！\nK8s Logo\nK8s 是一種開源的容器資源調度平台，能夠將部署流程自動化、擴展並管理不同容器間的工作負載。它的構想理念是「Automated container deployment, scaling, and management」，意即透過自動化功能提升應用程式的可靠性和減輕維運負擔，讓開發人員專注於軟體開發任務。\n另外 K8s 也是 Google 開發的，並且是 CNCF (Cloud Native Computing Foundation) 的一個專案，所以 K8s 也是一個非常受歡迎的容器管理系統。\n我們之前在 Docker 介紹 文章中，已經有介紹以往傳統虛擬機以及容器化的 Docker 差異以及優點，那當我們在管理容器時，其中一個容器出現故障，則需要啟動另一個容器，如果要用手動，會十分麻煩，所以這時就是 Kubernetes 的厲害的地方了，Kubernetes 提供：\nKubernetes 優點 輕量級：K8s 的輕量化特性讓應用程式能被輕易地部署至不同環境，例如：地端資料中心、公有雲或其他雲端混合環境。K8s 容器化的本質讓封裝在內的應用程式與其相依的資源能夠緊密結合，從而解決不同平台的兼容問題，並降低在不同基礎架構上部署的難度。 服務、系統部屬更方便：由於容器可在任何容器平台運行，因此無論是同時將多個 Container 部屬到一台機器，或是多個 Container 部屬至多台機器都不是問題。 自動化管理，重啟、擴張皆可行：且 K8s 可自動偵測、管理各 Container 的狀態，若有需要，可對 Container 執行自動擴展。而若偵測到有 Container 發生故障，也可自動重啟以確保服務正確且持續地運行。 彈性化運用：K8s 中每個服務、系統皆可獨立部屬，因此不會因為其中一個系統出現錯誤而影響整個運作，甚至各 Container 也可依各自需求來修改，運用上擁有高度彈性化。 Kubernetes 是如何幫我們管理以及部署 Container ? 要了解 Kubernetes 如何運作，就要先了解它的元件以及架構：\nKubernetes 元件介紹 那我們由小的往大的來做介紹：依序是 Pod、Worker Node、Master Node、Cluster\nPod Kubernetes 運作中最小的單位，一個 Pod 會對應到一個應用服務 (Application)，舉例來說一個 Pod 可能會對應到一個 Nginx Server。\n每個 Pod 都有一個定義文件，也就是屬於這個 Pod 的 yaml 檔。 一個 Pod 裡面可以有一個或多個 Container，但一般情況一個 Pod 最好只有一個 Container。 同一個 Pod 中的 Containers 共享相同的資源以及網路，彼此透過 local port number 溝通。 Worker Node Kubernetes 運作的最小硬體單位，一個 Worker Node (簡稱 Node) 對應到一台機器，可以是實體例如你的筆電、或是虛擬機，例如：GCP 上的一台 Computer Engine。\nMaster Node (Control Plane) 負責各個 Worker Node 的管理，可稱作是 K8S 的發號施令的中樞。\n其他更詳細介紹，可以參考我之前寫的文章：Kubernetes (K8s) 介紹 - 基本\nCluster Cluster 也叫叢集，可以管理眾多機器的存在，在一般的系統架設中我們不會只有一台機器而已，通常都是多個機器一起運行，在沒有 Kubernetes 的時候就必須要土法煉鋼的一台一台機器去更新，但有了 Kubernetes 我們就可以透過 Cluster 進行控管，只要更新 Master 旗下的機器，也會一併將更新的內容放上去，十分方便。\nK8s 元件 圖片來源\nKubernetes 小試身手 為了大家能夠更了解 Kubernetes 的使用，我們來實際操作一下： (但由於 K8s 建立以及部署需要一些時間，所以這邊會直接拿我擁有的環境做測試，其他更詳細的操作可以參考我之前寫的文章：Kubernetes (K8s) 介紹 - 基本、Kubernetes (K8s) 介紹 - 進階 (Service、Ingress、StatefulSet、Deployment、ReplicaSet、ConfigMap))\n首先，我們接續上面的 Docker 小試身手 的範例程式碼：\n先執行 docker buildx build --platform=linux/amd64 -t 880831ian/0529-arm64:latest .，將 Image 建立起來 (這邊會多 buildx 跟 platform 是因為我的電腦是 Mac M 系列處理器，系統架構是 arm64，所以要放到 GKE 上面跑，需要多指令平台)。\n接著執行 docker push 880831ian/0529-arm64:latest，將 Image 上傳到 Docker Hub 上。\n進入 k8s 資料夾，裡面有幾個檔案，分別是：namespace.yaml、deployment.yaml、service.yaml、ingress.yaml。\n先執行 kubectl apply -f namespace.yaml，這個指令是用來建立一個 namespace，這樣我們就可以將我們的服務放到這個 namespace 下。\n接著執行 kubectl apply -f .，將其他服務也建立到 GKE 上。\n最後我們打開瀏覽器，輸入 https://myapp.pin-yi.me，就可以看到我們寫的首頁了。 (此為範例，講座結束後會關閉服務)\n看完 Kubernetes 部署服務的方式，是不是覺得有點麻煩呢？還需要手動去部署，這時候就需要 CI/CD 來幫助我們自動化部署服務了！","什麼是-cicd#什麼是 CI/CD":"一樣先來了解一下為什麼要 CI/CD？\n目前大多數的企業公司都是採用敏捷開發的方式，所以在追求快速開發、快速部署，以及大量的測試時會消耗很多開發人員的時間和精力，所以這時候就需要 CI/CD 來幫助我們自動化部署服務。\n接著再介紹 CI/CD 之前還有一個名詞要跟大家介紹，那就是 DevOps。\n什麼是 DevOps DevOps 是一種結合軟體開發人員 (Development) 和 IT 運維技術人員 (Operations) 的文化，目的是縮短開發和運營之間的距離，並且提高開發和運營的效率。而 CI/CD 工具就是為了此概念產生的自動化工具，透過持續整合 (CI) 和持續部署 (CD) 的方式，在開發階段能自動協助開發人員偵測程式碼問題，並部署到 Server 上。\nCI/CD cycle\nCI (Continuous Integration) 持續整合 持續整合，顧名思義，就是當開發人員完成一個階段性的程式碼後就經由自動化工具測試、驗證，協助偵測程式碼問題，並建置出即將部署的版本（Build）。\nCD (Continuous Deployment) 持續部署 持續部署可以說是 CI 的下一階段，經過 CI 測試後所構建的程式碼可以透過 CD 工具部署至伺服器，減少人工部署的時間。\nCI/CD 常用的工具 GitHub GitHub 算是目前最受歡迎的程式碼管理平台，其 CI/CD 服務稱為 GitHub Action，提供了多項控制 API，能夠幫助開發者編排、掌握工作流程，在提交程式碼後自動編譯、測試並部署至伺服器，也提供了很多現成的 Action 可以使用。\nGitLab GitLab 算是公司企業內部比較主流的程式碼管理平台，其 CI/CD 服務稱為 GitLab CI/CD，其 CI/CD Pipeline 功能簡單又實用，使用者只需要設定於專案根目錄下的「.gitlab-ci.yml」檔，便可以開始驅動各種 Pipeline 協助您完成自動化測試及部署\nCI/CD 小試身手 我們今天會使用 GitGub Action 來進行 CI/CD 範例\n這邊也跟上面 Kubernetes 小試身手 一樣，由於 K8s 建立以及部署需要一些時間，所以這邊會直接拿我擁有的環境做測試。\n首先，我們接續上面的 Docker 小試身手 的範例程式碼：\n打開 .github/workflows/google.yml 這個檔案，裡面是我已經寫好的 GitHub Action 的流程。 這邊說明一下這個檔案的內容 (下面拆開說明)：\nname: Build and Deploy to GKE on: push: branches: [\"main\"] env: PROJECT_ID: pin-yi-project GAR_LOCATION: asia-east1 GKE_CLUSTER: cluster GKE_ZONE: asia-east1-b DEPLOYMENT_NAME: myapp REPOSITORY: pin-yi-image IMAGE: myapp NAMESPACE: myapp jobs: setup-build-publish-deploy: name: Setup, Build, Publish, and Deploy runs-on: ubuntu-latest environment: production 這邊主要是說明這個 Action 的名稱，以及當 push 到 main branch 時，就會觸發這個 Action，還有對應的 env，env 包含了 GCP 專案 ID、Image 儲存的地點、GKE 的 Cluster 名稱、GKE 的 Zone、部署的名稱、Image 的名稱、Namespace 的名稱。\n接著是 job 的設定，設定名稱、要用什麼 image 當做基底，以及環境等等。job 可以把它理解成每個要做的小事情，例如 Build Image、Push Image、Deploy Image 等等。\nsteps: - name: Checkout uses: actions/checkout@v3 - id: \"auth\" uses: \"google-github-actions/auth@v2\" with: credentials_json: \"${{ secrets.GCP_CREDENTIALS }}\" - name: Docker configuration run: |- echo '${{ secrets.GCP_CREDENTIALS }}' | docker login -u _json_key --password-stdin https://$GAR_LOCATION-docker.pkg.dev - name: Set up GKE credentials uses: google-github-actions/get-gke-credentials@v2 with: cluster_name: ${{ env.GKE_CLUSTER }} location: ${{ env.GKE_ZONE }} - name: Build run: |- docker build \\ --tag \"$GAR_LOCATION-docker.pkg.dev/$PROJECT_ID/$REPOSITORY/$IMAGE:$(echo $GITHUB_SHA | head -c7)\" \\ --build-arg GITHUB_SHA=\"$GITHUB_SHA\" \\ --build-arg GITHUB_REF=\"$GITHUB_REF\" . - name: Add Image Tag run: docker tag $GAR_LOCATION-docker.pkg.dev/$PROJECT_ID/$REPOSITORY/$IMAGE:$(echo $GITHUB_SHA | head -c7) $GAR_LOCATION-docker.pkg.dev/$PROJECT_ID/$REPOSITORY/$IMAGE:latest - name: Publish run: |- docker push \"$GAR_LOCATION-docker.pkg.dev/$PROJECT_ID/$REPOSITORY/$IMAGE:$(echo $GITHUB_SHA | head -c7)\" \u0026\u0026 \\ docker push \"$GAR_LOCATION-docker.pkg.dev/$PROJECT_ID/$REPOSITORY/$IMAGE:latest\" - name: Set Image run: |- kubectl set image deployment/$DEPLOYMENT_NAME \\ myapp=\"$GAR_LOCATION-docker.pkg.dev/$PROJECT_ID/$REPOSITORY/$IMAGE:$(echo $GITHUB_SHA | head -c7)\" -n $NAMESPACE - name: Deploy run: |- kubectl rollout status deployment/$DEPLOYMENT_NAME -n $NAMESPACE kubectl get services -o wide 接著就是各種的 job，有先 checkout 程式碼、登入 GCP、登入 Docker、設定 GKE 的 credentials、Build Image、Push Image、Deploy Image 等等。\n接著我們嘗試調整 index.html 的內容，然後 push 到 GitHub 上，就可以看到 GitHub Action 會自動幫我們 Build Image、Push Image、Deploy Image 到 GKE 上。\n最後我們打開瀏覽器，輸入 https://myapp.pin-yi.me，就可以看到我們調整的首頁了。(此為範例，講座結束後會關閉服務)","參考資料#參考資料":"地端是什麼？可以吃嗎？：https://wanchunghuang.com/what-is-on-premises/\n雲端 VS 地端，數位轉型怎麼選？：https://blog.hackmd.io/zh/blog/2022/02/22/cloud-vs-on-premises\n什麼是雲端？ | 雲端定義： https://www.cloudflare.com/zh-tw/learning/cloud/what-is-the-cloud/\n企業如何挑選合適的雲端服務？公有雲、私有雲差異比較總整理：https://aws.amazon.com/tw/events/taiwan/techblogs/public-cloud-private-cloud/\n給資料科學家的 Docker 指南：3 種活用 Docker 的方式（上）：https://leemeng.tw/3-ways-you-can-leverage-the-power-of-docker-in-data-science-part-1-learn-the-basic.html\nDocker 是什麼？Docker 基本觀念介紹與容器和虛擬機的比較：https://www.omniwaresoft.com.tw/product-news/docker-news/docker-introduction/\nCI/CD 是什麼？一篇認識 CI/CD 工具及優勢，將日常瑣事自動化：https://www.wingwill.com.tw/zh-tw/%E9%83%A8%E8%90%BD%E6%A0%BC/%E9%9B%B2%E5%9C%B0%E6%B7%B7%E5%90%88%E6%87%89%E7%94%A8/cicd%E5%B7%A5%E5%85%B7/\n[DevOps] CI/CD 介紹 - 基礎概念與導入準備：https://enzochang.com/cicd-introduction/\nKubernetes（K8s）是什麼？基礎介紹+3 大優點解析：https://www.metaage.com.tw/news/technology/293\n我之前寫過的所有文章：https://pin-yi.me/blog","如何最快恢復網站#如何最快恢復網站":"如何最快恢復網站，這是一個非常重要的議題，因為網站的停擺會造成業務的損失，也會讓使用者對網站的信任度下降，所以我們需要有一個很好的應急計畫，來應對這種狀況。\n網站停擺的原因 那在討論如何恢復前，我們要先了解會導致網站停擺的原因有哪些？\n伺服器供應商故障 這邊的伺服器供應商，如果以雲端的供應商來說，也就是 GCP、AWS、Azure 等等，這些供應商可能會因為硬體故障、軟體錯誤、網路問題等原因而停擺。 當然地端的機器也可能會因為主機的供應商導致網站中斷，例如：Windows 自動更新 xD。\n網站伺服器故障 網站的伺服器當然也可能會因為硬體故障、軟體錯誤、網路問題等原因而停擺。這也是最常見的原因之一。 這個的伺服器也可以是地端的伺服器，也可以是雲端的伺服器。\n我們以雲端的 K8s 來舉例，如果 K8s 配置錯誤，會是資源不足，或是 Pod 的數量不夠，都會導致網站停擺。\n網站程式碼錯誤 這邊的程式碼，大家可以把他理解成網站運行的 Code，例如：PHP、Python、JavaScript 等等，有可能程式碼寫了一個無窮迴圈，或是什麼特殊的 Bug，導致資源衝高，也會導致網站停擺。\n網站被駭客攻擊 當然網站會依照使用需求，開放給不同的使用者，如果是一般開在公網上的網站，就有可能會被駭客攻擊，例如：DDos 攻擊、SQL Injection 等等，這些攻擊都有可能導致網站停擺，還有可能導致資料外洩。\n網站被判定成非法網站 這邊的非法網站，是指網站內容違反了法律規定，例如：色情、賭博、毒品等等，這些內容都有可能導致網站被封鎖，或是被檢舉，進而導致網站停擺。\n網站解析被擋 圖片來源\n網站恢復(避免)的方法 伺服器供應商故障 雲端供應商故障的發生機率在這幾個裡面來說，算是最少的，因為雲端供應商有很多的備援機制，當然為了避免意外，在有能力之餘，也可以考慮多個雲端供應商來當作備援，這樣就可以避免單一供應商故障。\n我們以雲端的 GCP 來舉例，如果 GCP 的服務有問題，可以到 Google Cloud Status Dashboard 來查看目前服務的狀況。\n分享一下最近發生的案例：https://status.cloud.google.com/incidents/xVSEV3kVaJBmS7SZbnre\n另外，GCP 也有提供 SLA，當服務有問題並超出一開始 Goolge 所承諾的服務水準時，可以向 GCP 提出申請，來獲得賠償。\nGoogle Cloud Platform Service Level Agreements\n網站伺服器故障 會出現錯誤的原因有很多，例如：硬體故障、軟體錯誤、網路問題等等，這邊我們可以透過對服務系統進行不同的監控來提早發現問題，並且提早解決。當然也建議多做備份，當網站出現問題時，可以快速的恢復。平常也要演練災難復原計畫。\n網站程式碼錯誤 程式碼的錯誤，要再上線前多做測試，例如：單元測試、整合測試、壓力測試等等，也要先做好程式碼的規範，例如：程式碼的風格、程式碼的註解等等，避免出現一些不必要的問題。\n網站被駭客攻擊 網路安全是很重要的，所以從入口的防火牆設計、黑白名單，再者到程式碼的安全性，例如：SQL Injection、XSS、CSRF 等等，都要做好防護，並且要定期的去更新程式碼裡面使用的套件，才可以避免被駭客攻擊。\n另外在雲端的使用上有一點要特別注意，就是權限的設定劃分要明確，確保最小權限原則，避免帳號金鑰外流導致駭客取得權限。\n網站被判定成非法網站 這邊當然不是討論去架設相關的網站，導致封鎖要怎麼辦 (◕ܫ◕)，我們這邊討論的是，不一定真的是架設非法的網站，才會被封鎖，我們來看一下這個新聞：Google 被刑事局認定「涉及詐騙」網傻眼！台灣大曝真相 這邊建議使用的網址要是可辨識，不要使用一些奇怪的網址，才可以避免被誤判，以及如果使用雲端供應商提供的 IP 以前，也先可以先去掃描一下 IP 是否有被組織或是單位確認是非法 IP。如果真的被封鎖，可以向相關單位申訴，來恢復網站。","網站維運日常#網站維運日常":"這邊我就以目前我在公司的經驗來跟大家分享一下，網站維運日常都會做哪些事情：\n主要分為以下幾件事情：\n監控 我們需要做各種的監控，例如：網站的流量、CPU 使用率、記憶體使用率等等，這些都是我們需要監控的項目。除此以外也需要監控 RD 使用的程式語言有對應的指標，例如：PHP 的 FPM 的進程等等指標。\n監控的目的是，讓我們服務在出現問題前，就可以提早透過告警知道有可能潛在的問題，並且提早解決。避免被使用者發現問題，進而影響使用者的體驗。\n做各種的監控\n查 Log 我們每個服務都需要去紀錄 Log，不管是使用者的操作、系統、還是程式的錯誤，都需要去紀錄下來，這樣當網站出現問題時，我們可以透過 Log 來查找問題，並且解決問題。\n當然 Log 也不是一個簡單的議題，Log 數量很多，在不影響效能以及費用的情況下，要怎麼只存精簡且重要的 Log，也是一個需要思考的問題。\n查 Log\n協助 RD 調整設定以及排查問題 由於雲端使用 K8s 微服務的架構，所以會有很多 K8s 上的設定，例如：服務入口的 Nginx 倒轉、timeout 設定的問題、IP 是來源哪一個服務等等，有些時候我們還需要配合 RD 來進行排查程式碼，才能真正解決問題。\n所以要成為一個優秀的 SRE 也是需要具備一定的程式能力，才能夠更好的協助 RD 來解決問題。\n不要變成 Site Restart Engineer 工程師 (雖然有時候真的 restart 就好了 ( ͡° ͜ʖ ͡°))\nSite Restart Engineer XDDD\n開發能夠在維運上更方便的工具 除了上面說的已經可以讓我們身心俱疲的幾件日常，我們也會開發一些讓我們維護上可以更方便的一些工具，盡量都變成自動化的方式，去減少我們的工作量。\n例如：\n簡單的壓測網站 HTTP 狀態碼工具：chece_url_http_code 自動修復 elasticsearch 錯誤工具：auto_repair_es 計算哪時候可以下班的工具：can_get_off_work XD ","要走軟體工程師需要具備哪些技能#要走軟體工程師需要具備哪些技能？":"下面是我準備的小 bonus，是出自我自己的經驗，以及我偷偷觀察我合作的 RD 夥伴們，得出的小小心得，當然這只是我自己的看法，不一定是對的，希望能夠幫助大家。\n當然，想成為一個軟體工程師，你一定要會程式語言，例如 PHP、Python、JavaScript 等等，假如你對於未來的發展方向還不確定，或是不知道哪個程式語言是未來的趨勢，可以看看 stackoverflow 每年的調查報告，來了解目前最流行的程式語言是哪些： https://survey.stackoverflow.co/2023/#most-popular-technologies-language\n但除了這個基本的寫 Code 技能外，我還建議能夠提早學習到以下幾項技能，會對你未來的發展有很大的幫助：\nGit 版本控制 Git 可以說是現代軟體工程師必備的技能之一，可以幫助你管理程式碼的版本，當你的程式碼出現問題時，可以很快的回復到之前的版本，也是你跟其他工程師協作的最好工具。\n因為目前，你可能都是自己一個人寫 Code，所以不會碰到同時間有多個人在調整程式時，導致程式碼衝突的問題，當你沒有使用 Git 時，你會不知道這是誰改的，這個是什麼時候改的，這個改動是為了什麼，這樣會讓你的程式碼變得非常雜亂，也會讓你的程式碼變得非常難以維護。\n寫 Blog 培養寫 Blog 的習慣，可以幫助你整理自己的知識，也可以幫助你記錄自己的學習過程，像我當初會寫 Blog 還有一個原因，是因為我記憶不好，如果我不寫下來，就會常常忘記一樣的問題 (๑•́ ₃ •̀๑)。\n當然在找工作面試時，它也可以當作自己的作品集，讓面試官能夠了解你、並知道你的學習能力。\n溝通能力 在職場上，溝通能力是非常重要的一環，因為你不可能一個人在公司裡面工作，你一定會跟其他人合作，所以你要學會如何跟其他人溝通，如何表達自己的想法，如何聆聽別人的意見，這些都是非常重要的技能。\n每個行業都是，但我認為軟體工程師更需要這個技能，因為軟體工程師的工作是非常複雜的，你需要跟 PM、RD、QA、UI/UX、SRE 等等不同的部門合作，所以你要學會如何跟不同的人合作，這樣才能夠讓你的工作更順利。","講座資訊#講座資訊":" 時間：2024/05/29 19:00 - 21:00 地點：朝陽科技大學 \u0026 Google Meet 線上連結 Google Developer Student Clubs 相關連結：https://gdsc.community.dev/events/details/developer-student-clubs-chaoyang-university-of-technology-taichung-taiwan-presents-wang-zhan-fang-hu-yu-devopsji-shu-jiang-zuo-ft-zhuang-pin-yi-gong-cheng-shi/ 簡報：https://docs.google.com/presentation/d/10yH_zQikWeYz5pyv4st6ICMs6F2F9F_Vks1ngDaNROY/edit?usp=sharing 講座錄影：https://drive.google.com/file/d/1-TRJ3mAYehmPSr8Sg4p09dA7t4mor4CY/view?usp=drive_link ","雲端和地端的差別#雲端和地端的差別":"首先，想問一下大家，你們知道雲端和地端的差別嗎？\n地端 我們先來説説「地端」是什麼？地端的英文是 On-Premises，簡稱 On-Prem，代表所有安裝、運行都在個人電腦或公司自己內部的伺服器上所執行的運行方式。\n最好的例子就是你電腦裡面裝的 Windows 作業系統、Office 文書軟體、或是公司內部的 ERP 系統等等。這些軟體都是安裝在你的電腦或公司伺服器上，並且由你自己或公司的 IT 團隊負責管理，所以這也是目前最普遍的ㄧ種軟體運行方式。\n地端的優點是什麼？ 控制權：你可以完全控制你的伺服器和軟體，不用擔心第三方服務商的服務品質，想把機器擺在哪裡就放哪裡、用什麼型號都可以自己決定。 安全性：企業或是個人可以自行控制資料的存取權限，不用擔心第三方服務商的資料外洩問題。 成本：一次性購買伺服器和軟體，不需要每個月支付雲端服務費用。 地端的缺點是什麼？ 成本：一次性購買伺服器和軟體，需要花費大量的資金，且不確定是否符合需求使用。 維護：需要自行管理伺服器和軟體，包括硬體維護、軟體更新、資料備份等等。 擴充性：當使用者或是需求增加時，需要自行擴充伺服器和軟體，且需要花費大量的時間和金錢。 那相信大家已經大致了解地端的定義了，接下來我們來看看「雲端」是什麼？\n雲端 「雲端」的英文是 Cloud，不需要安裝在個人電腦或公司伺服器上，而是透過網際網路存取的伺服器，以及在這些伺服器上執行的軟體或是資料庫。常見的例子有 Google 雲端硬碟、Dropbox、或是 Google Cloud、AWS、Azure 等雲端服務商。\n如何使用雲端 圖片來源\n雲端得以實現，是因為一種稱為虛擬化的技術。虛擬化允許在一台實體的電腦上建立許多模擬「虛擬」電腦，其行為如同具有自己硬體的實體電腦，這類電腦的術語稱為虛擬機器。虛擬主機簡單介紹到這邊，後面的 Docker 介紹 會在提到。\n雲端會有很多資料中心，一個資料中心又有很多台的伺服器，一個伺服器可以執行許多虛擬「伺服器」，能夠為許多組織或是客戶提供不同的服務，即使個別伺服器故障，雲端供應商會在多個機器及多個區域備份服務。Google Data Center 影片連結\nGoogle Data Center 圖片來源\n雲端又可以分為以下幾種：\n公有雲（Public Cloud） 故名思義，公有雲由第三方雲端服務商提供，如 Google Cloud、AWS、Azure 等。這些服務商提供的平台允許用戶租用伺服器、資料庫、儲存空間等資源，用戶只需一組帳號和密碼即可訪問。公有雲的特點包括：\n彈性和可擴展性：用戶可以根據需求動態調整資源的規模。 成本效益：按需付費模式，企業可以根據實際使用情況支付費用。Google Cloud Pricing Calculator 全球覆蓋：可選擇部署的地區範圍廣泛。Google Cloud 據點 除了彈性和成本效益，最重要的還有服務商安全管理和維護及 SLA 的優勢。Google Cloud Platform Service Level Agreements\nTop 10 Cloud Service Providers Globally in 2024 圖片來源\n私有雲（Private Cloud） 私有雲是由企業自建或由第三方供應商為特定企業提供的雲端基礎設施，僅供企業內部使用。其特點包括：\n專屬使用：企業獨享全部資源，不與其他用戶共享。 高可控和安全性：企業可以自行管理和控制數據，確保數據安全性和合規性。 高成本：需要自行購買和維護硬體和基礎設施，初期投資和運營成本較高。 What Is Private Cloud? 圖片來源\n聽完公有雲和私有雲的介紹，有沒有可以結合兩種雲端的優點，來達到更好的效果呢？\n混合雲（Hybrid Cloud） 混合雲是一種結合了公有雲和私有雲優勢的雲端架構，允許企業在公有雲和私有雲之間靈活地配置和管理資源。這意味著企業可以將一些工作負載和應用程式部署在公有雲上，以利用其彈性和成本效益，同時將敏感資料和關鍵業務應用部署在私有雲或內部資料中心，以確保安全性和控制。\n所以以下幾個就是混合雲的優點：\n彈性和可擴展性：企業可以根據需求動態調整公有雲和私有雲的使用比例。例如，當工作負載增加時，可以暫時使用公有雲資源來擴展運算能力，而不必立即擴展內部基礎設施。\n成本效益：通過利用公有雲的按需付費模式，企業可以降低運營成本，避免在私有雲上過度投資。而私有雲則可以用來運行穩定且持久的工作負載。\n安全性和合規性：敏感數據和應用可以保存在私有雲或內部資料中心，以滿足企業的安全和合規要求。同時，非敏感的工作負載可以部署在公有雲上，以利用其便利性和成本優勢。\n高可用性和災難恢復：混合雲架構允許企業在公有雲和私有雲之間設置備份方案，增強系統的高可用性和災難恢復能力。\n所以混合雲也是目前最多企業採用的雲端架構，可以兼顧公有雲和私有雲的優勢，並根據實際需求靈活調整資源的使用。\n雲端還是地端好，要如何選擇？ 沒有一個明確的答案，要看你的需求和預算，以及公司的規模和發展方向。\n假設你是一個小型公司，可能沒有太多資金去購買伺服器和軟體，也沒有太多人力去管理、或是者你的公司業務是跨國的，需要在全球提供服務以及需求高可用性，或是長時間有大量的流量，例如：遊戲公司，那麼雲端就是一個不錯的選擇。\n如果公司資料敏感性高，需要自行控制資料存取權限，或是公司有自己的 IT 團隊，有能力管理伺服器和軟體，那麼地端就是一個不錯的選擇。\n我們了解了雲端和地端的差別，接下來了解一下有什麼技術可以幫助我們更好的部署我們的服務呢？"},"title":"朝陽科技大學 - 安全前瞻：網站防護與 DevOps 技術講座"},"/blog/docker/":{"data":{"":"此分類包含 Docker 相關的文章。\n使用 Prometheus 和 Grafana 打造監控預警系統 (Docker 篇) Docker 介紹 (如何使用 Docker-compose 建置 PHP+MySQl+Nginx 環境 "},"title":"Docker 相關"},"/blog/docker/docker/":{"data":{"":"","docker-基本概念#Docker 基本概念":" Dockerfile 映像檔 (Image) 容器 (Container) 倉庫 (Repository) 這四個是 Docker 最基本的組成，了解後就可以知道整個 Docker 的生命週期。\nDockerfile 開發人員在使用 Docker 時發現，大多現成的 Docker 映像檔無法滿足他們的需求，因此需要一種能夠生成映像檔的工具。Dockerfile 是一種簡易的文件檔，裡面包含了建立新映像檔所需的指令。\nDockerfile 語法主要由 Command（命令）和 Argument （參數選擇）兩大元素組成。以下是一個簡易的 Dockerfile 示意圖：\n命令式語法＋選擇參數（Command + Argument）\n映像檔 (Image) Docker 映像檔是一個唯獨的模板。\n例如：一個映像檔可在包含一個完整的 Linux 作業系統環境，裡面可以只安裝 Nginx 或使用者會使用到的其他應用程式。\n我們會使用映像檔來建立 Docker 的容器 (Container) ，Docker 也提供很簡單的機制來建立映像檔或是更新現有的映像檔，也可以去下載別人已經做好的映像檔。\n容器 (Container) Docker 是利用容器來執行應用程式。\n每一個容器都是由映像檔所建立的的執行程式。它可以被啟動、開始、停止、刪除。且每一個容器都是相互隔離的，不會相互影響。","docker-實作#Docker 實作":"本章節會介紹 Docker 三大組件映像檔 (image)、容器 (Container)、倉庫 (Repository) 要如何實際操作，以及他們的關聯性是什麼～ 映像檔 (Image) 本小節會介紹有關映像檔的內容，包括：\n如何從倉庫取得映像檔 如何管理本地主機上的映像檔 映像檔實作 (Dockerfile) 如何從倉庫取得映像檔 我們可以先到 Docker hub 上面看看有什麼服務或程式想要下載來做使用 [ 詳細介紹會放到倉庫 (Repository) 章節]，找到想要的服務，我們可以下 docker pull {要下載的服務、程式名稱} ，我們這邊就先下載 Mysql 這個映像檔。\nmysql $ docker pull mysql Using default tag: latest latest: Pulling from library/mysql 15115158dd02: Pull complete .... 省略 .... Digest: sha256:b17a66b49277a68066559416cf44a185cfee538d0e16b5624781019bc716c122 Status: Downloaded newer image for mysql:latest docker.io/library/mysql:latest 由於我們下載的沒有加任何的 tag ，也就是版本，所以我們都是下載最新版 latest ，如果想要下載特定版本，可以在服務名稱後面加上 :{版本} ，就可以下載對應的版本囉！\n如果有標記 Official Image 就代表是官方釋出的映像檔 ~ 在穩定性以及安全上更有保障，所以大家可以優先下載歐！\nDocker hub 下載 Image\n管理本地主機上的映像檔 查看映像檔 (images) 當我們下載好映像檔後，可以使用 docker images 來列出本機已下載的映像檔。\n$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE php latest d6229b88aa29 3 days ago 484MB mysql latest 826efd84393b 6 days ago 521MB nginx latest c919045c4c2b 13 days ago 142MB 我們來看看都列出哪些訊息吧！\nREPOSITORY：來自於哪一個倉庫，像是 php、mysql、nginx。 TAG：映像檔的標籤，因為我們都下載最新版本所以是 latest。 IMAGE ID：它的唯一 ID 號碼。 CREATED：建立時間。 VIRTUAL SIZE：映像檔大小。 儲存映像檔 (save) 想要儲存映像檔，可以使用 docker save {映像檔名稱} \u003e {檔案名稱}.tar，來做儲存。\n$ docker save mysql \u003e mysql.tar $ ls | grep 'mysql' mysql.tar 刪除映像檔 (rmi) 想要刪除映像檔，可以使用 docker rmi {映像檔名稱} 來做刪除。\n$ docker rmi demo-image Untagged:demo-image:latest Deleted: sha256:1f56acbcbe9ec613a37e26934a84d98bed73879059f424dc69754520086baa37 想要同時刪除映像檔時，可以先用 docker images -aq 列出全部映像檔的 IMAGE ID，再一起刪除。\ndocker rmi -f $(docker images -aq) 注意！刪除映像檔(Image)時，要先用 docker rm {容器} 去刪除所有依賴這個映像檔的容器。 接著要怎麼建立自己的映像檔呢？我們要使用 Dockerfile 來建立映像檔。\n映像檔實作 撰寫 Dockerfile 映像檔 Dockerfile 是一種文字格式的設定檔，可以透過 Dockerfile 快速建立自訂的映像檔，換句話說，Dockerfile 就像是建置 Docker Image 的腳本。\n舉個例子：可以把自己想像成一位設計師，設計好房子的格局、擺飾等，畫好設計圖 (Dockerfile) 後，最後請師傅 (Docker) 依你的構思完成就可以了。\n我們就來開始實作一個 Dockerfile 吧 ~\n我們先創建一個來放 Dockerfile 的資料夾，可以直接在路徑下建立出映像檔 mkdir demo-dockerfile cd demo-dockerfile/ Dockerfile 結構，大致可以分為四個部分\n基礎映像檔資訊 維護者資訊 映像檔操作指令 容器啟動時需執行的指令 我們有說過 Dockerfile 是一個文字格式的設定檔，所以我們用 vim 來編寫 Dockerfile。 vim Dockerfile # 基礎映像檔資訊 FROM nginx:latest # 維護者資訊 LABEL maintainer=\"880831ian@gmail.com\" # 映像檔操作指令 RUN apt-get update -y\\ \u0026\u0026 apt-get install nginx -y # 運行時容器提供服務的通道 EXPOSE 80 # 容器啟動時需執行的指令 CMD [\"nginx\",\"-g\",\"daemon off;\"] FROM nginx:latest\n第一行為必要的指定基礎映像檔，這邊使用 nginx 作為基礎映像檔，我們用最新版本，所以是 latest。\nLABEL maintainer=\"880831ian@gmail.com\"\n維護者資訊想不也是不可以少的，這邊也可以輸入 Email 資訊，只是要注意的是此資訊會寫入產出映像檔的 Author 名稱屬性中。\nRUN apt-get update -y\\ \u0026\u0026 apt-get install nginx -y\n這邊是最重要的部分，想要在映像檔案上設定或安裝都需要將命令寫在這，格式必須依 RUN ，RUN 指令後面放 Linux 指令，如果指令太長可以使用\\來換行。 -y 是在安裝 Nginx，會同意所有進行中所出現的問題。\nEXPOSE 80\n設定運行時容器提供服務的通道。\nCMD [\"nginx\",\"-g\",\"daemon off;\"]\n最後就是啟動指定容器時預設執行的指令，格式是 CMD [“executable”,“param1”,“param2”]。\nDocker 運行 Nginx 時為什麼要使用 daemon off;\n因為 Docker 容器啟動時，默認會把容器內部第一個進程，作爲 docker 容器是否正常運行的依據，如果 docker 容器 pid = 1 到進程就掛了，docker 就會退出！\nDocker 未執行自定義的 CMD 之前， Nginx 的 pid 是 1，執行到 CMD 之後，Nginx 就在後台運行，bash 或是 sh 的腳本就會變成 pid =1 。\n所以一但執行完 CMD，Nginx 容器就會退出了，所以才需要加上 -g daemon off;。\n在 Nginx 官方的 Docker Repository 也有說明，在 Complex configuration 內。\n順便說一下使用 Dockerfile 的優點：1 . 可以進行 Git 版控，讓你管理或分享更方便 2 . 佔用容量小，因為只是純文字檔而已。\n使用 Dockerfile 建立映像檔 我們已經撰寫完 Dockerfile 檔案了，接下來要執行來產生映像檔，我們要使用 docker build 來建立，我們一起來看看吧\ndocker build -t demo-image . 因為我們在 dockerfile 的目錄下，所以直接使用 “.” 來做建立動作，也可以使用 -f 來指定 dockerfile 的路徑位置。使用 -t 來設定映像檔的名稱，我們這邊取名叫 demo-image。\n[+] Building 2.7s (7/7) FINISHED =\u003e [internal] load build definition from Dockerfile 0.0s =\u003e =\u003e transferring dockerfile: 44B 0.0s =\u003e [internal] load .dockerignore 0.0s =\u003e =\u003e transferring context: 2B 0.0s =\u003e [internal] load metadata for docker.io/library/nginx:latest 2.6s =\u003e [auth] library/ubuntu:pull token for registry-1.docker.io 0.0s =\u003e [1/2] FROM docker.io/library/nginx:latest@sha256:8ae9bafbb64f63a50caab98fd3a5e37b3eb837a3e0780b78e5218e63193961f 0.0s =\u003e CACHED [2/2] RUN apt-get update -y \u0026\u0026 apt-get install nginx -y 0.0s =\u003e exporting to image 0.0s =\u003e =\u003e exporting layers 0.0s =\u003e =\u003e writing image sha256:1f56acbcbe9ec613a37e26934a84d98bed73879059f424dc69754520086baa37 0.0s =\u003e =\u003e naming to docker.io/demo-image 完成後我們使用 docker images 來看看是否建立成功。\n$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE demo-image latest 1f56acbcbe9e 2 hours ago 166MB php latest d6229b88aa29 4 days ago 484MB mysql latest 826efd84393b 6 days ago 521MB nginx latest c919045c4c2b 13 days ago 142MB 執行容器的部分，我們放到容器 (Container)的章節在介紹 ~\n容器 (Container) 我們在介紹指令前，先來了解一下 Dockerfile、Docker Image、Docker Container 這三個的關係，可以先參考以下圖片\n容器 Container 組成\n我們在啟動 Container 時，會有這三個部分組成，最底層是映像檔 (Image)，這一層主要是透過撰寫 Dockerfile 之後 build 出來的 Docker Image，就像我們前面說的它是一個唯獨的檔案。執行啟動了 Docker Container，就會加上第二層，就是需要先 Init Container 的設定，例如是 hostname、環境變數、網路連接等系統設定，最後最上層再加上一層讓使用者可以在此層去讀寫資料。\n有關於容器 (Container) 的指令非常多，光是簡單的 run 就有很多參數，我們先列出比較常用且基本的 Container 指令～\nContainer 執行時的操作 執行容器 (run) 我們想要創建一個新的容器並運行，就可以使用 docker run，我們來看看他可以使用哪些參數吧！\ndocker run [OPTIONS] IMAGE [COMMAND] [ARG...] OPTIONS 說明： 參數 描述 -d 後台運行容器 -i 命令互動模式，通常與 -t 同時使用 -t 為容器重新分配一個假裝的輸入終端，通常與 -i 同時使用 -p 指定容器與本機的 Port ，格式是 主機 Port : 容器 Port –name=\"{名稱}\" 為容器設定名稱 –net=\"{網路類型}\" 指定容器的網路連接類型，支援 bridge/host/none/container 四種模式 –link=\"{其他容器}\" 添加連接到另一個容器 –volume,-v 將容器檔案路徑映射到本地端，格式是 本機路徑：容器路徑 我們啟動我們下載好的 Nginx 來試試看吧！\n$ docker run -d -p 7777:80 --name=\"demo-nginx\" -v /Users/ian_zhuang/Desktop/data:/var/www/html nginx 31a4a4a56e3ef2fb75d538c4c9eea4914ac506a84a6ff97e1fbbb6c3213cc6b7 我們將 Nginx 容器在背景執行，且將預設的 80 Port 與本機的 7777 Port 綁在一起，讓我們在本機瀏覽 7777 Port 會直接導向容器的 80Port，設定容器的名字叫做 “demo-nginx\" ，我們 Nginx 容器的檔案路徑映射到本地端的桌面 data 資料夾，我們就可以在本機新增檔案同步到容器中。\n顯示容器 (ps) 使用 docker ps 來檢查一下是否啟動成功 (ps 可以顯示映像檔的基本資訊，如果沒有加 -a 只會顯示執行中的容器)\n$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 31a4a4a56e3e demo-image \"nginx -g 'daemon of…\" About a minute ago Up About a minute 0.0.0.0:7777-\u003e80/tcp demo-nginx 接著我們測試是否有把桌面 data 資料夾掛到容器的路徑，我們先在 data 新增一個 hello.html ，裡面隨意輸入，瀏覽一下 http://127.0.0.1:7777/hello.html ，看看是否成功。\n容器 Container -volume 測試\n顯示容器紀錄 (logs) 想要看到我們執行 Container 的紀錄，可以使用 logs 指令來顯示。\n$ docker logs demo-nginx /docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration /docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/ /docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh ... 省略 ... 2022/03/15 09:27:51 [notice] 1#1: start worker process 33 2022/03/15 09:27:51 [notice] 1#1: start worker process 34 2022/03/15 09:27:51 [notice] 1#1: start worker process 35 刪除容器 (rm -f) 想要刪除不需要的 Container，可以使用 rm 指令來做刪除，-f 是強制刪除容器。\n$ docker rm -f demo-nginx demo-nginx 進入容器 (exec) 想要進入 container 來查看資料或是修改檔案，可以使用 exec 來進入容器中。\n$ docker exec -it demo-nginx /bin/bash root@31a4a4a56e3e:/# ls bin boot dev\tdocker-entrypoint.d docker-entrypoint.sh etc\thome lib lib64 media mnt opt proc root run sbin srv sys tmp usr var root@31a4a4a56e3e:/# cd usr/bin/ root@31a4a4a56e3e:/usr/bin# pwd /usr/bin 匯出檔案 (export) 我們前面有提到說，如果刪除了容器，以前寫入的資料也會不見，如果想要輸出資料，可以使用 export 將可讀可寫的那一層匯成檔案。\n$ docker export demo-nginx \u003e demo-nginx.tar $ ls | grep 'demo' demo-nginx.tar save 跟 export 的區別：\n還記得我們在儲存映像檔的時候有介紹到 save 嗎，那他跟 export 的區別是什麼呢？我們可以理解成\nsave 是把 Docker Image 原始檔做儲存，export 是把修改 Docker Image 的內容都一併儲存。\n匯入檔案 (import) 有匯出檔案，當然也有匯入檔案拉，可以使用 import 將我們匯出的檔案匯入 Docker Image 裡面。\n$ cat ~/Desktop/demo-nginx.tar| docker import - import-nginx sha256:7106935f0bfbdbb84f9eb20edb8cdb2c53207f5e0963f6a4e89d8267e9d98c56 $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE import-nginx latest 7106935f0bfb 19 seconds ago 140MB Container 的狀態 檢查容器狀態 (inspect) 想要查看容器的狀態數據，可以使用 inspect 來顯示。\n[ { \"Id\": \"sha256:1f56acbcbe9ec613a37e26934a84d98bed73879059f424dc69754520086baa37\", \"RepoTags\": [ \"demo-image:latest\" ], \"RepoDigests\": [], \"Parent\": \"\", \"Comment\": \"buildkit.dockerfile.v0\", \"Created\": \"2022-03-15T03:02:26.8321887Z\", \"Container\": \"\", \"ContainerConfig\": { \"Hostname\": \"\", \"Domainname\": \"\", \"User\": \"\", .... 省略 .... 查看容器的 CPU、記憶體及網路使用 (stats) 想要查看容器的 CPU、記憶體及網路使用，可以使用 stats 來顯示。\nCONTAINER ID NAME CPU % MEM USAGE / LIMIT MEM % NET I/O BLOCK I/O PIDS 31a4a4a56e3e demo-nginx 0.00% 5.797MiB / 1.939GiB 0.29% 6.29kB / 2.83kB 1.46MB / 20.5kB 5 倉庫 (Repository) 我們在映像檔的章節有使用 docker pull 來下載別人的映像檔來使用，那我們要如何把我們做好的上傳上去 Docker hub 呢！(由於 Docker hub 是公開平台，所以大家都可以自由的下載映像檔，所以是公司機密的映像檔，就要避免上傳歐！)\n$ docker login Authenticating with existing credentials... Login Succeeded 由於我有下載桌面版的 Docker ，所以登入時不需要再另外設定！\n我們在上傳到 Docker hub 之前，需要先修改 Image 的 tag ，格式 docker tag {Image Name} {DockerHub帳號}/{想要取的 Image Name}\n$ docker images | grep 'demo-image' demo-image latest 1f56acbcbe9e 24 hours ago 166MB $ docker tag demo-image 880831ian/demo-image $ docker images | grep 'demo-image' 880831ian/demo-image latest 1f56acbcbe9e 24 hours ago 166MB demo-image latest 1f56acbcbe9e 24 hours ago 166MB 接下來就使用 docker push，將映像檔上傳到 Docker hub 上！\n$ docker push 880831ian/demo-image Using default tag: latest The push refers to repository [docker.io/880831ian/demo-image] 7722c88c8d69: Pushed 68a85fa9d77e: Mounted from library/ubuntu latest: digest: sha256:814caacaf3dad3eccb43dc9bcad635d0473bd5946295d40ca1ec23d13a5f6d0f size: 741 我們也登入 Docker hub 看一下，是不是真的上傳成功了～\nrepository","docker-進階#Docker 進階":"本章節會分成三個常用功能來說明\nVolumes 介紹 Network 模式介紹和比較 Docker-compose 介紹與實作 Volumes 介紹 我們在先前介紹 Container 時，也有說到，Container 會分成 Image 層、Init 層以及使用者可讀可寫層的這三層。當我們將 Container 刪除後，存放在 Docker Container 上的資料也會不見，雖然可以用 export 來儲存，但我們應該在根本上解決問題。\n所以我們可以使用兩種方式來解決！\n在執行 docker run 指令時加入 -v 參數，將 Container 的檔案路徑映射到本地端的檔案路徑。 在撰寫 Dockerfile 時，加入 VOLUME 指令，可以將資料存放在實體主機上。使用這個方法還要搭配我們介紹 Container 的狀態 \u003e 檢查容器狀態 (inspect) ，來查詢本地端檔案的存放路徑在哪。 使用 -v 指令將容器映射到本地端 在使用 docker run 指令時，使用 -v 將容器檔案路徑映射到本地的檔案路徑。\n$ docker run -it -v /Users/ian_zhuang/Desktop/data:/storage centos /bin/bash latest: Pulling from library/centos Digest: sha256:a27fd8080b517143cbbbab9dfb7c8571c40d67d534bbdee55bd6c473f432b177 Status: Downloaded newer image for centos:latest [root@a99b41fba3ca /]# 我們在著面的 data 資料夾隨意新增檔案或是資料，再進入 Docker Container 內的 /storage (該檔案是因為使用 -v 而新增的資料夾) ，看看有沒有同步新增。\n[root@a99b41fba3ca /]# cd storage [root@a99b41fba3ca storage]# pwd /storage [root@a99b41fba3ca storage]# ls 1.html\t2.html\t3.html Dockerfile VOLUME 使用 我們先打開上次的 Dockerfile 檔案，在基礎映像檔資訊的底下使用 VOLUME 指令，加入 storage 資料夾，再將 Image Build 起來，啟動 Container，在 /storage 裡面隨便新增資料，最後我們在用 docker inspect 指令來找映射在本地端的路徑。\n在基礎映像檔資訊的底下使用 VOLUME 指令，加入 storage 資料夾 # 基礎映像檔資訊 FROM ubuntu:latest VOLUME [\"/storage\"] .... 省略 .... 建立映像檔 $ docker build -t demo-image:v2 . [+] Building 1.3s (6/6) FINISHED =\u003e [internal] load build definition from Dockerfile 0.0s =\u003e =\u003e transferring dockerfile: 44B 0.0s =\u003e [internal] load .dockerignore 0.0s =\u003e =\u003e transferring context: 2B 0.0s =\u003e [internal] load metadata for docker.io/library/ubuntu:latest 1.2s =\u003e [1/2] FROM docker.io/library/ubuntu:latest@sha256:8ae9bafbb64f63a50caab98fd3a5e37b3eb837a3e0780b78e5218e63193961f9 0.0s =\u003e CACHED [2/2] RUN apt-get update -y \u0026\u0026 apt-get install nginx -y 0.0s =\u003e exporting to image 0.0s =\u003e =\u003e exporting layers 0.0s =\u003e =\u003e writing image sha256:0618bb2685ecfe200d9df4a91380d482031352d0e00cbfdf70fcd063aa8654fa 0.0s =\u003e =\u003e naming to docker.io/library/demo-image:v2 啟動 Container，並在 /storage 內新增隨意資料 $ docker run -it demo-image:v2 /bin/bash root@4f8712562dfe:/# echo \"Hello ian\" \u003e /storage/helloworld.txt root@4f8712562dfe:/# ll /storage total 12 drwxr-xr-x 2 root root 4096 Mar 16 05:39 ./ drwxr-xr-x 1 root root 4096 Mar 16 05:38 ../ -rw-r--r-- 1 root root 10 Mar 16 05:39 helloworld.txt 使用 inspect 指令，來找到 Volume 在本地端映射的資料夾路徑，看看裡面有沒有我們在 Container 裡面新增的資料吧 $ docker inspect -f '{{.Mounts}}' 4f8712562dfe [{volume 4fe10ca3f...省略 /var/lib/docker/volumes/4fe10ca3f234633164d9b3c541893c68db1b4f98806525076a2edd5c1c7863c4/_data /storage local true }] $ docker run -it --privileged --pid=host debian nsenter -t 1 -m -u -n -i sh / # cd /var/lib/docker/volumes/4fe10ca3f234633164d9b3c541893c68db1b4f98806525076a2edd5c1c7863c4/_data /var/lib/docker/volumes/4fe10ca3f234633164d9b3c541893c68db1b4f98806525076a2edd5c1c7863c4/_data # ls helloworld.txt mac OS 找不到 /var/lib/docker/volumes： 由於 macOS 下的 docker 實際是在 vm 裡又多加一層，所以沒辦法直接訪問 /var/lib/docker/volumes，必須先透過以下指令進入 VM 中。\ndocker run -it --privileged --pid=host debian nsenter -t 1 -m -u -n -i sh 詳細內容可以參考 Where is /var/lib/docker on Mac/OS X。\n容器與容器之間資料共享 如何啟用容器與容器之間的資料共享，可以用以下方式\n先啟動第一個容器指令如下 docker run -it -v /data --name=container1 centos /bin/bash [root@ a0307ce757ca /]# 另二個容器指令如下 docker run -it --volumes-from container1 --name=container2 centos /bin/bash [root@720d57983cd4 /]# --volumes-from 參數指定 container1 的資料會與 container2 做共享。 我們在第一個容器，進入 /data 資料夾，隨機輸入資料 [root@a0307ce757ca /]# cd /data/ [root@a0307ce757ca data]# echo \"ian~\" \u003e hello.txt 再來看一下第二個容器 /data 資料夾，是否有我們在容器(a0307ce757ca)產生的資料 [root@720d57983cd4 /]# cat /data/hello.txt ian~ Network 模式和比較 在執行 docker run 其中一個參數是 --net ，他可以設定 Container 要使用哪一種的網路模式，以下分別說明這些網路模式\nnone：在執行 Container 時，網路功能是關閉的，所以無法與此 Container 連線。 container：使用相同的 Network Namespace ，假設 Container 1 的 IP 是 172.17.0.2，那 Container 2 的 IP 也是 172.17.0.2。 host：Container 的網路設定和實體主機使用相同的網路設定，所以 Container 裡面也可以修改實體機器的網路設定，因此使用此模式需要考慮網路安全性上的問題。 bridge：Docker 預設就是此網路模式，這個網路模式就像是 NAT 的網路模式，例如實體主機的 IP 是 192.168.1.10 它會對應到 Container 裡面的 172.17.0.2，在啟動 Docker 的服務時會有一個 docker0 的網路卡來做此網路的橋接。 overlay：Container 之間可以在不同的實體機器上做連線，例如 Ｈ ost 1 有一個 Container 1 ，然後 Host 2 有一個 Container 2，Container 1 可以直接使用 overlay 的網路模式和 Container 2 做網路連線。 macvlan：可以直接分配實體網卡的 MAC address 給特定的 Container，讓 Container 透過實體的網卡使用網路。 那我們就來實作每一個網路模式吧！\nnone 我們使用 docker run指令，在後面加入參數 --net=none ，我們建立 jonlabelle/network-tools (裡面有很多網路測試工具)。\n$ docker run -it --net=none jonlabelle/network-tools [docker@network-tools]$ ifconfig lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 .... 省略 .... 可以看到我們使用 ifconfig 查詢，只有本地端的 127.0.0.1 IP，我們在使用 ping 來測試 google 網站吧\n[docker@network-tools]$ ping www.google.com ping: www.google.com: Try again container 我們先啟動一個名為 container1 的容器\ndocker run --name container1 -it jonlabelle/network-tools 再開啟另一個 Terminal 來啟動 container2 的容器，並設定相同的 Network Namespace\ndocker run --name container2 --net=container:container1 -it jonlabelle/network-tools 一樣使用 ifconfig 查詢，可以發現兩個容器的 IP 都是相同的\neth0 Link encap:Ethernet HWaddr 02:42:AC:11:00:02 inet addr:172.17.0.2 Bcast:172.17.255.255 Mask:255.255.0.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 .... 省略 .... host 在執行 docker run 指令時，在後面加入參數 --net=host ，來測試 host 模式\ndocker run --net=host -it jonlabelle/network-tools 可以看到 Container 的網路資訊和實體主機的網路資訊是相同的結果\n[docker@network-tools]$ ifconfig br-36a27cab1817 Link encap:Ethernet HWaddr 02:42:20:DF:DB:FD inet addr:172.20.0.1 Bcast:172.20.255.255 Mask:255.255.0.0 UP BROADCAST MULTICAST MTU:1500 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 .... 省略 .... host 網路模式\nbridge 在執行 docker run 指令時，在後面加入參數 --net=bridge ，來測試 bridge 模式\ndocker run --net=bridge -it jonlabelle/network-tools docker 會新增一個虛擬網卡作為容器網路對外的出口，預設名稱為 docker0，docker0 會跟本機的對外網卡(圖中的 eth0 )相連，藉此取得對外連線的能力，也因為每一個容器都會使用一個 veth device 與 docker0 相連，所以也具備對外連線的能力。\nbridge 網路模式\noverlay 下圖是說明 Host1 實體主機裡面有 Container1，然後 Host2 實體主機裡面有 Container2，可以透過 Docker overlay 模式將 Container1 和 Container2 連接做溝通。另外還需要一個 Consol 來存連線的資料庫，在使用 overlay 時要先在 Docker 做設定，這樣才能存放 overlay 網路模式的連線資訊。\noverlay 網路模式\nmacvlan macvlan 的原理就是在本機的網卡上虛擬出很多個子網卡，通過不同的 MAC 位置在數據鏈路層進行網路資料的轉發。\nmacvlan 網路模式\nDocker-compose 我們在執行多個容器時，需要重複的下 run 指令來執行，以及容器與容器之間要做關聯也要記得每一個之間要怎麼連結，會變得很麻煩且不易管理，所以有了 docker-compose 可以將多個容器組合起來，變成一個強大的功能。\n只要寫一個 docker-compose.yml 把所有會使用到的 Docker image 以及每一個容器之間的關聯或是網路的設定都寫上去，最後再使用 docker-compose up 指令，就可以把所有的容器都執行起來囉！\n我們就直接來實作我們這次的標題，要使用 docker-compose 來整合 PHP MySQL Nginx 環境。\n我們先開啟一個資料夾，取名叫 docker-compose ，來放置我們的 docker-compose 檔案 接著新增 docker-compose.yml 檔案，要準備來撰寫我們的設定檔囉！ 由於內容有點長，所以我分段說明，(這邊有放已經寫好的檔案歐) 檔案目錄\nDocker-compose.yml docker-volume html index.php info.php nginx Dockerfile default.conf php Dockerfile version: \"3.8\" services: ... 省略 .... 可以看到一開頭，會先寫版本，這邊代表的是會使用 3.8 版本的設定檔，詳細版本對照可以參考 Compose file versions and upgrading\nservices 可以設定用來啟動多的容器，裡面我們總共放了三個容器，分別是 nginx、php、mysql 。\n那我們來看看 nginx 裡面放了什麼吧！我會依照程式碼往下說明，有不清楚的可以底下留言！\nnginx nginx: build: ./nginx/ container_name: nginx ports: - 7777:80 volumes: - ./docker-volume/log/:/var/log/nginx/ nginx 的 build 就是要執行這個 nginx 容器的映像檔，還記得我們也可以使用 Dockerfile 來撰寫映像檔案嗎！?\n由於我們還要設定其他內容，所以特別另外拉一個 nginx 資料夾來放置，裡面放了兩個檔案，分別是 Dockerfile、default.conf。\nDockerfile 檔案裡面會使用 nginx 版本 1.20 ，並將 default.conf 複製到容器的 /etc/nginx/conf.d/default.conf 來取代設定。\n以及我們使用 ports 將容器 80 Port 指向本機 7777 Port ，格式是 本機 Port : 容器 Port，\n再使用 volumes 來設定我們 nginx 容器 log 資料夾映射到本機的 ./docker-volume/log/ 資料夾。\nphp php: build: ./php/ container_name: php expose: - 9000 volumes: - ./docker-volume/html/:/var/www/html/ php 的 build 是要執行這個 php 容器的映像檔，由於我們還要設定其他內容，所以特別另外拉一個 php 資料夾來放置 Dokcerfile。\nDockerfile 檔案裡面會使用 php 版本 7.4-fpm，並且在容器執行 docker-php-ext-install、mysqli。\n並將 Port 9000 發佈於本機，再使用 volumes 來設定 /var/www/html 網站根目錄映射到本機的 ./docker-volume/html/ 資料夾。\nmysql mysql: image: mysql:8.0.28 container_name: mysql volumes: - ./docker-volume/mysql/:/var/lib/mysql environment: MYSQL_ROOT_PASSWORD: secret MYSQL_DATABASE: mydb MYSQL_USER: myuser MYSQL_PASSWORD: password mysql 使用的映像檔是 mysql 版本是 8.0.28，我們為了要保留資料庫的資料，所以將容器的 /var/lib/mysql 映射到本地端 ./docker-volume/mysql 資料夾。\n最後的環境變數，設定 root 帳號的登入密碼，以及要使用的資料庫、使用者的帳號、使用者的密碼。\n最後在上面的 (這邊有放已經寫好的檔案歐) 裡面還有多一個 docker-volume/html 的資料夾，就是我們剛剛映射到本地端的資料夾，資料夾內已經放有連線測試的檔案，輸入網址 http://127.0.0.1:7777/index.php，如果開啟後有顯示下方畫面，就代表我們成功用 docker-compose 將 PHP MySQL Nginx 整合再一起囉！\n測試是否成功用 docker-compose 整合 PHP MySQL Nginx","什麼是-docker-#什麼是 Docker ?":"Docker 是一種軟體平台，它可以快速建立、測試和部署應用程式。為什麼可以快速建立呢？因為 Docker 會將軟體封裝到名為『容器』的標準單位。其中會包含程式庫、系統工具、程式碼、執行軟體所需的所有項目。 剛剛有提到容器 (Container)，是一種虛擬化技術，它高效率虛擬化及易於遷移和擴展的特性，非常適合現代雲端的開發及佈署。那 Container 與傳統的虛擬機有什麼差別呢？我們來看看下面這張圖\nContainer 與 VM 的差異\n可以看到 Container 是以應用程式為單位，而 VM 則是以作業系統為單位。Container 是一個封裝了相依性資源與應用程式的執行環境 ; VM 則是一個配置好 CPU、RAM 與 Storage 的作業系統，為了更好的做區別，我把 Container、VM 兩個差別用表格來說明～\n區別比較 Container VM 單位 應用程式 作業系統 適用服務 多使用於微服務 使用較大型的服務 硬體資源 是以程式為單位，需要的硬體資源很少 VM 會先佔用 CPU、RAM 等等硬體資源，不管有沒有使用都會先佔用 造成衝突 Container 間是彼此隔離的，因此在同一台機器可以執行不同版本的服務 會因為版本不同造成環境衝突 系統支援數量 單機支援上千個容器 一般最多幾十個 優點 1 . Image 較小，通常都幾 MB\n2 . 啟動速度快，通常幾秒就可以生成一個 Container\n3 . 更新較為容易，只需要利用新的 Image 重新啟動就會更新了 1 . 因為硬體層以上都虛擬化，因此安全性相對較高\n2 . 系統選擇較多，在 VM 可以選擇不同的 OS\n3 . 不需要降低應用程式內服務的耦合性，不需要將程式內的服務個別拆開來部署 缺點 1 . 安全性較 VM 差，因為環境與硬體都與本機共用\n2 . 在同一台機器中，每一個 Container 的 OS 都是相同的，無法一個為 Windows、一個為 Linux，還是依賴 Host OS\n3 . Container 通常會切成微服務的方式作部署，在各元件中的網路連結會比較複雜 1 . Image 的大小通常 GB 以上，比 Container 大很多\n2 . 啟動速度通常要花幾分鐘，因此服務重啟速度較慢\n3 . 資源使用較多，因為不只程式本身，還要將一部分資源分給 VM 的作業系統 總結\n更快速的交付和部署：對於開發和維運人員來說，最希望就是一次建立或設定，可以再任意地方正常運行。開發者可以使用一個標準的映像檔來建立一套開發容器，開發完成之後，維運人員可以直接使用這個容器來部署程式。Docker 容器很輕很快！容器的啟動時間都是幾秒中的事情，大量地節約開發、測試、部署的時間。\n更有效率的虛擬化：Docker 容器的執行不需要額外的虛擬化支援，它是核心層級的虛擬化，因此可以實作更高的效能和效率。\n更輕鬆的遷移和擴展：Docker 容器幾乎可以在任意的平台上執行，包括實體機器、虛擬機、公有雲、私有雲、個人電腦、伺服器等。 這種兼容性可以讓使用者把一個服務從一個平台直接遷移到另外一個。\n更簡單的管理：使用 Docker，只需要小小的修改，就可以替代以往大量的更新工作。所有的修改都以增量的方式被分發和更新，從而實作自動化並且有效率的管理。","參考資料#參考資料":"Docker 官網：https://docs.docker.com/get-started/\n什麼是 Docker？：https://aws.amazon.com/tw/docker/\nDocker－－從入門到實踐：https://philipzheng.gitbook.io/docker_practice/#yuan-chu-chu-ji-can-kao-zi-liao\nDockerfile 建立自訂映像檔 — 架起網站快速又簡單（一）：https://medium.com/@jackercleaninglab/dockerfile-%E5%BB%BA%E7%AB%8B%E8%87%AA%E8%A8%82%E6%98%A0%E5%83%8F%E6%AA%94-%E6%9E%B6%E8%B5%B7%E7%B6%B2%E7%AB%99%E5%BF%AB%E9%80%9F%E5%8F%88%E7%B0%A1%E5%96%AE-%E4%B8%80-22b2743f97b9\n用 30 天來介紹和使用 Docker：https://ithelp.ithome.com.tw/users/20103456/ironman/1320\nDocker 網路簡介：https://godleon.github.io/blog/Docker/docker-network-overview/\nDocker-compose Giving static IP in network mode : bridge：https://stackoverflow.com/questions/61949319/docker-compose-giving-static-ip-in-network-mode-bridge"},"title":"Docker 介紹 (如何使用 Docker-compose 建置 PHP+MySQl+Nginx 環境)"},"/blog/docker/prometheus-grafana-docker/":{"data":{"":"還記得我們上次架設 EFK 來獲得容器的日誌嗎!? 身為一個 SRE 除了收集日誌外，還需要監控每個系統或是服務的運行狀況，並在警急情況即時通知相關人員作為應對處理。所以透過好的 Monitoring/Alert System 了解目前 Server 硬體系統狀況和整個 Service 的網路狀況是一件非常重要的一件事情。\n在眾多的 Monitor 工具中，Prometheus 是一個很方便且完善的監控預警框架 TSDB (Time Series Database) 時間序列資料庫，可以快速且容易的建立不同維度的指標 (Metrics) 和整合不同的 Alert Tool 以及資訊視覺化圖表的監控工具並提供自帶的 PromQL 進行 query 查詢。\nPrometheus Logo\n我們先來看看 Prometheus 的架構圖，可以更了解 Prometheus 整體的定位：\nPrometheus 架構圖 (圖片來源：使用 Prometheus 和 Grafana 打造 Flask Web App 監控預警系統)\n有一個 Prometheus server 主體，會去 Prometheus Client Pull 相關的指標 (Metrics)，若是短期的 Job 例如 CronJob 在還來不及 Pull 資料回來可能就已經完成任務了、清洗掉資料。所以會有一個 pushgateway 接收 Job Push 過來的相關資訊，Prometheus Server 再從其中拉取資料。 (圖片左半部)\nService Discovery 可以更好的蒐集 Kubernetes 相關的資訊。 (圖片上半部)\nPrometheus Server 主體會將資料儲存在 Local On-Disk Time Series Database 或是可以串接 Remote Storage Systems。(圖片下半部)\nPrometheus Server 資料拉回來後可以使用本身自帶的 Web UI 或是 Grafana 等其他的 Client 來呈現。(圖片右下半部)\n當抓取資料的值超過 Alert Rule 所設定的閥值 (threshold) 時，Alertmanager 就會將訊息送出，可以透過 Email、Slack 等訊息通知，提醒相關人員進行處理。(圖片右上半部)\nPrometheus 可能在儲存擴展上比不上其他 Time Series Database，但在整合各種第三方的 Data Source 上十分方便，且在支援雲端服務和 Container 容器相關工具也十分友好。但在圖片的表現上就相較於單薄，所以會搭配我們接下來要介紹的 Grafanac 精美儀表板工具來進行資訊視覺化和圖表的呈現。\nGrafana Logo\nGrafana 是由 Grafana Lab 經營的一個非常精美的儀表板系統，可以整合各種不同的 Data Source，例如：Prometheus、Elasticsearch、MySQL、PostgreSQL 等。透過不同種指標 (Metrics) 呈現在 Dashboard 上。如果還是不太清楚，可以把 Prometheus Grafana 分別想成 Prometheus 是 EFK 的 Elasticsearch，Grafana 想成是 EFK 的 Kibana。\n今天我們要透過 Docker-Compose 搭配 Nginx 實作一個簡單的 Web Service 範例，並整合 Prometheus 和 Grafana 來建立一個 Web Service 監控預警系統。\n此文章程式碼也會同步到 Github ，需要的也可以去查看歐！要記得先確定一下自己的版本 Github 程式碼連結 😆","nginx-指標-metrics-描述#Nginx 指標 (Metrics) 描述":"我們在 http://localhost:9113/metrics 中可以看到許多指標 (Metrics) 那他們各代表什麼意思呢？我把它整理成表格讓大家可以選擇要使用的指標 (Metrics)：\n指標 描述 nginx_connections_accepted 接受用戶端的連接總數量 nginx_connections_active 當前用戶端連接數量 nginx_connections_handled Handled 狀態的連接數量 nginx_connections_reading 正在讀取的用戶端連接數量 nginx_connections_waiting 正在等待中的用戶端連接數量 nginx_connections_writing 正在寫入的用戶端連接數量 nginx_http_requests_total 客戶端總請求數量 nginx_up Nginx Exporter 是否正常運行 nginxexporter_build_info Nginx Exporter 的構建資訊 ","參考資料#參考資料":"使用 Prometheus 和 Grafana 打造 Flask Web App 監控預警系統：https://blog.techbridge.cc/2019/08/26/how-to-use-prometheus-grafana-in-flask-app/\nNginx Exporter 接入：https://cloud.tencent.com/document/product/1416/56039\n通過 nginx-prometheus-exporter 監控 nginx 指標：https://maxidea.gitbook.io/k8s-testing/prometheus-he-grafana-de-dan-ji-bian-pai/tong-guo-nginxprometheusexporter-jian-kong-nginx\n使用 nginx-prometheus-exporter 監控 nginx：https://www.cnblogs.com/rongfengliang/p/13580534.html\n使用阿里雲 Prometheus 監控 Nginx（新版）：https://help.aliyun.com/document_detail/171819.html?spm=5176.22414175.sslink.29.6c9e1df9DdpLPP\nGrafana Image Renderer：https://grafana.com/grafana/plugins/grafana-image-renderer/\ngrafana 的 image render 设置：https://blog.csdn.net/dandanfengyun/article/details/115346594","實作#實作":"接下來會依照執行的流程來跟大家說明歐！那要開始囉 😁\n我們要建立一個 Nginx 來模擬受監控的服務，我們要透過 nginx-prometheus-exporter 來讓 Prometheus 抓到資料最後傳給 Grafana，所以我們在 Docker-compose 裡面會有 nginx、nginx-prometheus-exporter、prometheus、grafana、grafana-image-renderer 幾個容器，我們先看一下程式碼，再來說明程式碼設定了哪些東西吧！\nDocker-compose.yamlversion: \"3.8\" services: nginx: build: ./nginx/ container_name: nginx ports: - 8080:8080 nginx-prometheus-exporter: image: nginx/nginx-prometheus-exporter:0.10 container_name: nginx-prometheus-exporter command: -nginx.scrape-uri http://nginx:8080/stub_status ports: - 9113:9113 depends_on: - nginx prometheus: image: prom/prometheus:v2.35.0 container_name: prometheus volumes: - ./prometheus.yaml:/etc/prometheus/prometheus.yaml - ./prometheus_data:/prometheus command: - \"--config.file=/etc/prometheus/prometheus.yaml\" ports: - \"9090:9090\" renderer: image: grafana/grafana-image-renderer:3.4.2 environment: BROWSER_TZ: Asia/Taipei ports: - \"8081:8081\" grafana: image: grafana/grafana:8.2.5 container_name: grafana volumes: - ./grafana_data:/var/lib/grafana environment: GF_SECURITY_ADMIN_PASSWORD: pass GF_RENDERING_SERVER_URL: http://renderer:8081/render GF_RENDERING_CALLBACK_URL: http://grafana:3000/ GF_LOG_FILTERS: rendering:debug depends_on: - prometheus - renderer ports: - \"3000:3000\" nginx：因為 Nginx 會通過 stub_status 頁面來開放對外的監控指標。所以我們要另外寫一個 Dockerfile 設定檔，先將 conf 放入 Nginx 中。 nginx-prometheus-exporter：這裡要注意的是需要使用 command 來設定 nginx.scrapt-url，我們設定 http://nginx:8080/stub_status，他的預設 Port 是 9113，並設定依賴 depends_no，要 nginx 先啟動後才會執行 nginx-prometheus-exporter。 prometheus：將 prometheus.yaml 設定檔放入 /etc/prometheus/prometheus.yaml，以及掛載一個 /prometheus_data 來永久保存 prometheus 的資料，最後 command 加入 --config.file 設定。 renderer：這是 grafana 顯示圖片的套件，我們使用 3.4.2 版本，記得要設定環境變數，照片顯示的時間才會正確，並開啟 8081 Port 讓 grafana 訪問。 grafana：一樣我們先掛載一個 /grafana_data 來永久保存 grafana 的設定，在環境變數中設定預設帳號 admin 的密碼是 pass，設定 renderer 套件的服務位置是 http://renderer:8081/render 以及回傳到 http://grafana:3000/，並設定依賴 depends_on prometheus 跟 renderer，最後設定 grafana 要呈現的畫面 3000 Port。 nginx/DockerfileFROM nginx:1.21.6 COPY ./status.conf /etc/nginx/conf.d/status.conf 選擇我們要使用的 nginx image 版本，並將我們的設定檔，複製到容器內。\nnginx/status.confserver { listen 8080; server_name localhost; location /stub_status { stub_status on; access_log off; } } 這邊最重要的就是要設定 /stub_status 路徑，並開啟 stub_status ，這樣才可以讓 nginx-prometheus-exporter 抓到資料！(要怎麼知道 Nginx 是否開啟 stub_status，可以使用 nginx -V 2\u003e\u00261 | grep -o with-http_stub_status_module 指令檢查，我們這次裝的 Image 已經有幫我們啟動)\nprometheus.yamlglobal: scrape_interval: 5s # Server 抓取頻率 external_labels: monitor: \"my-monitor\" scrape_configs: - job_name: \"prometheus\" static_configs: - targets: [\"localhost:9090\"] - job_name: \"nginx_exporter\" static_configs: - targets: [\"nginx-prometheus-exporter:9113\"] 這邊是 prometheus 的設定檔，例如有 scrape_interval 代表 Server 每次抓取資料的頻率，或是設定 monitor 的 labels，下面的 configs，分別設定了 prometheus 它的 targets 是 [\"localhost:9090\"] 以及 nginx_exporter 它的 targets 是 [\"nginx-prometheus-exporter:9113\"]。\ntest.sh#!/bin/bash docker=\"docker exec nginx\" for i in {1..10} do $docker curl http://nginx:8080/stub_status -s done 這個是我自己另外寫的測試程式，在本機執行後他會訪問 nginx 容器內部，並模擬 nginx 流量，讓我們在 Grafana 可以清楚看到資料。\n執行/測試 當我們都寫好設定檔後，在專案目錄下，也就是有 Docker-compose 路徑下，使用 docker-compose up -d 來啟動容器：\n啟動容器\n接下來我們依序檢查容器是否都有正常運作，開啟瀏覽器瀏覽 http://localhost:9113/metrics 查看是否有出現跟下面圖片差不多的內容：\n檢查 Nginx 以及 nginx-prometheus-exporter 的設定\n如果有出現，恭喜你完成了 Nginx 以及 nginx-prometheus-exporter 的設定，我們將 Nginx 的 stub_status 服務，透過 http://nginx:8080/stub_status 讓 nginx-prometheus-exporter 可以抓到圖片中的這些指標 (Metrics)。\nPrometheus 接著我們瀏覽 http://localhost:9090/targets，看看我們的 Prometheus 有沒有設定正確，抓到我們設定好的 targets：\n檢查 Prometheus targets\n如果兩個出現的都是 綠色的 UP 就代表正常有抓到資料囉！\n那要怎麼測試才知道有抓到資料呢？我們可以先用 Prometheus 內建的圖形化介面來檢查，在瀏覽器瀏覽 http://localhost:9090/graph 就可以看到下面的畫面：\nPrometheus 內建的圖形化介面\n我們選擇 Graph，並在上面的搜尋欄，打上 nginx_connections_accepted 按下右邊的 Execute 就會產生一張圖表，圖表裡面只有一條綠色的線，那這個線是什麼呢？它就是我們剛剛在 http://localhost:9113/metrics 其中一個指標 (Metrics)，它代表 Nginx 接受用戶端連接總數量：\nPrometheus 內建的圖形化介面\n這個功能就是把我們所收到的 Nginx 指標 (Metrics)，轉換成圖表讓我們可以知道他的變化。\n為了更明顯的看出變化，這時候就要使用我所寫好的 test.sh 腳本，使用 sh test.sh 來執行，再回來觀察圖型是否變化：\n經過測試顯示的 nginx_connections_accepted 圖形\n可以發現剛剛原本只有 1 個的連接數因為我們模擬總共跑了 10 次，所以連接數變成 11 了！\nGrafana Prometheus 的圖形化比較單調，所以我們使用 Grafana 來美化我們的儀表板，瀏覽器瀏覽 http://localhost:3000/ ，可以看到一個登入頁面：帳號是 admin，密碼是我們在環境變數中所設定的 pass：\nGrafana 登入頁面\n登入後我們看到首頁，選擇 Add your first data source 來新增資料來源：\nGrafana 新增資料來源\n選擇第一個 Prometheus，我們到 HTTP 的 URL 設定 http://prometheus:9090 其他設定在我們測試環境中，不需要去調整，滑到最下面按下 Save \u0026 test：\nGrafana 新增資料來源\n接著我們要來設計我們的儀表板，在 Grafana 除了自己設計以外，還可以 Import 別人做好的儀表板。\n我們點選左側欄位的 ＋ 符號 \u003e 裡面的 Import，可以在這邊 Import 別人做好的儀表板，使用方式也很簡單，只需要先去 Grafana Labs dashboard 裡面找到自己要使用的儀表板，右側會有一個 ID，把 ID 貼上我們的 Grafana 就 Import 成功囉！很神奇吧 XD\n我們要使用的儀表板是別人已經做好的 NGINX exporter，它的 ID 是 12708，把 ID 貼入後，按下 Load，就會有 NGINX exporter 的基本資訊，我們在最下面的 Prometheus 選擇我們要使用的 data source，就是我們剛剛先設定好的 Prometheus，最後按下 Import，就完成拉。\nGrafana 載入別人做好的儀表板\n如果設定都沒有錯誤的話，應該可以看到下面這個畫面，最上面是監測 Nginx 服務的狀態，以及下方有不同的指標在顯示：\nGrafana 儀表板\n接下來我們一次用 test.sh 來測試一下是否有成功抓到資料：\n測試 Grafana 是否成功抓到資料\n可以看到在我們使用完測試腳本後，在該時段的資料有明顯的不一樣，代表我們有成功抓到資料 😄\n此外也可以將 Nginx 服務暫停，看看儀表板上方的 NGINX Status 狀態是否改變：\n測試暫停 Nginx 查看 Grafana 儀表板 NGINX Status\nAlerting 警報 當然除了監控以外，我們還需要有警報系統，因為我們不可能每天都一直盯著儀表板看哪裡有錯誤，所以我們要設定警報的規則，以及警報要發送到哪裡，接著我們一起看下去吧：\n我們先點左側的 Alerting 🔔 \u003e 點選 Notification channels 來新增我們要發送到哪裡。這次我們一樣使用 Telegram，我們在 type 下拉式選單選擇 Telegram，輸入我們的 BOT API Token 以及 Chat ID，儲存之前可以點選 test 來測試！\n怎麼使用 Telegram Bot：請參考這一篇 Ansible 介紹與實作 (Inventory、Playbooks、Module、Template、Handlers) 來取得 BOT API Token 以及 Chat ID。 Alerting 設定\nAlerting 測試結果\n接著我們來設計一個屬於我們的控制板 (Panel)，順便幫他加上 Alerting，稍後也用 test.sh，看看他會不會自動發出提醒到 Telegram Bot 😬\n首先點選左側欄位的 ＋ 符號 \u003e 裡面的 Create，在選擇 Add an empty panel：\nCreate Panel\n再 Query 的 A Metrics browser 輸入 nginx_connections_accepted 一樣來取得 Nginx 接受用戶端連接總數量的圖表，到右上角選擇 Last 5 minutes，旁邊的圖型我們選擇 Graph (old)，下面的 Title 可以修改一下這個圖表的名稱，最後按下 Save，就可以看到我們建好一個控制板囉 🥳\n設定 Panel\n接著我們來設定 Alert，可以看到剛剛在 Query 旁邊有一個 Alert，點進去後按 Create Alert，我們先修改 Evaluate every 後面的 For 改為 1m (代表當數值超過我們所設定的閥值後，狀態會從 OK 變成 Pending，這時候還不會發送警報，會等待我們現在設定的 1m 分鐘後，情況還是沒有好轉，才會發送通知)，再 Conditions 後面欄位加入 10 (我們所設定的閥值，代表 nginx_connections_accepted 超過 10 就會進入 Pending 狀態)，往下滑 Notifications 的 Send to 選擇我們上面所建立的 channels 名稱，按下 Save。\n設定好 Alert 的控制板\n接著執行 test.sh 兩次，讓 nginx_connections_accepted 超過我們所設定的閥值，可以看到控制板超過 10 以上變成紅色：\n超過閥值，控制板變成紅色\n接著等待幾分鐘後，狀態會從 OK 綠色變成黃色的 Pending，最後轉成紅色的 Alert，這時候 Telegram 就會收到通知囉 ❌\n自動發送通知到 Telegram Bot，並附上控制板圖片","檔案結構#檔案結構":" docker-compose.yaml nginx Dockerfile status.conf prometheus.yaml test.sh 這是主要的結構，簡單說明一下：\ndocker-compose.yaml：會放置要產生的 nginx、nginx-prometheus-exporter、prometheus、grafana、grafana-image-renderer 容器設定檔。 nginx/Dockerfile：因為在 nginx 要使用 stub_status 需要多安裝一些設定，所以用 Dockerfile 另外寫 nginx 的映像檔。 nginx/status.conf：nginx 的設定檔。 prometheus.yaml：prometheus 的設定檔。 test.sh：測試用檔案(後續會教大家如何使用)。 ","版本資訊#版本資訊":" macOS：11.6 Docker：Docker version 20.10.14, build a224086 Nginx：1.21.6 Prometheus：v.2.35.0 nginx-prometheus-exporter：0.10 Grafana：8.2.5 (最新版本是 8.5.2，但選擇 8.2.5，是因為 8.3.0 後 Alerting 沒有辦法附上圖片，詳細原因可以參考 Add “include image” option into Grafana Alerting ) grafana/grafana-image-renderer：3.4.2 "},"title":"使用 Prometheus 和 Grafana 打造監控預警系統 (Docker 篇)"},"/blog/gcp/":{"data":{"":"此分類包含 Google Cloud Platform 相關的文章。\nGCP Load Balancer 介紹 GCS Bucket CORS 錯誤解決方法 Google Kubernetes Engine Local ephemeral storage 計算方式 GCP Memorystore HA 高可用性 failover 測試 GCP Prometheus Samples Ingested 計算方式及如何減少費用 Google Kubernetes Engine CronJob 會有短暫時間沒有執行 Job 如何過濾 GCP LOG，減少 Cloud Logging API 的花費 Google Cloud Platform (GCP) - IAM 與管理 Google Cloud Platform (GCP) - Compute Engine Google Cloud Platform (GCP) - Cloud Source Repositories Google Cloud Platform (GCP) - Container Registry Google Cloud Platform (GCP) - Cloud Build "},"title":"Google Cloud Platform"},"/blog/gcp/cloud-build/":{"data":{"":"跟大家介紹一下今天的主題 Cloud Build，Cloud Build 可以幫我們做持續建構、測試和部署，我們可以把它想成簡易版的 Jenkins，從整個映像檔案打包到部署，也就幾分鐘的事情，且內建許多指令。\n我們今天文章，需要使用前幾天提到的 Cloud Source Repositories 、Compute Engine、Container Registry，我們需要先透過 GitLab 將程式鏡像到 Cloud Source Repositories，再透過 Cloud Build 觸發將 GitLab 上面的 Dockerfile 建置到 Container Registry 中，再部署到 Compute Engine VM 上。大家可以參考流程圖，會更清楚今天的流程！那我們就開始囉 🥸\n流程圖","cloud-source-repositories-測試#Cloud Source Repositories 測試":"前面 GitLab 鏡像設定，請先參考上上篇 Google Cloud Platform (GCP) - Cloud Source Repositories，上上篇會帶大家從 GitLab 鏡像到 Cloud Source Repositories，所以我們就接續之前的內容，繼續往下開始學習吧～\n開啟 GCP，選擇左側的 menu \u003e 點擊 Cloud Build \u003e 選擇 觸發條件，點擊 建立觸發條件 按鈕。 輸入觸發條件的名稱，事件可以設定我們要怎麼進行觸發，我們這邊先選擇 推送至分支的版本 來觸發，在來源選擇上上篇建立好的 Cloud Source Repositories 存放區，分支版本我們先使用預設的 master，也就是推程式到 master 他會就觸發 Cloud Build： 建立觸發條件 1\n設定類型我們選擇 Cloud Build 設定檔，他也是 Cloud Build 專用的設定檔，後面會帶大家寫一份 Cloud Build，位置當然是使用我們 Cloud Source Repositories 存放區，以及可以依照專案來修改 cloudbuild.yaml 放在專案的哪裡，最後都沒問題，就按下建立： 建立觸發條件 2\n撰寫 cloudbuild.yaml 設定檔 在開始建立檔案前，先來跟大家說說檔案內有哪些設定吧：\n首先 Cloud Build 建構器是裝有常用的程式語言和工具的容器映像。我們可以配置 Cloud Build，讓建構器中運行特定命令，我舉個例子讓大家了解：\n以下程式碼是來自 Docker Hub 的 ubnutu 映像檔中所執行的命令：\nsteps: - name: \"ubuntu\" args: [\"echo\", \"hello world\"] 可以看到我們配置文件中 steps 參數是指我們要建構的步驟， name 字段指定 Docker 映像檔的位置，以及 args 字段中是指定映像檔運行的命令。\n我們的 Cloud Build 一樣會需要用 name 來指定建構容器的映像檔，以及使用 args 來執行我們映像檔所要運行的命令。\n我們的 Cloud Build 設定檔中的 name 常用的建構器映像檔如下：\nBuilder 名稱 bazel gcr.io/cloud-builders/bazel docker gcr.io/cloud-builders/docker git gcr.io/cloud-builders/git gcloud gcr.io/cloud-builders/gcloud gke-deploy gcr.io/cloud-builders/gke-deploy 接著我們來試著寫一個 cloudbuild.yaml 來建構我們的 nginx 服務，並部署到 Compute Engine 上。\n我們先回到 Gitlab 該專案下的目錄，新增 cloudbuild.yaml 檔案，將複製以下內容：\nsteps: # Docker Build - name: \"gcr.io/cloud-builders/docker\" args: [\"build\", \"-t\", \"gcr.io/$PROJECT_ID/ian-test:ian-nginx-test\", \".\"] # Docker Push - name: \"gcr.io/cloud-builders/docker\" args: [\"push\", \"gcr.io/$PROJECT_ID/ian-test:ian-nginx-test\"] # Build VM - name: \"gcr.io/google.com/cloudsdktool/cloud-sdk\" entrypoint: \"gcloud\" args: [ \"compute\", \"instances\", \"create-with-container\", \"ian-test-vm\", \"--container-image\", \"gcr.io/$PROJECT_ID/ian-test:ian-nginx-test\", ] env: - \"CLOUDSDK_COMPUTE_REGION=asia-east1\" - \"CLOUDSDK_COMPUTE_ZONE=asia-east1-b\" 我們一個一個來說說的這個 cloudbuild.yaml 裡面的設定吧！(我以前面的註解來區分)\nDocker Build：這邊的 name 我們用 gcr.io/cloud-builders/docker，代表我們將使用 docker 建構器，args 這邊下的意思是要把與 cloudbuild.yaml 放在一起的 Dokcerfile 給 build 起來，並改名為 gcr.io/$PROJECT_ID/ian-test:ian-nginx-test。 Docker Push：這邊一樣使用 gcr.io/cloud-builders/docker，args 指令部分變成我們要把他 push 到 gcr.io/$PROJECT_ID/ian-test 這個 Cloud Source Repositories，其中這個映像檔案的 tag 為 ian-nginx-test。 Build VM：這邊我們使用 gcr.io/google.com/cloudsdktool/cloud-sdk，可以透過它來建立 VM，並且執行 tag 名為 ian-nginx-test 的映像檔，後面環境變數是來設定 VM 的區域等等。 ian-test 是 Container Registry 資料夾名稱，ian-nginx-test 是 Container Registry 映像檔的 tag，ian-test-vm 是我們建立 VM 的名字，所以要記得改成自己的命名歐！ 撰寫 Dockerfile 接下來剛剛有提到 Docker Build 會將我們放在一起的 Dockerfile 給 build 起來，所以我們也要先寫好要用的 Dockerfile：\nFROM nginx:latest COPY ./index.html /usr/share/nginx/html/ 我們的 Dockerfile 很簡單，簡單寫了要使用的映像檔，以及將我們等等要測試的 index 複製到裡面\n撰寫測試 index.html \u003c!DOCTYPE html\u003e \u003chtml lang=\"en\"\u003e \u003chead\u003e \u003cmeta charset=\"UTF-8\" /\u003e \u003cmeta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\" /\u003e \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" /\u003e \u003ctitle\u003e測試測試測試\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e 我是測試檔案 \u003c/body\u003e \u003c/html\u003e 這個測試檔案，會蓋過 nginx 的預設畫面，當我們成功將 VM 建立後，瀏覽 80 Port 時，應該會跳出這個測試的網頁。\nCommit 到 GitLab 當我們都新增好檔案後，我們就將程式推到我們前幾篇的 Cloud Source Repositories 已經鏡像的 GitLab 中，接著就是等待見證奇蹟的時候了ＸＤ\n當我們推送後，我們先檢查 Cloud Source Repositories 是否有成功從 GitLab 鏡像過來： Cloud Source Repositories\n檢查看看 Container Registry 是否多了名為 ian-test 的資料夾，且裡面有一個 tag 為 ian-nginx-test 的映像檔： Container Registry\n檢查一下 Cloud Build 的觸發條件是不是在運作，最後成功可以看到類似下方圖片內容： Cloud Build 的觸發條件\n檢查 Compute Engine 的 VM 是否有成功被建立： Compute Engine\n最後就是測試這個映像檔案，是不是我們所 Build 的，測試方法很簡單，我們剛剛在 Dockerfile 有複製我們自己寫的 index.html 檔案，去蓋掉原本 nginx 的預設檔，所以我們可以瀏覽上面圖片的外部 IP，就可以看到我們所改的頁面囉！ 測試用 index.html","參考資料#參考資料":"Cloud Builder：https://cloud.google.com/build/docs/cloud-builders\nDay27 - 用 Cloud Build 實作 CI 部分：https://ithelp.ithome.com.tw/articles/10224727"},"title":"Google Cloud Platform (GCP) - Cloud Build"},"/blog/gcp/cloud-source-repositories/":{"data":{"":"跟大家介紹一下今天的主題 Cloud Source Repositories，聽到 Source Repositories 是不是感覺跟什麼東西很像呀，沒錯，就跟我們的 GitHub or GitLab 一樣，可以用來存放我們的程式碼的儲存庫，我們來看看官方怎麼介紹他吧：\n官方介紹 Cloud Source Repositories\n很好歐，非常簡單明瞭 🤣，沒錯，Cloud Source Repositories 就是託管在 Google Cloud 上功能齊全(？)的私有 Git 儲存庫。為什麼會打一個問號呢？是因為他其實沒有那麼好用，所以我們通常的做法，還是會依靠 GitHab 或是 GitLab 來存放程式碼，再透過鏡像 (mirror) 的方式到 Google Cloud Source Repositories。 那我們就開始囉～","cloud-source-repositories-測試#Cloud Source Repositories 測試":"建立 GitLab Project 首先，我們用 GitLab 來當示範，如何透過鏡像 (mirror) 到 Cloud Source Repositories 上面，我們先在 GitLab 上建立一個 Project：\n建立 GitLab Project\n使用 gcloud 指令建立 Source Repo 首先，一定要先裝 gcloud 指令到本機，這個步驟，前面文章也有說過，這邊就不在說明，我們先使用一下指令來查看目前所在的 GCP 專案： gcloud config get-value project 正常來說，如果有先用 config 設定好，會直接跳出你目前的專案 ID，如果沒有跳出來，請使用下面指令來設定：\ngcloud config set project \u003cproject id\u003e 接著我們要啟動該專案的 Cloud Source Repositories API： gcloud services enable sourcerepo.googleapis.com 創建 Cloud Source Repositories gcloud source repos create \u003crepo name\u003e 完成後，開啟 GCP 檢查一下是否有建立成功～點擊左側 menu \u003e Source Repositories， 開啟 Source Repositories\n成功建立 Source Repositories\n將程式碼新增至存放區中 我們要在這一步來設定鏡像 (mirror)，首先我們看剛剛上面建立好的 Source Repositories，其中有一個手動產生的憑證，點選 產生及儲存 Git 憑證 產生及儲存 Git 憑證\n點完後會需要先登入你的 GCP 帳號，登入完後會出現以下內容： Configure Git\n接著把藍色框框內的輸入到終端機內 Configure Git\n接著請複製以下指令貼到終端機內，會生成憑證密碼： grep 'source.developers.google.com' ~/.gitcookies | tail -1 | cut -d= -f2 生成憑證密碼\n接著請複製以下指令貼到終端機內，將用戶名存儲在 CSR_USER 環境變量中： CSR_USER=$(grep 'source.developers.google.com' ~/.gitcookies | \\ tail -1 | cut -d$'\\t' -f7 | cut -d= -f1) 用戶名存儲在 CSR_USER 環境變量中\n接著請複製以下指令貼到終端機內，將 GCP 存儲庫的 URL 存儲在 CSR_REPO 環境變量中 (repo name 要改成你在 gcp 上面的 repo)： CSR_REPO=$(gcloud source repos describe \u003crepo name\u003e --format=\"value(url)\") 將 GCP 存儲庫的 URL 存儲在 CSR_REPO 環境變量中\n接著請複製以下指令貼到終端機內，將存儲庫的 URL（包括用戶名）印到終端機上： echo $CSR_REPO | sed \"s/:\\/\\//:\\/\\/${CSR_USER}@/\" 存儲庫的 URL（包括用戶名）印到終端機上\n經過上面操作，我們可以在第 4 步驟拿到密碼，以及在第 7 步驟拿到完整的 GCP URL，接著我們要到 GItLab Mirror 來設定鏡像。\n到 GitLab Mirror 設定鏡像 先從右側 muen \u003e 選擇 Settings \u003e 點選 Repository，找到 Mirroring repositories GitLab Mirror 設定鏡像\n將剛剛拿到的 URL 以及密碼各別輸入 Git repository URL 以及 Password，記得要選擇 Mirror direction，因為我們是要將 gitlab 的鏡像到 GCP 的 Cloud Source Repositories，所以我們要選擇 PUSH，最後按下 Mirror repository： GitLab Mirror 設定鏡像\n如果沒有跳出錯誤，基本上是沒有問題了！\nGitLab Mirror 檢查\n就可以試著在 gitlab 上面推程式，看看有沒有跑到 Cloud Source Repositories 上面囉！ GitLab 推程式測試","參考資料#參考資料":"Cloud Source Repositories documentation\nMirroring GitLab repositories to Cloud Source Repositories"},"title":"Google Cloud Platform (GCP) - Cloud Source Repositories"},"/blog/gcp/compute-engine/":{"data":{"":"跟大家介紹一下今天的主題 Google Compute Engine(GCE)，GCE 是 Google Cloud 上的基礎架構服務 (IaaS)，該平台可以提供大規模的虛擬機器以及相關的基礎建設 (包含硬碟、網路、附載平衡器… 等等)來建置及運作您的服務，那我們可以將 GCE 服務的主要功能劃分成以下幾點：","google-compute-engine-測試#Google Compute Engine 測試":"首先我們使用 cloudskillsboost 提供的 Creating a Virtual Machine 來做練習，打開後，請先登入自己的 Google 帳號，接著點選左上角的 Start Lab，會跳出與下面圖片類似的內容：\n測試用的帳號密碼\n新增新的 VM 實例 點選 Open Google Console 按鈕來開啟 GCP 主控台 登入帳號就使用上面圖片所提供的帳號密碼來進行登入 登入成功會進入 GCP 主控台，點選左側的 menu \u003e Compute Engine \u003e VM Instances，可以參考下方圖片 新增 VM 實例\n點選 CREATE INSTANCE，請依照下方表格來進行設定： 標題 設定值 說明 Name gcelab 虛擬機實例的名稱 Region us-central1（愛荷華州） 有關區域的更多信息，請參閱 Compute Engine 指南 Regions and zones Zone us-central1-f Series N1 Machine type n1-standard-2 這是一個 2 vCPU、7.5 GB RAM 實例 Boot disk New balanced persistent disk/10 GB/Debian GNU/Linux 10 Firewall Allow HTTP taffic VM 實例 (Name、Region、Zone、Series)\nVM 實例 (Machine type、Boot disk、Firewall)\n新增完後，大約需要等待一分鐘，新的虛擬機就會列在 VM Instances 頁面上。 VM 實例\n安裝 NGINX Web 服務器 點擊 VM instances 實例最後的 SSH，會開啟 SSH 用戶端 在 SSH 終端，要先獲得 root 訪問權限，才更方便的進行後續的動作，請先使用一下指令： sudo su - 利用 root 用戶，來更新操作系統： apt-get update 更新操作系統\n安裝 NGINX： apt-get nginx -y 安裝 NGINX\n最後確認 NGINX 是否運行： ps auwx | grep nginx 確認 NGINX 是否運行\n可以打開瀏覽器瀏覽 http://外部 IP/ 或是使用 curl 外部IP，外部 IP 會在跟剛剛 VM 實例的 External IP 欄位呦～ curl 外部 IP\n完成後，記得可以點選 Check my progress 來檢查進度吧！ Check my progress\n使用 gcloud 創建一個新實例 我們剛剛是使用網頁版來新增，當然也可以使用 gcloud 指令來新增，這個工具有預先裝在 Google Cloud Shell 中。Cloud Shell 是一個基於 Debian 的虛擬機，包含了常用的開發工具 (gcloud、git 等工具)，另外你也可以將 gcloud 下載至本機上來做使用，請閱讀 gcloud 命令行工具指南\n在 Cloud Shell 中，使用 gcloud 來新增新的虛擬機實例： gcloud compute instances create gcelab2 --machine-type n1-standard-2 --zone us-central1-f 就會跳出以下圖片的內容，過一陣子去查看 VM Instances 也可以看到我們所新增的虛擬機實例歐～\nCheck my progress\n到這邊就完成了我們 Google Compute Engine 測試囉～我們知道可以新增 GCE 將實體主機的內容，移植到雲端上囉！希望大家會喜歡今天的文章 🥰","google-compute-engine-特色#Google Compute Engine 特色":"穩健的網路功能 提供使用者擁有穩健的網路功能，以運行各項應用程式及服務。\n自訂網路與預設網路 GCE 包含內部與外部的網路連線能力，讓使用者可以透過自訂規劃來建置屬於自己服務適用的網路，而在 GCE 服務開通當下，也提供預設的 default network，內建常用的路由與防火牆設定 (例如：SSH、RDP、ICMP… 等)，供入門使用者直接使用。\n防火牆規則 除了預設防火牆規則外，使用者也可以透過自建防火牆來開放可以連入的 IP。\n各區域的 HTTP(S) 的負載平衡 為 Layer 7 的負載平衡設備，透過設定可以串連多台主機或是主機群組，讓服務不再只是依賴於單點存在的伺服器。Layer 7 的負載平衡更可以識別路由規劃，進一步可以提供不同路由的重導規則設定，也可以提供 CDN 的 Cache 功能。\n網路的負載平衡 為 Layer4 的負載平衡設備，可以透過 Protocol 與 Port 的方式來重導外部流量到 GCE 主機或主機群，並可以透過 Health Check 的方式，讓流量僅通過健康的主機，避免服務中斷。\n子網路 透過 CIDR 的方式設定 GCE Network 中的各子網路範圍，並可以透過路由的方式串接各子網路中間的通訊，讓 GCE 網路的設計規劃可以更有彈性，也可以讓子網路的規劃來實作更安全的雲端網路架構。","參考資料#參考資料":"Compute Engine 基本介紹：https://gdgcloud-taipei.gitbook.io/google-cloud-platform-in-practice/google-cloud-shang-de-yun-suan-fu-wu/compute-engine/compute-engine-ji-ben-jie-shao\nCreating a Virtual Machine：https://www.cloudskillsboost.google/focuses/3563?locale=zh_TW\u0026parent=catalog"},"title":"Google Cloud Platform (GCP) - Compute Engine"},"/blog/gcp/container-registry/":{"data":{"":"跟大家介紹一下今天的主題 Container Registry，Container Registry 是儲存、管理和保護 Docker 容器映像檔的存放區，可以讓團隊透過同一項服務服務集中管理 Docker 映像檔、也可以執行安全漏洞分析，還能透過精密的存取權管理機制，來決定誰可以存取哪些內容。簡單來說他就是一個讓我們存放 Docker 映像檔的地方，他有以下幾個特點：","container-registry-特點#Container Registry 特點":" 安全的私人 Docker 註冊資料庫：只需要幾分鐘的時間，即可開始在 Google Cloud Platform 中使用安全的私人 Docker 映像檔儲存空間，控管能夠存取、檢視或下載映像檔的人員，並在受到 Google 安全防護機制保護的基礎架構上穩定執行。 自動建立及部署映像檔：在您修訂 Cloud Source Repositories 中的程式碼時，系統會自動建立映像檔並推送至私人註冊資料庫。您可以輕鬆設定持續整合/持續推送軟體更新管道，並整合至 Cloud Build，或是直接將管道部署至 Google Kubernetes Engine、App Engine、Cloud Functions 或 Firebase。(這個就是我們在下一篇文章會使用到的功能) 深度安全漏洞掃描：在軟體部署週期的早期階段找出安全漏洞，藉此確保您可以安全地部署容器映像檔。資料庫會持續重新整理，讓您的安全漏洞掃描作業取得最新型惡意軟體的相關資訊。 鎖定有風險的映像檔：運用二進位授權的原生整合功能來定義政策，避免部署與所設定政策相衝突的映像檔。您可以觸發容器映像檔的自動鎖定功能，禁止將有風險的映像檔部署至 Google Kubernetes Engine。 原生支援 Docker：您不僅可以視需求定義多個註冊資料庫，也能使用標準的 Docker 指令列介面，將 Docker 映像檔推送和提取到您的私人 Container Registry。Docker 提供依據名稱和標記搜尋映像檔的功能，讓您可以順暢使用。 快速的高可用性存取能力：您可使用我們遍布全球的區域性私人存放區，於全世界皆能享有最快速的回應時間。您可以就近將映像檔儲存在位於歐洲、亞洲或美國的運算執行個體中，並透過 Google 的高效能全球網路快速完成部署作業。 上面有提到 Container Registry 其中一個特點就是掃描映像檔，會檢查有沒有奇怪的內容或是錯誤，讓我們在部署前可以先做檢查，底下是整個的流程圖：\nContainer Registry 掃描流程圖\n我們可以延續上一篇文章 Cloud Source Repositories 來說明上面圖片，一開始先 commit 到 GCP 上，也可以把它當成 Cloud Source Repositories，就著透過會透過下一篇要講的 Cloud Build 來建置，並且掃描，如果沒有問題就會 Pubish 到 Container Registry 存放囉～\n我們就簡單說明要怎麼查看我們的 Container Registry，點選則左側的 menu \u003e 選擇 Container Registry \u003e 點擊 映像檔：\nContainer Registry\n一開始可能沒有任何的資料夾，之後當我們 Build 時，可以新增特定的資料夾來放我們的映像檔，可以參考下方圖片：\nContainer Registry 資料夾\n進入該資料夾後，就可以看到裡面的映像檔案：\nContainer Registry 映像檔"},"title":"Google Cloud Platform (GCP) - Container Registry"},"/blog/gcp/gcp-lb-introduce/":{"data":{"":"","#":"最近公司在導入 Multi-Zone，有發現大量的跨域費用產生，主要是不同 Cluster 或是 Cluster 跟 VM 之間的跨域流量，與 Google TAM 討論，他們有提出可以嘗試用 Service load balancing policy 的 Waterfall by zone，或是之後會推出的 Zone affinity，這些都會需要使用到 Load Balancer，簡單整理一下發現 GCP 的 Load Balancer 其實有很多種，因此，這篇文章就來介紹一下 GCP 的 Load Balancer。 (如果 Service load balancing policy 測試有結果，也會再寫一篇文章來介紹)\nGCP 的 Load Balancer 主要分為三種，分別是：\nApplication Load Balancer (ALB) – 應用程式負載平衡器 層級：第 7 層（L7） 支援協定：HTTP、HTTPS 功能： 基於內容的路由（如 URL 路徑、主機名稱） SSL/TLS 終止 整合 Cloud CDN、Cloud Armor 支援全球負載平衡（Premium Tier） 適用場景：需要進行應用層路由、SSL 終止，以及全球流量分配的 Web 應用程式。 Application Load Balancer 又可以再另外分為五種 (包含內、外網以及不同的 Region)：\nGlobal external Application Load Balancer 將此負載平衡器用於具有全球分散用戶或多個區域的後端服務的外部 HTTP(S) 工作負載。 (官方建議使用)\n資訊 中文：全球外部應用程式負載平衡器 縮寫：GLB 內部/外部：Public facing (external) 區域：Global Load balancing scheme：EXTERNAL_MANAGED 是否支援 Cloud CDN：✅ 是否支援 Cloud Armor：✅ 是否支援 Service load balancing policy：✅ 是否支援 SSL：✅ 是否支援 PROXY protocol：❌ 特點 與 GKE 相容，使用閘道或獨立 NEG 支援先進的流量管理 只能使用 Premium 等級的 Network Service 可以跨多個專案以及區域來存取後端 Global external Application Load Balancer\nClassic Application Load Balancer 此負載平衡器在高階層中是全球性的。在 Premium 網路服務層，此負載平衡器提供多區域負載平衡，嘗試將流量引導至具有容量的最近的健康後端，並儘可能靠近使用者終止 HTTP(S) 流量。\n在 Standard 網路服務層中，此負載平衡器只能將流量分配到單一區域內的後端。(建議不要再使用該 LB)\n資訊 中文：經典應用程式負載平衡器 縮寫：CLB 內部/外部：Public facing (external) 區域：Global Load balancing scheme：EXTERNAL 是否支援 Cloud CDN：✅ 是否支援 Cloud Armor：✅ 是否支援 Service load balancing policy：❌ 是否支援 SSL：✅ 是否支援 PROXY protocol：❌ 特點 與 GKE 相容，使用閘道、Ingress 或獨立 NEG 可以選擇 Standard 或是 Premium 等級的 Network Service 可以跨多個區域來存取後端 (選擇 Standrad 不能用) 有支援 Cloud CDN，但選擇 Standrad 則不能用 Classic Application Load Balancer\nRegional external Application Load Balancer 此負載平衡器包含現有的經典應用程式負載平衡器， 以及先進的流量管理能力。 如果您只想從一個地理位置提供內容，請使用此負載平衡器。\n資訊 中文：區域外部應用程式負載平衡器 縮寫：ALB 內部/外部：Public facing (external) 區域：Regional Load balancing scheme：EXTERNAL_MANAGED 是否支援 Cloud CDN：❌ 是否支援 Cloud Armor：✅ 是否支援 Service load balancing policy：❌ 是否支援 SSL：✅ 是否支援 PROXY protocol：❌ 特點 與 GKE 相容，使用獨立 NEG 支援先進的流量管理 可以選擇 Standard 或是 Premium 等級的 Network Service 可以跨多個專案，但只能選擇該區域的資源 (且沒辦法接 Bucket) Regional external Application Load Balancer\nCross-Region internal Application Load Balancer 這是一個多區域負載平衡器，它基於開源 Envoy 代理實現為託管服務。跨區域模式使您能夠將流量負載平衡到全球分佈的後端服務，包括確保流量定向到最近的後端的流量管理。此負載平衡器還具有高可用性。將後端放置在多個區域有助於避免單一區域故障。如果一個區域的後端發生故障，流量可以轉移到另一個區域。\n資訊 中文：跨區域內部應用程式負載平衡器 縮寫：Cross-Region internal ALB 內部/外部：Internal 區域：Global Load balancing scheme：INTERNAL_MANAGED 是否支援 Cloud CDN：❌ 是否支援 Cloud Armor：❌ 是否支援 Service load balancing policy：✅ 是否支援 SSL：✅ 是否支援 PROXY protocol：❌ 特點 始終可在全球範圍內訪問。 VPC 中任何 Google Cloud 區域的用戶端都可以將流量傳送到負載平衡器 負載平衡器可以將流量傳送到任何區域的後端 (可以跨多個專案) 自動故障轉移到同一或不同區域的健康後端 Cross-Region internal Application Load Balancer\nRegional internal Application Load Balancer 這是一個區域負載平衡器，它基於開源 Envoy 代理程式作為託管服務實作。區域模式可確保所有用戶端和後端都來自指定區域，這在您需要區域合規性時很有幫助。此負載平衡器具備基於 HTTP(S)參數的豐富流量控制功能。負載平衡器配置完成後，它會自動指派 Envoy 代理程式來滿足您的流量需求。\n資訊 中文：區域內部應用程式負載平衡器 縮寫：ILB 內部/外部：Internal 區域：Regional Load balancing scheme：INTERNAL_MANAGED 是否支援 Cloud CDN：❌ 是否支援 Cloud Armor：✅ 是否支援 Service load balancing policy：❌ 是否支援 SSL：✅ 是否支援 PROXY protocol：❌ 特點 預設無法由不同 Region 進行訪問，需要額外開啟全球訪問設定 負載平衡器只能將流量傳送到與負載平衡器的代理程式位於相同區域的後端 (可以跨多個專案) 自動故障轉移到同一區域內的健康後端 Regional internal Application Load Balancer\nProxy Network Load Balancer (PNLB) – 代理網路負載平衡器 層級：第 4 層（L4） 支援協定：TCP、SSL 功能： 作為反向代理，終止 TCP 或 SSL 連線 支援 SSL/TLS 終止（僅限 SSL Proxy 模式） 可選擇全球（Premium Tier）或區域性（Standard Tier）部署 適用場景：需要處理加密的 TCP 流量，並在負載平衡器層級終止 SSL 的應用程式。 Proxy Network Load Balancer 又可以再另外分為五種 (包含內、外網以及不同的 Region)：\nGlobal external Proxy Network Load Balancer 此負載平衡器適用於需要全球可用性和高效能的 TCP/SSL 應用程式。它支援使用 Zonal NEGs（包括 VM 和 GKE Pod）作為後端，並整合 Google Cloud Armor 進行安全防護。\n資訊 中文：全球外部代理網路負載平衡器 縮寫：Global Proxy NLB 內部/外部：Public facing (external) 區域：Global Load balancing scheme：EXTERNAL_MANAGED 是否支援 Cloud CDN：❌ 是否支援 Cloud Armor：✅ 是否支援 Service load balancing policy：✅ 是否支援 SSL：✅ 是否支援 PROXY protocol：✅ 特點 只能使用 Premium 等級的 Network Service 支援 TCP 和 SSL 協定 支援 SSL/TLS 卸載 支援使用 Zonal NEGs（包括 VM 和 GKE Pod）作為後端 整合 Google Cloud Armor 進行 DDoS 防護 Global external Proxy Network Load Balancer\nClassic Proxy Network Load Balancer 此負載平衡器適用於現有使用傳統實例群組（Instance Groups）的應用程式。在 Premium 網路服務層中，它可以作為全球性的負載平衡器；在 Standard 網路服務層中，僅限於區域性部署。它支援 TCP 和 SSL 協定，並支援 SSL/TLS 卸載。\n資訊 中文：經典代理網路負載平衡器 縮寫：Classic Proxy NLB 內部/外部：Public facing (external) 區域：Global Load balancing scheme：EXTERNAL 是否支援 Cloud CDN：❌ 是否支援 Cloud Armor：✅ 是否支援 Service load balancing policy：❌ 是否支援 SSL：✅ 是否支援 PROXY protocol：✅ 特點 可以選擇 Standard 或是 Premium 等級的 Network Service 支援 TCP 和 SSL 協定 支援 SSL/TLS 卸載 支援使用傳統實例群組作為後端 在 Premium Tier 中可作為全球性負載平衡器，在 Standard Tier 中僅限於區域性部署 Classic Proxy Network Load Balancer\nRegional external Proxy Network Load Balancer 此負載平衡器適用於需要在單一區域內處理 TCP 流量的應用程式。它在該區域內提供外部 IP，並將進來的 TCP 流量轉發至後端服務。此負載平衡器支援 TCP 協定，並可選擇使用 Premium 或 Standard 網路服務層級。\n資訊 中文：區域外部代理網路負載平衡器 縮寫：Regional Proxy NLB 內部/外部：Public facing (external) 區域：Regional Load balancing scheme：EXTERNAL_MANAGED 是否支援 Cloud CDN：❌ 是否支援 Cloud Armor：❌ 是否支援 Service load balancing policy：❌ 是否支援 SSL：❌ 是否支援 PROXY protocol：✅ 特點 可以選擇 Standard 或是 Premium 等級的 Network Service 支援 TCP 協定 支援使用 Compute Engine 虛擬機（VM）作為後端服務 相較於全球性負載平衡器，區域性負載平衡器的成本較低，適合預算有限的應用程式 Regional external Proxy Network Load Balancer\nCross-Region internal Proxy Network Load Balancer 這是一個多區域負載平衡器，它基於開源 Envoy 代理實現為託管服務。跨區域模式可讓您將流量負載平衡到全球分佈的後端服務，包括確保流量定向到最近的後端的流量管理。此負載平衡器還具有高可用性。將後端放置在多個區域有助於避免單一區域故障。如果一個區域的後端發生故障，流量可以轉移到另一個區域。\n資訊 中文：跨區域內部代理網路負載平衡器 縮寫：Cross-Region internal Proxy NLB 內部/外部：Internal 區域：Global Load balancing scheme：INTERNAL_MANAGED 是否支援 Cloud CDN：❌ 是否支援 Cloud Armor：❌ 是否支援 Service load balancing policy：✅ 是否支援 SSL：❌ 是否支援 PROXY protocol：✅ 特點 始終可在全球範圍內訪問。 VPC 中任何 Google Cloud 區域的用戶端都可以將流量傳送到負載平衡器 負載平衡器可以將流量傳送到任何區域的後端 自動故障轉移到同一或不同區域的健康後端 Cross-Region internal Proxy Network Load Balancer\nRegional internal Proxy Network Load Balancer 這是一個區域負載平衡器，它基於開源 Envoy 代理程式作為託管服務實作。區域模式可確保所有用戶端和後端都來自指定區域，這在您需要區域合規性時很有幫助。\n資訊 中文：區域內部代理網路負載平衡器 縮寫：Regional internal Proxy NLB 內部/外部：Internal 區域：Regional Load balancing scheme：INTERNAL_MANAGED 是否支援 Cloud CDN：❌ 是否支援 Cloud Armor：❌ 是否支援 Service load balancing policy：❌ 是否支援 SSL：❌ 是否支援 PROXY protocol：✅ 特點 預設無法由不同 Region 進行訪問，需要額外開啟全球訪問設定 負載平衡器只能將流量傳送到與負載平衡器的代理程式位於相同區域的後端 自動故障轉移到同一區域內的健康後端 Regional internal Proxy Network Load Balancer\nPassthrough Network Load Balancer (NLB) – 直通網路負載平衡器 層級：第 4 層（L4） 支援協定：TCP、UDP、ESP、GRE、ICMP、ICMPv6 等 功能： 不終止連線，將流量直接傳遞給後端 保留原始封包資訊（來源 IP、目的地 IP 等） 僅支援區域性部署 適用場景：需要低延遲、高效能，並保留原始封包資訊的內部服務，如資料庫、內部微服務通訊等。 Passthrough Network Load Balancer 主要分為兩種 (內、外網)：\nExternal passthrough Network Load Balancer 這是一種區域性的第 4 層（L4）負載平衡器，將來自網際網路的流量分配至同一區域內的後端服務。它不進行代理或 SSL/TLS 卸載，並保留原始的客戶端 IP 位址。\n資訊 中文：外部直通網路負載平衡器 縮寫：External NLB 內部/外部：Public facing (external) 區域：Regional Load balancing scheme：EXTERNAL 是否支援 Cloud CDN：❌ 是否支援 Cloud Armor：✅ 是否支援 Service load balancing policy：✅ 是否支援 SSL：❌ 是否支援 PROXY protocol：❌ 特點 支援 TCP、UDP、ESP、GRE、ICMP 和 ICMPv6 等協定 保留原始客戶端 IP 位址，適用於需要此資訊的應用程式 支援使用區域性的後端服務或目標集（Target Pool）作為後端 可與 Google Cloud Armor 整合，提供進階的 DDoS 防護 支援 IPv4 和 IPv6 流量 適用於需要低延遲和高效能的應用場景 External passthrough Network Load Balancer\nInternal passthrough Network Load Balancer 這是一種區域性的第 4 層（L4）負載平衡器，將流量分配至同一 VPC 網路內的後端服務。它僅在內部網路中運作，適用於內部服務之間的通訊。\n資訊 中文：內部直通網路負載平衡器 縮寫：Internal NLB 內部/外部：Internal 區域：Regional Load balancing scheme：INTERNAL 是否支援 Cloud CDN：❌ 是否支援 Cloud Armor：❌ 是否支援 Service load balancing policy：❌ 是否支援 SSL：❌ 是否支援 PROXY protocol：❌ 特點 支援 TCP、UDP、ICMP、ICMPv6、SCTP、ESP、AH 和 GRE 等協定 保留原始客戶端 IP 位址，適用於需要此資訊的內部應用程式 支援使用區域性的後端服務作為後端 可作為靜態路由的下一跳，實現更靈活的流量控制 支援與 Service Directory 整合，方便服務發現與管理 適用於需要高效能和低延遲的內部服務通訊 Internal passthrough Network Load Balancer","參考資料#參考資料":"Network Service Tiers：https://cloud.google.com/network-tiers?hl=zh-tw\nExternal Application Load Balancer：https://cloud.google.com/load-balancing/docs/https\nInternal Application Load Balancer：https://cloud.google.com/load-balancing/docs/l7-internal\nProxy Network Load Balancer：https://cloud.google.com/load-balancing/docs/proxy-network-load-balancer\nInternal proxy Network Load Balancer：https://cloud.google.com/load-balancing/docs/tcp/internal-proxy\nPassthrough Network Load Balancer：https://cloud.google.com/load-balancing/docs/passthrough-network-load-balancer\nInternal passthrough Network Load Balancer：https://cloud.google.com/load-balancing/docs/internal","總結#總結":" 負載平衡器類型 層級 協定 功能 適用場景 Application Load Balancer (ALB) L7 HTTP, HTTPS 基於內容的路由、SSL/TLS 終止、整合 Cloud CDN 和 Cloud Armor Web 應用程式需要應用層路由和 SSL 終止 Proxy Network Load Balancer (PNLB) L4 TCP, SSL 反向代理、SSL/TLS 終止 處理加密的 TCP 流量，並在負載平衡器層級終止 SSL 的應用程式 Passthrough Network Load Balancer (NLB) L4 TCP, UDP, ESP, GRE, ICMP 等 不終止連線，保留原始封包資訊 低延遲、高效能的內部服務，如資料庫、內部微服務通訊等 "},"title":"GCP Load Balancer 介紹"},"/blog/gcp/gcp-log-reduce-cloud-logging-api/":{"data":{"":"當我們使用 GCP 的 Cloud Logging 服務來查看 Log 時，有時候會有一些我們不需要顯示出來的，或是從來都不會去查詢的 Log，再者是 GCP 本身的錯誤導致大量噴錯的 Log ，這些 Log 都會導致 Cloud Logging 的費用增加。","介紹-cloud-logging#介紹 Cloud Logging":"先來簡單說一下 Cloud Logging 這項服務的基本架構，請看圖：\nCloud Logging 基本架構\n可以看到 Logs Data 會透過 API 再經過 _Default log sink (router) 存到相應命名的 log bucket (預設配置)，圖中 _Required 以及 _Default 的 log sink 都是 GCP 自動創建的接收器，下面簡述一下它們的區別：\n_Required 日誌儲存桶 Cloud Logging 會將以下類型的 Log 存到 _Required 儲存桶\n管理員活動審核 Log\n系統事件審核 Log\nAccess Transparency Log\nCloud Logging 會將 _Required 儲存桶 Log 保留 400 天，無法調整該期限，且無法修改或刪除 _Required 儲存桶，也沒辦法停用 _Required log sink 接受器路由到 _Required 儲存桶的設定。\n_Default 日誌儲存桶 只要不是存在 _Required 日誌儲存桶的 Log 就會透過 _Default log sink 接受器路由到 _Default 儲存桶。\n除非有另外配置自定義設定， 否則 _Default 日誌儲存桶 Log 只會保留 30 天，也一樣無法刪除 _Default 日誌儲存桶。此外 Cloud Logging 的費用是以存在 _Default 日誌儲存桶來計算。\n功能 價格 每月免費額度 Logging 提取 提取的 Log $0.5/GiB 每個項目前 50 GiB Logging 儲存 保留超過 30 天的 Log，每月每 GiB $0.01 在默認保留期限的 Log 不會有額外儲存費用 查看該專案使用的 Cloud Logging API 費用：https://console.cloud.google.com/apis/api/logging.googleapis.com/cost","參考資料#參考資料":"Routing and storage overview https://cloud.google.com/logging/docs/routing/overview","過濾-log#過濾 Log":"那現在知道 Cloud Logging 的架構，那當我們遇到需要過濾 Log 時，我們可以使用以下步驟來過濾以節省 Cloud Logging API 費用：\n範例說明 這次範例是 google 在 2023/07/06 所發佈的 Service Health，當 GKE 版本大於 1.24 以上，就會噴\nFailed to get record: decoder: failed to decode payload: EOF\ncannot parse ‘0609 05:31:59.491654’ after %L\ninvalid time format %m%d %H:%M:%S.%L%z for ‘0609 05:31:59.490647’\n這三種類型的錯誤 Log，在等待官方修復前，官方的建議是先將他給過濾掉，避免一直刷噴錢 ┐(´д`)┌\n附上當時的 Service Health 連結：https://status.cloud.google.com/incidents/y5XvpyBXFhsphSt4DfiE\n我們在上面架構圖有說到，Log 會透過 log sink 路由到 bucket，所以我們要將過濾條件加在 log router 上：\n選擇 Log Router 選擇 Log Router\n選擇 Log Router Sinks 選擇 _Default 的 Log Router Sinks，點選右邊按鈕的 Edit Sink\n選擇 _Default 的 Log Router Sinks\n設定 Sink details 第一個是 details，可以輸入說明，這邊輸入：google 有 bug 會噴大量的意外 LOG，怕費用飆高，先用官方建議的來過濾 LOG，詳細可以看： https://status.cloud.google.com/incidents/y5XvpyBXFhsphSt4DfiE\n輸入 Sink details 說明\n選擇 Sink Service 跟 Bucket 接著 sink 服務選擇 Logging bucket，以及對應儲存的 log bucket (這邊基本上都是預設)\n選擇 Logging bucket\n設定 include Log 選擇那些可以被 include 到 sink 接收器的 LOG 格式 (這邊基本上都是預設)\n預設的 LOG 格式\n設定 filter Log 這邊就是我們要輸入過濾的地方，先點擊 ADD EXCLUSION，輸入過濾的名稱，以及過濾的內容格式，我們輸入 google 在 Service Health 所提供的格式，最後按下 UPDATE SINK\n新增要過濾的 LOG 格式\n設定完成 等待更新完成，就可以看到我們已經在接收器上設定好過濾條件囉～\n查看詳細接收器設定\n檢查 Log 是否過濾成功 最後再檢查一下 Log 是不是沒有收到該錯誤的 Log 內容\n檢查 LOG 是否不會再出現"},"title":"如何過濾 GCP LOG，減少 Cloud Logging API 的花費"},"/blog/gcp/gcp-memorystore-failover/":{"data":{"":"此文章主要針對 GCP Memorystore failover 來做測試，測試 Memorystore 高可用性 (HA) 在標準 Standard Tier failover 故障轉移需要多久，以及不同 replica 條件下轉移是否會有差異等等。","參考資料#參考資料":"Memorystore for Redis FAQ：https://cloud.google.com/memorystore/docs/redis/faq\nHigh availability for Memorystore for Redis：https://cloud.google.com/memorystore/docs/redis/high-availability-for-memorystore-for-redis#when_a_failover_is_triggered\nAbout manual failover：https://cloud.google.com/memorystore/docs/redis/about-manual-failover#stackdriver_verification\nInitiate a manual failover：https://cloud.google.com/memorystore/docs/redis/initiate-manual-failover\nGeneral best practices：https://cloud.google.com/memorystore/docs/redis/general-best-practices\nExponential backoff：https://cloud.google.com/memorystore/docs/redis/exponential-backoff","實作測試-1m1s#實作測試 (1m1s)":"可以只用之前 IaC 文章來建立，或是用 UI 來建立 Memorystore，這邊用 IaC 來示範：\nterraform { source = \"${get_path_to_repo_root()}/modules/memorystore\" } include { path = find_in_parent_folders() } inputs = { name = \"ian-1m1s\" memory_size_gb = 5 region = \"asia-east1\" network = \"bbin-XXX\" replica_count = 1 redis_version = \"REDIS_6_X\" redis_configs = { \"maxmemory-policy\" = \"allkeys-lru\" } } 這邊 memorystore module 預設是 Standard Tier，另外小提醒，如果要使用 Replica memory_size_gb 最小必須是 5G。\n設定監控 建立完成後，我們也先把監控給拉出來，設定可以參考 About manual failover，主要是選擇 Cloud Memorystore Redis \u003e replication \u003e Node role\n監控圖表\n可以看到 node-0 跟 node-1，數字 1 代表 Primary (node-0)、數字 0 代表 Replica (node-1)\n查看 describe 我們也可以下指令查看：\ngcloud redis instances describe ian-1m1s --region=asia-east1 --project={project_id} describe 畫面\n可以看到 currentLocationId 代表現在 Primary (node-0) asia-east1-a，locationId 代表最初配置 Primary 區域，alternativeLocationId 則是最初配置 Replica 的區域，因此我們也可以在故障轉移後，來查看區域是否變化。詳細可以參考：gcloud verification\n接著我們可以參考 Initiate a manual failover 文章，手動來觸發 failover，這邊提醒一下 手動 failover 有分兩種資料保護模式：limited-data-loss、force-data-loss，詳細請看 Optional data protection mode，這邊測試就不討論兩種保護模式，都使用預設的 limited-data-loss 來測試。\n寫 shell 測試 redis 連線 另外，我們在寫一個 shell 到 GKE 裡面 (用同 VPC 打) 打 Memorystore Redis 的 Primary Endpoint 以及 Read Endpoint，看看當 failover 是不是會有連不上的問題 ～\napiVersion: v1 kind: Pod metadata: name: redis-test namespace: default spec: initContainers: - name: copy-scripts image: busybox command: [ \"sh\", \"-c\", \"cp /config/redis.sh /scripts/ \u0026\u0026 chmod +x /scripts/redis.sh\", ] volumeMounts: - name: config-volume mountPath: /config - name: scripts mountPath: /scripts containers: - name: redis-cli image: redis:alpine command: [\"sh\", \"-c\", \"while true; do /scripts/redis.sh; sleep 1; done\"] env: - name: READ_PRIMARY_HOST value: \"172.18.0.69\" - name: READ_REPLICA_HOST value: \"172.18.0.68\" - name: REDIS_PASSWORD value: \"XXXX\" volumeMounts: - name: scripts mountPath: /scripts workingDir: /scripts volumes: - name: config-volume configMap: name: redis-scripts - name: scripts emptyDir: {} --- apiVersion: v1 kind: ConfigMap metadata: name: redis-scripts namespace: default data: redis.sh: | #!/bin/sh ## 顏色設定 GREEN=\"\\033[1;32m\" RED=\"\\033[1;31m\" NC=\"\\033[0m\" READ_PRIMARY_HOST=${READ_PRIMARY_HOST} READ_REPLICA_HOST=${READ_REPLICA_HOST} READ_REPLICA_PORT=6379 REDIS_PASSWORD=${REDIS_PASSWORD} while true; do TIMESTAMP=$(TZ=\"Asia/Taipei\" date +\"%Y-%m-%d %H:%M:%S\") PRIMARY_OUTPUT=$(timeout 1 redis-cli -h $READ_PRIMARY_HOST -p $READ_REPLICA_PORT -a $REDIS_PASSWORD --no-auth-warning PING) 1\u003e/dev/null if [ $? -eq 0 ]; then echo -e \"[$TIMESTAMP] PRIMARY PING ${GREEN}SUCCESS${NC}：\" \"$PRIMARY_OUTPUT\" else echo -e \"[$TIMESTAMP] PRIMARY PING ${RED}FAILED${NC}\" fi REPLICA_OUTPUT=$(timeout 1 redis-cli -h $READ_REPLICA_HOST -p $READ_REPLICA_PORT -a $REDIS_PASSWORD --no-auth-warning PING) 1\u003e/dev/null if [ $? -eq 0 ]; then echo -e \"[$TIMESTAMP] REPLICA PING ${GREEN}SUCCESS${NC}：\" \"$REPLICA_OUTPUT\" else echo -e \"[$TIMESTAMP] REPLICA PING ${RED}FAILED${NC}\" fi sleep 1 done 開始手動觸發 failover 我們準備好監控以及也先看好 describe 後，就可以下指令來手動觸發 failover：\ngcloud redis instances failover ian-1m1s --data-protection-mode=limited-data-loss --region=asia-east1 --project={project_id} 可以看到目前 ian-1m1s 就開始 Failing over 了～\nmemorystore UI 畫面","實作測試-1m2s#實作測試 (1m2s)":"由於設定都差不多，所以上述 1m1s 設定這邊就不重複說明，記得把 IaC 的 replica_count 改成 2，以及 shell 的 IP 跟 Password 要記得換！\n直接看監控部分：可以看到現在 node-2 是 1，代表 node-2 是 Primary，其他 node-1、node-0 都是 Replica。\n監控畫面\n接著看 describe：currentLocationId 是 asia-east1-c，也就是 node-2 (Primary)。\ndescribe 畫面\n一樣下指令來跑 shell，並觀察監控、describe 來有 shell： 變成 node-0 變成 Primary，node-2 變成 Replica，node-1 一樣是 Replica。\n監控畫面\ncurrentLocationId 變成是 asia-east1-a，Primary 從 node-2 變成 node-0。\ndescribe 畫面\n觀察 shell 發現，有出現 FAILED 都是 Primary，Replica 都正常。\nshell 畫面","後續觀察#後續觀察":"查看監控 首先我們先看監控部分，可以看到原本的 node-1 是 Primary 後來掉下來的同時，由 node-2 變成 Primary 最後完成故障轉移。\n監控畫面\n再來看 describe 的部分，可以發現原本的 currentLocationId 從 Primary (node-0) asia-east1-a 變成(node-1) asia-east1-b，完成故障轉移，因此更換區域。\ndescribe 畫面\n查看腳本 最後，查看一下腳本的執行紀錄可以發現：在 18:12:58， 出現第一筆 Primary PING 不到的錯誤，到 18:13:18 才恢復，代表故障轉移時間約 20 左右 (最後面會測試 5 次來取平均)，其流程是 Primary 會先連不到，接著變成 Primary 跟 Replica 都連不到，最後剩下 Replica，到都正常。\nshell 畫面","整理故障轉移時間#整理故障轉移時間":" 測測次數 1m1s (秒) 1m2s (秒) #1 16 13 #2 19 10 #3 22 6 #4 20 10 #5 18 9 可以發現，多一個 Replica，在故障轉移的時間可以更快速。","文件說明#文件說明":"首先我們可以先閱讀 Google Memorystore for Redis FAQ 文章，可以得知 Standard Tier 在 failover 轉移，大約會花 30 秒左右。\nGoogle Memorystore for Redis FAQ\n我們也可以從 High availability for Memorystore for Redis 文章中知道，當 Primary 資料庫發生故障時，就會發生故障轉移。在轉移期間，Primary Endpoint 和 Read Endpoint 會自動重新導向新的 Primary 資料庫 和 Replica。這時候 Primary Endpoint 和 Read Endpoint 連線都會被刪除。\nwhen a failover is triggered\n因次會導致有幾秒鐘連不到 Redis。重新連接時，將使用原本的 IP 位址即可自動重新定向到新的服務上。故障轉移後，不需要調整連線設定。\n接下來我們來實際測試看看，我們分成兩個測試組，分別為：1m1s、1m2s 的方式，來看看故障轉移時，需要花多久，還有不同 Replica 條件下會不會有差異。","最佳實踐#最佳實踐":"所以我們知道，每當發生 Failover 時，一定會出現斷線的問題，官方在 Memorystore for Redis 的 General best practices 中有提到需要重新連線的操作和場景，以下都會導致與 Redis instance 網路連線中斷：\nVersion upgrade\nScaling up/down\nImporting\nManual failover\nSystem maintenance\nCertificate Authority rotation for Redis instances that have in-transit encryption enabled\nEmergency failover\n所以我們需要再設計應用程式時，考量到這一點，可以參考 Exponential backoff ，要加上重試邏輯，讓應用程式自動重新連線並持續正常運作。"},"title":"GCP Memorystore HA 高可用性 failover 測試"},"/blog/gcp/gcp-prometheus-sample-ingested-calculate/":{"data":{"":"最近在優化公司帳單費用，發現公司有幾個 project 裡面的 Prometheus Samples Ingested 費用很高，此文章會針對費用如何計算以及如何去減少費用來做說明。\nPrometheus Samples Ingested 就是 Prometheus 攝取的樣本數，因為上述幾個專案，我們都是使用 Google Managed Prometheus (GMP) 的方式來接 Metrics，所以可以先推測是 Metrics 導致此費用增加。\nGoogle 帳單 (project_1)\nGoogle 帳單 (project_2)","如何減少花費#如何減少花費":"我們可以從上面的表中知道，是 phpfpm_process_state/gauge 這個 metric，總共收了 30.71 B 也就是大約 1842.60 美元的花費是裡面最高的。所以我們就先這對如何減少這個 metric 來做說明：\n要先知道 phpfpm_process_state/gauge 這個 metric 是怎麼來的呢？\n首先我們可以從 metric 名稱知道它是 phpfpm 的 process 狀態。我們有開啟 Cluster 的 Managed Service for Prometheus，來使用 Google 管理的 Prometheus (GMP)， 並在服務上面設定 phpfpm-exporter，並使用 PodMonitoring 來將 Pod 上的 metrics 接到 GMP 上。\n所以我們可以先到有 phpfpm-exporter 的 Pod 隨意的 Container 去看看這個 metrics，下 curl -s 127.0.0.1:9253/metrics 指令來查看 (請依照設定 port 去查看，這邊不詳細列出)，可以看到會有很多 phpfpm-exporter 的 metrics。\nphpfpm-exporter metrics\n其中我們也可以找到 phpfpm_process_state 這個 metrics，這邊我們發現一個比較特別的事情，一般的 metrics 都只有一筆，但 phpfpm_process_state 它有 6 個狀態。\nphpfpm-exporter metrics\n所以代表送到 GMP 的 metrics 也會比其他 phpfpm 相關的 metrics 還多 6 倍，這也可以解釋為什麼他是所有 metrics 裡面費用最高的，以及 phpfpm_process_state 費用是其他 phpfpm 像是：phpfpm_process_request_duration 的 6 倍了。\nMetrics Management phpfpm metrics\n因此我們可以來計算一下費用，phpfpm_process_XXX 的 metrics 它會因為 process 的數量而改變，以 phpfpm_process_state 這個 metrics 來計算，計算公式就是：\nprocess 數量 * 6 (上面說的6個狀態) * Pod 數量 * PodMonitoring interval 的秒數\n(PodMonitoring interval 的秒數這個我們後續再說)\n可以先看下圖，我們這個的 api 有 30 的 process，乘 6 個狀態，可以看到每次的搜集就會收集 180 筆的 phpfpm_process_state。\nphpfpm status\n我們也可以透過 GCP 的 Metrics explorer 查詢到對應的數值。\nphpfpm status\n剛剛提到的 PodMonitoring interval 的秒數，在 project_1 的 PodMonitoring 設定間隔是 60s，所以上面才不需要另外乘，但像是 project_1 的秒數都是 5s，所以我們還需要再多乘上 12 (60/5) 才是我們在 GCP 的 Metrics explorer 看到每分鐘的數量。\n因此，我們可以得知費用會跟 process (範例的 metrics)、裡面的 metrics 有幾個、Pod 數量、 PodMonitoring 間隔時間有關，跟 Google 的文件說明也是一樣的\nhttps://cloud.google.com/stackdriver/pricing?hl=zh-tw#pricing_examples_samples\n所以要減少費用，可以調整上述影響 metrics 數量的變因，另一種方式就是，針對需要收集哪些 metrics 來做過濾，可以參考此文件：Get started with managed collection，過濾不要的 metrics ，避免送到 GMP 上，而有多餘的費用，我們可以在 PodMonitoring 上去來過濾要送出 metrics，如下：\napiVersion: monitoring.googleapis.com/v1 kind: PodMonitoring metadata: name: {{ $.Values.deployment.name }} namespace: {{ $.Release.Namespace }} spec: selector: matchLabels: app: {{ $.Values.deployment.name }} endpoints: - port: metrics interval: {{ .endpoints.metrics_interval_sec }} metricRelabeling: - action: drop regex: phpfpm_process_state sourceLabels: [__name__] 透過 metricRelabeling 使用 drop 方式，搭配正規表示法來過濾 name 是 phpfpm_process_state 的 metrics，另外也可以使用 keep 的方式，決定要送什麼到 GMP 上。\n過濾後，指標不見惹\n上圖就是在 PodMonitoring 上新增 drop 來過濾 phpfpm_process_state，也可以看 Samples billable volume 變成 0 Sample，就代表我們把這個 metrics 給過濾掉拉～～～ 也就可以省錢囉xDD\nMetrics Management phpfpm metrics","費用計算說明#費用計算說明":"首先我們先來計算一下費用是不是正確，Prometheus Samples Ingested 的 SKU 是 A4E4-DF03-CDB6，可以透過這個頁面來查詢：\nGoogle Cloud Platform SKUs\n我們可以看到如果是 0 ~ 50,000,000,000 的 Samples，會以每 1,000,000/0.06 USD 來計算，其他以此類推，為了更方便的計算，我有簡單寫一個腳本來計算費用：\ncalc_metrics.sh #! /bin/bash if [[ $# -ne 1 ]]; then echo \"請使用： $0 \u003csamples\u003e 來計算費用\" exit 1 fi if [[ $1 =~ [a-zA-Z] ]]; then number=$(echo \"$1\" | sed -E 's/([0-9]+(\\.[0-9]+)?)\\s*[a-zA-Z]+/\\1/') unit=$(echo \"$1\" | sed -E 's/[0-9]+(\\.[0-9]+)?\\s*([a-zA-Z]+)/\\2/') case $unit in K) factor=1000 ;; M) factor=1000000 ;; B) factor=1000000000 ;; *) echo \"未知單位: $unit\" exit 1 ;; esac result=$(printf \"%'d\" $(printf \"%.0f\" $(echo \"$number * $factor\" | bc))) else result=$1 fi echo \"Samples：${result}\" result=$(echo \"${result}\" | tr -d ',') if [[ ${result} -le 50000000000 ]]; then cost=$(echo \"scale=2; ${result} * 0.06 / 1000000\" | bc) elif [[ ${result} -le 250000000000 ]]; then cost=$(echo \"scale=2; ${result} * 0.048 / 1000000\" | bc) elif [[ ${result} -le 500000000000 ]]; then cost=$(echo \"scale=2; ${result} * 0.036 / 1000000\" | bc) else cost=$(echo \"scale=2; ${result} * 0.024 / 1000000\" | bc) fi echo \"費用：$cost USD\" 我們分別帶入 project_1 (106,676,274,756) 以及 project_2 (66,631,967,760) 的 Samples 來計算看看金額是否正確。\n用腳本檢查金額是否與帳單一樣\n計算完與實際的帳單費用差不多，接著我們可以打開 GCP 的 Metrics Management 來查看一下，我們用了哪些 Metrics 導致費用這麼高。\n以下以 project_1 為例：\n打開 Metrics Management 後，將時間選擇前 30 天(30d)，與上面帳單選擇一樣，接著可以看到 Billable samples ingested 這邊，這裡就是指我們 30 天總共收了多少筆的 samples，也可以把他理解收了多少筆的 Metrics。也可以看到底下表格的 Samples billable volume 可以透過排序知道是誰使用最多。這邊的 B 代表 10 億，也就是 1000000000，所以我們上個月收了 1066 多億筆的 samples。\n查看 Metrics Management\n可以使用剛剛的腳本來計算 (沒錯，它也支援數字單位的轉換 xD)，計算如下：\n查看 Metrics Management\n所以可以得知，帳單 Prometheus Samples Ingested 這個 SKU 會這麼高就是這邊的 Billable samples ingested 所導致，那我們現在可以依照下面的表格 Metrics 知道是哪個 Metrics 花錢最兇，在針對這些 Metrics 來進行調整，以達到減少費用的目的。"},"title":"GCP Prometheus Samples Ingested 計算方式及如何減少費用"},"/blog/gcp/gcs-cors/":{"data":{"":"最近公司有需求需要透過前端去打 GCS Bucket 的檔案，但會遇到 CORS 錯誤，所以寫一篇來記錄此問題的解決方法。\n有先簡單寫一個前端頁面，可以透過 Axios 去打後端，詳細程式可以點我查看，我們也另外建立一個公開的 GCS Bucket，並放一個測試用的 JSON 檔案 (都有附在此專案中)。\n公開的 GCS Bucket URL 及內容 (bucket 會用 ian-test-demo 來示範，請自行修改成自己的 bucket 名稱) curl 測試\n使用測試程式需要先做以下步驟：\n執行 cd code; docker build -t gcs-cors-test .\n執行 docker run -d -p 8080:80 --name gcs-cors-test gcs-cors-test\n開啟瀏覽器 127.0.0.1:8080\n開啟後，我們輸入剛剛公開 GCS Bucket URL 到輸入欄位，開啟 F12 Network，並按下測試\n使用程式來測試\n就會發現，出現 CORS error 錯誤，我們可以查看下方的錯誤說明\n發現 CORS 錯誤\n因為我們從 http://localhost:8080 要打到 https://storage.googleapis.com/ian-test-demo/hello.json ，觸發了瀏覽器的 CORS 限制，所以導致噴錯，CORS 的說明詳細可以直接參考：https://www.explainthis.io/zh-hant/swe/what-is-cors\n那要怎麼解決呢，根據 Google 文件，我們需要為 Bucket 配置 cors 設定\n我們可以先下此指令來查看該 bucket 是否有設定 cors： gcloud storage buckets describe gs://ian-test-demo --format=\"default(cors_config)\"\ngcloud 指令查看 cors 設定\n如果還沒設定就會顯示 null\n那我們先來寫一下 cors 的設定檔案\n[ { \"origin\": [\"*\"], \"method\": [\"GET\", \"POST\", \"PUT\", \"DELETE\"], \"responseHeader\": [\"Content-Type\", \"Authorization\"], \"maxAgeSeconds\": 30 } ] (可以再根據自己需求去調整，先以這樣來 demo)\n寫完後，我們需要設定到 Bucket 上面，可以用此指令將 cors.json 設定到指定的 Bucket 上：gcloud storage buckets update gs://ian-test-demo --cors-file=cors.json\n下完指令後，我們可以在用 describe 來確認是否設定完成，正常會如下顯示：\n設定及檢查\n接著我們就可以來測試看看是否還會碰到 CORS error 的問題了\n再次使用前端程式測試\n另外，如果不需要設定 CORS 時，可以用以下指令移除該 Bucket 的 CORS 設定：gcloud storage buckets update gs://ian-test-demo --clear-cors\n移除 CORS 設定","參考#參考":"Set up and view CORS configurations\nCORS configuration examples"},"title":"GCS Bucket CORS 錯誤解決方法"},"/blog/gcp/gke-cronjob-not-working/":{"data":{"":"前陣子公司建立在 Google Kubernetes Engine 叢集上的 CronJob 服務會有短暫時間沒有執行 Job。先前情提要一下，此 CronJob 的設定是每分鐘都會執行 (圖一)，所以理當來說 Log 應該要可以看到每分鐘都有此 CronJob 的紀錄，但有時候會發生 CronJob 短暫時間都沒有執行的狀況，找了一陣子都沒有找到原因，最後開支援單請 Google 那邊協助查看，終於找到原因拉 😍。那就跟我一起看一下發生的過程，以及 Google 幫我們找到的原因，以及要如何解決等等～\n(圖一) CronJob schedule 時間為每分鐘執行","參考資料#參考資料":"[1] Standard cluster upgrades：https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-upgrades\n[2] maintenance-windows-and-exclusions：https://cloud.google.com/kubernetes-engine/docs/concepts/maintenance-windows-and-exclusions","問題發生以及問題原因#問題發生以及問題原因":"我們使用 Google Cloud Platform 裡面的記錄功能，可以看到 (圖二)，在每分鐘執行的 Log 中，有短暫時間沒有執行 Job，但這個時間除了 CronJob 以外，其他的服務都是好的。\n(圖二) Google Cloud Platform 記錄有短暫沒有執行\n找了一陣子都沒有找到原因，於是我們開支援單請 Google 那邊協助查看，Google 那邊找了一陣子後終於找到原因拉！！！我們一起來看看吧 (圖三)\n(圖三) Google 支援單回覆\n就如同 Google 所說，使用提供的指令參數來查詢，叢集在該時段有發生 master control plane 的升級，如 (圖四)，再加上我們建的這個 cluster 是使用 zonal cluster (圖五)，所以叢集只有一個 control plane，當 control plane 在更新時，會無法部署新的 workload，導致該 CronJob 沒有執行 Job。參考資料 [1]\n(圖四) 發生 master control plane 的升級\n(圖五) 有問題的叢集位置類型\nGoogle 的建議是可以考慮使用另一個 regional cluster，讓 master node 在更新時不會只在單一地區，或是一樣使用舊的 zonal cluster，透過設定 Maintenance window 或者 Maintenance exclusions 來降低服務受到 workload 的影響。參考資料 [2]\n就算把 node_pool 裡面的自動升級給停掉，也沒有辦法解決此問題！因為此 master control plane (也就是 master node) 的升級，不是 worker node 的 node pool 升級，是由 GKE 負責維護的，所以他們會定期升級 control plane，也沒辦法停止此類的升級。\n若已經建立好 zonal cluster 後，想要改成 regional cluster ，是沒有辦法使用修改的方式，一定只能重建 cluster，所以大家在建立時要注意～","解決問題#解決問題":"最後我們選擇將叢集給整個重建，來確保 CronJob 不會有沒有執行到的狀況發生，重建叢集跟搬服務的過程很辛苦的 😰 希望大家不要發生 QQ，最後我們來看一下重建完後，在 master control plane 更新的時候，還會不會有 CronJob 沒有執行的情況發生。\n新叢集使用 regional cluster 來建立，在 2/1 也有 master control plane 的升級。 (圖六)\n(圖六) 發生 master control plane 的升級\n查看 CronJob 執行的紀錄可以發現並沒有 Job 沒有執行的情況發生。 (圖七)\n(圖七) 叢集位置類型\n(圖八) 新叢集位置類型"},"title":"Google Kubernetes Engine CronJob 會有短暫時間沒有執行 Job"},"/blog/gcp/gke-local-ephemeral-storage/":{"data":{"":"此文章主要針對 Google Kubernetes Engine Local ephemeral storage 的計算方式來做介紹。","local-ephemeral-storage-計算方式#Local ephemeral storage 計算方式":"根據官方文件 Local ephemeral storage reservation，我們可以知道 GKE 會為每個 node 提供本地的臨時儲存空間，當這個儲存空間在 node 故障刪除時，資料也會被一起遺失。\nGKE 會依照下列的方式計算本機預留的暫存空間，也就是被偷走的空間 /ᐠ .ᆺ. ᐟ\\ﾉ：\nEVICTION_THRESHOLD + SYSTEM_RESERVATION EVICTION_THRESHOLD 是驅逐閾值 ; SYSTEM_RESERVATION 是系統保留空間。\nEVICTION_THRESHOLD 驅逐閥值計算 在預設情況下，臨時儲存空間是由開機磁碟的大小來決定的，驅逐閾值是開機磁碟大小的 10%。\nEVICTION_THRESHOLD = 10% * BOOT_DISK_CAPACITY SYSTEM_RESERVATION 系統保留空間計算 系統保留空間也跟開機磁碟大小有關：\nSYSTEM_RESERVATION = Min(50% * BOOT_DISK_CAPACITY, 6GiB + 35% * BOOT_DISK_CAPACITY, 100 GiB) 會計算 50% * BOOT_DISK_CAPACITY、6GiB + 35% * BOOT_DISK_CAPACITY、 100 GiB 三者中的最小值。奇怪的計算方式 (*´･д･)?。\n計算可用空間 最後將開機磁碟大小減去 EVICTION_THRESHOLD 與 SYSTEM_RESERVATION 就可以得到可用空間：\nBOOT_DISK_CAPACITY - (EVICTION_THRESHOLD + SYSTEM_RESERVATION) 計算範例 根據上面的公式我們可以知道 被偷走的空間有多少，下面舉例兩個不同的開機磁碟大小來計算：\n開機磁碟大小 100GB 先計算 EVICTION_THRESHOLD：\nEVICTION_THRESHOLD = 10% * 100GB = 10GB 再計算 SYSTEM_RESERVATION：\nSYSTEM_RESERVATION = Min(50% * 100GB, 6GiB + 35% * 100GB, 100 GiB) = Min(50GB, 6GiB + 35GB, 100GB) = Min(50GB, 41GB, 100GB) = 41GB 接著將 EVICTION_THRESHOLD 與 SYSTEM_RESERVATION 相加：\nEVICTION_THRESHOLD + SYSTEM_RESERVATION = 10GB + 41GB = 51GB 就可以得出被偷走的空間為 51GB，代表我們建立 100GB 的開機磁碟時，實際上只有 49GB 空間可以使用。\n開機磁碟大小 300GB 先計算 EVICTION_THRESHOLD：\nEVICTION_THRESHOLD = 10% * 300GB = 30GB 再計算 SYSTEM_RESERVATION：\nSYSTEM_RESERVATION = Min(50% * 300GB, 6GiB + 35% * 300GB, 100 GiB) = Min(150GB, 6GiB + 105GB, 100GB) = Min(150GB, 111GB, 100GB) = 100GB 接著將 EVICTION_THRESHOLD 與 SYSTEM_RESERVATION 相加：\nEVICTION_THRESHOLD + SYSTEM_RESERVATION = 30GB + 100GB = 130GB 就可以得出被偷走的空間為 130GB，代表我們建立 300GB 的開機磁碟時，實際上只有 170GB 可以使用。\n了解計算方式後，那為什麼會出現 Local ephemeral storage 不足的問題呢？\n後來發現，因為我的多數服務都會使用 gcsfuse 掛載 GCS bucket，而 gcsfuse 預設 ephemeral storage request 會使用 5GB，所以以 100 GB 的開機磁碟，我一個 node 只要超過 9 個 Pod，就會導致 Local ephemeral storage 不足的問題。詳細就請參考：Configure resources for the sidecar container","前情提要#前情提要":"為什麼會突然在看這個主題呢？\n主要是這幾天在優化自己 GKE 的 Node Pool 規格時，原先是開 e2-standard-2 / 100GB 的 node，數量是 8 ~ 12 顆 node (透過自動擴展)，改成 n2-standard-4 / 100GB 的 node，數量是 6 ~ 10 顆 node (透過自動擴展)。\n發現明明 CPU/MEM 資源都夠且更多，但 node 數量卻還是跟原始規格一樣長到 9 顆，使用 Cloud logging 來查看 unschedulable，發現與 Local ephemeral storage 有關，因此就來了解一下 Local ephemeral storage 是如何計算的。\n我們在 Cloud Logging 用以下指令來查看為何 node 會需要長新的，而不是進到還有資源的 node 上：\nunschedulable severity=WARNING \"ephemeral-storage\" Cloud Logging 查詢結果\n可以看到是因為 Local ephemeral storage 不足，導致無法排程，才會長新的 node。\n但這個很奇怪，我們明明建立了 100GB 的 node，為什麼會不足呢？而且我們還是開 spot instance，理論上 node 會被收回，node disk 也會被清空，所以不應該會出現這個問題才對。\n因此我們就先來了解 Local ephemeral storage 是如何計算的。","參考資料#參考資料":"Local ephemeral storage reservation：https://cloud.google.com/kubernetes-engine/docs/concepts/plan-node-sizes#local_ephemeral_storage_reservation\nConfigure resources for the sidecar container：https://cloud.google.com/kubernetes-engine/docs/how-to/persistent-volumes/cloud-storage-fuse-csi-driver#sidecar-container-resources","哪裡可以查看-local-ephemeral-storage-使用量#哪裡可以查看 Local ephemeral storage 使用量？":" 可以從 GKE 的 Node Pool UI 介面查看： GKE Node Pool 頁面\n可以看到實際上 101.2 GB 的 node，可用空間只有 47.06 GB，與我們上面的計算方式差不多。\n可以透過 kubectl describe node 來查看： kubectl describe node 頁面\n可以看到 Capacity ephemeral-storage: 98831908Ki，換算成 GB 大約是 101.19 GB。\nAllocatable ephemeral-storage: 47060071478，換算成 GB 大約是 47.06 GB。\nkubectl describe node 頁面\n也可以從 describe 看到目前 request ephemeral-storage 設定的大小，來避免 Local ephemeral storage 出現不足的問題。"},"title":"Google Kubernetes Engine Local ephemeral storage 計算方式"},"/blog/gcp/iam/":{"data":{"":"IAM 的全名是 Identity and Access Management，當我們藉由 IAM，可以授與特定 Google Cloud 資源的 精細 訪問權限，並防止對其他資源的訪問。疑！？為什麼是精細？我們接著看下去，我們可以採用最小權限安全原則，該原則要求任何人都不應擁有超出實際所需的權限。","iam-測試#IAM 測試":"接下來，我們來測試看看 IAM 實際設定以及用途吧！ 我們會使用 GCP 所以提供的 [Qwiklabs] Cloud IAM：Qwik Start 來進行測試，之後的步驟會跟 Cloud IAM：Qwik Start 內容一樣，所以大家可以邊操作邊參考呦！那我們開始囉 🙃\n進入網頁後，請先登入自己的 Google 帳號，接著點選左上角的 Start Lab，會跳出與下面圖片類似的內容：\n測試用的帳號密碼\n這邊會提供兩組的帳號及密碼，分別是 Username1 以及 Username2 (後面會以 Username 1 跟 2 來說明)，密碼會用共用，且會在同一個專案下。\n用第一個 Username1 登入 GCP 點選 Open Google Console 按鈕來開啟 GCP 主控台 登入的帳號就輸入第一個 Username1 ，密碼輸入共用密碼 ，最後登入 登入成功會進入 GCP 主控台，會跳出下方圖片內容，國家選擇台灣，點選同意 Terms of Service，最後按 AGREE AND CONTINUE 登入 Username 1 GCP 主控台\n用第二個 Username2 登入 GCP 步驟與第一個相同，這邊就不在重複，但建議使用無痕，避免 Username1 跟 Username2 搶來搶去，以及登入帳號要選 Username2，應該不會選錯吧 🤣 Username1 IAM 主控台 到 Username1 的 GCP 主控台首頁 點選左側的 menu \u003e IAM 與管理 點擊頁面上方的 +ADD 按鈕，可以從下拉選單去查看各式各項的專案相關角色，可以看到超級多的角色設定，所以我們一開始才會說他是可以設定 ** 精細** 的訪問權限 我們選擇 Base，右側有 4 種角色，分別是瀏覽者 (Browser)、編輯者 (Editor)、所有者 (Owner)、檢視者 (Viewer)，詳細區別請看下面 👇👇 ADD IAM\n此表是取 Google Cloud IAM 文章基本角色中的定義，簡單說明 4 種角色的差別：\n角色名稱 權限 role/viewer (檢視者) 不影響狀態的只讀操作權限，例如：查看 (但不修改) 現有資源或是資料 role/editor (編輯者) 所有查看者權限，以及修改狀態的操作權限，例如：更改現有資源 role/owner (擁有者) 以下操作的所有編輯權限： 1. 管理項目和項目內的所有資源角色及權限 2. 為項目設置帳單 role/browser (瀏覽者) 讀取權限以及瀏覽項目的層次結構，包含資料夾、組織和 IAM 政策。但此角色不包含查看項目中資源的權限 我們的 Username1 為 owner，Username2 為 viewer\nGoogle 建議：Base 角色包含所有 Google Cloud 服務的數千個權限。除非別無選擇，否則不要授與用戶 Base 角色，請設定最有限的自訂義角色或是可以滿足該需求的角色即可 切換到 Username2 IAM 主控台 我們可以在表格裡面收尋 Username1 跟 Username2 的帳號，可以看一下他們授與的角色是否與上面說的一致，大概可以參考以下圖片：\nUsername1 跟 Username2 權限\n在 Username2 因為是 Viewer 權限，所以點擊上面的 + ADD，不會反應，會跳出以下照片內容：\nUsername2 權限不足\n再切回 Username1 Cloud Storage 接下來我們要建立一個 GCS 儲存空間，點選 menu \u003e Cloud Storage \u003e Browser 點選 Create a bucket 給予他一個獨特的名稱，以及在 Choose where to store your data 選擇 Multi-Region 最後點選 CREATE Create a bucket\n上傳範例檔案 進入新建立的 Cloud Storage，點選 Upload fiiles 按鈕 上傳一個 txt 檔案，可以先取名為 sample.txt，或是上傳後使用最後三個小點圖案內的 Rename 來修改名稱 Upload sample.txt fiiles\n驗證專案檢視者存取權限 我們再切換回 Username2 的主控台，選擇 menu \u003e Cloud Storage \u003e Browser 就可以看到跟上面一樣的儲存區 Username2 被授與 “檢視者 Viewer” 角色，這個角色有不影響狀態的只讀權限。這個範例中說明這個功能，他僅能檢視，沒有辦法上傳\n移除專案存取權限 我們再次切回 Username 主控台，選擇 menu \u003e IAM 與管理 ，找到 Username2 旁邊的鉛筆圖案 修改 Username2 權限\n點擊角色名稱的垃圾桶來移除 Username2 的檢視者權限，點擊 Save 移除 Username2 權限\n這個動作要完成生效到所有服務上，所以會需要一點時間，詳細可以參考點我 檢查 Username2 是否有存取權限 切換到 Username2 主控台，選擇 menu \u003e Cloud Storage \u003e Browser 會發現出現以下的錯誤訊息，代表我們移除權限成功 Username2 沒有權限\n新增儲存角色 我們再次切換到 Username1 主控版，選擇 menu \u003e \u003e IAM 與管理，點選上方 + ADD，在 New principals 上貼上 Username2 的帳號，Role 選擇 Storage Object Viewer 新增 Username2 角色\n查看 Username2 權限\n檢查 Username2 是否有存取權限 切換到 Username2 主控台，因為 Username2 沒有專案檢視者的角色，所以看不到專案以及任何的資源，但這個使用者對我們剛剛設定的 Cloud Storage 有特別的存取權 打開右上角的 Activate Cloud Shell 命令列工具 ，如下圖： 開啟 Activate Cloud Shell 命令列工具\n輸入以下指令 gsutil ls gs://\u003c剛剛建的 Cloud Storage 名稱\u003e 如果出現跟下方圖片一樣，就代表我們設定成功囉！\nUsername2 Storage Object Viewer 權限\n最後的最後，如果有跟我們一步一步來的朋友，在 [Qwiklabs] Cloud IAM：Qwik Start 頁面中，應該會看到一個叫 Check my progress 的按鈕，做完每一步驟，都可以點一下，他會自動去判斷你是否有完成這項動作歐！\nCheck my progress\n到這邊就完成了我們在 IAM 的測試囉～我們知道 IAM 可以設定很多的角色，以及測試了查看、新增、修改、刪除角色的功能，希望大家會喜歡今天的文章 🥰","iam-的工作原理#IAM 的工作原理":"首先我們先來了解一下 IAM 的工作原理，藉由 IAM，我們可以定義誰 (哪一個身份) 對哪些資源有哪種的訪問權限 (角色) 來管理訪問權限控制。什麼是資源？例如， Compute Engine 虛擬機 (GCE)、Google Kubernetes Engine (GKE) 集群和 Cloud Storage 存儲分區都是 Google Cloud 資源，我們用於整理資源的組織或資料夾、項目等也都是資源\nGCP IAM Logo\n我們可以把它理解成\n什麼 『 人 』，可以對什麼『 資源 』，做什麼『 事情 』 IAM 不會直接向用戶授與資源的訪問權限，而是將權限分成多個角色，然後將這些角色授與經過身份驗證的主帳號。(以前 IAM 會將主帳號稱為成員，目前部分 API 仍然使用此術語。)\nIAM 中的權限管理\n可以看到這張圖片，訪問權限管理主要包含三個部分：\n主帳號 (Principal)：主帳號可以是 Google 帳號 (針對用戶)、服務帳號 (針對應用和計算工作負載)、Google 群組或 Workspace 帳號或可以訪問資源的 Cloud Identity 網域等等 角色 (Role)：一個角色對應一組權限，權限決定了可以對資源執行的操作。向主帳號授與某個腳色， 代表授與該角色包含的所有權限給主帳號 政策 (Policy)：允許政策 (Allow Policy) 是將一個或多個主體綁定在各個角色，當想要定義誰 (主體) 對資源擁有何種類型的訪問 (角色) 時，可以創建允許政策並將其附加到資源。 ","參考資料#參考資料":"IAM 概覽：https://cloud.google.com/iam/docs/overview\n什麼是 Cloud IAM？GCP 權限管理服務介紹：https://blog.cloud-ace.tw/identity-security/what-is-cloud-iam/"},"title":"Google Cloud Platform (GCP) - IAM 與管理"},"/blog/git-or-cicd/":{"data":{"":"此分類包含 Git 或 CICD 相關的文章。\n如何啟用 GitLab 的 Package Registry 以及將儲存位置從伺服器改到 GCS 上 如何合併多個 commit，且推到遠端呢？ 部署 Laravel 於 Heroku 搭配 GitLab CI/CD 如何從頭打造專屬的 GitLab CI/CD Ansible 介紹與實作 (Inventory、Playbooks、Module、Template、Handlers) 使用 Jenkins 設定 GitHub 觸發程序並通知 Telegram Bot Jenkins 及 Ansible IT 自動化 CI/CD 介紹 Git 介紹 "},"title":"Git 或 CICD 相關"},"/blog/git-or-cicd/ansible/":{"data":{"":"本篇文章是接續前面兩篇 Jenkins 及 Ansible IT 自動化 CI/CD 介紹 跟 使用 Jenkins 設定 GitHub 觸發程序並通知 Telegram Bot 文章，歡迎大家先去觀看前面兩篇文章 🤪\n本篇所使用到的程式碼都會整理於 GitHub 連結，大家有興趣可以去瀏覽看看歐！","ansible-安裝與實作#Ansible 安裝與實作":"安裝之前先讓大家看一下版本吧！大家要記得檢查自己的版本與教學是否相同，如果不同，記得要先查看官網是否有修改內容。\n版本 macOS：11.6 Docker：Docker version 20.10.14, build a224086 Aansible：ansible [core 2.12.5] 如何安裝 Ansible 在控制主機 由於 Ansible 是一套開源的軟體，所以在目前大部分主流作業系統上都可以透過對應的套件管理 (package manager) 進行安裝。\n本人使用 macOS ，所以這邊僅列出 masOS 安裝方式，其他的可以參考官方的安裝指南。\nmacOS 安裝可以使用兩種方式，官方較推薦使用 pip 來做安裝：\nPip Install Packages (pip 官方較推薦) $ sudo pip install ansible Homebrew (brew) $ sudo brew install ansible 安裝完後，可以使用 --version 指令來檢查是否安裝完成：\n$ ansible --version ansible [core 2.12.5] config file = None configured module search path = ['/Users/ian_zhuang/.ansible/plugins/modules', '/usr/share/ansible/plugins/modules'] ansible python module location = /usr/local/Cellar/ansible/5.7.1/libexec/lib/python3.10/site-packages/ansible ansible collection location = /Users/ian_zhuang/.ansible/collections:/usr/share/ansible/collections executable location = /usr/local/bin/ansible python version = 3.10.4 (main, Apr 26 2022, 19:43:24) [Clang 13.0.0 (clang-1300.0.29.30)] jinja version = 3.1.2 libyaml = True 如何安裝 Ansible 在被控節點 不需要！！！ 透過 Ansible 進行管理的被控節點完全不需要安裝 Ansible。我們只需要確保這個節點可以透過 SSH 與控制主機做溝通，並安裝 Python 2.6 以上版本就可以透過控制主機來進行部署及管理了。\n那我們為了要模擬，所以我們使用 Docker 來模擬 Managed Node，首先老樣子，一樣先寫一個 Dockerfile 來建立我們的映像檔，此映像檔是微調 chusiang/ansible-managed-node.dockerfile 的內容，修改 ubuntu 版本以及內容作調整，我會把程式碼放在 GitHub 連結 ，以及 DockerHub 連結，歡迎大家前去下載使用。\nFROM ubuntu:22.10 LABEL maintainer=\"880831ian@gmail.com\" # Update the index of available packages. RUN apt-get update # Install the requires package. RUN apt-get install -y openssh-server sudo curl wget bash-completion openssl \u0026\u0026 apt-get clean # Setting the sshd. RUN mkdir /var/run/sshd RUN echo 'root:root' | chpasswd RUN sed -i 's/PermitRootLogin without-password/PermitRootLogin yes/' /etc/ssh/sshd_config # SSH login fix. Otherwise user is kicked off after login RUN sed 's@session\\s*required\\s*pam_loginuid.so@session optional pam_loginuid.so@g' -i /etc/pam.d/sshd ENV NOTVISIBLE \"in users profile\" RUN echo \"export VISIBLE=now\" \u003e\u003e /etc/profile # Create a new user. # # - username: docker # - password: docker RUN useradd --create-home --shell /bin/bash \\ --password $(openssl passwd -1 docker) docker # Add sudo permission. RUN echo 'docker ALL=(ALL) NOPASSWD: ALL' \u003e\u003e /etc/sudoers # Setting ssh public key. RUN wget https://raw.githubusercontent.com/chusiang/ansible-jupyter.dockerfile/master/files/ssh/id_rsa.pub \\ -O /tmp/authorized_keys \u0026\u0026 \\ mkdir /home/docker/.ssh \u0026\u0026 \\ mv /tmp/authorized_keys /home/docker/.ssh/ \u0026\u0026 \\ chown -R docker:docker /home/docker/.ssh/ \u0026\u0026 \\ chmod 644 /home/docker/.ssh/authorized_keys \u0026\u0026 \\ chmod 700 /home/docker/.ssh EXPOSE 22 # Run ssh server daemon. CMD [\"/usr/sbin/sshd\", \"-D\"] 接下來將它包成 Image 並啟動他：\n$ docker build -t ansible-ubuntu-server . \u0026\u0026 docker run --name server1 -d -p 8888:22 ansible-ubuntu-server 64c51235e34a7ba42c0c45e690201dd80248c9aac76c3b855c99cf63f7f0af7c 可以用 exec 進入容器：\ndocker exec -it server1 /bin/bash 如何讓 Ansible 操控 Docker 容器？ 我們在工作目錄下，新增一個 ansible.cfg：\n[defaults] inventory = hosts remote_user = docker host_key_checking = False 設定 inventory hosts：\n[local] server1 ansible_ssh_host=127.0.0.1 ansible_ssh_port=8888 ansible_ssh_pass=docker 其中 8888 是我們在啟動時所開放的 Port，也可以自行更改。\nansible_ssh_host：設為本機的 IP。 ansible_ssh_port：設為 docker ps 取得的 SSH Port 也就是我們的 8888。 ansible_ssh_pass：因為我們沒有連線用的金鑰，所以直接使用密碼方式做連結。(建議只用於練習環境使用) Hello World On Managed Node 當我們都設置完成後，就可以使用 Terminal 用 Docker 建立好的 Ansible 來練習了！\n$ ansible all -m command -a 'echo Hello World on Docker.' server1 | CHANGED | rc=0 \u003e\u003e Hello World on Docker. ansible 安裝時常見問題\nQ1. server1 | FAILED | rc=-1 » to use the ‘ssh’ connection type with passwords or pkcs11_provider, you must install the sshpass program\nAns1. 會遇到這個問題是因為需要多安裝 sshpass，一般系統安裝 sshpass 很簡單，但在 macOS 上稍微麻煩，詳細可以參考這篇文章。\nQ2. ~paramiko/transport.py:236: CryptographyDeprecationWarning: Blowfish has been deprecated\nAns2. 在我安裝過程中，發現上前幾天才出現這個 Bug 詳細情形可以參考 GitHub issues，目前解決辦法有降板或是先將錯誤訊息給註解掉，之後再等新的版本出來再更新，大家可以自行選擇，我這邊是直接把出現問題的 transport.py 內容註解掉，大概位於 236 行，可以看下方圖片。\nCryptographyDeprecationWarning 錯誤訊息修正","ansible-是如何運作的#Ansible 是如何運作的？":"在 Ansible 世界裡，我們會透過 Inventory 檔案 來定義有哪些的 Managed Node，並藉由 SSH 與 Python 來進行溝通。那我們先來看一張圖：\nAnsible 運作原理 (圖片來源：七分鐘掌握 Ansible 核心觀念)\n誒 😱 突然多了很多新名詞，沒關係我來一一解釋，首先我們先從 Managed Node 是什麼，以及圖片中的 Control machine 開始說起吧！\n什麼是控制主機及被控節點？ 在 Ansible 裡，我們會把所有機器的角色做以下的區分：\n控制主機 (Control Machine)：顧名思義，這類主機可以透過運行 Ansible 的劇本 (Playbooks) 對被控節點進行部署。 被控節點 (Managed Node)：也稱為遙控節點 (Remote Node)。相對於控制主機，這類節點就是我們透過 Ansible 進行部署的對象。 所以代表我們在操作這邊就是 Control Machine，要部署的機器就是 Managed Node，透過 SSH 來做連線。但什麽是 Inventory 跟 Playbooks 呢？\n什麼是 Ansible Inventory Inventory 這個單字本身有詳細目錄、清單和列表的意思。在這裡我們可以把它理解成一份主機列表，可以透過它來定義每個 Managed Node 的代號、IP 位址、連線設定和群組。\n$ vim hosts # ansible_ssh_host：遠端 SSH 主機位址 # ansible_ssh_port：遠端 SSH Port # ansible_ssh_user：遠端 SSH 使用者名稱 # ansible_ssh_private_key_file：本機 SSH 私鑰檔案路徑 # ansible_ssh_pass：遠端 SSH 密碼 (建議使用私鑰) [local] server1 ansible_ssh_host=127.0.0.1 ansible_ssh_port=55000 ansible_ssh_pass=docker 所以我們可以在這邊輸入很多個主機來做管理，可以把它想成一個設定檔。\n什麼是 Ansible Playbooks 再談 Ansible Playbooks 之前，先說明我們要怎麼去操作 Ansible？一般來說，我們可以使用 Ad-Hoc Commands 和 Playbooks 兩種方式來操作 Ansible。\nAd-Hoc Commands 是什麼？ Ad hoc 可以翻譯成簡短地指令，也就是我們常用的指令模式，最常見的 ping和echo 為例。\nping $ ansible all -m ping server1 | SUCCESS =\u003e { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": false, \"ping\": \"pong\" } echo $ ansible all -m command -a \"echo Hello World\" server1 | CHANGED | rc=0 \u003e\u003e Hello World 從上面的例子中可以看到 Ad-Hoc Commands 一次只能處理一件事情，這是它與 Playbooks 最大的差異。\nPlaybooks 是什麼？ Playbooks 就是字面上的意思為劇本，我們可以先透過寫好的劇本 (Playbooks) 來讓各個 Managed Node 進行指定的動作 (Plays) 和任務 (Tasks)。\n簡而言之，Playbooks 就是 Ansible 的腳本 (Script)，而且比傳統 Shell Script 還要強大好幾百倍的腳本！此外它是使用 YAML 格式，寫 Code 就如同寫文件一樣，簡單易讀。\n有關詳細的動作 (Plays) 和任務 (Tasks)，等我們實際安裝好再來說明 😆","ansible-發送通知到-telegram-bot#Ansible 發送通知到 Telegram Bot":"剛剛看了很多內建的模組，當然 Ansible 還有很多好玩的模組可以使用，我們就跟 使用 Jenkins 設定 GitHub 觸發程序並通知 Telegram Bot 文章 一樣，將我們取得的內容傳送到 Telegram Bot 吧！那首先我們要先創造一個 Telegram Bot，在 Telegram 找到一個機器人叫 BotFather 的官方機器人帳號。並使用指令 /newbot，會看到一下畫面：\nTelegram 創建機器人\n他詢問你要幫機器人取叫什麼名稱，可以直接在輸入欄位輸入想要取的名稱，當然不能是別人已經取過的：\nTelegram 創建機器人\n看到它回覆你 Done! 代表成功了，接下來你會拿到一組 API Token，像我的是 5335968936:AAEDO_Tudhy0t577jtbF9TpgrzqOsL99h9c (已更換，大家放心 😂 )，接下來開啟瀏覽器輸入以下網址 https://api.telegram.org/bot{API Token}/getupdates，其中的 {API Token} 請帶入自己的 Token，直到出現 {\"ok\":true,\"result\":[]} 代表完成。\n接下來開啟你自己的 Bot ，打上 /start 指令，重新整理剛剛的網頁就可以看到以下這樣的文字：\n{\"ok\":true,\"result\":[{\"update_id\":606594112,\"message\":{\"message_id\":1,\"from\":{\"id\":493995679,\"is_bot\":false,\"first_name\":\"\\u54c1\\u6bc5\",\"last_name\":\"Ian\",\"username\":\"pinyichuchu\",\"language_code\":\"zh-hans\"},\"chat\":{\"id\":493995679,\"first_name\":\"\\u54c1\\u6bc5\",\"last_name\":\"Ian\",\"username\":\"pinyichuchu\",\"type\":\"private\"},\"date\":1652695148,\"text\":\"/start\",\"entities\":[{\"offset\":0,\"length\":6,\"type\":\"bot_command\"}]}} 這是你傳訊息給 Bot 它所收到的 API，資料很多沒關係，我們找到 id，像我的是 493995679，這個就是我跟機器人的聊天室，我們就先回到 Ansible 這邊吧！\n開啟一個新的檔案叫 send_notify_tg.yaml，打以下內容：\n--- - name: Send notify hosts: all tasks: - name: Send notify to Telegram community.general.telegram: token: \"9999999:XXXXXXXXXXXXXXXXXXXXXXX\" api_args: chat_id: 000000 parse_mode: \"markdown\" text: \"Your precious application has been deployed: https://example.com\" disable_web_page_preview: True disable_notification: True 可以看到我們使用的模組不是 Ansible 內建的，而是社群別人寫的，詳細可以參考 community.general.telegram module – module for sending notifications via telegram：\n其中 token 就是我們剛剛在 BotFather 那邊所拿到的 Token，chat_id 就是我們剛剛在網頁上看到的 id，把資料都輸入進去後，我們可以修改 text 內容，改成 “Send notify to Telegram 測試傳送通知”，接著執行 ansible-ploybook send_notify_tg.yaml ，看看能不能正常收到通知！\n發送通知至 Telegram Bot\n這樣子就成功透過 Ansible Module 傳送通知給 Telegram 囉！\n我們可能需要將機器人加入群組內，這時候需要更換一下 chat_id，先將機器人加入群組，再次到剛剛瀏覽器的網頁刷新，查看 chat 後面的 id 帶有 -，像是 -540226836 這樣，這個就是該群組的 ID，將 send_notify_tg.yaml 的 chat_id 修改成 -540226836 在測試看看，他就會在群組中發送通知囉！\n{\"update_id\":606594124,\"message\":{\"message_id\":14,\"from\":{\"id\":493995679,\"is_bot\":false,\"first_name\":\"\\u54c1\\u6bc5\",\"last_name\":\"Ian\",\"username\":\"pinyichuchu\",\"language_code\":\"zh-hans\"},\"chat\":{\"id\":-540226836,\"title\":\"\\u54c1\\u6bc5 \u0026 AnsibleSendMessageBot\",\"type\":\"group\",\"all_members_are_administrators\":true},\"date\":1652696181,\"group_chat_created\":true}} 發送通知至 Telegram 群組 Bot","使用-ansible-的-template-系統#使用 Ansible 的 Template 系統":"Template module 是常使用的檔案模組之一，我們在 常用的 Ansible Module 有哪些？ 文章中有提到，可以用它和變數 (Variables) 來操作檔案。\n我們只需要事先定義變數和模板 (Templates)，即可用它動態產生遠端的 Shell Script、設定檔 (Configure)等。換句話說，我們可以用一份 template 來開發 (Development)、測試 (Test)、正式環境 (Production) 等不同環境設定。\n舉例說明：\n建立 template 檔案 $ vim hello_world.txt.j2 Hello \"{{ dynamic_word }}\" 由於 Ansible 是就由 Jinja2 來實作 template 系統，所以需要使用 *.j2 的副檔名。 上面的 \"{{ dynamic_word }}\"\" 代表我們在 template 裡使用了名為 dynameic_word 的變數。 建立 playbook，並加入變數 vim template_demo.yaml --- - name: Play the template module hosts: localhost vars: dynamic_word: \"World\" tasks: - name: generation the hello_world.txt file ansible.builtin.template: src: hello_world.txt.j2 dest: /tmp/hello_world.txt - name: show file context command: cat /tmp/hello_world.txt register: result - name: print stdout debug: msg: \"{{ result.stdout_lines }}\" 在第 5 行，我們幫 dynamic_word 變數設了一個預設值 World。 在 8 行的第 1 個 task 裡，我們使用 template module，並指定了檔案的來源 (src) 和目的地 (dest)。 之後的 2 個 task 則是把 template module 產生的檔案給印出來。 直接使用 ansible-playbook template_demo.yaml 執行 Playbook。 Template Module 範例\n也可以透過 -e 參數將 dynamic_word 覆寫成 “ansible”\n$ ansible-playbook template_demo.yaml -e \"dynamic_word=ansible\" Template Module 使用 -e 覆寫參數\n如何切換不同環境 除了我們剛剛用 vars 來宣告變以外，還可以使用 vars_files 來 include 其他的變數檔：$ vim template_demo2.yaml --- - name: Play the template module hosts: localhost vars: env: \"development\" vars_files: - vars/{{ env }}.yml tasks: - name: generation the hello_world.txt file ansible.builtin.template: src: hello_world.txt.j2 dest: /tmp/hello_world.txt - name: show file context command: cat /tmp/hello_world.txt register: result - name: print stdout debug: msg: \"{{ result.stdout_lines }}\" 可以看到上面例子中第 7 行，就是我們使用 vars_files 來 include 其他的變數檔。\n建立 vars/development.yaml、vars/test.yaml、vars/production.yaml 檔案，接下來將依不同得環境 include 不同的檔案變數檔案 (vars files)，這樣就可以用一份 Playbook 切換環境了！ Development $ vim vars/development.yaml dynamic_word: \"development\" Test $ vim vars/test.yaml dynamic_word: \"test\" Production $ vim vars/production.yaml dynamic_word: \"production\" 執行 ansible-playbook template_demo2.yaml -e \"dynamic_word=Test\"，並有 -e 去修改各個環境。 Template Module 範例\nTemplate 系統是實務上很常見的手法之一，藉由它我們可以很輕鬆地讓開發、測試、正式環境無縫接軌。但若是在大型的 Playbook 裡切換環境，建議使用較為進階的 group_vars 跟 host_vars。","參考資料#參考資料":"現代 IT 人一定要知道的 Ansible 自動化組態技巧：https://chusiang.gitbooks.io/automate-with-ansible/content/\nAnsible 安裝：https://tso-liang-wu.gitbook.io/learn-ansible-and-jenkins-in-30-days/ansible/ansible/ansible-installation\n怎麼用 Docker 練習 Ansible？：https://chusiang.gitbooks.io/automate-with-ansible/content/05.how-to-practive-the-ansible-with-docker.html\ncommunity.general.telegram module – module for sending notifications via telegram：https://docs.ansible.com/ansible/latest/collections/community/general/telegram_module.html#ansible-collections-community-general-telegram-module","取得-managed-node-的-facts#取得 Managed node 的 facts":"還記得我們在執行任務 (Tasks) 時，明明只有兩個，但最後結果顯示三個嗎？是因為在使用 Playbooks 時，Ansible 會自動執行 Setup module 以蒐集各個 Managed node 的 facts。 這個 facts 就好比是系統變數一樣，從 IP 位址、作業系統、CPU 等資訊應有盡有。\nAd-Hoc Commands 通常我們都會先使用 Ad-Hoc Commands 來呼叫 setup 看看有哪些可用的資訊，這對於我們稍後撰寫較為複雜的 Playbooks 會很有幫助。\n可以藉由 less 快速搜尋所有的變數 $ ansible all -m setup | less server1 | SUCCESS =\u003e { \"ansible_facts\": { \"ansible_apparmor\": { \"status\": \"disabled\" }, \"ansible_architecture\": \"x86_64\", \"ansible_bios_date\": \"03/14/2014\", \"ansible_bios_vendor\": \"BHYVE\", \"ansible_bios_version\": \"1.00\", \"ansible_board_asset_tag\": \"NA\", \"ansible_board_name\": \"NA\", \"ansible_board_serial\": \"NA\", \"ansible_board_vendor\": \"NA\", \"ansible_board_version\": \"NA\", 搭配 filter 將發行版本 (distribution) 資訊給過濾出來 $ ansible all -m setup -a \"filter=ansible_distribution*\" server1 | SUCCESS =\u003e { \"ansible_facts\": { \"ansible_distribution\": \"Ubuntu\", \"ansible_distribution_file_parsed\": true, \"ansible_distribution_file_path\": \"/etc/os-release\", \"ansible_distribution_file_variety\": \"Debian\", \"ansible_distribution_major_version\": \"22\", \"ansible_distribution_release\": \"kinetic\", \"ansible_distribution_version\": \"22.10\", \"discovered_interpreter_python\": \"/usr/bin/python3\" }, \"changed\": false } 取得套件管理員的種類資訊，例子中取得的值是 apt $ ansible all -m setup -a \"filter=ansible_pkg_mgr\" server1 | SUCCESS =\u003e { \"ansible_facts\": { \"ansible_pkg_mgr\": \"apt\", \"discovered_interpreter_python\": \"/usr/bin/python3\" }, \"changed\": false } 轉寫 Playbooks 我來出個題目，我想要知道 Ansible 所使用的公鑰，並透過 Telegram Bot 發送到群組，要怎麼做呢！？\n首先要利用剛剛的 Ad-Hoc Commands filter，找到公鑰，再將公鑰透過 Telegram Bot 傳送，所以我們會有兩個 Tasks，那我們開始實作囉 🤓\n1.找到公鑰\n--- - name: Filter rsa_public \u0026 Send notify hosts: all tasks: - name: Filter setup rsa_public key ansible.builtin.setup: filter: - \"ansible_ssh_host_key_rsa_public\" register: result 可以看到我們將 filter setup 從 Ad-Hoc 轉成 Playbooks，並使用 result 來存在找到的公鑰。\n發送通知至 Telegram Bot - name: Send notify to Telegram community.general.telegram: token: \"5335968936:AAFhxxMRJy-rucGKgSE80Xss7qPq2iOHWlc\" api_args: chat_id: -540226836 parse_mode: \"markdown\" text: \"{{ result }}\" disable_web_page_preview: True disable_notification: True 老樣子，我們就使用上次 send_notify_tg.yaml 內的 Send notify to Telegram 任務來傳送通知。\n執行後，看看群組是否有收到我們找到的 ansible_ssh_host_key_rsa_public 通知。\n發送通知至 Telegram 群組 Bot","在-playbooks-使用-handlers#在 Playbooks 使用 Handlers":"Handlers 是我們在 Ansible Playbooks 裡很常用來重開系統服務 (Service) 的手法，我們這邊透過安裝 Nginx 來介紹它。\n那什麼是 Handlers 呢？Handler 本身是一種非同步的 callback function ; 在這裡則是指關聯於特定 tasks 的事件 (event) 觸發機制。當這些特定的 tasks 狀態為 被改變 (changed) 且都已被執行，才會觸發一次的 event。\n我們建立 setup_nginx.yaml --- - name: setup the nginx hosts: all become: true vars: username: \"PinYi\" mail: \"880831ian@gmail.com\" blog: \"https://pin-yi.me\" tasks: # 執行 'apt-get update' 指令。 - name: update apt repo cache apt: name=nginx update_cache=yes # 執行 'apt-get install nginx' 指令。 - name: install nginx with apt apt: name=nginx state=present # 於網頁根目錄 (DocumentRoot) 編輯 index.html。 - name: modify index.html ansible.builtin.template: src=templates/index.html.j2 dest=/var/www/html/index.html owner=www-data group=www-data mode=\"644\" backup=yes notify: restart nginx # handlers # # * 當確認事件有被觸發才會動作。 # * 一個 handler 可被多個 task 通知 (notify)，並於 tasks 跑完才會執行。 handlers: # 執行 'sudo service nginx restart' 指令。 - name: restart nginx service: name=nginx enabled=yes state=restarted # post_tasks: # # 在 tasks 之後執行的 tasks。 post_tasks: # 檢查網頁內容。 - name: review http state command: \"curl -s http://localhost\" register: web_context # 印出檢查結果。 - name: print http state debug: msg={{ web_context.stdout_lines }} 來說明一下上面這個 yaml 檔案：\n首先我們想要安裝 Nginx，我們給了三個參數，分別是 username、mail、blog，等等會帶入我們的 template。 我們一開始有 3 個 task，分別代表執行更新、安裝、編輯 index.html 檔案。 以及 1 個 handlers 他會等 modify index.html 有改變且執行後才會動作。 最後是 post_tasks 他是等 tasks 之後執行的 tasks。 接下建立 Nginx html 的 template：vim templates/index.html.j2 _____________________________________ / This is a ansible-playbook demo for \\ \\ automate-with-ansible at 2022/05/17./ ------------------------------------- \\ ^__^ \\ (oo)\\_______ (__)\\ )\\/\\ ||----w | || || [ {{ username }}@automate-with-ansible ~ ]$ [ {{ username }}@automate-with-ansible ~ ]$ [ {{ username }}@automate-with-ansible ~ ]$ cat .profile - {{ mail }} - {{ blog }} 執行 Playbook 可以看到因為我們 modify index.html 沒有被改變，notify 沒有通知 handlers，所以他不會執行 handlers 該段程式。(正常來說，修改 html 不需要重啟，此為範例 🤣 )\nHandlers 範例\n那我們修改一下 index.html 來測試一下會不會把 index.html 的狀態被改變，而讓 handlers 執行呢！我們隨意修改 index.html 內容，修改日期改成 05/17： Handlers 範例\n可以看到我們的 modify index.html 被改變了，所以 notify 通知 handlers 執行重新啟動。","在-playbooks-使用-loops#在 Playbooks 使用 loops":"在 Shell Script 中，我們會使用 for 和 while 等迴圈 (loop) 來簡化重複的程式碼，而在 Ansible 我們也可以使用 loop 來簡化重複的任務 (Tasks)。\n標準迴圈 首先我們先以簡單的方式重複印出三筆資料。\nShell Script 建立 for loop 的 Script $ vim bash_loop.sh #!/bin/bash for x in 0 1 2; do echo Loop $x done 在第 4 行，我們用 for，並 \b 代入 0,1,2 三個值到 $x 變數 在第 5 行，則用了 echo，印出訊息和 $x 變數 執行 Script：可以看到底下跑了 3 次的 loop $ chmod a+x bash_loop.sh $ ./bash_loop.sh Loop 0 Loop 1 Loop 2 Ansible Playbooks 我們需要透過 item 和 with_items 來使用 Ansible 的 loop，其 item 為預設名。在 Ansible 2.5 中添加了 loop，所以我們後續兩者都會提到 (目前兩者都可以使用！)\n建立 loop 的 playbook vim playbook_with_items.yaml --- - name: a basic loop with playbook hosts: localhost tasks: - name: print loop message ansible.builtin.debug: msg: \"Loop {{ item }}\" with_items: - 0 - 1 - 2 在第 6、7 行裡，我們用 debug module 來印出訊息，並定義 item 在第 8 ~ 11 行，則用了 with_item 將 0,1,2 的值傳入 item 執行 ansible-playbook playbook_with_items.yaml 後會得到： TASK [print loop message] ************************************************************************************************************* ok: [server1] =\u003e (item=0) =\u003e { \"msg\": \"Loop 0\" } ok: [server1] =\u003e (item=1) =\u003e { \"msg\": \"Loop 1\" } ok: [server1] =\u003e (item=2) =\u003e { \"msg\": \"Loop 2\" } 另一種 在 Ansible 新增的 loop\n建立 loop 的 playbook vim playbook_loop.yaml --- - name: a basic loop with playbook hosts: all tasks: - name: print loop message ansible.builtin.debug: msg: \"{{ item }} {{ my_idx }}\" loop: - Loop - Loop - Loop loop_control: index_var: my_idx 執行 ansible-playbook playbook_loop.yaml 後會得到： TASK [print loop message] ************************************************************************************************************* ok: [server1] =\u003e (item=0) =\u003e { \"msg\": \"Loop 0\" } ok: [server1] =\u003e (item=1) =\u003e { \"msg\": \"Loop 1\" } ok: [server1] =\u003e (item=2) =\u003e { \"msg\": \"Loop 2\" } 會使用 Loop 就可以減少我們在寫重複的程式碼，當然上面的只是簡單的範例，詳細請參考 Loops - Ansible Documentation。","常用的-ansible-module-有哪些#常用的 Ansible Module 有哪些？":"接下來簡單介紹一下比較常用到的 8 個 Module：\nansible.builtin.apt apt module 是給 Debian, Ubuntu 等作業系統使用的套件模組 (Packing Modules)，我們可以透過它管理 apt 套件。類似的有 apt-get、dpkg等。\n更新套件索引(快取)，等同於 apt-get update 指令 - name: Update repositories cache ansible.builtin.apt: update_cache: yes 安裝 vim 套件 - name: Install the package \"vim\" ansible.builtin.apt: name: vim state: present 移除 nano 套件 - name: Remove \"nano\" package ansible.builtin.apt: name: nano state: absent ansible.builtin.command command module 是可以在遠端上執行指令的指令模組，但它不支援變數 (variables) 和 \u003c、\u003e、|、;、\u0026，若有這類需求要改用 shell module。\n重新開機 - name: Reboot at now ansible.builtin.command: /sbin/shutdown -r now 當某個檔案不存在時才執行指令 - name: create .ssh directory ansible.builtin.command: mkdir .ssh creates=.ssh/ 先切換目錄再執行指令 - name: cat /etc/passwd ansible.builtin.command: cat passwd args: chdir: /etc ansible.builtin.copy copy moudule 是從本地複製檔案到遠端的檔案模組，若有使用變數需求，可以改用 template。它類似 Linux 指令的 scp。\n複製 ssh public key 到遠端 (chmod 644 /target/file) - name: copy ssh public key to remote node ansible.builtin.copy: src: files/id_rsa.pub dest: /home/docker/.ssh/authorized_keys owner: docker group: docker mode: 0644 複製 ssh public key 到遠端 (chmod u=rw,g=r,o=r /target/file) - name: copy ssh public key to remote node ansible.builtin.copy: src: files/id_rsa.pub dest: /home/docker/.ssh/authorized_keys owner: docker group: docker mode: \"u=rw,g=r,o=r\" 複製 nginx vhost 設定檔到遠端，並備份原有的檔案 - name: copy nginx vhost and backup the original ansible.builtin.copy: src: files/ironman.conf dest: /etc/nginx/sites-available/default owner: root group: root mode: 0644 backup: yes ansible.builtin.file file module 是在遠端建立和刪除檔案 (file)、目錄 (directory) 和軟連結 (symlinks) 的檔案模組。它類似的 Linux 指令為 chown、mkdir 和 touch。\n建立檔案 (touch)，並設定權限為 644 - name: touch a file, and set the permissions ansible.builtin.file: path: /etc/motd state: touch mode: \"u=rw,g=r,o=r\" 建立目錄 (mkdir)，並設定檔案擁有者為 docker - name: create a directory, and set the permissions ansible.builtin.file: path: /home/docker/.ssh/ state: directory owner: docker mode: \"700\" 建立軟連結 (ln) - name: create a symlink file ansible.builtin.file: src: /tmp dest: /home/docker/tmp state: link ansible.builtin.lineinfile lineinfile module 是個可用正規表示法對檔案進行插入或取代文字的檔案模組。它類似的 Linux 指令是 sed。\n移除 docker 使用者的 sudo 權限 - name: remove sudo permission of docker ansible.builtin.lineinfile: dest: /etc/sudoers state: absent regexp: \"^docker\" 在 /etc/hosts 檔案裡用 127.0.0.1 localhost 取代開頭為 127.0.0.1 的一行 - name: set localhost as 127.0.0.1 ansible.builtin.lineinfile: dest: /etc/hosts regexp: '^127\\.0\\.0\\.1' line: \"127.0.0.1 localhost\" owner: root group: root mode: 0644 ansible.builtin.service service module 是用來管理遠端系統服務的系統模組。它類似的 Linux 指令為 service。\n啟動 Nginx - name: start nginx service ansible.builtin.service: name: nginx state: started 停止 Nginx - name: stop nginx service ansible.builtin.service: name: nginx state: stopped 重開網路服務 - name: restart network service ansible.builtin.service: name: network state: restarted args: eth0 ansible.builtin.shell shell module 是可以在遠端用 /bin/sh 執行指令的指令模組，支援變數 (variables) 和 \u003c、\u003e、|、; 和 \u0026 等運算。\n藉由 ls 和 wc 檢查檔案數量 - name: check files number ansible.builtin.shell: ls /home/docker/ | wc -l 把所有的 Python 行程給砍掉 - name: kill all python process ansible.builtin.shell: kill -9 $(ps aux | grep python | awk '{ print $2 }') ansible.builtin.stat stat module 是用來檢查檔案狀態的檔案模組。其類似的 Linux 指令為 stat。\n檢查檔案是否存在，若不存在則建立他。 - name: check the 'vimrc' target exists ansible.builtin.stat: path: /home/docker/.vimrc register: stat_vimrc - name: touch vimrc file: path: /home/docker/.vimrc ansible.builtin.state: touch mode: \"u=rw,g=r,o=r\" when: stat_vimrc.stat.exists == false 取的某檔案的 md5sum - name: Use md5sum to calculate checksum ansible.builtin.stat: path: /path/to/something checksum_algorithm: md5sum 其他 其他還有很多可以使用的 Module ，詳情可以查看 Ansible.Builtin。","第一個-playbook#第一個 Playbook":"在我們都安裝好後，要來說說我們剛剛有偷偷提到的 Playbooks 的動作 (Plays) 和任務 (Tasks)。在一份 Playbooks 裡面，可以有多個 Play、多個 Task 和多個 Module：\nPlay：通常為某個特定的目的，例如： Setup a official website with Drupal 藉由 Drupal 建置官網 Restart the API Service 重開 API 服務 Task：要實行 Play 這個目的所需做的每個步驟，例如： Install the Nginx 安裝 Nginx Kill the djnago process 強制停止 django 的行程 Module：Ansible 所提供的各種操作方式，例如： apt: name=vim state=present 使用 apt 套件安裝 vim command: /sbin/shutdown -r now 使用 shutdown 的指令關機 有點聽不懂吧！我來舉個例子，我們最熟悉的 Hello World，先建立一個 helloworld.yaml 的檔案：\nhelloworld.yaml--- - name: say 'hello world' hosts: all tasks: - name: echo 'hello world' command: echo 'hello world' register: result - name: print stdout debug: msg: \"{{ result.stdout }}\" 可以看到這整個就是 Play，我們想要達到 say ‘hello world’ 的目的，其中有兩個 name 分別代表兩個 Task，也就是達成 Play 目的所需得步驟。最後 command 與 debug 就是我們的 Module 要怎麼達成這兩個步驟的操作方式。\nPlaybooks 組成結構\n我們使用 ansible-playbook 執行 Playbook，在這個範例中，我們執行了１個 Play、3 個 Tasks 和 2 個 Modules：\n$ ansible-playbook helloworld.yaml 執行 Playbooks\n我們剛剛明明只寫兩個 tasks，為什麼執行就變成三個 tasks？\n這是因為 Ansible 預設會使用 Setup task 來取得 Managed node 的 facts。關於 facts 的詳細說明，請滑到後面 取得-managed-node-的-facts 觀看 😬\n那如果沒有 Ansible 時，我們是怎麼操作的？我會附上 Shell Script 的做法，我們來比較看看吧！\nShell Script 建立 helloworld.sh 檔案 helloworld.sh#! /bin/bash echo \"Hello World\" 執行 helloworld.sh ./ helloworld.sh Hello World 看起來 Shell Script 已經夠用了，為什麼還要寫 Playbook 呢？這邊整理幾個理由給大家參考：\n用 Ansible 的 Module 可以把很多複雜的指令給標準化，例如不同的 Linux 發行版本在安裝套件時需代各種不同的參數。 在現有的雲原生 (cloud native) 的架構下，傳統的 Shell Script 已經不敷使用，一般而言 Shell Script 只能對一台機器 (instance) 進行操作。 "},"title":"Ansible 介紹與實作 (Inventory、Playbooks、Module、Template、Handlers)"},"/blog/git-or-cicd/git-introduce/":{"data":{"":"","git-常見問題#Git 常見問題":" Git 裡的 HEAD 是什麼？ HEAD 本身是一個指標，通常會指向某個本地端分支或是其他 Commit，所以也可以把 HEAD 當作目前所在的分支。\n刪除合併後的分支會發生什麼事情嗎？ 分支本身就像指標或是貼紙一樣，貼在某個 Commit 上面，分支並不是目錄或是檔案，所以當我們刪除已經合併過的分支，不會造成檔案或目錄跟著被刪除。","git-操作指令#Git 操作指令":"Git 的操作指令繁多，包含環境類、查看類、提交類、分支類、遠端類、合併類、還原類等等，所以才有了 Git GUI 工具，筆者很推薦 Gitkraken，雖然需要付費，但真的很方便，畫面也很乾淨簡潔。如果是學生的話，還可以使用 GitHub Student Developer Pack 免費使用歐！\nGit GUI 工具 (Gitkraken)\n接下來我們會依照環境類、查看類、提交類、分支類、遠端類、合併類、還原類依序下去介紹～\n環境類 使用每一個程式或工具，必須先把它安裝到自己電腦上對吧！但因為大家使用的系統都不一樣，所以這邊就不列出要怎麼進行安裝，可以參考 Git 安裝教學\n當我們安裝好後，我們就可以一起進入 Git 的世界囉！\ninit 首先，找一個你要開始進行版本控制 (Git) 的資料夾， 使用：\n$ git init 要記得要到版本控制的資料夾目錄下才使用這個指令歐！\n使用完後，會看到跑出下面這些文字：\nGit init\n此外資料夾內也會多一個隱藏檔案 .git ，他是用來存放 git 的紀錄，所以不要亂刪除歐：\n$ tree -a -d . └── .git ├── hooks ├── info ├── objects │ ├── info │ └── pack └── refs ├── heads └── tags 回到剛剛圖片，它說明默認會使用 master 這個來作為初始分支，並記得要使用 git config 來做設定，那 git config 是要做什麼用的呢！？\nconfig 在推送 Commit 的時候，會顯示使用者名稱以及電子郵件，所以要先在推送前設定好，這時就使用：\n$ git config --global user.name \"your name\" $ git config --global user.email \"your email\" 分別設定使用者名稱以及電子郵件，這樣共同使用版本控制的人，才分的出來誰是誰！ (若只要在單個專案下設定使用者名稱及電子郵件，就不需要設定 –global 參數)\n查看類 status 我們剛剛已經設定好 config ，如果想查看檔案的 git 狀態，就使用：\n$ git status 可以查看現在資料夾內有哪些檔案還沒加入版本控制，或是已經加入但還沒 Commit 變成新版本。\nGit status\n綠色代表已經加入版本控制但還沒有 Commit ，紅色代表尚未加入追蹤。\nlog 當我們想要查看 Commit 的幾個版本，或是是誰 Commit 的等等資訊，可以使用：\n$ git log Git log\n可以看到一串英文加數字，它是SHA-1 校驗碼也代表你推的這一個版本的識別 ID，也可以看到是由誰推送跟時間與推送的文字說明。\ndiff 當我們假設已經 Commit 後，想要比較不同版本內容的差異，就使用：\n$ git diff 116e 442c 116e 代表最新的版本，442c 代表上一個版本，可以看上面 log 的辨識 ID，因為校驗碼很長，最少需要前4個數字跟英文，才可以知道是哪一個版本。\nGit diff\n紅色代表最新版本因為我們是用 116e 最新版本來跟綠色上一個版本 442c 來做比較。\nreflog 如果我們在操作 Git 的時候執行錯誤，需要回復到前幾個版本，但還想要查看歷史紀錄，如果我們使用前面說的 git log 是看不到已經舊的紀錄，這時要使用：\n$ git reflog Git reflog (圖一)\nGit log (圖二)\n可以看到圖一是使用 reflog 就可以知道我們還原的紀錄，但使用 log 查看，會發現沒有辦法看到 test-2 的紀錄。\n提交類 add 當我們上面使用 git init 初始化資料夾後，還沒有開始進行版本控制，需要使用：\n$ git add . $ git add test.txt 將檔案加入版本控制的暫存區。它的格式是 git add [檔案名稱] ，如果想要把資料夾全部檔案都加入版本控制，可以使用 . 來加入。\ncommit 當我們新增完後，要將它提交出去，就要使用：\n$ git commit -m \"內容打這\" Git commit\n我們可以在雙引號內輸入我們修改的內容，方便其他人了解該版本的差異。\n分支類 branch 我們專案初始化後，一開始都只會有一個 master 分支，如果想要新增分支，可以使用：\n$ git branch \"分支名稱\" 會將所在分支的檔案狀態複製到新增的分支上，當該分支改動時，不會影響到原本的分支。\nGit branch\ncheckout 如果想要切換不同版本或是分支，就可以使用：\n$ git checkout \"分支名稱/分支ID\" 來切換不同的分支或是以前的版本。\nGit checkout\n如果想要同時建立分支並切換，可以使用：\n$ git chechout -b \"分支名稱\" -b 這個參數就等於是 git branch \"分支\" \u0026 git checkout \"分支\"。\nGit checkout -b\n遠端類 clone 如果我們遠端上已經有版本庫，想要下載到本地端，就可以使用：\n$ git clone [遠端網址] 會在下指令的當前路徑下，下載整個遠端的專案。\nremote 如果要新增遠端版本庫，就可以使用：\n$ git remote add [簡稱] [遠端網址] 取一個可以代表要新增的遠端 Git 版本庫簡稱。\nGit remote\n如果想要檢視已經設定好的遠端版本庫，就可以使用：\n$ git remote 他會列出每個遠端本版本庫的簡稱。\n也可以使用 git remote -v 指令，會顯示 Git 用來讀寫遠端簡稱時所用的網址：\nGit remote -v\npush 當我們已經設定好遠端版本庫的位址，我們如果想要將本地端的專案版本庫放到遠端，就可以使用：\n$ git push [簡稱] [分支名稱] 將本地端版本庫推到遠端的版本庫。\nGit push\n上面這張圖片，就是把本地端的 master 分支內容，推一份到 origin 這個地方 (可能是 GitHub 或公司內部 Git 伺服器)，並且在 origin 這個地方形成同名的 master 分支。\n但很多人不知道的是，其實 push 指令的完整型態長這樣：\n$ git push origin master:master 意思就是「把本地的 master 分支內容，推一份到 origin 上，並且在 origin 上建立一個 master 分支」\n如果我們把指令改為：\n$ git push origin master:dog 意思就會變成「把本地的 master 分支內容，推一份到 origin 上，並且在 origin 上建立一個 dog 分支」\npull 如果我們想要將遠端的專案 下載並合併 至本地端，就可以使用：\n$ git pull [簡稱] [分支名稱] 將遠端專案資料下載並合併到本地端。\nGit pull\nfetch 如果我們單純只想要將遠端的專案 下載 至本地端，就可以使用：\n$ git fetch [簡稱] [分支名稱] 將遠端專案資料下載到本地端。\nGit fetch\nclone、pull、fetch 差異 差異 clone fetch pull 功能 會把遠端整份專案都下載到本地端 只會下載，並不會合併 會下載且合併檔案 補充說明 適用於專案一開始時使用，如果 clone 之後要再更新，通常是執行 git fetch or git pull 假設我遠端叫 orgin，當執行時，Git 會比對本地端與遠端，會「下載 origin 上有但本地端沒有的檔案下來」 pull 與 fetch 做的事情差不多，多了一個進行合併的功能 合併類 merge 如果想要將不同分支內容合併，就可以使用：\n$ git merge [分支名稱] 像下面這張圖，我們將分支 123 合併到分支 master。(要記得先切換到要合併的主分支，才可以合併其他的分支進來)\nGit merge\nmerge (fast-fastward) merge 有一個參數叫做 fast-fastward，我們先看圖片再來說明：\nGit merge fast-fastward 圖一 (CS Visualized: Useful Git Commands)\nGit merge no fast-fastward 圖二 (CS Visualized: Useful Git Commands)\n圖一是我們的 fast-fastward，也是 Git 預設的合併方式，當我們要將 dev 合併到 master，fast-fastward 會將 dev 分支的 commit 紀錄合併到 master 上，然而圖二是不使用 fast-fastward 方式，會保留 dev 分支上的 commit 紀錄，並在 master 上新增一個。\nno fast-fastward 好處是可以完整保留每個分支的 commit 紀錄，壞處是假如 commit 紀錄只有一個，合併多次就會出現很多小叉路。要怎麽使用 no fast-fastward：\n$ git merge --no-ff [分支名稱] rebase (合併) 如果想要重新修改特定分支的「 基礎版本 」，要把另一個分支的變更，當成我這隻分支的基礎，就可以使用：\n$ git rebase [分支名稱] Git rebase (合併) (CS Visualized: Useful Git Commands)\nrebase (修改) 如果想要修改特定分支上任何一個版本資訊，就可以使用：\n$ git rebase -i [HEAD~?] 但要記得，如果再使用前，要先詢問是否有人正在使用此分支，因為 rebase 會改變歷史紀錄。\nGit rebase (修改) (CS Visualized: Useful Git Commands)\ncherry-pick 如果想要從某個分支，拉幾個 Commit 進來該分支，就可以使用：\n$ git cherry-pick [分支ID] 但做此動作，需要解決修改後的版本衝突。\n還原類 reset 如果想要還原任意 Commit，就可以使用：\n$ git reset [HEAD~?] 會還原選擇的 Commit，且檔案還是維持最新版本。\nreset 指令可以搭配參數使用，常見到的三種參數，方別是 --mixed、--soft、--hard，不同的參數執行之後會有稍微不太一樣的結果。\nmixed 模式：--mixed 是預設的參數，如果沒有特別加其他參數，got reset 會使用 --mix 模式。這個模式會把暫存區的檔案丟掉，但不會影響到工作目錄的檔案，也就是說 Commit 拆出來的檔案會留在工作目錄(實體的檔案)，但不會留在暫存區。 soft 模式：這個模式下的 reset，工作目錄跟暫存區檔案都不會被丟掉，所以看起來只有 HEAD 的移動而已。也因此，Commit 拆出來的檔案會直接放在暫存區。 hard 模式：在這個模式下，不管是工作目錄以及暫存區的檔案都會丟掉。 以下用表格在整理一次：\n模式 mixed 模式 soft 模式 hard 模式 工作目錄(實體的檔案) 不變 不變 丟掉 暫存區 丟掉 不變 丟掉 文字說明也不太懂對吧！沒錯我也是 😂，所以我整理了三種不同的範例，我們一起做看看吧！\n我們先開一個新專案，在 master 上面 commit 2 次，可以參考下方圖片：\nGit reset 示範\n我們用 git log 來看一下記錄：\nGit reset 示範 log 紀錄\n都設定好後，我們要來測試每個參數的不同之處，先以預設的 --mixed 來測試：\nreset - mixed 我們下 git reset --minxed 按 Tab 可以看要還原的 commit，我們之後的測試都是還原到 24aeb0d -- [HEAD^] add a.txt 這個，來觀察 add b.txt 這個 commit 的變化。\nGit reset mixed 模式\n所以我們的指令是 git reset --mixed 24aebo4，我們再來觀看看看，檔案狀態也就是 b.txt 以及暫存區狀態。\nGit reset mixed 模式\n可以看到使用 --mixed 模式，檔案 b.txt 還會存在，只是移除暫存區。\nreset - soft 我們指令是 git reset --soft 24aebo4：\nGit reset soft 模式\n可以看到使用 --soft 模式，檔案 b.txt 還會存在，且會在暫存區。\nreset - hard 我們指令是 git reset --hard 24aebo4：\nGit reset hard 模式\n可以看到使用 --hard 模式，檔案 b.txt 不見了，所以也不會在暫存區。\nrevert 如果想要還原任意 Commit，但又想保留在歷史紀錄，就可以使用：\n$ git revert [HEAD~?] 會還原選擇的 Commit，檔案也會還原到舊的版本\nreset 與 revert 差異 指令 reset revert 改變歷史狀態 是 否 說明 把目前狀態設定成某個指定的 Commit 狀態，通常適用於尚未推到遠端的 Commit 新增一個 Commit 來取消另一個 Commit 的內容，原本的 Commit 依舊會保留在歷史紀錄中。通常適用於已經推到遠端的 Commit 其他 tag 標籤是用於標記特定的點或是提交的歷史，通常會用來標記發佈版本的名稱或是編號，例如：v1.0。標籤看起來有點像是分支，但打上標籤的提交是固定的，不能隨意的變更位置。\nGit 中有兩種標籤類型：輕量標籤(lightweight tag)和標示標籤(annotated tag)，他們有什麼區別呢？我們分別列出他們不同之處。\n輕量標籤(lightweight tag)\n不可以變更的暫時標籤 可以添加名稱 標示標籤(annotated tag)\n可以添加打標籤者的名稱、email、日期 可以添加名稱 可以添加註解 可以添加簽名 一般情況下，標示標籤都會用在較為重要的提交上，如發布提交可以使用標示標籤來新增註解或簽名，另一方面，輕量標籤通常使用在本機端最為暫時性的使用或是一次性使用。\n我們分別來看一下要如何新增輕量標籤(lightweight tag)以及標示標籤(annotated tag)吧！\n我們先隨意在分支上推一次 commit ，如下圖，讓我們等等有 commit 可以來新增標籤：\n在隨意的分支推一個 commit\n輕量標籤 使用 tag 且不帶其他的參數來下指令：\n$ git tag lightweight bc4c597 lightweight 是我們 tag 名稱，bc4c597 是剛剛 commit 的 SHA-1\n接著我們使用 git show lightweight 來查看標籤：\n輕量標籤 lightweight\n可以發現因為我們使用「輕量標籤」，所以沒有存任何資訊，但可以在圖片第一行最後面看到我們使用的 tag。\n標示標籤 我們一樣使用 tag，但後面可以加上 -a -m 參數：\n$ git tag annotated bc4c597 -a -m \"可以備註\" -a 參數是請 Git 幫我們建立有附註的標籤，後面的 -m 則是跟我們 commit 一樣可以來輸入訊息\n接著我們使用 git show annotated 來查看標籤：\n標示標籤 annotated\n可以看到我們使用標示標籤，所以可以查看標籤是誰填寫、他的信箱、填寫時間以及他的備註內容。\n官方文件對於這兩種標籤的說明：\n有標示標籤主要用來做像是軟體版號之類的用途，而輕量標籤則是來於個人使用或暫時的標記用途。簡單來說，有標示標籤的好處是有更多關於這張標籤的資訊，假設不是很在乎這些資訊，使用一般的輕量標籤也是沒有問題的！","什麼是-git-#什麼是 Git ?":"不管是不是工程師，只要常常需要使用電腦工作，每天一定都會新增、修改、刪除許多檔案，我們看到這張圖：\n很多人的電腦裡面都有這樣的內容\n這張圖是一個菜鳥工程師在整理檔案時的方法，因為每一天都會對這份檔案做不同的處理，但為了保留以前的版本，所以也不會刪除舊的檔案，只好用日期或是版本來做分類，時間越久，檔案就累積越多，假如不小心刪除，也找不回來紀錄，也不清楚不同檔案的差異，所以有了 Git 這項工具。\nGit 為分散式版本控制系統，是為了更好管理Linux內核而開發的。 Git 的優點：免費開源、速度快、檔案體積小、分散式系統。 Git 的缺點：指令繁雜，但可以透過 GUI 工具解決。 Git 會紀錄哪些資料：更動前 vs 更動後的程式碼、修改者、修改時間、修改原因（修改者需要自行撰寫 commit message）。 我們也常常聽到 GitHub or GitLab 那跟 Git 是一樣的東西嗎？\nAns：GitHub(GitLab) 是基於 Web 的平台，結合了 Git 的版本控制功能，為開發團隊提供了儲存、分享、發布和合作開發項目的中心化雲存儲的場所。","參考資料#參考資料":"什麼是 Git？為什麼要學習它？：https://gitbook.tw/chapters/introduction/what-is-git\nGit：基本概念介紹與指令：https://medium.com/@tina2793778/git-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E4%BB%8B%E7%B4%B9%E8%88%87%E6%8C%87%E4%BB%A4-d5d85607cd7d\n[Git] 初始設定：https://ithelp.ithome.com.tw/articles/10240965\n連猴子都能懂的Git入門指南：https://backlog.com/git-tutorial/tw/\nGit教學】分支合併: merge 與 rebase 差異：https://www.maxlist.xyz/2020/05/02/git-merge-rebase/\nGit 面試題：https://gitbook.tw/interview\nReset、Revert 跟 Rebase 指令有什麼差別？：https://gitbook.tw/chapters/rewrite-history/reset-revert-and-rebase"},"title":"Git 介紹"},"/blog/git-or-cicd/git-merge-multiple-commit/":{"data":{"":"當我們在使用 Git 時，常常修改完內容後，會推 commit 到 github or gitlab，在一個分支上開發久了， commit 會累積很多，很雜且很亂，所以我們可以試著將 commit 給合併。\n大家可以使用這個檔案來做練習：點我 GoGo 😉\ngit commit\n可以看到上面這張圖，這個與範例檔案的 commit 相似(不同專案，所以 SHA-1 也會不同，為了模擬所以 commit 相同而已)，我們模擬在同一個分支底下，有很多的 commit，那我們試著把他給合併起來。先說明一下目前的 commit 狀況，我們在 master 分支上有 3 個 commit，且已經推到遠端上。所以我們本地修改後，還要讓遠端的也合併，這個步驟要怎麼做呢？大家可以先想想看，後面會告訴大家答案 🥰","參考資料#參考資料":"如何合併多個 commits：https://zerodie.github.io/blog/2012/01/19/git-rebase-i/\n【狀況題】聽說 git push -f 這個指令很可怕，什麼情況可以使用它呢？：https://gitbook.tw/chapters/github/using-force-push","合併本地端-commit#合併本地端 commit":"首先我們目的是想要讓 add 2.txt 與 add 3.txt 的 commit 合併成 add txt，可以先使用以下指令來找到他的 commit 的 SHA-1：\ngit log git log 查看 commit 的 SHA-1\n要怎麼合併呢？我們先使用 rebase 到不會變動的 commit，也就是 add 1.txt 這個 commit：\ngit rebase -i 3b5bab9d5fb65b965ae55236734103b178f9daf2 git rebase\n下完後，會跳出上面圖片內容，可以看到上面是 rebase interactive (-i) 要執行的指令，下面是每個指令的簡單說明，我們本次會使用的只有 pick 以及 squash，分別的意思是：\npick：會執行該 commit。 squash：會把這個版本的 commit 合併到前一個 commit。 所以我們要將它改成以下：\npick f8e5882 add 2.txt squash 3eb0ef4 add 3.txt 也就是將 3eb0ef4 這個版本的 commit 合併到 f8e5882 的 commit，對應我們的例子，將 add 3.txt 合併到 add 2.txt 這個 commit。\n儲存離開後，會跳出以下的畫面，他會告訴你原本兩個的 commit message 分別是 add 2.txt 以及 add 3.txt，這時候我們要輸入新的 commit message，也就是 add txt，建議可以把原本的訊息註解掉。\n輸入新的 commit\n儲存後，我們查看 git log，就可以看到我們將 add 2.txt 跟 add 3.txt 合併成 add txt 😝\n查看目前合併狀態的 git log","合併遠端-commit#合併遠端 commit":"可以看到下方是我們已經將本機端的 commit 給合併，但遠端還是一樣有 3 個 commit，如果我們就這樣直接推上去，只會多一次的 commit，所以我們該怎辦呢 ?\n遠端與本地端的 commit 不同\n我們就是要使用大家都害怕的：\ngit push -f 強制覆蓋掉分支上的內容，但切記切記，這個只適用於自己的分支上歐～不然會直接大爆炸 💣\n使用 git push -f 後的 commit"},"title":"如何合併多個 commit，且推到遠端呢？"},"/blog/git-or-cicd/gitlab-cicd/":{"data":{"":"自從上次學完 Jenkins 及 Ansible CI/CD，就覺得 CI/CD 實在太酷了！能夠自動化的去持續整合 (Continuous Integration, CI) 以及持續佈署 (Continuous Deployment,CD) 專案，再加上這幾天複習了 Git 的使用方法，突然想到，要怎麼設定我們將程式推到遠端的 Git Repo，能夠再搭配 CI/CD 去做測試，並且把程式碼自動部署到正式的服務機器設備上呢？\n那我們就開始囉！此篇會複習一下 CI/CD 並且說明 GitLab CI/CD 運作的原理，關於實作部分會放到下一篇文章 部署 Laravel 於 Heroku 搭配 GitLab CI/CD\n對了工商一下，剛剛有提到的 Jenkins 及 Ansible CI/CD 總共有 3 篇文章，還沒看過的可以先飛過去看一下歐 👇👇👇\nJenkins 及 Ansible IT 自動化 CI/CD 介紹\n使用 Jenkins 設定 GitHub 觸發程序並通知 Telegram Bot\nAnsible 介紹與實作 (Inventory、Playbooks、Module、Template、Handlers)\n老樣子文章也會同步到 Github，會附上範例中的程式碼，有需要的也可以去查看歐！ Github 程式碼連結 💓","gitlab-cicd#GitLab CI/CD":"GitLab CI/CD 是 GitLab 內建強大的工具，在 GitHub 被稱為 Github Actions，這個之後有空再來介紹 XD，回歸正題，GitLab CI/CD 可以讓我們持續整合和部署，且不需要使用第三方的應用程式來整合。我們來複習一下 持續整合 CI、持續部署 CD 他們是什麼吧：\n持續整合 CI 一個處於開發階段的專案或是軟體，它被我們放在 GitLab 的 Repository 裡，開發人員每天會推送不同的程式碼到 GitLab 上，GitLab 持續整合 CI 會在開發人員每次推送後，自動化的依照我們設定好的腳本進行建構與測試，從而減少開發中的專案發生錯誤的可能性。\n這種做法就被稱作持續整合，對於我們提交給專案的每一個更改、甚至是開發分支，它都是自動且持續地建構和測試，確保新加入的變動，符合我們在專案中所設計的所有測試。\n持續部署 CD 持續部署，讓我們不太需要手動的去部署專案服務，而是將其設置為自動化部署，完全不需要有人工去干涉，減少了人為的部署錯誤。\nGitLab CI/CD (圖片來源：GitLab Agile Planning)\n大致了解是如何運作之後，我們接著聊聊上面有提到的 設定好的腳本：\n.gitlab-ci.yml GitLab CI/CD 的工作原理是，要在專案根目錄新增一個名為 .gitlab-ci.yml 的文件 (記得文件名稱開頭有 . ，我一開始忘記要加，想說怎麼都沒有反應 😅 )，也就是我們上面說的 設定好的腳本，可以先將所需建構、測試和部署的腳本編寫完成，以及定義很多規則，例如執行命令的先後順序、部署應用程式的位置以及指定是否自動運行或是手動觸發腳本等。\n將 .gitlab-ci.yml 文件放入 Repository 裡，就會觸發 CI，負責管理的 GitLab-CI 就會依照 .gitlab-ci.yml 設定檔來啟動名為 GitLab Runner 的工具來運行腳本，這個 GitLab Runner 我們放到後面來說，我們先來說說 .gitlab-ci.yml 這個設定檔要怎麼編寫，以及編寫後的流程。\n這是一個示範的 .gitlab-ci.yml，選自優良的 GitLab XD，為了說明有小修改程式碼，程式碼也會放在 GitHub 上歐：\n.gitlab-ci.yml stages: - build - test - deploy cache: paths: - config/ build-job: stage: build script: - echo \"Hello, $GITLAB_USER_LOGIN!\" test-job1: stage: test script: - echo \"This job tests something\" test-job2: stage: test before_script: - echo \"This job tests something, but takes more time than test-job1.\" script: - echo \"After the echo commands complete, it runs the sleep command for 20 seconds\" - echo \"which simulates a test that runs 20 seconds longer than test-job1\" - sleep 20 deploy-prod: stage: deploy script: - echo \"This job deploys something from the $CI_COMMIT_BRANCH branch.\" 那我來簡單說明一下上面這些設定檔案的功能：\nstages：代表這個 CI 設定檔有三個 stage 要跑，一個是 build、一個 test、一個 deploy，他們的順序也決定 CI 運作的順序，由 build → test → deploy，假如 test 沒有通過，就不會執行 deploy。 cache：我們在寫 CI 時，常常需要裝 package，但我不想每次都重新跑一次，所以可以寫一個 cache，不要讓 GitLab 每次都重新拉新的 package。 build-job、test-job1、test-job2、deploy-prod：代表我有 4 個 job 要執行，每個 job 裡面有不同的任務，也是顯示在 Pipeline 的名稱。 stage：他現在要執行的階段，對應到 stages。 before_script：可以把它當先需要先執行的指令，後面才會執行主要的 script 指令。所以需要安裝的可以先寫在這裡面。 script：主指令，在實際運行的腳本中，通常會見到多行的指令被依序執行。 $CI_COMMIT_BRANC：當然 .gitlab-ci.yml 檔案也可以帶入參數，這個部分我們留到 部署 Laravel 於 Heroku 搭配 GitLab CI/CD 搭配實際操作來說明。 當然 .gitlab-ci.yml 有很多功能，上面只是簡單說明比較常用的，當你不確定自己寫的 CI 設定檔有沒有問題，沒關係就直接推上去，GitLab 還會先檢查一下設定檔是不是正確：\nGitLab CI/CD 檢查格式有錯\n當我們將 .gitlab-ci.yml 連同專案一起推到 GitLab 上後，我們可以看到它會開始執行我們所寫的腳本，會顯示整個執行過程：\nGitLab CI/CD 執行過程\n查看執行的狀態：\nGitLab CI/CD 狀態\n也可以在 GitLab Pipeline 看到執行的流程：\nGitLab CI/CD Pipeline\nGitLab CI Runner 我們上面有提到，我們在 CI 跑腳本，需要一個 Server 來代替 GitLab 來讓我們執行，這個 Server 我們稱為 Runner。我們來看一下整個執行的圖片：\nGitlab CI/CD 實際執行流程 (圖片來源：Gitlab-CI 入門實作教學 - 單元測試篇)\n那這個 Runner 有分成兩種：\n共享 Runner (Shared Runners) 自架 Runner (Specific Runners) 共享 Runner (Shared Runners) 因為本文章以及後續 部署 Laravel 於 Heroku 搭配 GitLab CI/CD 文章所使用的平台是 gitlab.com，由官方所提供，所以我們直接使用共享 Runner，可以在 repository Settings → CI / CD → Runners 中找到，有不少官方提供的共享 Runner 可以使用，也不需要做任何設定。\nGitLab CI/CD 共享 Runner\n但也有幾個缺點：\n因為是共享，所以 Server 資源也會共享，理論上多人使用的速度還是會比較慢。 以及如果是開源專案，是完全免費。但如果是私人專案，一個月有 400 分鐘的 CI 執行時間限制。 自架 Runner (Specific Runners) GitLab CI/CD 自架 Runner (圖片來源：Best Practice for DevOps on GitLab and GCP : GitLab Runner 簡介與安裝 - Day 7)\nGitLab Server 和 GitLab Runner 是 GitLab CI/CD 中不可或缺的兩者，但如果像公司是自架 GitLab，首先要先找一台電腦或是 Server 做為 Runner，那我們這邊以 Docker 作示範。\nGitLab Runner 的建議建置步驟如下：\n準備/安裝一個 GitLab Server (這邊我們直接使用 gitlab.com) 安裝一個與 GitLab Server 對應版本的 GitLab Runner 在安裝 GitLab Runner 的設備上設定 Executor 什麼是 Executor ?\n如果把 GitLab Runner 當成一個工廠來看，那 Executor 就是工廠內一個又一個的產線，同一個工廠內可以擁有不同種類的產線，Runner 與 Executor 之間的關係就是如此，這些產線會根據專案中 .gitlab-ci.yml 的內容，決定產線以及如何產出開發者期望的產品。\n另外 Executor 的種類非常多，可以看下方這些圖片，因為我們最常使用的就是 Docker，所以我們等等的範例，也是建置在 Docker 之上！\nGitLab Runner Executors\n那我們就開始來實作我們的 GitLab Runner 吧：\n首先，我們回去剛剛在 repository Settings → CI / CD → Runners 左側的 Specific runners GitLab Runner Executors\n可以看到一個註冊的 URL 以及 Token，這個我們在設定 Executor 會使用到！\n接下來開始安裝 GitLab Runner，我們使用 Docker，以下是 Docker 執行的指令：本此使用 gitlab-runner 版本是 alpine-v15.0.0 docker run -d --name gitlab-runner --restart always \\ -v ~/Shared/gitlab-runner/config:/etc/gitlab-runner \\ -v /var/run/docker.sock:/var/run/docker.sock \\ gitlab/gitlab-runner:alpine-v15.0.0 接著進入容器裡面，使用 docker exec -it gitlab-runner gitlab-runner register 來註冊，可以參考下方圖片，輸入 URL 以及 自己的 Token： GitLab Runner 註冊 Executors\n可以回到 gitlab.com 查看 Specific runners 下方是否多了我們剛剛所註冊的 GitLab-Runner GitLab Available specific runners\nGitLab CD GitLab CD 其實就是在 .gitlab-ci.yml 後面加上我們要部署的設定，透過 CI 整合完，我們可以設定他要部署到哪一台機器或是設備上這部分就放到下一篇文章直接用實作來告訴大家要怎麼使用吧！，請大家接續看下一篇 部署 Laravel 於 Heroku 搭配 GitLab CI/CD ，一起學習吧 GoGo !","參考資料#參考資料":"Get started with GitLab CI/CD：https://docs.gitlab.com/ee/ci/quick_start/\nBest Practice for DevOps on GitLab and GCP : GitLab CI/CD - Day 6：https://ithelp.ithome.com.tw/articles/10214114\nGitlab-CI 入門實作教學 - 單元測試篇：https://nick-chen.medium.com/gitlab-ci-%E5%85%A5%E9%96%80%E7%AD%86%E8%A8%98-%E5%96%AE%E5%85%83%E6%B8%AC%E8%A9%A6%E7%AF%87-156455e2ad9f\n如何使用 GitLab CI：https://medium.com/@mvpdw06/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-gitlab-ci-ebf0b68ce24b"},"title":"如何從頭打造專屬的 GitLab CI/CD"},"/blog/git-or-cicd/gitlab-package-registry-to-gcs/":{"data":{"":"今天接到一個案子，RD 部門之後想要使用 GitLab 的 Package Registry 功能來發布套件，且不想把它存在 GitLab 伺服器上，希望可以直接存到 GCP 的 Google Cloud Storage 上，所以才會有了此篇筆記來記錄一下整個過程。\n版本資訊\nGitLab 14.10 (有部分設定會於新版本棄用，請記得確認好自己的版本是否支援) 先說一下，我們的 GitLab 是使用 docker-compose 來建置，所以後續的實作內容都會以 docker-compose 的方式來介紹。","先查看尚未重啟的-gitlab-package#先查看尚未重啟的 GitLab Package":"由於公司 GitLab 預設有先開啟 packages_enabled，所以我就拿同事用 Helm 寫的 CI，來做測試。當更新 value.yaml 後會自動打包 Package 放到 Package Registry 中，我們直接進入到預設 Package Registry 的儲存位置，是在 /var/opt/gitlab/gitlab-rails/shared/packages/，用指令發現打包的 Package 的確存放於此 ，如下：\n檢查是否還有 Package 在預設儲存位置 (尚未遷移)","參考資料#參考資料":"GitLab Package Registry administration：https://docs.gitlab.com/14.10/ee/administration/packages/","啟動-gitlab-的-package-registry#啟動 GitLab 的 Package Registry":"首先，我們當然要先啟動這項 Package Registry 功能，才可以再之後使用它，我們先看一下 GitLab 啟動的 docker-compose.yml 檔案：\nversion: '3' services: gitlab: image: 'gitlab/gitlab-ee:14.10.5-ee.0' restart: always container_name: gitlab hostname: gitlab-pid logging: driver: \"json-file\" options: max-size: \"100m\" max-file: \"50\" environment: GITLAB_OMNIBUS_CONFIG: | external_url '${GITLAB_DOMAIN}' letsencrypt['enable'] = false gitlab_rails['initial_root_password'] = '${GITLAB_ROOT_PASSWORD}' gitlab_rails['gitlab_shell_ssh_port'] = '${GITLAB_HOST_SSH_PORT}' gitlab_rails['backup_keep_time'] = 79200 gitlab_rails['omniauth_allow_single_sign_on'] = ['google_oauth2'] gitlab_rails['omniauth_block_auto_created_users'] = false gitlab_rails['omniauth_sync_profile_from_provider'] = ['google_oauth2'] gitlab_rails['omniauth_sync_profile_attributes'] = ['name', 'email'] gitlab_rails['omniauth_providers'] = [ { 略過．．． } ] ports: - '${GITLAB_HOST_SSH_PORT}:22' - '${GITLAB_HOST_HTTP_PORT}:80' - '${GITLAB_HOST_HTTPS_PORT}:443' volumes: - './config:/etc/gitlab' - './logs:/var/log/gitlab' - './data:/var/opt/gitlab' 有些設定有略過或是省略不寫，大家就依照自己的設定來看就好～","新增-package-registry-設定#新增 Package Registry 設定":"我們在上方的 gitlab_rails['omniauth_providers'] = [ ... 略 ... ] 之後加上新增 Package Registry 設定內容：\ngitlab_rails['packages_enabled'] = true gitlab_rails['packages_object_store_enabled'] = true gitlab_rails['packages_object_store_remote_directory'] = \"GCS 名稱\" gitlab_rails['packages_object_store_direct_upload'] = true gitlab_rails['packages_object_store_background_upload'] = true gitlab_rails['packages_object_store_proxy_download'] = true gitlab_rails['packages_object_store_connection'] = { 'provider' =\u003e 'Google', 'google_project' =\u003e '專案 ID', 'google_json_key_location' =\u003e '/etc/gitlab/google_key.json' } packages_enabled：啟動 packages packages_object_store_enabled：啟動 packages 對象存儲 packages_object_store_remote_directory：設定 packages 對象存儲位置，這邊要輸入 GCS 的名稱 packages_object_store_direct_upload：設定是否可以直接上傳到對象存儲位置 packages_object_store_background_upload：設定是否以後台方式上傳到對象存儲位置 packages_object_store_proxy_download：設定是否可以透過代理伺服器進行套件下載 packages_object_store_connection：設定連接到對象存儲，由於我們要存到 GCS 上面，需要有這三項 provider、google_project、google_json_key_location 才可以將 packages 存到 GCS 上。如果想用其他的儲存位置，例如 Amazon S3、Azure Blob storage 可以參考 Object storage 詳細設定 ( 其中的 google_json_key_location 是要放可以讀寫 GCS 的 SA SECRET 檔案 ) ","重啟設定後再次檢查-gitlab-package#重啟設定後再次檢查 GitLab Package":"當我們重啟設定後，也有建立好可供我們權限 SA 的 GCS 後，會發現原本存在預設 /var/opt/gitlab/gitlab-rails/shared/packages/ 沒有自動跑到 GCS 上，是因為我們還需要手動下指令將他遷移過去，指令是 gitlab-rake \"gitlab:packages:migrate\"，最後等他跑完我們在檢查一下預設儲存位置就發現已經沒有 Package 了\n檢查是否還有 Package 在預設儲存位置 (已遷移)\n開 GCS 網站來看會發現原先在預設儲存位置的 Package 都可以跑到 GCS 上：\n查看已遷移到 Google Cloud Storage 的 Package"},"title":"如何啟用 GitLab 的 Package Registry 以及將儲存位置從伺服器改到 GCS 上"},"/blog/git-or-cicd/jenkins-ansible-it-cicd/":{"data":{"":"在軟體開發領域中，IT 自動化 (automation) 及 持續整合 (Continuous Integration, CI) 、持續佈署 (Continuous Deployment,CD) 是 DevOps 精神中很重要的兩個部分，此文章是參考 30 天入門 Ansible 及 Jenkins ，再加上自己測試後的筆記紀錄，歡迎大家可先閱讀作者原文，那我們就開始一起學習吧 👊\n我會分別介紹 Ansible 與 Jenkins 這兩個非常熱門的開源軟體再搭配實作來讓大家更了解他們，那在之前我們先來聊聊為什麼需要 IT 自動化以及什麼是持續整合/持續部署吧 🤠","ansible#Ansible":"Ansible 是一種 IT 自動化工具。它可以部署軟體、配置系統，並編排更高級的自動化任務，例如 CD (持續部署) 或 RollingUpdate 零停機的滾動更新。\n自動化簡化了複雜的任務，不僅使開發人員的工作更容易管理，也讓他們的注意力可以放在對團體更有價值的其他任務上。換句話說，它可以節省時間並提高效率。Ansible 使用簡單的 YAML 語法，且 Ansible 是一種輕量級且安全的解決方案，它的優點還有：\n使用 Ansible 不需要任何特殊的編程技能，因為使用的是 YAML 語法 Ansible 允許建立高複雜性的 IT 自動化。 因為不需要安裝其他套件，所以伺服器上有更多空間來容納應用服務的支援 Ansible 在設計上非常簡單且可靠一制性。 ","jenkins#Jenkins":"Jenkins 是使用 Java 編成語言編寫最受歡迎的開源自動化服務器。它促進了軟體開發過程中的持續整合、持續部署的自動化過程。\nJenkins 支持 1800 多個其他軟體套件，Jenkins 易於安裝和使用，它還提供方便瀏覽的項目管理儀表板，它的優點還有：\n免費開源 充滿活力的用戶社群 多種工具和技術集成 插件支持 易於安裝、配置和升級 監控外部工作 支持各種身份驗證方法、通知、版本控制等等 ","jenkins-與-ansible-介紹實作連結#Jenkins 與 Ansible 介紹實作連結":"由於 Jenkins 與 Ansible 介紹與實作教學文章較長，故個別分開一篇文章來做說明，大家可以去看自己有興趣的文章歐 😎\nJenkins：使用 Jenkins 設定 GitHub 觸發程序並通知 Telegram Bot Ansible：Ansible 介紹與實作 (Inventory、Playbooks、Module、Template、Handlers) ","jenkins-跟-ansible-比較#Jenkins 跟 Ansible 比較":" 名稱 Jenkins Ansible 套件 Jenkins 支持 1800 多種套件 支持較少套件 語言 支持 C、C++、Java、Perl、Python、Ruby 等 支持 C、Python、JavaScript、Ruby 等 費用 Jenkins 是免費的 Ansible 不是免費的，但有試用版(Red hat) 大小 重量級 輕量級 服務 基於伺服器的工具 基於雲上的工具 ","什麼是持續整合持續部署#什麼是持續整合/持續部署":"當環境搭建成功後，對於服務本身的維護以及監控也是開發流程中相當重要的一環。當我們從原始碼代管服務 (GitHub、GitLab)上取得原始碼後，要如何確保產品在發布前品質沒有問題，一直以來都是開發人員需要思考的一個課題。\n由於現在多數開發團隊都會透過版本控制來提交並整合開發人員各自修改的程式碼，若在合併分支時沒有把合併衝突 (conflict) 處理恰當，或是合併程式碼後產生某些邏輯錯誤，往往會到產品發佈後才發現不可預期的錯誤。\n所以有了持續整合/持續部署的機制下，我們可以透過高頻率的整合、測試並分析程式碼品質，在最短時間發現問題以及發生點，進而確保產品每一次的發布都是穩定且高品質的。\nCI/CD 流程圖 (作者打錯是 rsync 不是 rsyne) Day12 什麼是 CICD\n上面這張圖是簡化版的 CI/CD 流程圖，當我們 Developer 將程式 Push 到原始碼代管服務 (GitHub、GitLab)上，會經過 Webhook 給 Jenkins 這種自動化可以持續整合部署到各自的伺服器上。\n那 CI \u0026 CD 分別負責哪些工作呢？\n持續整合 Continuous Integration 持續整合的英文是 Continuous Integration 我們縮寫成 CI ，後續也會使用 CI 來做說明：\n流程： 程式建置\n開發人員在每一次的 Commit \u0026 Push 後，都能夠於統一的環境自動 Build 程式，透過此步驟可以避免每個開發人員因本機的環境或是套件版本不同導致出現異常。\n程式測試\n當程式編譯完後，透過單元測試測試新寫的功能是否正確，或者確定是否會影響現有功能，透過該步驟進行測試，可以避免開發人員遺忘先在本機檢查，作為雙重驗證之功用。\n目的： 降低人為疏失風險 減少人工手動的反覆動作 進行版本控制 增加系統一制性與透明化 持續佈署 Continuous Deployment 持續佈署的英文是 Continuous Deployment 我們縮寫成 CD ，後續也會使用 CD 來做說明：\n流程： 部署服務\n透過自動化方式，將寫好的程式碼更新到機器上並公開對外服務，另外需要確保套件版本＆資料庫資料的完整性，也會透過監控系統進行服務存活檢查，若服務異常會即時發送通知告知開發人員。\n目的： 保持每次更新程式都可以順暢完成 確保服務存活 我們了解了自動化與 CI/CD 的重要性與功用，那我們要怎麼去實現這些呢！我們先看下面這張圖：\nTop 5 DevOps Automation Tools in 2020\n這邊整理 2020 年適合用於 DevOps Automation 的工具，那我們本次教學會介紹最多人使用的 Jenkins 以及 Ansible 兩種：","參考資料#參考資料":"30 天入門 Ansible 及 Jenkins\nDay12 什麼是 CICD\nJenkins 和 Ansible 的對比和區別","為什麼需要-it-自動化-#為什麼需要 IT 自動化 ?":"當我們在開發任何軟體產品時，除了開發本身的過程需要花相當多的心力外，在產品部署的環節也是讓大家頭痛的一個部分。其環境的搭建或是參數的設定常常會因為一些小原因導致產品無法像在開發時一樣正常運作。尤其當需要部署的主機不只一台時，重複性的工作會花費我們大量的時間。再加上還會因為伺服器提供的作業環境不同、或是其他種種限制而必須做參數上的調整等等。\n這時候 IT 自動化 就顯得十分重要，透過自動化，不但可以幫助開發人員有效減少部署產品所需時間外，還可以在有限度的修改下分別針對不同環境去做調整。\nBest Automation Tools for DevOps"},"title":"Jenkins 及 Ansible IT 自動化 CI/CD 介紹"},"/blog/git-or-cicd/jenkins-github-tg-bot/":{"data":{"":"此文章是接續前面 Jenkins 及 Ansible IT 自動化 CI/CD 介紹 文章，此篇會實際安裝及實作 Jenkins，大家記得在學習前要先檢查自己的版本是否有新的更新！那我們開始囉 😘","jenkins-安裝與實作#Jenkins 安裝與實作":"我這次會使用 Docker-compose 來進行安裝，除了 Docker 以外也有不同的安裝方式，可以參考 Jenkins download and deployment，本次使用的環境版本如下：\n版本 macOS：11.6 Docker：Docker version 20.10.14, build a224086 Jenkins：jenkins/jenkins:lts-jdk11 yamllint：1.26.0 安裝 這邊會使用 Jenkins 提供的 官方 LTS 映像檔 來作為基底，因為我們要多安裝測試程式 yamllint，所以就自己寫一個 Docker-compose：(同樣的程式碼會放在 GitHub，也直接包成映像檔放在 DockerHub，歡迎大家自行取用)\nyamlint，它是語法檢查工具，可以用來檢查 yaml 檔案的語法是否正確以及符合規範，我們看一下實際操作的畫面：\nyamllint 測試\n可以看到如果不符合 yaml 規範就會跳出錯誤訊息。\n接下來先看一下整個 Docker-compose 結構以及各參數：\n. ├── Docker-compose.yaml ├── jenkins │ └── Dockerfile Docker-compose.yaml\nversion: \"3.8\" services: jenkins: build: ./jenkins/ container_name: jenkins ports: - 8080:8080 - 50000:50000 restart: always volumes: - ./jenkins_home:/var/jenkins_home 參數說明：\nbuild: ./jenkins/：因為要先安裝 yamllint，所以使用 Dockerfile 另外寫。 container_name:jenkins：容器的名稱。 ports: -8080:8080 - 50000:50000：8080 是待會我們瀏覽儀表板會使用到的 Port，如果本機上 8080 已經被佔用，可以自行更換，50000 是 Jenkins 所使用的 Port。 restart: always：當容器停止時，會自動重新啟動容器。 volumes: - ./jenkins_home:/var/jenkins_home：掛載目錄，就算刪除容器一樣可以保留其他設定。我將啟動 Docker-compose.yaml 的資料夾下多一個 jenkins_home 與容器內 /var/jenkins_home 做映射，大家可以自己去調整。 jenkins/Dockerfile\nFROM jenkins/jenkins:lts-jdk11 LABEL maintainer=\"880831ian@gmail.com\" USER root RUN apt-get upgrade -y\\ \u0026\u0026 apt-get update -y\\ \u0026\u0026 apt-get install yamllint -y 參數說明：\nFROM：我們使用 Jenkins 官方提供的 LTS 維護版本。 USER：因為要先安裝東西，所以直接給 root 權限。 RUN：先升級完後，再更新，最後再裝 yamllint。(-y 是同意所以詢問) 最後使用 docker-compose 來執行：\n$ docker-compose up -d 要在 Docker-compose.yaml 資料夾下指令才有用。\n下完指令後，他就會在背景開始安裝，可以試著用瀏覽器瀏覽 http://localhost:8080，查看有沒有跳出下面這個畫面：\n瀏覽器訪問 http://localhost:8080\n我們看到它需要輸入一組 Administrator password，我們要使用 docker logs 來查看，會發現最後會有寫 Please use the following password to proceed to installation 的地方：\n$ docker logs jenkins ************************************************************* ************************************************************* ************************************************************* Jenkins initial setup is required. An admin user has been created and a password generated. Please use the following password to proceed to installation: 70c0780f62a7441f90286be106908378 This may also be found at: /var/jenkins_home/secrets/initialAdminPassword ************************************************************* ************************************************************* ************************************************************* 其中的 70c0780f62a7441f90286be106908378 ，就是我們的 Administrator password，直接複製並貼到欄位後按 Continue。\n我們可以看到它詢問是否要安裝套件，我們選擇左邊 Install suggested plugins 安裝推薦的套件即可：\n安裝推薦的套件\n等待它安裝套件，安裝完後會自動跳到註冊畫面：\n等待安裝…\n輸入完基本的資料後，按 Save and Continue：\n創建 Admin 使用者\n最後看到下面這個畫面就代表我們安裝好囉！\nJenkins 儀表板\n建立第一個 Jenkins Job 我們已經成功安裝好並進入到 Jenkins 儀表板，我們先來建立第一個 Job，它的功用是告訴我們系統檔案的即時使用狀況，讓我們對 Jenkins 有初步的了解：\n點選儀表板的新增作業或是 Create a Job： 新增作業\n輸入 Job 專案名稱並選擇建立 Free-Style 軟體專案： 設定 Job 專案\n可以看到這邊有不同的專案類型可以選擇，Free-Style 以及 Pipeline 這兩種類型的專案基本上就涵蓋大部分的需求。Free-Style 類型的專案提供了非常大的彈性讓使用者來做原始碼管理以及建置。如果建置流程涉及多個專案，則可以使用 Pipeline 類型的專案來組合及定義建置邏輯。\n接下來設定專案組態： 設定專案組態\n要記得幫每一個專案都加上描述，讓其他人知道該專案的用途或是使用時機等。\n接著，在建置的下拉式欄位選擇 選擇建置步驟 \u003e 執行 Shell\n設定專案組態\n輸入 df -h 指令：\n設定專案組態\n我們這邊透過建置 執行 Shell 這個建置步驟來告訴 Jenkins，未來這個專案被建置，就會執行 df -h 這個指令。\n專案組態設置完後，我們點選左邊的馬上建置來建置剛剛建好的專案，如果我們設定上沒有問題，應該會在左下角的建置歷程這邊看到我們的第一個建置紀錄： 建置專案\n點進去後，再點 Console Output，可以看到這次建置的結果： 建置專案\n這樣我們的第一個 Jenkins Job 就設定完成囉！Jenkins 也確實的執行我們所設定的指令，並將系統的使用狀況呈現在終端機的輸出上。 以上就是我們第一個簡單的專案建置流程，當然，Jenkins 可以做到的事情不僅如此，在後面我們會透過安裝不同的套件來強化 Jenkins，來達到完整的持續整合 😁","原始碼管理與建置觸發程序#原始碼管理與建置觸發程序":"上面有提到 Jenkins 作為一個持續整合的工具，與原始碼管理系統的整合尤其重要。我們這一章節，會介紹如何在 Jenkins 上透過原始碼管理 (source code management,SCM) 系統，例如從 GitHub 獲得專案的原始碼，並設置建置觸發程序 (build triggers) 來實踐持續整合。\n建置專案 HTTPS 接下來我們先建立一個新的 Job，選擇 Free-style 模式，這次要在原始碼管理裡面選擇 Git，在 Repositories \u003e Repository URL 裡面輸入我們這次要測試的 repository URL：\n設定 Repository URL\nSSH 如果我們想要透過 SSH 來存取專案，我們會遇到以下狀況：\nSSH 尚未設定錯誤訊息\n會有錯誤訊息是因為我們還沒有把 Jenkins 與 GitHub 做 SSH 金鑰配對，所以 GitHub 拒絕 Jenkins 透過 SSH 存取。那要怎麼解決呢？\n最簡單的方法就是在 Jenkins 主機下建立 SSH 金鑰，並將公開金鑰 Key 加入到 GitHub 帳號中，有需要的朋友可以再自行使用，該範例使用 HTTPS 來做設定。\n建置觸發程序 由於我們還沒有定義任何建置的觸發程序，所以除非我們手動去操作 Jenkins，不然 Jenkins 並不會主動幫我們進行建置。因此，在建置觸發程序這個欄位內，我們可以自由設定我們希望 Jenkins 何時自動幫我們進行建置專案。那依照專案的屬性不同，我們也可以採用不同的建置時機。那最常見的有以下兩種：\n定期建置 在 Jenkins 中，我們是採用 Cron Format 的方式來定義建置行程。Cron Format 總共五個欄位，欄位與欄位之間可用空白或 Tab 鍵做區隔：\n分 (minute)：0 - 59 分 時 (hour)：0 - 23 時 日 (day of month)：1 - 31 日 月 (month)：1 - 12 月 星期 (day of week)：星期 0 - 7 (其中 0 與 7 都代表星期天) 假設我們希望每個 30 分鐘就建置一次當前專案，我們可以在定義規則裡面填入 H/15 * * * *：\n定期建置\nGitHub hook trigger for GITScm polling 這種觸發方式在持續整合時非常實用。我們可以讓 Jenkins 自動監測當在原始碼專案有任何的 push event 發生時就進行建置。為了要使用這種方式建置，需要以下幾個步驟來設定：\n新增 GitHub personal access token 進入 GitHub 首頁，點選右上角下拉選單，點選 Settings： GitHub hook trigger for GITScm polling 設定\n先點選左邊的 Developer settings \u003e 點選左邊的 Personal access tokens \u003e 點選右上角的 Generate new token，輸入 token 的描述並勾選 repo scope 以及 admin:repo_hook scope 跟 admin:org_hook scope，點選 Generate token： GitHub hook trigger for GITScm polling 設定\n會產生一個 token，請先把 token 複製下來，離開這個畫面後，token 就會看不到了！(此 token 已刪除 ✌️) GitHub hook trigger for GITScm polling 設定\n設定 Jenkins GitHub 進入 Jenkins 儀表板頁面，點選左邊管理 Jenkins \u003e System Configuration 的設定系統，往下滑找到 GitHub： GitHub hook trigger for GITScm polling 設定\n找到後，再 GitHub Servers 下拉欄位選擇 Add GitHub Server，會看到下面畫面，API URL 輸入 https://api.github.com，Credentials 點 Add： GitHub hook trigger for GITScm polling 設定\nKind 選擇 Secret Text，Secret 輸入剛剛存的 Personal Access Token，Description 簡單描述一下： GitHub hook trigger for GITScm polling 設定\n設定完後，點選一下右邊的 Test connection，如果我顯示 Credentials verified for user UserName, rate limit: xxx 就代表成功囉！ GitHub hook trigger for GITScm polling 設定\n設定專案組態 接著我們跳回來剛剛的專案組態，並勾選 GitHub hook trigger for GITScm polling： 設定專案組態\n寫一個 Shell Script 來建置專案： for file in $(find . -type f -name \"*yaml\") do yamllint $file done 設定專案組態\n這邊利用一個簡單的 Shell Script 迴圈來對所有 YAML file 進行 yamllint 的檢查，最後點選儲存離開。\nGitHub 上整合 Jenkins 先到 GitHub 被建置專案的頁面下點 Setting 標籤 \u003e 點選左邊的 Webhook \u003e 點選右上角的 Add webhook \u003e 輸入 Jenkins Hook URL 到 Payload URL：\n記得 Jenkins Hook URL 後面要加 /github-webhook/，小弟我卡在這裡很久 😢 😢\nWebhooks 設定\nJenkins Hook URL 設定\" 由於我們是將 Jenkins 運行在本機端，所以 Jenkins Hook URL http://localhost:8080 是 Private IP。GitHub 沒有辦法抓 Private IP，為了練習，我們可以透過 ngrok 這套簡單小工具來暫時將 http://localhost:8080 變成 Public IP。\nngrok 的使用方式很簡單，只要先下載 brew install ngrok/ngrok/ngrok ，並使用 ngrok http 8080 指令，將 Private IP 變成 Public IP：\nngrok 將 Private IP 變成 Public IP\n圖片中 https://2063-111-235-135-57.jp.ngrok.io 就是 Public 的 Jenkins Hook URL\n完成後，先點選 Recent Deliveries 檢查是否成功，底下的 Response 需要是 200，才是對的歐！(這裡一定要先檢查，不然後面會找問題到死 XD)\nWebhooks 設定\n測試 我們都設定好後，要開始來測試，我們可以直接先點選 馬上建置，來測試是否可以透過 GitHub personal access token，抓取 GitHub 的檔案。\n馬上建置\n可以看到在建置流程那邊發現建置失敗，點進去可以看詳細內容，\n建置失敗\n點選左側的 Console Output，可以看到我們有成功獲取 GitHub 上得專案，並且執行我們的 Shell 來檢查 yaml 的檔案格式，發現是因為格式有錯誤，所以建置才會失敗 ❌\nConsole Output\n接下來我們先修改一下 yaml 的檔案，後重新 push 到 Github 上，並觀察 Jenkins 會不會自動建置 ！(修改位置大家可以直接看 Commit 結果)\nConsole Output\n當你 push 完後，發現它會自動建置，請因為我們修改成正確格式，所以他也建置成功囉！\n也可以點選左側有一個新的 GitHub Hook Log ，可以看到我們成功透過 GitHub hook trigger for GITScm polling 偵測到有新的 event，透過 WebHook 讓 Jenkins 知道。\nGitHub Hook Log\n建置後觸發通知 當我們自動建置成功當然沒什麼問題，但如果失敗有可能就會影響後續程式的上線時間，所以我們希望建置完成後，可以收到通知，通知除了可以用 email 以外，也可以使用套件去串接我們常用的平台，例如 Telegram、Slack、Line 等等，接下來我會教大家要怎麼去串接這些服務，在開始之前要請大家先安裝兩個套件：\n安裝/設定 Build Timestamp Build Timestamp 這個套件可以幫我們在稍後傳送通知時加上當下的時間戳，那要怎麼安裝套件呢？先到儀表板首頁，點選左側的 管理 Jenkins \u003e 點選 管理外掛程式：\n管理 Jenkins \u003e 管理外掛程式\n再 Plugin Manager 的 可用的裡面搜尋 Build Timestamp，選擇後點下方的 Download now and install after restart，等待他安裝後會自動重啟。\n安裝 Build Timestamp\n重啟後從儀表板點選左側 管理 Jenkins \u003e 點選 設定系統，找到 Build Timestamp，開啟設定，並設定 Timezone 為 Asia/Taipei 以及 pattern yyyy-MM-dd HH:mm:ss z，這樣我們待會就可以使用 BUILD_TIMESTAMP 參數來獲取當下時間，記得要按下儲存歐！\n設定 Build Timestamp\n安裝/設定 Notify.Events Notify.Events 這個套件可以串接很多的平台，例如 Telegram、Slack、Line 等，也可以透過它寄發郵件，是一個十分方便的套件，但缺點是他需要註冊，免費版只有每個月 300 次的訊息傳輸量，但在我們測試階段已經十分夠用。一樣我們用剛剛的方法安裝 Notify.Events。\n安裝 Notify.Events\n安裝好後，Notify.Events 他不需要先設定，它可以依據不同的 Job 有不同的設定，所以我們開啟剛剛的 Job 組態，拉到最下面找到 建置後動作 ，選擇 Notify.Events：\n安裝 Notify.Events\n可以看到這邊要先輸入 Token，那 Token 就必須去官網註冊後設定。\n我們先開啟瀏覽器，搜尋 Notify.Events，註冊帳號後，在 Channels 點選 Create，輸入一下 Title 按下 Save。\nNotify.Events 官網設定\n完成後，應該可以看到以下畫面，這邊就可以讓我們選擇來源，以及要發送到哪裡：\nNotify.Events 官網設定\n我們先選擇 Sources，點選 Add source，可以看到很多來源，選擇 CI/CD and Version control ，再選擇 Jenkins：\nNotify.Events 官網設定\n點選 Next，就可以看到以下畫面，它告訴我們要將它提供的 Token 貼入設定檔，也就是我們剛剛在 Job 組態裡面的那個 Token：\nNotify.Events 官網設定\n設定好 Sources，接下來要設定接收方，回到剛剛 Notify.Events 的儀表板，點選 Subscribe，我們測試使用 Telegram 來當接收方，它會跳出一個視窗，告訴你要怎麼把他的機器人加成好友或是加入群組，這邊就依大家需要自行選擇，那我就將它加入群組，使用 /subscribe DRr0bIZ0 @NotifyEventsBot 指令來綁定\nNotify.Events 官網設定\n這時候我們都設定好了，我們回到 Job 組態的 Notify.Events 設定位置，將 Token 貼上去，它可以自訂訊息的模板，可以全部都一樣，也可以針對建置後的狀態，產生不同的訊息模板，我們來自定義設計一下：\nSuccess\n📢 Jenkins 建置通知 📣 時間：$BUILD_TIMESTAMP 🕐 名稱： \u003ca href=\"$PROJECT_URL\"\u003e$PROJECT_NAME\u003c/a\u003e 次數： \u003ca href=\"$BUILD_URL\"\u003e#$BUILD_NUMBER\u003c/a\u003e 建置狀態： 🟢 \u003cb\u003e$BUILD_STATUS\u003c/b\u003e 🟢 \u003ca href=\"$BUILD_URL/console\"\u003e建置日誌連結\u003c/a\u003e --------- 😍😍😍 --------- Notify.Events Success\nFailure\n📢 Jenkins 建置通知 📣 時間：$BUILD_TIMESTAMP 🕐 名稱： \u003ca href=\"$PROJECT_URL\"\u003e$PROJECT_NAME\u003c/a\u003e 次數： \u003ca href=\"$BUILD_URL\"\u003e#$BUILD_NUMBER\u003c/a\u003e 建置狀態： 🔴 \u003cb\u003e$BUILD_STATUS\u003c/b\u003e 🔴 \u003ca href=\"$BUILD_URL/console\"\u003e建置日誌連結\u003c/a\u003e --------- 😭😭😭 --------- Notify.Events Failure\nTelegram 通知測試 最後我們都設定好了，就來測試一下吧！我們先故意將程式碼格式用錯，讓他先跳出錯誤，再修改，來看看結果如何吧！(文字連結是對應的 Commit )\nTelegram 通知\n可以看到，我們分別兩次的測試，會依據我們建置後的結果，觸發不同的通知模板。","參考資料#參考資料":"30 天入門 Ansible 及 Jenkins\n[CI]設定 jenkins 連結 GitHub Private Repo by Webhook"},"title":"使用 Jenkins 設定 GitHub 觸發程序並通知 Telegram Bot"},"/blog/git-or-cicd/laravel-gitlab-cicd-heroku/":{"data":{"":"經過上一篇文章 如何從頭打造專屬的 GitLab CI/CD 的學習，讓我們了解到 GitLab CI/CD 的整個流程，接著我們本次要把 Laravel 給部署到 Heroku 透過 GitLab 的 CI/CD 去達成，不需要透過任何人工去測試，並上架程式到 HeroKu 上，全部都依賴 GitLab CI/CD，讓我們接著看下去吧！\n當然，此文章程式碼也會同步到 Github ，需要的也可以去查看歐！要記得先確定一下自己的版本 Github 程式碼連結 😏","gitlab-cd-建置#GitLab CD 建置":"我們玩完 CI 後，接著要把程式部署到伺服器或是雲端上，這時候我們不需要透過人工手動的方式，只需要有 CD 來幫我們自動化部署就可以拉！如果不太清楚，可以參考這張圖片：\nGitLab CI/CD workflow (圖片來源：GitLab)\n當我們剛剛進行 CI 的整合測試，最後經過 Review and approve 合併到主分支，這時候如果我們有設定 CD，CD 就會幫我們部署到服務上，我把 CD 流程轉成文字步驟說明：\n把新功能分支合併到 master 分支，代表功能已經可以上線 GitLab 觸發 Gitlab-CI 執行 pipeline Gitlab-CI 執行自動化測試 Gitlab-CI 測試成功後，執行部署到正式伺服器 回傳執行結果至 GitLab 那想要達成自動化部署之前，必須能在遠端用指令下達部署更新！簡單來說有兩件事情：\n要先整理再更新專案時需要哪些指令，並將其寫成腳本 需要獲得伺服器的授權，可以對伺服器下達更新專案的腳本 我們以現在 Laravel 專案來說，套用上面講的兩件事情：\n腳本製作：上線新版本大概要執行以下圖片的內容 Laravel 專案上線前會下達的指令\n對遠端伺服器下指令：通常使用 ssh 與 伺服器做溝通，所以先在伺服器產生授權金鑰給要遠端控制的電腦，如果要給 Gitlab-CI 控制的話，也需要把金鑰存在 GitLab 上，通常使用 ssh user@remote.server 'git pull' 來下達更新專案的指令 本篇我們要部署的是 PaaS 的 HeroKu，可以減少時間去架設環境，就可以達到我們想要的效果，那接著會帶大家從 Heroku 設定開始歐！先簡單介紹一下 Heroku：\nHeroku 是一個支援多種程式語言的雲平台即時服務(PaaS)， 是一種雲端運算服務，提供運算平台與解決方案服務，PaaS 提供使用者將雲端基礎設施部署與建立至使用者端，或者藉此獲得使用程式語言、程式庫與服務。使用者不需要管理與控制雲端基礎設施（包含網路、伺服器、作業系統或儲存），但需要控制上層的應用程式部署與應用代管的環境。\n創建 Heroku 專案 那要使用 Heroku 當然需要一組帳號拉，建立帳號我應該不用再多介紹了吧 🤡 我們直接到 Heroku 頁面，右上角 New，點選 Create new app，輸入本次專案名稱，我就取叫 laravel-gitlab-cicd-heroku (這個不能與別人重複，因為他會生成專屬網頁)， 進去後，點選右上角有一個 Open app，就會跳出這個專案專屬的網頁：\nHeroku 專屬網頁\n設定 HeroKu 與 GitLab 連線 先點選右上角個人頭像 → Account setrtings → 在 Account 往下滑 → API Key，點選 Reveal 並將該 API 記住，這是等等透過 GitLab 部署時會用到的 API Token：\n取得部署的 API Token\n回到 GitLab 專案底下，Settings → CI/CD → Variables，他可以將變數設定在這邊，再讓 .gitlab-ci.yml 來抓取變數，設定以下兩個變數：(詳細可以參考官網)\nKey 名稱(HEROKU_PRODUCTION_PROJECT_NAME)，Value 值(設定我們剛剛在 Heroku 部署的專案名稱，我的是 laravel-gitlab-cicd-heroku) Key 名稱(HEROKU_PRODUCTION_API_KEY)，Value 值(這個就是我們上面的 API Key，每個人都要用自己的歐！上面的我已經重設了 😎 ) gitLab 設定 Variables\n這邊要注意先把預設的 Protect variable 給關閉，他預設會只能在受保護的分支或標籤運行，但我們這此以簡單為主，所以這些設定都先關掉。 新增 Heroku 識別檔案 接下來我們要新增一個檔案名為 Procfile，它是 Heroku 部署更新時會啟動的對象，注意他沒有副檔名，我們在裡面輸入以下：(我們使用合併後的 master)\n新增 HeroKu 識別檔案\n它代表我們網頁服務使用 apache2 指令運行並把入口指向專案資料夾中的 laravel 專案的入口資料夾。\n修改 .gitlab-ci.yml 我們修改原本用來 CI 的腳本，來設定自動化部署的任務 Job 及腳本\n.gitlab-ci.ymlimage: lorisleiva/laravel-docker:latest Production_Deploy: stage: Production_Deploy before_script: - apk add ruby ruby-dev ruby-irb ruby-rake ruby-io-console ruby-bigdecimal ruby-json ruby-bundler yarn ruby-rdoc \u003e\u003e /dev/null - apk update - gem install dpl \u003e\u003e /dev/null script: - dpl --provider=heroku --app=$HEROKU_PRODUCTION_PROJECT_NAME --api-key=$HEROKU_PRODUCTION_API_KEY 最後上傳 GitLab 來觸發 Gitlab-CI 執行自動化部署 (上傳指令就不多說囉，想必大家都會了吧！，不會的話可以去看 Git 介紹，裡面有詳細的介紹 😍 )\n觸發 Gitlab-CI 執行自動化部署\n可以看到部署成功，我們也來看看 Runner 運作狀況：\nRunner 運作狀況\n看到他成功將服務給部署到 https://laravel-gitlab-cicd-heroku.herokuapp.com/。\n既然已經部署好了，當然要去看一下我們的網頁啊，但當我們打開部署好的網頁，會發現跳出 500 Error，雖然他與我們 CI/CD 沒有關係，但我們還是試著解決，那這個問題會發生是因為我們沒有給環境變數的 APP_KEY，這個 Key 可以在專案的 .env 取得，拿到後開啟 Heroku → Setting → Config vars 將 APP_KEY 設定上去。\nRunner 運作狀況\n最後重新整理 https://laravel-gitlab-cicd-heroku.herokuapp.com/，就可以看到我們部署上去的網站囉！\n透過 CD 部署到 Heroku 的 Laravel 首頁","gitlab-ci-建置#GitLab CI 建置":"上傳 Laravel 專案 接下來我們要上傳含有 Unit Test 專案到 GitLab 上，步驟如下，如果已經熟悉如何將專案推到 GitLab，可以直接跳到 在 GitLab 上執行單元測試\n在 gitlab.com 上點選建立專案，選擇 Create blank project，也可以直接瀏覽該網址 https://gitlab.com/projects/new#blank_project。 輸入專案名稱可以選擇 Project deployment target 為 Heroko，選擇 Public，最後按下 Create project 在 GitLab 上建立新專案\n於專案資料夾下加入 remote 遠端 GitLab，並 Push 將專案推上去。 將 Laravel 專案推到 GitLab 上\n成功推上去，可以到 GitLab 上，看到我們剛剛的專案！\n成功推到 GitLab 上\n在 GitLab 上執行單元測試 要在 GitLab 上執行 CI/CD 就需要有 Runner，這次我們選擇使用 gitlab.com 的 Shared runners，想要使用 Specific runners，可以查看上一篇 如何從頭打造專屬的 GitLab CI/CD 文章\n本次使用 Share runners\n接下來在專案的根目錄撰寫我們的 .gitlab-ci.yml 檔案，之後再次上傳 GitLab，當我們根目錄有此檔案，GitLab-CI 就會讀取並依照內容啟動 Runner 來執行工作：\n.gitlab-ci.ymlimage: lorisleiva/laravel-docker:latest Unit_test: before_script: - composer install --prefer-dist --no-ansi --no-interaction --no-progress --no-scripts script: - ./vendor/bin/phpunit --testsuit Unit --coverage-text --colors=never 說明一下這個 yml 檔內的設定是在做什麼：\nimage：因為我們執行 CI/CD 過程中，需要有 PHP、Compose、NPM 等工具，有這些套件管理工具就可以延伸去安裝更多套件，如果一開始沒有安裝，就會很麻煩，其中一個辦法就是去 Runner 環境修改並安裝，但因為方便以及我們這次使用 Share runners，所以不能修改別人的 Runner，另一個辦法是可以使用 image 關鍵字，可以讓 Runner 切換到另一個環境去執行工作 (Job)，我們這邊使用 lorisleiva/laravel-docker:latest ，他裡面已經幫我們安裝好上述的工具了！ Unit_test：這邊也是我們的 Job，那裡面主要是先用 composer install 去安裝我們需要的套件，最後在執行 phpunit 來做單元測試。 上傳 .gitlab-ci.yml 接著我們使用以下指令將含有 .gitlab-ci.yml 的專案上傳到 GitLab，並回到 GitLab 選擇 CI/CD，可以查看目前的 Pipelines，會有我們剛剛所新增的 Runner。\n將 .gitlab-ci.yml 推到 GitLab\n查看 Runner 已經進行執行單元測試檢測\n可以看到 Runner 先安裝我們的環境，再執行單元測試的腳本\n設置須通過測試才可以合併 當我們有了測試還不夠，要怎麼確保每隻要上線 (合併到主分支) 的程式都有經過測試才上線呢？\n接下來我們可以在 GitLab 裡面做這些設定，先到專案的 Setting → General → Merge requests → Merge checks 點選 Pipelines must succeed：\n點選 Pipelines must succeed 來確保程式合併前都必須經過測試\n測試是否可以阻擋未成功情況 我們先模擬要開發新功能，所以在 master 最新 commit 下，建立一個新分支 new\ngit checkout -b \"new\" 接著修改單元測試，故意新增錯誤的測試，開啟專案的 tests/Unit/ExampleTest.php，最下面加上紅色框框程式碼：\n新增錯誤測試，還模擬看看是否能成功擋住\nassertEquals 會檢查這兩個值是否相同，不同的話，就會跳出錯誤，所以我們故意輸入 1 和 2。\n並將它上傳到 GitLab，並發出 Merge Request 看看會有什麼結果！\n將新增錯誤的 ExampleTest 加入暫存，推到 GitLab\n並將 new 分支透過 Merge Request 來合併到 master\n可以看到我們合併在 Pipeline 測試時，因為 new 沒有通過測試，所以也沒有辦法進行合併！\n分支 new 沒有通過測試，所以沒有進行 Merge","參考資料#參考資料":"Gitlab-CI 入門實作教學 - 單元測試篇：https://nick-chen.medium.com/gitlab-ci-%E5%85%A5%E9%96%80%E7%AD%86%E8%A8%98-%E5%96%AE%E5%85%83%E6%B8%AC%E8%A9%A6%E7%AF%87-156455e2ad9f\nGitlab-CI 自動化部屬部署：https://medium.com/@nick03008/%E6%95%99%E5%AD%B8-gitlab-ci-%E5%85%A5%E9%96%80%E5%AF%A6%E4%BD%9C-%E8%87%AA%E5%8B%95%E5%8C%96%E9%83%A8%E7%BD%B2%E7%AF%87-ci-cd-%E7%B3%BB%E5%88%97%E5%88%86%E4%BA%AB%E6%96%87-cbb5100a73d4\n部署 Laravel 於 Heroku 搭配 Gitlab CI/CD：https://medium.com/@vip131430g/%E9%83%A8%E7%BD%B2-laravel-%E6%96%BC-heroku-%E6%90%AD%E9%85%8D-gitlab-ci-cd-6d59a66aebdb","建立-laravel-專案#建立 Laravel 專案":"請大家依照 Laravel 官方文件來建立 Laravel 環境，也可以看小弟我的文章拉 👆👆👆，請記得要先安裝好 php 以及 composer，接著按照以下步驟來建立。\n新建一個 Laravel 新專案\n這時候瀏覽 http://127.0.0.1:8000，如果都正確，應該會看到 Laravel 的首頁\nLaravel 首頁","測試本地-unit-test#測試本地 Unit Test":"接著我們剛剛有提到選用 Laravel 的原因是 Laravel 有 PHPUnit 單元測試可以使用，所以我們現在先在本地端來測試 Unit Test，專案預設有放一個單元測試在 tests/Unit/ExampleTest.php。我們先再次確認環境是否有安裝好，再來執行單元測試。\n在本地端執行單元測試\n執行後，應該都會是通過的畫面，如下圖：\n執行單元測試結果","版本資訊#版本資訊":" macOS：11.6 Docker：Docker version 20.10.14, build a224086 Laravel Installer：2.3.0 Laravel Framework：9.14.1 gitlab.com：GitLab Enterprise Edition 15.1.0-pre 首先，我們第一步驟就是先建立一個 Laravel 專案，至於為什麼要選擇用 Laravel 來當作 GitLab CI/CD 的範例呢？因為 Laravel 內建有 PHPUnit 的測試腳本，可以讓我們在 CI 測試時，更好的展現 CI 的功能！，有關於 Laravel 相關內容，這邊一樣推薦兩篇文章給大家閱讀：🤣\nLaravel 介紹 (使用 Laravel 從零到有開發出一個留言板功能並搭配 RESTful API 來實現 CRUD) Laravel 進階 (內建會員系統、驗證 RESTful API 是否登入、使用 Repository 設計模式) 又工商了一波 XD"},"title":"部署 Laravel 於 Heroku 搭配 GitLab CI/CD"},"/blog/kubernetes/":{"data":{"":"此分類包含 Kubernetes 相關的文章。\nKubernetes 開啟 Region 後，如何減少跨 Zone 網路費用 K8s Node Log Stdout Logrotate 回收機制 kube-dns vs CoreDNS 比較，DNS 的最佳實踐 Pod 出現 cURL error 6: Could not resolve host 在正式環境上踩到 StatefulSet 的雷，拿到 P1 的教訓 部署 Pod 遇到 container veth name provided (eth0) already exists 錯誤 Kubernetes (K8s) 自定義 PHP HorizontalPodAutoscaler (HPA) 指標 Kubernetes (K8s) HorizontalPodAutoscaler (HPA) 原理與實作 用大型社區來介紹 Kubernetes 元件 Kubernetes (K8s) 介紹 - 進階 (Service、Ingress、StatefulSet、Deployment、ReplicaSet、ConfigMap) Kubernetes (K8s) 介紹 - 基本 "},"title":"Kubernetes"},"/blog/kubernetes/gcp-k8s-hpa-php-custom-metrics/":{"data":{"":"此篇要介紹 HorizontalPodAutoscaler 的自定義指標，K8s 內建的指標 (metrics) 只支援 CPU 以及 Memory，如果我們今天想要使用其他的指標來讓 HPA 擴縮呢！？ 不知道什麼是 HorizontalPodAutoscaler 嗎？可以先查看：\nKubernetes (K8s) HorizontalPodAutoscaler (HPA) 原理與實作 這時候我們就必須使用自定義指標，我們一樣來說說他的工作原理吧：HorizontalPodAutoscaler 是怎麼取得自定義指標，以及是跟誰拿到指標的呢？我們從下圖得知：\nKubernetes HPA : ExternalMetrics+Prometheus\nHorizontalPodAutoscaler 會先訪問 K8s 的 API，並向 API 取得指標資料。這邊的 API 就是 custom.k8s.io/v1beta1。\n大致了解後，我們就來進入今天的重點，也就是透過自定義化 PHP 的指標來讓 HorizontalPodAutoscaler 進行擴縮，這次使用的平台是 Google Cloud Platform，前面介紹 GCP 服務的大家可以參考 Google Cloud Platform (GCP) 百科全書 - 介紹與開頭 [ EP.0 ]，我們這邊就直接跳到程式碼與操作部分。\n此文章程式碼也會同步到 Github ，需要的也可以去 clone 使用歐！ Github 程式碼連結 😆","參考資料#參考資料":"Autoscaling Deployments with Cloud Monitoring metric：https://cloud.google.com/kubernetes-engine/docs/tutorials/autoscaling-metrics#custom-prometheus_1\nGoogleCloudPlatform/k8s-stackdriver：https://github.com/GoogleCloudPlatform/k8s-stackdriver/tree/master/custom-metrics-stackdriver-adapter\nhipages/php-fpm_exporter：https://github.com/hipages/php-fpm_exporter\nScaling PHP-FPM with custom metrics on GKE/kubernetes：https://www.ashsmith.io/scaling-phpfpm-with-custom-metrics-gke","實作#實作":"安裝自定義的 Adapter 我們要先 Apply 自定義的 Adapter，這邊我們使用 Google 提供的 Stackdriver Adapter 來使用 (也可以直接使用 Github 程式碼中的 adapter_new_resource_model.yaml 檔案歐)：\nkubectl apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/k8s-stackdriver/master/custom-metrics-stackdriver-adapter/deploy/production/adapter_new_resource_model.yaml Deployment apiVersion: apps/v1 kind: Deployment metadata: name: demo labels: app: demo spec: replicas: 1 selector: matchLabels: app: demo template: metadata: labels: app: demo spec: containers: - name: php-fpm image: php:fpm workingDir: /var/www/service ports: - containerPort: 9000 resources: requests: cpu: 100m memory: 1G limits: cpu: 100m memory: 1G volumeMounts: - name: application mountPath: /var/www/service/ - name: php-fpm-config mountPath: /usr/local/etc/php-fpm.d/www.conf subPath: www.conf - name: nginx image: nginx:alpine workingDir: /var/www/service ports: - containerPort: 80 volumeMounts: - name: application mountPath: /var/www/service/ - name: nginx-config mountPath: /etc/nginx/conf.d/ - name: phpfpm-exporter image: hipages/php-fpm_exporter:latest env: - name: PHP_FPM_SCRAPE_URI value: \"tcp://localhost:9000/status\" - name: PHP_FPM_FIX_PROCESS_COUNT value: \"true\" resources: requests: cpu: 10m limits: cpu: 10m - name: prometheus-to-sd image: gcr.io/google-containers/prometheus-to-sd:v0.9.0 ports: - containerPort: 6060 protocol: TCP command: - /monitor - --stackdriver-prefix=custom.googleapis.com - --monitored-resource-type-prefix=k8s_ - --source=:http://localhost:9253 - --pod-id=$(POD_NAME) - --namespace-id=$(POD_NAMESPACE) resources: requests: cpu: 10m limits: cpu: 10m env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace volumes: - name: application emptyDir: {} - name: php-fpm-config configMap: name: php-fpm-conf - name: nginx-config configMap: name: nginx-conf 我來簡單說明一下，這是一個 Deployment，我們在每一個 Pod 裡面都放 4 個 Container，分別是 php-fpm、nginx、phpfpm-exporter、prometheus-to-sd\nphp-fpm 就是我們要使用的 php，nginx 會提供 9000 Port 讓 phpfpm-exporter 去抓到目前的 Process 數值，最後丟給 prometheus-to-sd，讓他去通知我們剛剛所安裝的 Adapter，就可以透過 API 讓 HPA 知道！聽不太懂嗎？沒關係，幫大家畫了一張圖，請參考下方圖片：\n架構圖\n可以看到 Deployment 裡面，我們使用 ConfigMap 來掛載 php 的 www.conf 以及 nginx 的設定檔，那我們接下來就寫一份 ConfigMap 吧！\nConfigMap apiVersion: v1 kind: ConfigMap metadata: name: php-fpm-conf data: www.conf: | [www] user = 900 group = 900 listen = 9000 listen.owner = 900 listen.group = 900 listen.mode = 0660 pm = dynamic pm.max_children = 150 pm.max_requests = 300 pm.start_servers = 24 pm.min_spare_servers = 24 pm.max_spare_servers = 126 pm.status_path = /status ping.path = /ping ping.response = OK catch_workers_output = yes request_terminate_timeout = 300 clear_env = no --- apiVersion: v1 kind: ConfigMap metadata: name: nginx-conf namespace: ian data: nginx.conf: | server { listen 80; listen [::]:80; server_name _; root /var/www/service/; index index.php; location / { try_files $uri $uri/ /index.php$is_args$args; } location ~ ^/(status|ping)$ { fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; } } 這份 ConfigMap.yaml 檔案裡面分成 php-fpm-conf 來放 www.conf，以及 nginx-conf 來放 nginx.conf 檔案，這邊要注意的是 www.conf 記得要加上 pm.status_path = /status，phpfpm-exporter 是透過這個頁面來獲得 Process 數量。\nHorizontalPodAutoscaler apiVersion: autoscaling/v2beta2 kind: HorizontalPodAutoscaler metadata: name: demo-hpa spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: demo minReplicas: 1 maxReplicas: 10 metrics: - type: Pods pods: metric: name: phpfpm_active_processes target: averageValue: 50 type: AverageValue - type: Pods pods: metric: name: phpfpm_idle_processes target: averageValue: 50 type: AverageValue - type: Pods pods: metric: name: phpfpm_total_processes target: averageValue: 50 type: AverageValue - type: Pods pods: metric: name: phpfpm_accepted_connections target: averageValue: 50 type: AverageValue 這份 HorizontalPodAutoscaler 我們使用的版本是 autoscaling/v2beta2， v2beta1 跟 v2beta2 的設定檔語法有些不同！\n可以看到我們 metrics.pods.metric.name 分別是 phpfpm_active_processes (活動的進程個數)、phpfpm_idle_processes (空閒的進程個數)、phpfpm_total_processes (總共的進程個數)、phpfpm_accepted_connections (當前的連接數量)，如果超過我們所設定的 target 值，HPA 就會作動。\n我們依序把 Deployment \u003e ConfigMap \u003e HorizontalPodAutoscaler 的 yaml 檔案給 Apply，可以觀察一下 Pod 是否都有正常啟動：\nPod 是否正常啟動\n我們到 HorizontalPodAutoscaler 查看我們所設定的 metrics 是否都有抓到目前的值：\nmetrics 是否都有抓到目前的值\n我們也可以用 Google Cloud Platform 的監控來查看：\nGoogle Cloud Platform 的監控\n以上就完成 自定義 HorizontalPodAutoscaler 指標囉～ 😍"},"title":"Kubernetes (K8s) 自定義 PHP HorizontalPodAutoscaler (HPA) 指標"},"/blog/kubernetes/k8s-advanced/":{"data":{"":"前面我們在基本篇中，為了使 Pod 能夠與本機連線，使用了 port-forward，還有另一種方法就是今天要介紹的第一個主題：Service\n那我們會先複習一下 port-forward，再來介紹 Service，當然後面也會有實際操作，請大家跟我繼續一起學下去吧 ！","port-forward#port-forward":"port-forward 簡單來說就是把本機的某一個 Port 與 Pod 所開放對外的 Port 做映射，就像是我們在 Docker 跑 container 時會使用 -p 來連結機器與 container 的 port 一樣～\n使用的方法也很簡單且方便，使用 kubectl port-forward \u003cpod\u003e \u003cexternal-port\u003e:\u003cpod-port\u003e，我們拿 Kubernetes 基本篇最後的範例，來做說明：\n$ kubectl port-forward kubernetes-demo-pod 3000:3000 Forwarding from 127.0.0.1:3000 -\u003e 3000 Forwarding from [::1]:3000 -\u003e 3000 Handling connection for 3000 我們把 kubernetes-demo-pod 這個 pod，用 port-forward 設定本機 3000 port 與 pod 3000 port 做映射，當我們瀏覽 http://localhost:3000 就可以看到 pod 裡面的內容了！\n雖然很方便，可以馬上就開好要映射的 port，但缺點就是每次建立 pod 時都需要手動去打指令來設定 port，且時間久了，也會忘記本機上哪些 port 有被使用到，因此這邊推薦使用 Service 來取代 port-forward，那我們來看看 Service 是什麼吧。","什麼是-configmap-#什麼是 ConfigMap ?":"看到 Config 應該會聯想到與設定檔有關，沒錯 ConfigMap 通常都是用來存放設定檔用的，換句話來說這個物件會直接連結一個或多個檔案，而 ConfigMap 通常都是用來存放偏向部署面的設定檔，像是資料庫的初始化設定、nginx 設定檔等等，這種不用被包進去 image 內，但需要讓 container 可以使用的檔案。\nConfigMap 特性 一個 ConfigMap 物件可以放一個或多個設定檔：我們上面有提到它是用來存放設定檔用的，會直接連接該設定檔。 無需修改程式碼，可以替換不同環境的設定檔：由於設定檔都交由 ConfigMap 管理，並不是包在 image 內，因此可以藉由修改 ConfigMap 的方式來達到不用更新 Pod 內容就可以更換設定檔的作用。 統一存放所有的設定檔：一個 ConfigMap 可以連結一個以上的設定檔，因此也可以將該專案會用到的所有設定檔通通存放在同一個 ConfigMap 物件中進行管理。 如何建立 ConfigMap？ 由於 ConfigMap 可以直接存入設定檔，所以我們以現有設定檔為基準，接下來要用 create 這個參數來建立 ConfipMap 物件出來，這時可能會想，之前都是使用 apply 怎麼這次要改用 create呢！？ 雖然兩者都有建立的意思，但背後實作的技術完全不同：\ncreate 使用的是 Imperative Management，Imperative Management 會告訴 Kubernetes 我目前的動作要做什麼，例如：create、delete、replace 某個物件。 apply 是使用 Declarative Management，Declarative Management 是用宣告的方式來建立物件，更白話一點就是我希望這個物件要長怎麼樣，所以 apply 通常都會搭配 yaml 檔，而這份 yaml 檔就會在 kind 這個設定值告訴 Kubernetes 這個物件要長成什麼樣子。 因為我們這次要直接使用現有的設定檔來建立 ConfigMap，所以這時候不能使用 apply 的方式，只能使用 create 來建立，指令也很簡單：\n$ kubectl create configmap \u003cconfigmapName\u003e --from-file=\u003cfilePath\u003e 建立 ConfigMap\n建立完成使用 get 來查詢是否正確建立 ConfigMap：\n使用 get 查詢是否正確 ConfigMap\n最後可以下 describe 這個參數來查看 ConfigMap 的內容，會發現裡面就是我們設定檔的完整內容：\n使用 describe 查看 ConfigMap 的內容","什麼是-deployment-#什麼是 Deployment ?":"Deployment 是一種負責管理 ReplicaSet 以及控制 Pod 更新的物件，在先前的文章都沒有提到 Pod 的更新，是因為 Pod 無法直接做更新，必須砍掉重建才會是新的內容，有了 Deployment 之後我們就可以很方便的進行 Pod 的更新了！\n由於 ReplicaSet 本身也會控制 Pod，所以整個看起來會像是 Deployment 控制 Pod，但其實真正控制 Pod 的是 ReplicaSet ~\nDeployment 與 ReplicaSet 架構圖\nDeployment 的特性 部署一個應用服務 上面我們提到 Deployment 可以幫助 Pod 進行更新，通常在開發一個產品的時候一定會不斷的更新，透過 Deployment 我們可以快速的更新 Pod 內部的 container，所以通常在部署應用的時候都會使用 Deployment 來進行部署。\n在服務升級過程中可以達成無停機服務遷移 (Zero downtime deployment) 在 Deployment 幫 Pod 內部 container 進行更新的過程有一個專有名稱叫做 RollingUpdate ，RollingUpdate 翻成中文的意思是滾動更新，在更新的過程中 Deployment 會先產生一個新的 ReplicaSet 而這個 ReplicaSet 內部的 Pod 會運行新的內容，待新的 Pod 被 Kubernetes 確認可以正常運行後 Deployment 才會將舊的 ReplicaSet 進行取代的動作，這樣就完成了無停機服務遷移了。\n可以 Rollback 到先前版本 每一次的 Deployment 在進行 RollingUpdate 的時候都會把當前的 ReplicaSet 做一個版本控制的紀錄，就像是 git commit 一樣，所以我們也可以利用這些紀錄來快速恢復成以前的版本，這些 Pod 也就會變成先前的內容。\n講完基本的介紹後，接下來要介紹的是要如何撰寫以及建立一個 Deployment：\nDeployment 搭配 ReplicaSet 寫法 Github 程式碼連結\napiVersion: apps/v1 kind: Deployment metadata: name: kubernetes-deployment spec: replicas: 3 strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 1 minReadySeconds: 60 revisionHistoryLimit: 10 template: metadata: labels: app: demo spec: containers: - name: kubernetes-demo-container image: 880831ian/kubernetes-demo ports: - containerPort: 3000 selector: matchLabels: app: demo 看到 Deployment 是不是覺得跟 Replication Controller 非常相似呢？其實 Deployment 就多了再 RollingUpdate 時的設定以及 ReplicaSet 的設定而已，下面來說明一下這些設定：\n首先一開始的 apiVersion 的值已經不是 v1 了，改成 apps/v1，由於 Kubernetes 針對 Deployment、RollingUpdate、ReplicaSet 等等設定了新的 apiVersion 值，通常都用 apps/v1 都是用來設定跟應用程式有關的架設，所以 Deployment 這邊要記得改成 apps/v1 歐！\n在 spec 的地方有看到 strategy 新的設定值，這個主要用來設定 Deployment 更新的策略，這裡的 strategy.type 有兩種設定：\nRollingUpdate 此為預設值，先建立新的 ReplicaSet 並控制新內容的 Pod，待新 Pod 也可以正常運作後，才會通知 ReplicaSet 將原有的 Pod 給移除，由於過程中會有新舊兩種 Pod 同時上線，因此會有一段時間是新舊內容會隨機出現的情形發生。\n這邊可以看到除了 type 以外還寫了 maxSurage 以及 maxUnavailablle，這兩個設定值為 RollingUpdate 過程的設定，接下來一樣說明一下兩個設定的功能：\nmacSurge：代表 ReplicaSet 可以產生比 Deployment 設定中的 replica 所設定的數量還多幾個出來，多新增 Pod 的好處是在 RollingUpdate 過程中可以減少舊內容顯示在頁面的機率。\nmacUnavailable：代表在 RollingUpdate 過程中，可以允許多少的 Pod 無法使用，假設 macSurge 設定非 0，maxUnavailable 也要設定非 0。\nRecreate 先通知當前 ReplicaSet 把舊的 Pod 砍掉再產生新的 ReplicaSet 並控制新內容的 Pod，由於先砍掉 Pod 才建立新的 Pod ，所以中間有一段時間伺服器會無法連線。\n也因為 Recreate 會砍掉重建，因此 Recreate 無法像 RollingUpdate 設定 maxSurge 以及 macUnavailable。\n講完 Deployment 的更新流程設定後，接下來要講 Deployment 完成更新後的設定，這邊有兩種設定：\nminReadySeconds minReadySeconds 代表當新的 Pod 建立好並且運行的 container 沒有 crash 的情況下，最少需要多少時間可以開始接受 Request，預設為 0 秒，代表當 Pod 一被建立起來，就可以馬上開始接受 Request，假設今天 container 在剛運行的時後需要花時間做初始化，這時候就可以利用 minReadySeconds 讓此 Pod 不會馬上接受到 Request ，這個是選填的設定。\nrevisionHistoryLimit 每次 Deployment 在進行更新的時候，都會產生一個新的 ReplicaSet 用來進行版本控制，在 Deployment 中這個專有名稱為 revision，所以這個設定就是要設定最多只會有多少個 revision，這個也是選填的設定。\nDeployment 建立 老樣子，使用 apply 來建立 Deployment，我們可使用 kubectl get deploy 來查看是否建立成功：\n$ kubectl get deploy NAME READY UP-TO-DATE AVAILABLE AGE kubernetes-deployment 3/3 3 3 8m31s 由於 Deployment 直接管理 ReplcaSet，因此我們可以查看 ReplcaSet 是否也有被建立起來：\n$ kubectl get rs NAME DESIRED CURRENT READY AGE kubernetes-deployment-dc5c59fdb 3 3 3 10m 可以看到 ReplcaSet 後面會自動加上一小段亂數，這邊是 Deployment 在建立 ReplicaSet 的時候加進去的，這樣之後可以更方便的利用 ReplcaSet 進行 rollback 的動作。\n由於 ReplicaSet 會直接管理 Pod，因此我們也可以查看 Pod 是否有被建立起來：\n$ kubectl get po NAME READY STATUS RESTARTS AGE kubernetes-deployment-dc5c59fdb-cjbz6 1/1 Running 0 12m kubernetes-deployment-dc5c59fdb-mjvd7 1/1 Running 0 12m kubernetes-deployment-dc5c59fdb-r92zt 1/1 Running 0 11m 那我們用 minikube dashboard 來查看一下：\nminikube dashboard 查看 Deployment 與 ReplicaSet\nDeployment 的最後要來說的是要如何更新底下的 Pod 呢，大家就接著往下看囉！\n如何更新 Deployment 內部的 Pod 大家都知道 Pod 是 Kubernetes 最小的運行單位，所以更新 Pod 的意思就是把內部運行的 container 進行更新，也就是說我們只要更新 Pod 的 image 就可以順利的讓 Pod 運行最新的內容，Deployment 就是運用這個原理才進行 Pod 的內容更新，方法也很簡單只要利用 Set 這個參數就可以了，可以使用 kubectl set -h 可以看 set 這個參數真正的用法。\nkubectl set\n可以看到 set 後面還要接 SUBCOMMAND，而 SUBCOMMAND 就是 Available Commands 的內容，由於我們要更新的是 image 所以這邊的 SUBCOMMAND 會是 image，完整指令是：\n$ kubectl set image deployment/kubernetes-deployment kubernetes-demo-container=880831ian/kubernetes-demo:v1 --record deployment.apps/kubernetes-deployment image updated 使用 set 來更新 Pod，格式是 kubectl set image deployment/\u003cdeployment name\u003e \u003cpod name\u003e=\u003c要更新的 image\u003e，後面加上 --record 參數，這樣會紀錄每次更新的時候到底更新哪些內容，這樣日後要進行 rollback 也會比較容易知道要 rollback 回哪個 revision，由於要顯示差異，所以在 dockerhub 上又多推一個版本 v1，將原本柴犬的圖片改成 kubernetes logo，等於我們更新是從 kubernetes-demo:latest 更新至 kubernetes-demo:v1，我們來看看它的變化吧！\n這是還沒更新 Pod 前的 Deployment 與 ReplicaSet：\nDeployment 與 ReplicaSet\n我們用 set 下完更新指令後，可以查看 ReplicaSet 以及 Pod 在更新過程中的變化：\nReplicaSet 以及 Pod 在更新過程中的變化\n可以發現 strategy 為 RollingUpdate 的時候並不會把舊有的 Pod 移除反而會讓新舊 Pod 同時上線，以達到無停機服務的作用，但這樣在網頁中就有可能會同時出現新舊內容。\n因為 strategy 為 RollingUpdate，所以會同時出現新舊內容\n最後等到新的 Pod 已經建立完成，且正常運作，ReplicaSet 就會把舊的 Pod 給移掉：\n新的 Pod 建立成功，並刪除舊 Pod\n再次瀏覽就可以發現已經變成新的內容了！代表我們完成更新動作～\n更新至 kubernetes-demo:v1\nDeployment 回朔版本 我們講完更新後，接著要講如何 rollback 回以前的版本，首先我們必須使用 rollout 這個參數，一樣使用 kubectl rollout -h 可以查看 rollout 的用法，跟 set 十分相似。\n更新至 kubernetes-demo:v1\n由於我們這邊要 rollback ，所以 SUBCOMMAND 會使用 undo，我們上面有用 --record 可以查看要 rollback 的版本，所以我們這邊寫法會長像這樣：\n$ kubectl rollout undo deployment/kubernetes-deployment --to-revision=1 deployment.apps/kubernetes-deployment rolled back 格式是 kubectl rollout undo deployment/\u003cdeployment name\u003e --to-revision=\u003c--record 版本\u003e\n就可以看到又回復成以前的柴犬囉 \u003e\u003c\n成功還原柴犬\n為甚麼不用 Replication Controller 最後要討論的是為什麼要不用 Replication Controller 而是改用 ReplicaSet + Deployment？\n由於實際再使用 Kubernetes 時架構會比現在練習的還要複雜，所以用 ReplicaSet 讓 selector 用更彈性的方式選取 Pod 會是比較好的做法。","什麼是-ingress-#什麼是 Ingress ?":"還記得我們在 Service - NodePort 時，需要打 \u003cip\u003e:\u003cport\u003e ，但現在網站除了網域以外，基本上不會需要自己去打 IP 以及 Port 了吧！那為了解決這個問題，有了 Ingreess。\nIngress 可以幫助我們統一對外的 port number，並根據 hostname 或是 pathname 來決定請求要轉發到哪一個 Service 上，之後就可以利用該 Service 連接到 Pod 來處理服務。\n我們先來看一下一般的 Service ：\nService 圖片來源：[Day 19] 在 Kubernetes 中實現負載平衡 - Ingress Controller\n可以看到當多個 Service 同時運行時，Node 都需要有對應的 port number 去對應每個 Server 的 port number。像是 GCP 這種雲端服務，每台機器都會配置屬於自己的防火牆。這也代表，不論新增、刪除 Service 物件，都必須要額外多調整防火牆的設定，Port 的管理也想對複雜。\n若是使用 Ingress，我們只需要開放一個對外的 port numer，Ingree 可以在設定檔中設定不同的路徑，決定要將使用者的請求傳送到哪一個 Service 物件上：\nIngress 圖片來源：[Day 19] 在 Kubernetes 中實現負載平衡 - Ingress Controller\n這樣的設計，除了讓維運人員不需要維護多個 port 或是頻繁的更改防火牆外，可以自訂條件的功能，也使得請求的導向可以更加彈性。\nIngress 功能 將不同\"路徑\"的請求對應到不同的 Service 物件 若沒有設定網域，則該機器上的所有網域只要透過此路徑都可以連接到指定的 Service 物件。\n將不同\"網域\"的請求對應到不同的 Service 物件 若沒有設定路徑，則會以 / 路徑連接到指定的 Service 物件。\n支援 SSL Termination SSL 全名是傳輸層安全性協定，而網站通常都會利用 https 進行加密以確保資料安全，但 Service 與 Pod 之間的溝通都是以無加密方式傳輸，所以 Ingress 就支援解密，讓 Service 與 Pod 可以正常溝通傳遞資料。\nminikube 啟動 Ingress 由於 minikube 預設沒有啟動 Ingress 功能，因此需要額外使用 minikube addons enable ingress 讓 minikube 啟動 Ingress (Ingress 也需要先安裝 Hyperfix)：\nIngress 圖片來源：[Day 19] 在 Kubernetes 中實現負載平衡 - Ingress Controller\n設定 /etc/hosts 加入 Ingress 基本上就需要網域才可以使用，但我們在本機上做練習，所以只要修改本機的 host 檔案就可以了(加入 minikube ip 以及想要的網域名稱)。\n$ vim /etc/hosts 192.168.64.11 test.tw 192.168.64.11 test-test.tw 因為有兩種方法，第一種 (設定網域以及路徑)跟第二種 (只設定路徑沒有設定網域)，所以底下也會分成兩種來說明！\n設定網域以及路徑 Github 程式碼連結\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: kubernetes-demo-ingress spec: rules: - host: \"test.tw\" http: paths: - path: / pathType: Prefix backend: service: name: kubernetes-demo-service port: number: 3000 只設定路徑沒有設定網域 Github 程式碼連結\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: kubernetes-demo-ingress spec: rules: - http: paths: - path: / pathType: Prefix backend: service: name: kubernetes-demo-service port: number: 3000 Ingress 整體寫法與 Service 差不多，只差在 spec 的細部設定，我們就來說一下 spec 設定吧：\n對了～ Ingress 的 apiVersion 不像是 Pod 跟 Service 一樣使用 v1 ，因為目前支援 Ingress 的 API 只有 networking.k8s.io/v1，詳細可以參考 kubernetes Ingress 官網\nrules 這代表這個 Ingress 的轉發規則，此 Ingress 所有的設定都必須寫載 rules 內。\nhost 設定可以連接到 Service 物件的網路名稱。\npath 設定可以連接到 Service 物件的路徑名稱。\npathType 分為 Prefix 和 Exact 兩種，Prefix：前綴符合就符合規則 ; Exact：需要完全一致才行，包含大小寫。\ntype path request path macth Prefix / 全部路徑 Yes Exact /aa /aa Yes Exact /bb /cc No Prefix /aa /aa,/aa/ Yes Prefix /aa/cc /aa/ccc No service.name 設定連接到的 Service 名稱，這裡要填寫的就是 Service 中在 metadata 內寫的 name。\nservice.port.number 設定要經由哪個 port 連接到 Service 物件，就像是 Service 的 Port 要連接到 Pod 的 targetPort。\n建立 Ingress 一樣我們分成兩個做說明\n設定網域以及路徑 一樣使用 apply 來建立：\n$ kubectl apply -f ingress.yaml ingress.networking.k8s.io/kubernetes-demo-ingress created 使用 kubectl get ing 來查詢 Ingress 狀況：\n$ kubectl get ing NAME CLASS HOSTS ADDRESS PORTS AGE kubernetes-demo-ingress nginx test.tw 192.168.64.11 80 10m 接下來我們分別測試寫在 /etc/host 裡面的兩個網域，使用瀏覽器搜尋 test.tw 跟 test-test.tw：\ntest.tw 成功顯示柴犬\ntest-test.tw 顯示 404\n從上面結果可以知道，因為我們有設定 domain，所以只有符合的才會連接到 Service 。\n只設定路徑沒有設定網域 一樣使用 apply 來建立：\n$ kubectl apply -f ingress.yaml ingress.networking.k8s.io/kubernetes-demo-ingress created 使用 kubectl get ing 來查詢 Ingress 狀況：\n$ kubectl get ing NAME CLASS HOSTS ADDRESS PORTS AGE kubernetes-demo-ingress nginx * 192.168.64.11 80 5m56s 接下來我們分別測試寫在 /etc/host 裡面的兩個網域，使用瀏覽器搜尋 test.tw 跟 test-test.tw：\ntest.tw 成功顯示柴犬\ntest-test.tw 顯示 404\n從上面結果可以知道，可以發現 hosts 的部分是 * ，這代表所有背後指向 192.168.64.11 這個 IP 的網域都可以直接連接到 Service 物件。","什麼是-replicaset-#什麼是 ReplicaSet ?":"看到 Replica 大家應該就知道這個跟控制 Pod 的數量有關係了！其實 Replica 跟 Replication Controller 很像，ReplcaSet 提供了更彈性的 selector，在 Replication Controller 中我們必須要把完整的 key/value Label 寫上去，在 ReplicaSet 不用這個麻煩，但 ReplicaSet 一樣也可以寫上完整的 Label ，這個就看開發者要怎麼去設計了！\nReplcaSet 的特性 上面有提到 ReplcaSet 非常彈性的 selector，這邊要說的是 ReplcaSet 是如何讓 selector 變得更加彈性，這裡一共要介紹兩種 ReplcaSet 的 selector 寫法：\nmatchLabels matchLabels 其實跟一般的 selector 做的事情一模一樣，也要寫出完整的 Label 出來，整體大概會長像這樣：\nselector: matchLabels: app: demo matchExpressions matchExpressions 則是提供更彈性的選取條件，每一筆條件主要由 key、operator、value 組成，並且使用一個 { } 包起來，看起來很像 JS 的 物件型態，整體大概會長像這樣：\nselector: matchExpression: - { key: app, operator: In, values: [demo, test] } 看起來似乎很複雜但其實很容易理解，上面的示範中 Expression 翻成中文就是只要 Label 的 key 是 app 且值符合 [demo, test] 陣列中的其中一個值的 Pod 就會被選取到。\n我們在隨便取得例子：假設有一個 matchExpression 長得像這樣：\nselector: matchExpression: - { key: env, operator: NotIn, values: [hello, hi] } 我們可以得知要選取 Label 的 key 為 env 且值不能是 [hello, hi] 其中一個值的 Pod。","什麼是-replication-controller-#什麼是 Replication Controller ?":"大家看到 Controller 就知道 Replication Controller 也是一種 Controller 負責控制 Replication，而 Replication 翻成中文是複製的意思，在 Kubernetes 中 Replication 代表同一種 Pod 的複製品。\n這邊要帶給大家認識一個重要的設定：replica，replica 就是複製品的意思，透過這個設定我們就可以快速產生一樣內容的 Pod，舉例來說：今天設定了 replica: 3 就代表會產生兩個內容一樣的 Pod 出來。\nKubernetes StatefulSet 架構\nReplication Controller 用途 上面有提到 Replication Controller 可以利用設定 replica 的方式快速建立 Pod 數量，除了建立之外 Replication Controller 也確保 Pod 數量與我們設定的 replica 一致，假如今天不小心刪除其中一個 Pod，這時候 Replication Controller 會自動再產生一個新的 Pod 來補齊刪除的 Pod 空缺，所以我們可以善用 Replication Controller 來讓系統更佳穩定。\nReplication Controller 寫法 我們來修改一下之前的 kubernetes-demo.yaml Pod 檔案：(Github 程式碼連結)\napiVersion: v1 kind: ReplicationController metadata: name: kubernetes-demo spec: replicas: 3 selector: app: demo template: metadata: labels: app: demo spec: containers: - name: kubernetes-demo-container image: 880831ian/kubernetes-demo ports: - containerPort: 3000 這個設定檔看似複雜但其實很簡單，可以發現 template 區塊內的設定基本上就是 Pod 的設定，再加上一些屬於 Replication Controller 的設定。\n由於我們要建立的是 Replication Controller，因此在一開始的 spec 要填的是 Replication Controller 的設定，所以 replica 會擺在第一個 spec 內。\n可以再看到 selector，前面提到 Replication Controller 要控制的就是 Pod 的數量，所以這邊的 selector 就是要選取 Pod，就跟我們在 Service 要選取 Pod 的一樣。\n最後一個新的設定：template，template 就是用來定義 Pod 的資訊，所以 Pod 的內容像是 metadata、spec 等等都會寫在 template 內，所以可以把 template 想像成不需要寫 apiVersion 跟 kind 的 Pod 壓模檔，有了這個觀念再來看 template 內的描述就很簡單，只是把 Pod 的內容複製過來而已，而 template 內的 spec 就是寫上 Pod 的 container 資訊。\nReplication Controller 建立 老樣子，也是使用 apply 來建立壓模檔：\n$ kubectl apply -f kubernetes-demo.yaml replicationcontroller/kubernetes-demo created 來查看一下 Replication Controller 是否有成功建立起來，可以使用 kubectl get rc 來查詢：\n$ kubectl get rc NAME DESIRED CURRENT READY AGE kubernetes-demo 3 3 3 28s 接下來可以查看 Pod 是否有出現 3 個，所以使用 kubectl get po 來查詢：\n$ kubectl get po NAME READY STATUS RESTARTS AGE kubernetes-demo-4zkxm 1/1 Running 0 37s kubernetes-demo-cp8jt 1/1 Running 0 37s kubernetes-demo-pt9px 1/1 Running 0 37s 可以看到為了不要讓名稱重複，所以 Replication Controller 會在每一個 Pod 名稱後面加入亂數。\n接下來我們用 minikube dashboard 來測試一下，是否刪除其中一個 Pod 後，Replication Controller 會自動建立新的：\n刪除隨機一個 Pod\n當我們隨機刪除一個 Pod 時，被刪除的 Pod 會 Terminating 準備刪除，且啟動一個新的 Pod ContainerCreating：\nPod 服務\n當新的 Pod 啟動成功後，舊的 Pod 才會被刪除，所以可以確保我們的服務穩定度。\nPod 服務\n綜上所述，可以知道 Replication Controller 真的會控制 Pod 數量，那我們刪掉一個 Pod 他就重生一個，這樣不會永遠都刪不完嗎？其實我們可以把 Replication Controller 砍掉就好了，而 Replication Controller 刪除時，也會自動終止底下的 Pod ，最後 Pod 都會自動刪除。\n但其實 Kubernetes 官方不建議使用 Replication Controller 的方式來控制 Pod，而是建議使用 Deployment 搭配 ReplicaSet 來控制，我們接下來要介紹的主題就是：Deployment 跟 ReplicaSet 。","什麼是-secrets-#什麼是 Secrets ?":"看到 Secrets 這個名字就知道這是非常機密的物件，相較於 ConfigMap 是用來存放部署面的檔案，Secrets 通常都是用來存機密的資料，像是使用者帳號、SSL 憑證等。\nSecrets 特性 上面 ConfigMap 提到的特性 Secrets 一樣，比較特別的是 Secrets 會將內部的資料進行 base64 編碼。因為重新編碼所以可以確保資料相較於 ConfigMap 下安全一些，所以建議如果是機密性的資料就存在 Secrets 裡面吧！\n如何建立 Secrets？ 我們一樣用現有的檔案來做基準作為示範，由於上面 ConfigMap 只存入一個檔案而已，所以這邊 Secrets 我們改成存入多個檔案：\n一樣用 create 的參數進行 Secrets 建立，但這邊要多加一個 SUBCOMMAND 叫 generic，generic 代表意思是從本機檔案、目錄建立 Secrets，接下來只要下：\nkubectl create secret generic \u003csecretName\u003e --from-file=\u003cfilePath\u003e 建立 Secrets\n建立完成使用 get 來查詢是否正確建立 Secrets：\n使用 get 查詢是否正確 Secrets\n最後一樣可以下 describe 這個參數來查看 Secrets 的內容，但因為加密所以不會顯示原本內容，只會看到的確有兩個檔案：\n使用 describe 查看 Secrets 的內容","什麼是-service-#什麼是 Service ?":"service 他其實就是建立的一個網路連線通道，可以讓應用程式正確的連結到正在運行的 pods，而 service 又有 4 種的表現形式，我們接下來會一個一個簡單介紹：\nClusterIP 它是 service 的預設值，所以沒有設定時，預設就是使用該方式做連線，它代表這個 service 只能在相同的 cluster 內使用，無法讓外部做使用。\nNodePort 簡單來說它可以從外部連線到內部使用。假設本機有其他服務，例如：nginx 之類的服務，還有架一個 K8s 的 cluster ，這時候只要設定好 NodePort，就可以讓本機使用 K8s cluster 來使用內部的服務。\nExternalName 主要是為了讓不同 namespace ，以 ClusterIP 所生成的 service 可以利用 ExternalName 設定外部名稱，藉以連到指定的 namespace Service。\nLoadBalancer 這個屬性是強化版的 NodePort，除了 NodePort 可以讓外部連線的優點以外，同時也建立負載平衡的機制來分散流量，很可惜 LoadBalaner 只提供雲端服務，例如：GCP、AWS 等等都有支援，目前 minikube 要使用 LoadBalancer 需要先啟動 tunnel 才能做使用。tunnel 是什麼呢？我們後面會說明！\nk8s service 流程圖 Kubernetes 那些事 — Service 篇\nNodePort 實作 那我們來用 Service (NodePort) 改寫基本篇的連線問題 ：(Github 程式碼連結)\nservice.yaml apiVersion: v1 kind: Service metadata: name: kubernetes-demo-service labels: app: demo spec: type: NodePort ports: - protocol: TCP port: 3000 targetPort: 3000 nodePort: 30001 selector: app: demo 結構與 kubernetes-demo.yaml 相同，以下簡單說明不同之處：\nkind\n該元件的屬性，此設定檔的類型是：Service\nspec\ntype：指定此 Service 要使用的方法，這邊我們使用 NodePort。 ports.protocol：此為連線的網路協議，預設值為 TCP，當然也可以使用 UDP。 ports.port：此為建立好的 Service 要以哪個 Port 連接到 Pod 上。 ports.targetPort：此為目標 Pod 的 Port ，通常 port 跟 targetPort 一樣。 ports.nodePort：此為機器上的 Port 要對應到該 Service 上，這個設定要 nodePort 形式的 Service 才會有效果，假設今天沒有設定 nodePort ，Kubernetes 會自動開一個機器上的 Port 來對應該 Service ，範圍是在 30000 - 37267 之間。 selector.app：如果要使 Service 連接到正確的 Pod 就必須利用 selector，只要原封不動的把 Pod 的 Labels 複製上去即可。 接下來使用 kubectl apply 建立 service：\n$ kubectl apply -f service.yaml k8s 建立 service (NodePort)\n接下來取得 minikube Node IP，可以使用：\n$ minikube ip 192.168.64.11 打開瀏覽器搜尋 192.168.64.11:30001 就可以看到我們可愛的柴犬囉 \u003e\u003c\n成功顯示柴犬\nLoadBalancer 實作 那我們來用 Service (LoadBalancer) 改寫基本篇的連線問題 ：(Github 程式碼連結)\nservice.yaml apiVersion: v1 kind: Service metadata: name: kubernetes-demo-service labels: app: demo spec: type: LoadBalancer ports: - protocol: TCP port: 3000 targetPort: 3000 selector: app: demo 結構與 kubernetes-demo.yaml 相同，以下簡單說明不同之處：\nkind\n該元件的屬性，此設定檔的類型是：Service\nspec\ntype：指定此 Service 要使用的方法，這邊我們使用 LoadBalancer。 ports.protocol：此為連線的網路協議，預設值為 TCP，當然也可以使用 UDP。 ports.port：此為建立好的 Service 要以哪個 Port 連接到 Pod 上。 ports.targetPort：此為目標 Pod 的 Port ，通常 port 跟 targetPort 一樣。 selector.app：如果要使 Service 連接到正確的 Pod 就必須利用 selector，只要原封不動的把 Pod 的 Labels 複製上去即可。 接下來使用 kubectl apply 建立 service：\n$ kubectl apply -f service.yaml k8s 建立 service (LoadBalancer)\n可以看到 dashboard 有我們剛剛啟動的 Service，但是啟動後前面的燈是黃色的，是因為 minikube LoadBalancer 需要透過 tunnel 才可以使用，可以參考 minikube 官網說明：\nminikube 官網 說明 LoadBalancer minikube tunnel\n所以我們需要使用 minikube tunnel 來啟動 tunnel\n$ minikube tunnel ✅ Tunnel successfully started 📌 NOTE: Please do not close this terminal as this process must stay alive for the tunnel to be accessible ... 🏃 Starting tunnel for service kubernetes-demo-service. 就可以看到燈號已經變成綠燈，在外部 Endpoints 多一個連結，可以直接點開\nk8s service\n就可以看到我們可愛的柴犬囉 \u003e\u003c\n成功顯示柴犬","什麼是-stateless-和-stateful#什麼是 Stateless 和 Stateful":"Stateless Stateless 顧名思義就是無狀態，我們可以想成我們每次與伺服器要資料的過程中都不會被伺服器記錄狀態，每一次的 Request 都是獨立的，彼此是沒有關聯性的，也就是我們當下獲得的資料只能當下使用沒有辦法保存，靜態網頁通常都是一種 Stateless 的應用。\n舉個例子來說：今天我想要查詢火車時刻表，我可以藉由 Google 搜尋火車時刻表，並點選連結，或是直接在瀏覽器輸入 https://tip.railway.gov.tw/ ，這兩種結果最後都會一致，並不會因為我的操作不同而產生不同結果，這就是一種 Stateless 的表現。\nStateful Stateful 就是 Stateless 的相反，也就是每次的 Request 都會被記錄下來，日後都可以進行存取，Stateful 最常見的例子是資料庫，所以我們可以理解成 Stateful 背後一定會有一個負責更新內容的儲存空間。\n幾個例子來說：今天我們想要查看 Google 雲端硬碟的內容，我們必須先登入自己的帳號才可以查看內容，這種有操作先後順序才會有結果的就是一種 Stateful 的表現。\nStateless vs Stateful\n那為什麼要提到 Stateless 跟 Stateful 呢？\n因為跟 Pod 有很大的關係，在 Kubernetes 中 Pod 就屬於 Stateless 的，我們前面有提到 Stateless 的特性就是每次的 Request 都是獨立的，這樣有一個好處是可以快速的擴充。\n在 Kubernetes - 基本篇中的 Pod 有提到：Pod 是 Kubernetes 中最小的單位，由於 Pod 是屬於 Stateless 的，即便今天同一種內容的 Pod 有很多個也沒有關係，因為每次的 Request 都是獨立的，多個 Pod 就多個連線的端點而已。\nKubernetes 的 Stateful 上面有說到 Kubernetes 的 Pod 是 Stateless 的，那難道 Kubernetes 沒有辦法做 Stateful 應用嗎？其實是可以的，Kubernetes 為了 Stateful 有特別開啟一個類別叫：StatefulSet\n這邊會簡單說明一下 StatefulSet 的架構：\nStatefulSet StatefulSet 一共有兩個重要的部分：\nPersistent Volume Claim 前面有說到 Stateful 背後有一個更新內容的儲存空間，在 Kubernetes 中負責管理儲存的空間是 Volume，作用與 Docker 的 Volume 幾乎一模一樣，但 Kuberntes 的 Volume 只是在 Pod 中暫時存放的儲存空間，當 Pod 移除之後這個儲存空間就會消失，為了要在 Kubernetes 中建立一個像是資料庫可以永久儲存的空間，這個 Volume 不能被包含在 Pod 中，而這個就是 Persistent Volume (PV)。\nPersistent Volume Claim (PVC) 就是負責連接 Persistent Volume (PV) 的物件，所以可以想像一下今天有多少的 Persistent Volume 就會有多少的 Persistent Volume Claim。\nHeadless Service 還記得在 Service 有提到 ClusterIP 嗎？其實每個 Service 都會有自己一組的 ClusterIP (ExternalName 形式的除外)，所以 Headless 的意思其實就是不要有 ClusterIP，方法也很簡單，直接在設定檔中加入 ClusterIP: None 就可以了！\n這麼做有什麼好處？由於 Headless Service 並沒有直接跟 Pod 有對應關係，因此 Service 本身沒有 ClusterIP，所以 Kubernetes 內部在溝通時就沒有辦法把我們設定好的 Service 名稱進行 IP 轉換，不過 Headless Service 會將內部的 Pod 的都建立屬於自己的 domain，所以我們可以自由的選擇要連接到哪一個 Pod。\n這時候你會說可以用手動來連接呀？但因為 Service 一般是跟著 Pod 的 Label ，所以一個 Service 都會連接許多個 Pod，這樣我們就沒有辦法針對某個 Pod 來做事情，所以 Headless Service 在 Stateful 中也會被建立。\n我們一直強調說 Kubernetes 最小的單位是 Pod，即便是 StatefulSet 也會有 Pod，只是這個 Pod 會歸 StatefulSet 管理，綜合上面所述可以知道一個 StatefulSet 裡面除了執行的 Pod 外還會有負責跟 Persistent Volume 連接的 Persistent Volume Claim，整體的 StatefulSet 架構會長得像這樣：\nKubernetes StatefulSet 架構\n基本上 StatefulSet 中在 Pod 的管理上都是與 Deployment 相同，基於相同的 container spec 來進行; 而其中的差別在於 StatefulSet controller 會為每一個 Pod 產生一個固定識別資訊，不會因為 Pod 改變後而有所變動。\n什麼時候需要使用 StatefulSet ? 如何研判哪些 Application 需要使用 StatefulSet 來部署？只要符合以下條件，就需要使用 StatefulSet 來進行部署\n需要穩定 \u0026 唯一的網路識別 (pod 改變後的 pod name \u0026 hostname 都不會變動) 需要穩定的 Persistent storage (pod 改變後還是能存取相同的資料，基本上用 PVC 就可以解決) 部署 \u0026 擴展的時候，每個 Pod 的產生都是有其順序且逐一慢慢完成的 進行更新操作時，也是與上面的需求相同 StatefulSet 有什麼限制？ v1.5 以前版本不支援，v1.5 ~ v1.9 之間是 beta，v1.9 後正式支援 storage 的部分一定要綁定 PVC，並綁定到特定的 StorageClass or 預先配置好的 Persistent Volume，確保 Pod 被刪除後資料依然存在。 需要額外定義一個 Headless Service 與 StatefulSet 搭配，確保 Pod 有固定的 network identity。 network identity：\n代表可以直接透過 domain name 直接取的 Pod IP; 實現的方法則是部署一個 ClusterIP=None 的 Service，讓 Cluster 內部存取 Service 時，可以直接連到 Pod 而不是 Service IP。\nStatefulSet 寫法 要寫一個 StatefulSet ，有幾個重要的部分必須涵蓋：\nApplication \u0026 Persistent Volume Claim Headless Service .spec.selector 所定義的內容 (matchLabels) 必須與 .spec.template.metadata.labels 相同。 其他部分都與 Deployment 幾乎相同 ~\n我們先來看看要怎麼定義 Application \u0026 Persistent Volume Claim (把 Service 跟 StatefulSet 寫在一起) (Github 程式碼連結)\napiVersion: v1 kind: Service metadata: name: nginx labels: app: nginx spec: ports: - port: 80 name: web clusterIP: None selector: app: nginx --- apiVersion: apps/v1 kind: StatefulSet metadata: name: web spec: serviceName: \"nginx\" replicas: 2 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: k8s.gcr.io/nginx-slim:0.8 ports: - containerPort: 80 name: web volumeMounts: - name: www mountPath: /usr/share/nginx/html volumeClaimTemplates: - metadata: name: www spec: accessModes: [\"ReadWriteOnce\"] resources: requests: storage: 1Gi 我們先打開一個 Terminal 來觀察 StatefulSet 創建 Pod 的過程：\n$ kubectl get pods -w -l app=nginx 我們一樣使用 kubectl apply 來創建定義在 web.yaml 中的 Headless Service 和 StatefulSet。\n$ kubectl apply -f web.yaml 我們看一下剛剛的 StatefulSet 創建 Pod 的過程，可以發現我們擁有 N 的副本的 StatefulSet ， Pod 部署時會按照 {0 …. N-1} 的序號依序創建。\nNAME READY STATUS RESTARTS AGE web-0 0/1 Pending 0 0s web-0 0/1 Pending 0 0s web-0 0/1 ContainerCreating 0 0s web-0 1/1 Running 0 2s web-1 0/1 Pending 0 0s web-1 0/1 Pending 0 0s web-1 0/1 ContainerCreating 0 0s web-1 1/1 Running 0 3s 可以發現 web-1 Pod 是在 web-0 Pod 處在 Running 狀態才會被啟動。此外，可以發現就跟我們上面講的一樣，StatefulSet 中的 Pod 擁有一個獨一無二的身份標記，基於 StatefulSet 控制器分配給每個 Pod 的唯一順序索引。\nPod 名稱的格式是 \u003c statefulset name \u003e-\u003c ordinal index \u003e，像我們 web 這個 StatefulSet 有兩個副本，所以它創建了兩個 Pod：web-0、web-1。\nStatefulSet 測試 我們知道 StatefulSet 它有使用穩定的網路身份以及 PV 的永久儲存，那我們就分別來測試看看：\nStatefulSet 穩定的網路身份 我們先使用 kubectl exec 在每個 Pod 中執行 hostname\n$ for i in 0 1; do kubectl exec \"web-$i\" -- sh -c 'hostname'; done web-0 web-1 再使用 kubectl run 來運行一個提供 nslookup 命令的容器，通過對 Pod 的主機名執行 nslookup，我們可以檢查他在集群內部的 DNS 位置。\n$ kubectl run -i --tty --image busybox:1.28 dns-test --restart=Never --rm /bin/sh 啟動一個新的 shell ，並運行 nslookup web-0.nginx 跟 nslookup web-1.nginx：\n/ # nslookup web-0.nginx Server: 10.96.0.10 Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local Name: web-0.nginx Address 1: 172.17.0.5 web-0.nginx.default.svc.cluster.local / # nslookup web-1.nginx Server: 10.96.0.10 Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local Name: web-1.nginx Address 1: 172.17.0.6 web-1.nginx.default.svc.cluster.local 可以看到 Headless Service 的 CHANCE 指向 SRV 記錄 (記錄每個 Running 的 Pod)。SRV 紀錄指向一個包含 Pod IP 位址的記錄表。\n我們使用 kubectl delete pod -l app=nginx 刪除 Pod 後，會發現 Pod 的序號、主機名、SRV 條目和記錄名稱都沒有改變！\nStatefulSet 永久儲存 我們先查看 web-0 跟 web-1 的 PersistentVolumeClaims：\n$ kubectl get pvc -l app=nginx NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE www-web-0 Bound pvc-e9a23104-d018-4d73-8cd9-89e5ea67f96c 1Gi RWO standard 39m www-web-1 Bound pvc-00dd6c87-6d95-4d04-9c1b-49b43441f4a1 1Gi RWO standard 38m StatefulSet 控制器創建兩個 PersistentVolumeClaims，綁定兩個 PersistentVolumes，因為我們配置是動態提供 PersistentVolume，所有的 PersistentVolume 都是自動創建和綁定的。\nNginx web 服務器默認會加載位於 /usr/share/nginx/html/index.html 的 index 文件。因此我們在 spec 中的 volumeMounts 將 /usr/share/nginx/html 資料夾由一個 PersistentVolume 支持。\n那我們將 Pod 主機名稱寫入 index.html ，再刪掉 Pod 看看寫入內容是否還會存在：\n$ for i in 0 1; do kubectl exec \"web-$i\" -- sh -c 'echo \"$(hostname)\" \u003e /usr/share/nginx/html/index.html'; done $ for i in 0 1; do kubectl exec -i -t \"web-$i\" -- curl http://localhost/; done web-0 web-1 當我們刪除 Pod 後，如果沒有使用 PersistentVolumeClaims 去綁定 PersistentVolumes 的話，資料就會消失，那我們來看看有綁定的結果：\n使用 kubectl delete pod -l app=nginx 刪除 Pod：\npod \"web-0\" deleted pod \"web-1\" deleted 再次使用 kubectl get pod -w -l app=nginx 來檢查 Pod 的狀態：\nweb-0 0/1 ContainerCreating 0 0s web-0 1/1 Running 0 2s web-1 0/1 Pending 0 0s web-1 0/1 Pending 0 0s web-1 0/1 ContainerCreating 0 0s web-1 1/1 Running 0 1s 一樣使用 for i in 0 1; do kubectl exec -i -t \"web-$i\" -- curl http://localhost/; done 來查看：\nweb-0 web-1 可以發現 web-0、web-1 雖然重新啟動，但依舊會監聽它們主機名，因為和它們的 PersistentVolumeClaim 相關聯的 PersistentVolume 被重新掛載到了各自的 volumeMount 上。","參考資料#參考資料":"Kubernetes 那些事 — Service 篇：https://medium.com/andy-blog/kubernetes-%E9%82%A3%E4%BA%9B%E4%BA%8B-service-%E7%AF%87-d19d4c6e945f\nKubernetes 那些事 — Ingress 篇（一）：https://medium.com/andy-blog/kubernetes-%E9%82%A3%E4%BA%9B%E4%BA%8B-ingress-%E7%AF%87-%E4%B8%80-92944d4bf97d\nKubernetes 那些事 — Ingress 篇（二）：https://medium.com/andy-blog/kubernetes-%E9%82%A3%E4%BA%9B%E4%BA%8B-ingress-%E7%AF%87-%E4%BA%8C-559c7a41404b\nKubernetes 那些事 — Stateless 與 Stateful：https://medium.com/andy-blog/kubernetes-%E9%82%A3%E4%BA%9B%E4%BA%8B-stateless-%E8%88%87stateful-2c68cebdd635\nkubernetes ReplicationController：https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/\nKubernetes 那些事 — Replication Controller：https://medium.com/andy-blog/kubernetes-%E9%82%A3%E4%BA%9B%E4%BA%8B-replication-controller-5c8592d37083\nKubernetes 那些事 — Deployment 與 ReplicaSet（一）：https://medium.com/andy-blog/kubernetes-%E9%82%A3%E4%BA%9B%E4%BA%8B-deployment-%E8%88%87-replicaset-%E4%B8%80-406234a63d43\nKubernetes 那些事 — Deployment 與 ReplicaSet（二）：https://medium.com/andy-blog/kubernetes-%E9%82%A3%E4%BA%9B%E4%BA%8B-deployment-%E8%88%87-replicaset-%E4%BA%8C-f60146c878e4\nKubernetes 那些事 — Deployment 與 ReplicaSet（三）：https://medium.com/andy-blog/kubernetes-%E9%82%A3%E4%BA%9B%E4%BA%8B-deployment-%E8%88%87-replicaset-%E4%B8%89-142b2863eb94\nkubernetes Deployments：https://kubernetes.io/docs/concepts/workloads/controllers/deployment/\n[Kubernetes] StatefulSet Overview：https://godleon.github.io/blog/Kubernetes/k8s-StatefulSets-Overview/"},"title":"Kubernetes (K8s) 介紹 - 進階 (Service、Ingress、StatefulSet、Deployment、ReplicaSet、ConfigMap)"},"/blog/kubernetes/k8s-hpa/":{"data":{"":"此篇是要介紹 HorizontalPodAutoscaler (HPA) 的原理以及實作內容，那我們先來說明一下 HorizontalPodAutoscaler 是什麼吧！","horizontalpodautoscaler-原理#HorizontalPodAutoscaler 原理":"HorizontalPodAutoscaler (HPA) 中文可以叫水平 Pod 自動擴縮，他會自動更新工作負載資源 (Deployment 或 StatefulSet)，其目的是透過自動擴縮工作負載來滿足使用需求。\n簡單來說他會根據你現在的負載去調整你的 Pod 數量，當目前的負載超過配置的設定時，HPA 會指示工作負載資源 (Deployment 或 StatefulSet) 擴增，來避免塞爆同一個負載資源。\n如果負載減少，且 Pod 數量高於配置的最小值，HPA 也會指示工作負載資源 (Deployment 或 StatefulSet) 慢慢縮減。其中水平 Pod 自動擴縮不適用 DaemonSet 工作負載資源。\nHPA 是如何工作呢？ HorizontalPodAutoscaler 工作流程圖\nKubernetes 將水平 Pod 自動擴縮定義為一個間歇運行的控制迴路，他不是連續的，其間隔是由 kube-controller-manager 的 --horizontal-pod-autoscaler-sync-period 參數來配置，預設是 15 秒，controller-manager 會依據每一個 HPA 定義的 scaleTargetRef 來找到是哪一個工作負載資源需要進行水平 Pod 自動擴縮，然後根據目標資源的 .spec.selector 標籤選擇對應的 Pod，並從資源指標 API 或自定義指標獲取 API，目前總共有三種的資源指標，分別是 CPU、Memory、自定義指標。\n有關於其他 Kubernetes 觀念部分，可以先查看：\nKubernetes : Kubernetes (K8s) 介紹 - 基本 kubernetes : Kubernetes (K8s) 介紹 - 進階 (Service、Ingress、StatefulSet、Deployment、ReplicaSet、ConfigMap) 此文章程式碼也會同步到 Github ，需要的也可以去 clone 使用歐！要記得先確定一下自己的版本 Github 程式碼連結 😆","參考資料#參考資料":"Horizontal Pod Autoscaling：https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/\nHorizontalPodAutoscaler Walkthrough：https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/","實作#實作":"這次實作要使用的叢集是 Minikube，所以照以前文章一樣，我們先啟動 Minikube。本次會使用 K8s 的管理工具：k8slens 來做為輔助，大家有興趣也可以先去下載來使用歐 😘\n啟動 Minikube 叢集 啟動 Minikube 叢集 minikube start --vm-driver=docker 啟動 Minikube\n設定 Metrics Server 由於我們 HorizontalPodAutoscaler 會根據現在的負載來判斷是否要新增新的 Pod 來解決負載資源用完的問題，所以第一個條件就是要先獲的目前的負載資源使用量，這時候我們必須先在 K8s 叢集上安裝 Metrics Server，透過他讓我們可以知道目前的負載使用量！\n先到 kubernetes-sigs/metrics-server 下載最新的 components.yaml 檔案下來，以我這次示範的版本為例，大家可以點我下載 👇\n下載完，需要先修改兩個地方，才能 apply 到 Minikube 叢集，第一個是修改 kubelet-preferred-address-types=InternalIP，以及新增 kubelet-insecure-tls=ture 讓 Metrics Server 禁用 TLS 證書驗證，詳細可以參考以下照片：\n修改 components.yaml\n接著 apply 這個 yaml 檔案： apply components.yaml\n可以檢查一下 deployment.apps/metrics-server 是否有成功建立或是 Pod 是否有問題：\n檢查 metrics-server 是否有問題\n開始撰寫實作檔案 接著我們就依照官方的 HorizontalPodAutoscaler Walkthrough 的文章開始囉！首先先寫一個 index.php，這個檔案是用於後續測試 HPA 負載使用量的程式： index.php\n新增 Dockerfile，我們使用 php:5-apache 的 image，並複製剛剛寫的 index.php 到容器內： Dockerfile\n將該 Dockerfile Build 起來，推到 DockerHub 上，這部分就不多說明，有興趣可以參考以前文章，大家可以直接使用 880831ian/php-apache 我推好的 image 來使用。 新增 Deployment.yaml，裡面會使用我們剛剛打包推到 DockerHub 上的 image： apiVersion: apps/v1 kind: Deployment metadata: name: php-apache spec: selector: matchLabels: run: php-apache replicas: 1 template: metadata: labels: run: php-apache spec: containers: - name: php-apache image: 880831ian/php-apache ports: - containerPort: 80 resources: limits: cpu: 500m requests: cpu: 200m --- apiVersion: v1 kind: Service metadata: name: php-apache labels: run: php-apache spec: ports: - port: 80 selector: run: php-apache 這邊大家可以先記得我們在 containers.resources 有設定 cpu 的 requests 為 200m。後續在計算副本數時會再提到 😬\n最後將 Deployment.yaml 給 apply，檢查一下是否有正常啟動： k8slens 檢查是否正常\n使用 kubectl autoscale 來幫助我們創建 HorizontalPodAutoscaler： kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10 我們在 Deployment.yaml 裡面 containers.resources 有設定 cpu 的 requests 是 200m，也就是 200 milli-cores，當我們現在設定 HPA 的平均 CPU 使用率為 50%，所以我們只要超過 200m / 2 = 100m，也就是 requests 超過 100 milli-cores 就會產生新的 Pod。這邊最少 Pod 為 1，最多為 10。後續等測試時，會帶大家計算他是如何產生 Pod 的 😏 查看目前 HPA 使用量，因為我們這個 php-apache 還沒有任何的訪問，所以是 0% / 50% (承上面所說，所以這邊的值是 0 / 100 milli-cores)，後面也可以看到我們所設定最高跟最低的 Pod，以及目前的副本數。 查看目前 HPA 使用量\n當然我們除了用剛剛的指令以外，我們也可以自己寫 HPA 的 yaml 檔案。我這邊先使用 kubectl get hpa php-apache -o yaml \u003e hpa.yaml 來將剛剛用指令 run 起來的 HPA 變成 yaml 檔，我們來看看裡面有哪些內容吧： apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: creationTimestamp: \"2022-07-12T03:16:11Z\" name: php-apache namespace: default resourceVersion: \"19454\" uid: dde68e68-9b6e-46a8-b50f-5525b8ec3bdf spec: maxReplicas: 10 metrics: - resource: name: cpu target: averageUtilization: 50 type: Utilization type: Resource minReplicas: 1 scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: php-apache 後面狀態省略.... apiVersion：要記得使用 autoscaling/v2 kind：HorizontalPodAutoscaler maxReplicas：是我們剛剛用指令的最大 Pod 副本數 metrics：我們指標設定 cpu resource，其設定平均使用率為 50 % (百分比) minReplicas：剛剛用指令的最小 Pod 副本數 scaleTargetRef：設定我們這個 HPA 是依照 php-apache 這個 Deployment。 ","常見問題及解決辦法#常見問題及解決辦法":" ℹ️ Q1 . 出現 x509: cannot validate certificate for 192.168.XXX.XXX because it doesn’t contain any IP SANs 錯誤\nAns 1：是因爲沒有加入 kubelet-insecure-tls=ture 讓 Metrics Server 禁用 TLS 證書驗證，才導致錯誤發生。","測試-hpa#測試 HPA":" 接下來我們都設定好後，我們要來模擬增加負載，看看 HPA 的後續動作，首先我們先使用以下指令來持續觀察 HPA： kubectl get hpa php-apache --watch NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE php-apache Deployment/php-apache 0%/50% 1 10 1 4h40m 以及執行以下指令，該指令是建一個新的 Pod，由新的 Pod 無限循環的去向 php-apahe 服務發出請求 kubectl run -i --tty load-generator --rm --image=busybox:1.28 --restart=Never -- /bin/sh -c \"while sleep 0.01; do wget -q -O- http://php-apache; done\" 模擬增加負載\n我們使用 k8slens 觀察，會發現當負載使用量超過我們前面所設定的 50%，也就是 100 milli-cores 時，他就會自動長新的 Pod 出來： HPA 自動長 Pod\n我們切回去看觀察 HPA 指令 kubectl get hpa php-apache --watch： 觀察 HPA\n我們剛剛有說超過 50%，也就是 100 milli-cores 時，會長新的 Pod。我們以上面圖片的來說明，來計算期望需要幾個副本?\n官方有提供一個公式： 期望副本数 = ceil[目前副本数 * (目前指標 / 期望指標)] ，可以看到我們負載從 0% 從到 250%，也就是我們實際上是從 0 變成 500 milli-cores (250 / 50 _ 100 )。我們將值帶入公式內，目前的副本：1 _ (目前指標：500 / 期望指標是：100)得出來的值是 5，如果有小數，因為前面有ceil，所以會取整數(不可能開半個 Pod 對吧 🙄)，最後以這個例子來說，最終得到的 期望副本数 = 5\n根據我們計算出來的副本數，他就會依照計算結果，幫我們自動生成該數量的 Pod，來減緩同一個 Pod 的負載量，接著我們先中斷測試指令，再繼續觀察 HPA ： 觀察 HPA\n可以發現因為我們將測試指令中斷後，負載使用量會慢慢降低，但負載降低後，副本數不會馬上變回去，因為怕如果又有大量的使用量會導致 Pod 來不及長出來，所以預設是 5 分鐘 (--horizontal-pod-autoscaler-downscale-stabilization) 才會減至原來的 Pod 數量\nPod 自動減少","版本資訊#版本資訊":" macOS：11.6 Minikube：v1.25.2 Kubectl：Client Version：v1.24.1、Server Version：v1.23.3 Metrics Server 3.8.2 "},"title":"Kubernetes (K8s) HorizontalPodAutoscaler (HPA) 原理與實作"},"/blog/kubernetes/k8s-node-log-stdout-logrotate/":{"data":{"":"我們知道可觀測性的三大支柱是 Log、Metics、Trace，其中 Log 是最基礎的一環，透過 Log 我們可以了解應用程式的運作狀況，當應用程式出現問題時，透過 Log 可以快速定位問題，進而解決問題。\n以我們公司為例，我們主要使用 GCP，現在也開始用 AWS 來當作 DR 的環境。在 GCP 我們原本會使用 Log Exporter 來查看服務及系統的 Log，但為了更方便的管理，我們改使用 Datadog 來統一收集 Log (AWS 也是)。\n要如何傳遞給 Datadog，我們依照 Datadog 官方文件建議，將 Log Stdout，讓 Datadog agent 去收集 Log，這樣就可以統一管理 Log 了，詳細可參考 Datadog Kubernetes log collection。","node-logrotate#Node Logrotate":"當然，有在使用過相關 Log 工具，對 Logrotate 一定不陌生，第一個想法就想說，會不會 Node 上有對應的 Logrotate 設定，去定期的去處理 /var/log 的檔案，因次我們這邊針對 GKE(GCP) 以及 EKS(AWS) 來做說明：\nGKE 首先，我們先連線到 GKE Node 上，進到 /etc/logrotate.d 底下，可以看到以下圖片的設定，在 GKE 裡面只有一個 Logrotate 設定檔，他會針對整個 /var/log 底下的 log 檔案做處理。\nGKE /etc/logrotate.d 底下檔案設定\nEKS 一樣的步驟，進到 /etc/logrotate.d 底下，可以看到以下圖片的設定，EKS 會有多個 Logrotate 設定檔，但卻沒有對 /var/log/pods 底下的 log 檔案做處理，所以 EKS 需要額外去寫 DameonSet 來處理 /var/log/pods 底下的 log 檔案回收嗎？\nEKS /etc/logrotate.d 底下檔案設定","參考資料#參考資料":"Logging Architecture 日誌架構：https://kubernetes.io/docs/concepts/cluster-administration/logging/#log-rotation\nDatadog Kubernetes log collection：https://docs.datadoghq.com/containers/kubernetes/log/?tab=datadogoperator","尋找答案#尋找答案":"最後在 Kubernetes 官方文件中 Logging Architecture 有看到兩個設定值，分別是 containerLogMaxSize (預設 10Mi)，以及 containerLogMaxFiles(預設 5)。kubelet 會根據這兩個設定值來決定是否要回收 Log。\n如果 Log 檔案大小超過 containerLogMaxSize，kubelet 就會重新開始寫新的 Log 檔案，如果 Log 檔案數量超過 containerLogMaxFiles，kubelet 會刪除最舊的 Log 檔案。\n那要怎麼查看當前 node 的 kubelet 設定值呢？可以透過以下指令：\njournalctl -u kubelet --no-pager | grep \"container-log-max\" 就可以看到底下兩個設定值：（圖片上面是 EKS，下面是 GKE）\n查看 containerLogMaxSize 跟 containerLogMaxFiles 當前設定值\n可以發現都跟預設值一樣。\n那我們也直接進到 /var/log/pods 底下，來查看是否是依照這個規律去運作。\n進入 /var/log/pods\n可以看到，就跟上面說的資料夾名稱是由 {namespace}_{pod name}_xxx 來命名，我們進去到隨機一個資料夾來查看 Log 檔案。\nGKE EKS 可以看到，GKE 跟 EKS 檔案數量都是 5 個，且每個都是小於 10Mi (因為計算誤差，所以有些會顯示 11、12)，所以可以確定是依照 containerLogMaxSize 跟 containerLogMaxFiles 在運作的。\n也可以看到 GKE Log，第一次查看最舊的檔案是 0.log.20250317-081814.gz，第二次查看最舊的檔案就變成 0.log.20250317-081844.gz，就代表超過 5 份 Log 檔案，kubelet 就會刪除最舊的 Log 檔案。","疑問#疑問":"我之前ㄧ直以為 Container Log Stdout 不會寫實體檔，後來發現，其實它還是會寫，但當 Node 重啟，或是 Pod 被刪除時，才會移除，要怎麼查看，我們可以進到 GKE or EKS 的 node 機器上，到 /var/log/pods/ 路徑底下，就可以看到運作在該 node 上的 pod，資料夾名稱會是以 {namespace}_{pod name}_xxx 來命名。\nHow nodes handle container logs 圖片：Logging Architecture\n所以就好奇，是誰來管理這些 Log 的回收機制呢？如果沒有回收機制，Log 不會把 Node 的 Disk 給寫爆嗎？"},"title":"K8s Node Log Stdout Logrotate 回收機制"},"/blog/kubernetes/k8s-open-region-how-to-reduce-cross-zone-costs/":{"data":{"":"由於最近公司開始導入 Multi Zone 架構，這時候會出現一個問題，就是明明我是同一個 GKE，但因為有多個 Zone，導致開始會有一些跨 Zone 的網路傳輸費用產生 (每 Gib 會收 0.01 美元)，可以參考下方圖片：\n跨域費用說明\n所以此文章會針對要如何減少跨 Zone 網路費用進行測試以及提供解決辦法。","參考資料#參考資料":"Google Cloud 內部 VM 之間的資料移轉定價：https://cloud.google.com/vpc/network-pricing?hl=zh-TW\nTopology Aware Routing：https://kubernetes.io/docs/concepts/services-networking/topology-aware-routing/\nAWS 相關文件 (AWS 也會有類似問題)：https://aws.amazon.com/tw/blogs/containers/exploring-the-effect-of-topology-aware-hints-on-network-traffic-in-amazon-elastic-kubernetes-service/","建置-gke-環境#建置 GKE 環境":" [!NOTE] GKE 資訊： Tier / Mode：Standard\n版本：1.32.2-gke.1297002\nLocation type：Regional\nDefault node zones：[\"asia-east1-a\", \"asia-east1-b\"]\n使用的 Module 請參考：880831ian/IaC terraform { source = \"${get_path_to_repo_root()}/modules/gke\" } include { path = find_in_parent_folders(\"root.hcl\") } inputs = { name = \"test\" regional = true region = \"asia-east1\" zones = [\"asia-east1-a\", \"asia-east1-b\"] release_channel = \"REGULAR\" master_ipv4_cidr_block = \"172.16.1.16/28\" network_project_id = \"xxxxx\" network = \"default\" subnetwork = \"default-asia-east1\" gcs_fuse_csi_driver = false enable_maintenance = false monitoring_enable_managed_prometheus = false deletion_protection = false node_pools = [ { name = \"test-pool\" machine_type = \"e2-small\" min_count = 1 max_count = 1 } ] } (這邊小提醒，如果使用多個 Zone，在 node 的 CA 記得要用 min_count 跟 max_count，他才是依照 Zone 去長，也就是 test-pool 會分別在 Zone A 跟 Zone B 去長歐)","測試連線#測試連線":"原生 Service 測試 建立完後，啟動兩個 nginx 服務，分別叫做 nginx-a 跟 nginx-b，各會有兩個 pod，並使用 podAntiAffinity 來限制同一個服務能建立在兩個 Zone 上面。\n相關的程式碼都會放在 880831ian/k8s-open-region-how-to-reduce-cross-zone-costs\n程式碼 下面附上其中一個 nginx-a 服務 yaml (nginx-b 也跟 nginx-a 一樣，只有名稱改變)\napiVersion: apps/v1 kind: Deployment metadata: name: nginx-a spec: replicas: 2 selector: matchLabels: app: nginx-a template: metadata: labels: app: nginx-a spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - nginx-a topologyKey: topology.kubernetes.io/zone containers: - name: nginx image: nginx:latest ports: - containerPort: 80 volumeMounts: - name: nginx-conf mountPath: /etc/nginx/conf.d volumes: - name: nginx-conf configMap: name: nginx-conf --- apiVersion: v1 kind: Service metadata: name: nginx-a spec: selector: app: nginx-a ports: - protocol: TCP port: 80 targetPort: 80 --- apiVersion: v1 kind: ConfigMap metadata: name: nginx-conf data: default.conf: | server { listen 80; location / { return 200 \"$hostname\\n\"; } } 顯示部署完的服務狀態 我們使用指令，列出四個 pod 各別的 node 以及所在的 Zone\nprintf \"%-30s %-35s %-20s\\n\" \"POD\" \"NODE\" \"ZONE\" echo \"---------------------------------------------------------------------------------------------\" kubectl get pod -o=custom-columns='POD:metadata.name,NODE:spec.nodeName' --no-headers | while read pod node; do zone=$(kubectl get node \"$node\" -o=jsonpath='{.metadata.labels.topology\\.kubernetes\\.io/zone}') printf \"%-30s %-35s %-20s\\n\" \"$pod\" \"$node\" \"$zone\" done Pod 所在 Node 以及 Zone\n接著，我們從 nginx-a-98f749dc6-jzl8m (Zone A) 發起連線打 nginx-b 的 svc，觀察一下他會打到同樣是 Zone A 的 nginx-b-795d6c85b8-c2k45，還是 Zone B 的 nginx-b-795d6c85b8-tncfm，或是隨機分配呢？\n我們使用以下指令打 1000 次並觀察次數\nfor i in {1..1000}; do curl -s http://nginx-b.default.svc.cluster.local done | sort | uniq -c Pod 所在 Node 以及 Zone\n為了測試準確性，我們重複測試 3 次\n同 nginx-a Zone A 打 nginx-b svc nginx-b Zone A nginx-b Zone B 第一次打 1000 次 482 518 第二次打 1000 次 493 507 第三次打 1000 次 498 502 就上面測試結果可以得知，他是隨機去分配到 Zone A 或是 Zone B 的 Nginx-b Pod，因為 Service 本來就會依照預設的 Round-Robin 來去分配。\n使用 Topology Aware Routing 測試 簡單說明一下，Topology Aware Routing 縮寫是 (TAR/TAH) 中文叫做：拓樸感知路由\n在 K8s 1.27 以前叫作 Topology Aware Hints，所以縮寫才會是 TAH，邏輯是需要在 Service 的 annotation 加上 service.kubernetes.io/topology-mode: Auto （1.27 Version），以前版本設定方式會不同。\n它可以幫助網路流量盡可能的保持在同一個 Zone 裡面，Pod 之前優先使用同 Zone 流量來提高可靠性、性能以及減少成本。\n設定完成後，會顯示來至 endpoint-slice-controller\n當設定上去後，因為 EndpointSlice 這邊會記錄每個 Pod 是屬於哪個 Zone，當並提供給 kube-proxy 知道，進而讓流量往同樣的 Zone 走。\n顯示部署完的服務狀態 那我們一樣用同樣的步驟再來測試一次：\n使用一樣的指令，先列出四個 pod 各別的 node 以及所在的 zone (服務都有重啟過)\nprintf \"%-30s %-35s %-20s\\n\" \"POD\" \"NODE\" \"ZONE\" echo \"---------------------------------------------------------------------------------------------\" kubectl get pod -o=custom-columns='POD:metadata.name,NODE:spec.nodeName' --no-headers | while read pod node; do zone=$(kubectl get node \"$node\" -o=jsonpath='{.metadata.labels.topology\\.kubernetes\\.io/zone}') printf \"%-30s %-35s %-20s\\n\" \"$pod\" \"$node\" \"$zone\" done 測試結果\n我們從 nginx-a-fd5bdb699-6qnwb (Zone A) 發起連線打 nginx-b 的 svc，一樣觀察他會打到同樣是 zone A 的 nginx-b-795d6c85b8-2q7m2，還是 Zone B 的 nginx-b-795d6c85b8-82427？\n我們使用以下指令打 1000 次並觀察次數\nfor i in {1..1000}; do curl -s http://nginx-b.default.svc.cluster.local done | sort | uniq -c Pod 所在 Node 以及 Zone\n為了公平性，我們也比照原本測試的，重複測試 3 次\n同 nginx-a Zone A 打 nginx-b svc nginx-b Zone A nginx-b Zone B 第一次打 1000 次 1000 0 第二次打 1000 次 1000 0 第三次打 1000 次 1000 0 [!TIP] 所以可以得知： 只要 Service 有加上該 annotation，就可以透過 EndpointSlice 走到同樣的 Zone。\n另外，我們也來測試看看，如果打的過程中，將 nginx-b Zone A nginx-b-795d6c85b8-2q7m2 Pod 給移除，那觀察流量會不會自動轉到 Zone B 呢？！\nPod 所在 Node 以及 Zone\n測試結果：當一開始進到 nginx-b-795d6c85b8-2q7m2 的流量，遇到 nginx-b-795d6c85b8-2q7m2 服務中斷，由於該 Nginx-b 在 Zone A 沒有其他 Pod，所以就會切回去使用 Zone B 的 Nginx-b 服務 (nginx-b-795d6c85b8-82427)，但當 Zone A 的 Nginx-b 服務好了，又會再走回 Zone A (nginx-b-795d6c85b8-w2vr4) \u003c 是後面長出來的 Pod。\n使用的建議 確保 K8s 版本在 1.27 以上，1.27 以下需要使用 service.kubernetes.io/topology-aware-hints。\n每個服務要均勻分配在每個 Zone 上面。\n確保服務在更新時，同 Zone 上面的數量足夠，避免切到其他 Zone 導致費用產生。"},"title":"Kubernetes 開啟 Region 後，如何減少跨 Zone 網路費用"},"/blog/kubernetes/k8s-plain/":{"data":{"":"前陣子對於 Kubernetes 部分內容還不是很清楚，在網路上閒逛的時候發現一篇很有趣的文章，標題是 『 給行銷跟業務的 Kubernetes 101 中翻中介紹 』，點進去後才發現，作者 Phil Huang 將 Kubernetes 元件的內容用大型住戶社區來介紹，讓我更清楚每一個元件的意思，以下我會用我理解的以及作者的思考邏輯來去寫這篇筆記，再次感謝作者文章 😍","kubernetes-元件白話文#Kubernetes 元件白話文":"\n大型社區示意圖 (圖片來源：蘋果地產)\n可以看到上面這張圖片，他是一個很典型的大型社區，我們這次的 Kubernetes 元件白話文，會以大型社區來當作現實中的元件，以社區的例子來說明 Kubernetes 。\n當然，我們之前的文章也有介紹過 Kubernetes，有興趣可以先飛回去看看！\nKubernetes (K8s) 介紹 - 基本\nKubernetes (K8s) 介紹 - 進階 (Service、Ingress、StatefulSet、Deployment、ReplicaSet、ConfigMap)\nKubernetes (K8s) 搭配 EFK 實作 (Deployment、StatefulSet、DaemonSet)\n我們先想像一下我們建立 Kubernetes 完整的叢集服務，就好比是建立一個大型的社區，所以以下會將名詞與社區來做連結，那．．．開始囉！\nKubernetes 元件 Kubernetes：建立這個大型社區的藍圖，規劃的社區內的大大小小的設計。 虛擬化平台、公有雲平台、實體機器：就是我們要蓋社區的地皮。(虛擬化平台：vSphere/Proxmox/VMware、公有雲平台：AWS/GCP/Azuer、實體機器：就是實體機器 😂) OS 作業系統：每棟大樓的骨架和地基。 Master Node：住戶管委會所居住大樓 (真好自己有所屬的大樓ＸＤ)，為了保證他們不會吵架，所以建議最少需要三棟。 Etcd Cluster：管委會的人，一樣為了怕一黨獨大(?，所以建議最少需要有三位管委，且互相投票選出一個頭頭。 Worker Node：就是偶們住戶所居住的大樓。 Pod：住戶，所以我們一棟 Worker Node 大樓，可以有很多 Pod 住戶。 Pod IP：每個住戶的門牌，既然是門牌，代表他也不會重複。 Container Registry：包裹集中的存放中心。 Container Images：還沒有被打開的包裹。 Containers：已經被打開且正在使用的包裹，那每個 Pod 住戶裡面，可以有一個或很多個以上的 Containers 包裹。 Service：社區裡面的社團，例如：羽球社、麻將社等等，可以集合大家的地方。 Service Mesh：社團的聯絡名冊，會記住誰是哪一個社團的成員。 Ingress Controller：社區的大門，可以指定讓社區成員都固定走同一個或是多個門的入口管控。 Egress Controller：社區的後門，可以指定讓社區成員都固定走同一個或是多個門的出口管控。 Internal DNS / Service Discovery：大樓住戶的電話簿。 External DNS：指向各大樓的路標。 OCI (Open Container Initiative)：制定大樓鋼筋水泥或是行李箱標準的組織。 CRI (Container Runtime Interface)：大樓鋼筋水泥的廠商。 CNI (Container Network Interface)：大樓水電系統的廠商。 CSI (Container Storage Interface)：大樓空間規劃的廠商。 Bastion：專門維護社區的工程車。 常用套件 Promethus：社區整體的監控中心 (Meterics)，一堆攝影機的管理室 XD Grafana：監控中心裡面大型的 LED 儀表板。 Elaticsearch：社區整體的情資中心 (Logging)，這應該是一群愛八卦的大媽吧 😏 Kibana：情資中心裡面大型的 LED 訊息版。 ","參考資料#參考資料":"給行銷跟業務的 Kubernetes 101 中翻中介紹","常見問題#常見問題":"作者也有整理了一些常見的問題，我把它整理一下挑選出幾個我自己一開始也會有疑問的問題，我們一起來看看吧！一樣我會用我所理解的意思來介紹 ~\nQ1：為什麼 Kubernetes 的最小單位是 Pod 這個要怎麼理解？ Ans ： 試想一下，難道管委會會管你的包裹裡面內容物放什麼嗎 XD\nQ2：Docker 在 Kubernetes 的角色是什麼？ Ans ： 是眾多的 CRI (Container Runtime Interface) 選擇之一，也就是大樓的鋼筋水泥廠商有很多間，有一間叫做 Docker 的廠商特別有名。\nQ3：呈上，那 CRI/CNI/CSI 是不是也可以替換成其他的廠商？ Ans ： 當然可以，現在可以看到越來越多廠商都開始支援 Kubernetes 就是因為這個原因，因為一般情況下，Kubernetes 並不會特別限定 大樓鋼筋水泥廠商、大樓水電系統廠商、大樓空間規劃廠商，只要有符合特定的標準即可，但要留意 Kubernetes 的版號也會受到這三個介面支援發行的版號所影響，要留意相容性的問題！\nQ4：整個住戶社區最重要的角色是什麼？ Ans ： 那三棟管委會大樓，和裡面的的三位委員，三位掛掉一位還可以維持正常運作，掛掉兩位會維持唯獨運作。\nQ5：Kubernetes、VM、Container 的差異性 Ans ： 我們可以建立好一個大樓(VM)，隨意放置一個或是多個包裹(Containers)，必須手動管理這些包裹，如果資源不足或是這棟大樓倒了，就沒有辦法自動轉移這些包裹的內容了。\n但如果是 Kubernetes，我們就可以建立多個大樓(VM)，透過 Kubernetes 所規定的放置計畫 (例如：Deployment、DaemonSet)，我們將可以統一調度這些 Pod，當某棟大樓資源不夠或是這棟大樓倒時，可以根據 Kubernetes 規則來進行搬遷或是擴充大樓。\nQ6：為什麼有人會把 Container 跟 Pod 混在一起講 Ans ： 因為大部分的情況下，都是一個住戶(Pod)放一個包裹(Container)，，才會導致這樣的誤會。但實際上，一個住戶(Pod)是可以放一個或多個包裹在裡面的！\nQ7：什麼是 Node Scaling ? Ans ： 可以把它理解成，當住戶太多時，Kubernetes 會自動或手動興建大樓，來把多出來的用戶給塞進去。\nQ8：在 Q5 有提到放置計畫是什麼意思？ Ans ： 只要你有需要將包裹放在社區內，都必須提出部署計畫(Deployment) 給管委會審核，只要通過審核，他們依照你事先聲明的計畫，盡最大可能性來放置包裹。\n所以當我們有任何變更計畫時，都需要重新提交一份新的部署計畫給管委會重新審核和接受。"},"title":"用大型社區來介紹 Kubernetes 元件"},"/blog/kubernetes/k8s-statefulset-podmanagementpolicy/":{"data":{"":"此文章要來記錄一下前陣子在公司的正式環境踩到 StatefulSet 的雷，事情是這樣的，我們有些服務，是使用 StatefulSet 來建置，至於為什麼不用 Deployment，這個說來話長 (也不是因為需要特定的 Pod 名稱、或是網路標記等等)，我們這邊先不討論，這個 StatefulSet 服務是 Nginx + PHP-FPM，為了避免流量進入到 processes 已經被用光的 Pod 中，我們在 StatefulSet 的 PHP Container 上有設定 readiness，readiness 的設定長得像以下：\nreadinessProbe: exec: command: - \"/bin/bash\" - \"-c\" - | CHECK_INFO=$(curl -s -w 'http code:\\t%{http_code}\\n' 127.0.0.1/status) HTTP_CODE=$(echo -e \"${CHECK_INFO}\" | awk '/http code:/ {print $3}') IDLE_PROCESSES=$(echo -e \"${CHECK_INFO}\" | awk '/idle processes:/ {print $3}') [[ $HTTP_CODE -eq 200 \u0026\u0026 $IDLE_PROCESSES -ge 10 ]] || exit 1 我們會用 curl 來打 /status，檢查回傳的 http code 是否為 200，以及 idle processes 是否大於等於 10，如果不符合，就會回傳 1，讓他被標記不健康，讓 Kubernetes 停止流量到不健康的容器，以確保流量被路由到其他健康的副本。","參考資料#參考資料":"Kubernetes — 健康檢查：https://medium.com/learn-or-die/kubernetes-%E5%81%A5%E5%BA%B7%E6%AA%A2%E6%9F%A5-59ee2a798115\nPod Management Policies：https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#pod-management-policies","問題#問題":"當天遇到的情況是，我們上程式後，Pod 都一切正常，當流量開始進來後，發現 10 個 Pod 會開始偶發的噴 Readiness probe failed，查看監控發現 processes 越來越低，最後被反應服務有問題，我們查看 Hpa 的紀錄的確有觸發到 40 個 Pod，只是查看 Pod 數還是依樣卡在 10 個，當下我們有嘗試使用調整 yaml 在 apply，發現 StatefulSet 的 yaml 也已經更新了，但 Pod 還是一樣卡在 10 個，也有使用 kubectl 下 kubectl scale sts [服務名稱] --replicas=0，想要切換 Pod 數也沒有辦法。\n當下我們有先 Call Google 的 Support 一起找原因，Google 是建議我們 readiness 的條件不要設的太嚴格，可以加上 timeoutSeconds: 秒數，但對於 Pod 卡住，還是沒有找到原因，後來我們查了一下 StatefulSet 的文件發現，StatefulSet 有一個設定 podManagementPolicy，預設是 OrderedReady，他必須等待前面的 Pod 是 Ready 狀態，才會再繼續建立新的，也就是說我們的 StatefulSet 已經卡住，導致就算 Hpa 觸發要長到 40 個 Pod 也沒有用。","測試結果#測試結果":"最後我們就使用兩種模式來測試看看，已下是測試結果(透過 P1 才知道的設定ＱＱ)：\n有將測試的 StatefulSet 放在 Github，可以點我查看 (可以調整 readinessProbe 的 httpGet.Path 故意把他用壞)\n使用 OrderedReady 模式 StatefulSet 在 podManagementPolicy 預設 OrderedReady 的模式，故意讓 readiness 卡住時 (Pod 卡住時)：\n當下的 StatefulSet 設定： StatefulSet 設定\nPod 狀態： Pod 狀態\n使用指令調整 Pod 數量 我們這時候下指令調整 Pod 數量，看看會發生什麼事：\nkubectl scale sts my-statefulset --replicas=5 我們先看 StatefulSet 的 yaml 可以看到 Pod replicas 已經改變，也可以看 generation 有更新，代表 StatefulSet 本身有接收到調整設定的請求。\n下指令調整後的 StatefulSet 設定\n看了一下 Pod 數量，也是一樣卡住，且 Pod 數量也沒有變化。\n下指令調整後的 Pod 狀態\n使用 yaml 調整 Pod 數量 我們直接調整 StatefulSet yaml 的 Pod 數量，看看會發生什麼事：\n一樣我們先看 StatefulSet 的 yaml 可以看到 Pod replicas 已經改變(這裡應該切別的 Pod 數量，切回 3 個好像沒有意義 xD)，也可以看 generation 有更新。\n使用 yaml 調整後的 StatefulSet 設定\n看了一下 Pod 數量，也是一樣卡住，且 Pod 數量也沒有變化。\n使用 yaml 調整後的 Pod 狀態\n所以代表在 OrderedReady 的模式下，Pod 卡住時，無法對 Pod 進行任何操作，必須要手動刪除卡住的 Pod 才吃得到最新的設定。\n使用 Parallel 模式 StatefulSet 在 podManagementPolicy Parallel 的模式，故意讓 readiness 卡住時 (Pod 卡住時)：\n當下的 StatefulSet 設定： StatefulSet 設定\nPod 狀態： Pod 狀態\n使用指令調整 Pod 數量 我們這時候下指令調整 Pod 數量，看看會發生什麼事：\nkubectl scale sts my-statefulset --replicas=5 我們先看 StatefulSet 的 yaml 可以看到 Pod replicas 已經改變，也可以看 generation 有更新，代表 StatefulSet 本身有接收到調整設定的請求。\n下指令調整後的 StatefulSet 設定\n看了一下 Pod 數量，就算 my-statefulset-2 卡住，還是可以擴到 5 個 Pod。\n下指令調整後的 Pod 狀態\n使用 yaml 調整 Pod 數量 我們直接調整 StatefulSet yaml 的 Pod 數量，看看會發生什麼事：\n一樣我們先看 StatefulSet 的 yaml 可以看到 Pod replicas 已經改變，也可以看 generation 有更新。\n使用 yaml 調整後的 StatefulSet 設定\n看了一下 Pod 數量，也不會管其他 Pod 是否 Ready，一樣可以縮小成 2 個 Pod。\n使用 yaml 調整後的 Pod 狀態","結論#結論":"後來我們重新檢查了一下為什麼 processes 會用完，結果發現是 RD 的程式邏輯，導致每筆 Request 必須等待前一筆 Request 做完，才會開始動作，讓 processes 一直被占用，沒辦法即時消化，導致 processes 用完，又加上服務是使用 StatefulSet，預設模式的 OrderedReady，必須等待前一個 Pod 是 Ready 才可以自動擴縮，所以當我們 Hpa 想要擴縮，來增加可用的 processes 數量，也因為沒辦法擴縮，最後導致這一連串的問題 😕。\n另外，如果想要從 OrderedReady 模式切成 Parallel 模式 (反正過來也是)，必須先將原本的 StatefulSet 給刪除，才可以調整：\nOrderedReady 模式切成 Parallel 模式","解決辦法#解決辦法":"當下想趕快解決 readiness 這個問題，調整 timeoutSeconds 後，單純 apply 是沒有用的，要記得刪掉卡住的 Pod，讓他重新建立，才會套用新的設定 (但我們當下太在意為甚麼 Pod 會卡住，沒有想到要先把 readiness 問題修掉 xD，我們當下的解法是先將流量導到地端正常的服務上)。\n另外 Google 也說，假如我們還是必須使用 StatefulSet 來建立服務，建議我們把 podManagementPolicy 改成 Parallel，它會有點像是 Deployment 的感覺，不會等待其他 Pod 變成 Ready 狀態，所以可以讓我們就算在 readiness 卡住的情況下，也可以自動擴縮服務。\nℹ️ StatefulSet podManagementPolicy 參數說明\nOrderedReady (預設) Pods 會按照順序一個接一個地被創建。即，n+1 號 Pod 不會在 n 號 Pod 成功創建且 Ready 之前開始創建。 在縮小 StatefulSet 的大小時，Pods 會按照反向順序一個接一個地被終止。即，n 號 Pod 不會在 n+1 號 Pod 完全終止之前開始終止。 這確保了 Pods 的啟動和終止的順序性。\nParallel 所有 Pods 會同時地被創建或終止。 當 StatefulSet 擴展時，新的 Pods 會立即開始創建，不用等待其他 Pods 成為 Ready 狀態。 當縮小 StatefulSet 的大小時，要終止的 Pods 會立即開始終止，不用等待其他 Pods 先終止。 這種策略提供了快速的擴展和縮小操作，但缺乏順序性保證。"},"title":"正式環境上踩到 StatefulSet 的雷，拿到 P1 的教訓"},"/blog/kubernetes/k8s/":{"data":{"":"","kubernetes-元件介紹與說明#Kubernetes 元件介紹與說明":"Kubernetes 是如何幫我們管理以及部署 Container ? 要了解 Kubernetes 如何運作，就要先了解它的元件以及架構：\n那我們由小的往大的來做介紹：依序是 Pod、Worker Node、Master Node、Cluster\nPod Kubernetes 運作中最小的單位，一個 Pod 會對應到一個應用服務 (Application)，舉例來說一個 Pod 可能會對應到一個 NginxServer。\n每個 Pod 都有一個定義文件，也就是屬於這個 Pod 的 yaml 檔。 一個 Pod 裡面可以有一個或多個 Container，但一般情況一個 Pod 最好只有一個 Container。 同一個 Pod 中的 Containers 共享相同的資源以及網路，彼此透過 local port number 溝通。 Worker Node Kubernetes 運作的最小硬體單位，一個 Worker Node (簡稱 Node) 對應到一台機器，可以是實體例如你的筆電、或是虛擬機例如 GCP 上的一台 Computer Engine。\n每一個 Node 都有三個組件所組成：kubelet、kube-proxy、Container Runtime\nkubelet 該 Node 的管理員，負責管理該 Node 上的所有 Pods 的狀態並負責與 Master Node 溝通。\nkube-proxy 該 Node 的傳訊員，負責更新 Node 的 iptables，讓 Kubernetes 中不在該 Node 的其他物件可以得知該 Node 上的所有 Pods 的最新狀態。\nContainer Runtime 該 Node 真正負責容器執行的程式，K8s 預設是 Docker，但也支援其他 Runtime Engine，例如 Mirantis Container Runtime、CRI-O、containerd\n常見誤解：\n很多人認為 Kubernetes 是 docker container 的管理工具，包含我一開始也是這樣認為，但其實 Kubernetes 是用來管理容器化 (containerized applications) 並不是專屬於 docker 獨享，作為一個 container orchestrator 的角色，Kubernetes 希望能夠管理所有容器化的應用程式\nMaster Node Kubernetes 運作的指揮中心，可以簡化看成一個特殊化的 Node 來負責管理所有其他的 Node。\n一個 Master Node (簡稱 Master) 中有四個組件組成：kube-apiserver、etcd、kube-scheduler、kube-controller-manager\nkube-apiserver 管理整個 Kubernetes 所需 API 的接口 (Endpoint)，例如從 Command Line 下 kubectl 指令就會把指令送到這裡。 負責 Node 之間的構通橋樑，每個 Node 彼此不能直接溝通，必須要透過 apiserver 轉介。 負責 Kubernetes 中的請求的身份認證與授權。 etcd etcd 是兼具一制性和高可用性的分散式鍵值數據庫，可以保存 Kubernetes 所有 Cluster 的後台數據庫。 kube-scheduler 整個 Kubernetes 的 Pods 調度員，scheduler 會監視新建立但還沒有被指定要跑在哪個 Node 上的 Pod ，並根據每個 Node 上面資源規定，硬體限制等條件去協調出一個最適合放置 Node 來讓該 Pod 運行。 kube-controller-manager 負責管理並運行 Kubernetes controller 的組件，簡單來說 controller 就是 Kubernetes 裡一個負責監視 Cluster 狀態的 Process，例如：Node Controller、Replication Controller。 這些 Process 會在 Cluster 與預期狀態 (desire state) 不符時嘗試更新現有狀態 (current state)。例如：現在要多開一台機器以應付突然增加的流量，那我的預期狀態就會更新成 N+1，現有狀態是 N，這時相對應的 controller 就會想辦法多開一台機器。 controller-manager 的監視與嘗試更新也都需要透過訪問 kube-apiserver 達成。 Cluster Cluster 也叫叢集，可以管理眾多機器的存在，在一般的系統架設中我們不會只有一台機器而已，通常都是多個機器一起運行同一種內容，在沒有 Kubernetes 的時候就必須要土法煉鋼的一台一台機器去更新，但有了 Kubernetes 我們就可以透過 Cluster 進行控管，只要更新 Master 旗下的機器，也會一併將更新的內容放上去，十分方便。在 Kubernetes 中多個 Worker Node 與 Master Node 的集合。","什麼是-annotation-#什麼是 Annotation ?":"前面提到的 Label 功用其目的是要讓 Kubernetes 知道可以去更方便管理的，那我們如果想要貼標籤但不想讓 Kubernetes 知道，要怎麼做呢？\n這時我們就可以用 Annotation，透過 Annotation 可以將標籤單純給開發人員查看，那聽起來 Annotation 好像沒有什麼實質上的用途，因為 Kubernetes 不會採用這些標籤，但其實 Annotation 還是有用的歐！後續文章會再提到 \u003e\u003c\n那既然 Label 跟 Annotation 有相似，所以寫法想必也是差不多吧：\nannotations: author: Pin-YI contact: 880831ian@gmail.com 一樣也是定義一組具有辨識度的 key/value ，我們這邊就先放 author、contact\n那 Label 與 Annotation 要放在 Pod 的哪一處呢？\n還記得我們上面說 metadata 是用來擺描述性資料的地方嗎，所以不管是 Label 或是 Annotation 都是放在 metadata 中歐！\nmetadata: name: kubernetes-demo-pod labels: app: demo annotations: author: Pin-YI contact: 880831ian@gmail.com ","什麼是-kubernetes-k8s#什麼是 Kubernetes (K8s)?":"Kubernetes 也可以叫 K8s，這個名稱來源希臘語，意思是舵手或是飛行員，所以我們可以看到它的 logo 是一個船舵的標誌，之所以叫 K8s 是因為 Kubernetes 的 k 到 s 中間有 8 的英文字母，為了方便，大家常以這個名稱來稱呼他！\nkubernetes logo\nKubernetes 是一種開源可用來自動化部屬、擴展以及管理多個容器的系統，適用於當容器數量增加，需要穩定容器環境，以及管理資源或權限分配的狀況。\n我們之前在 Docker 介紹 文章中，已經有介紹以往傳統虛擬機以及容器化的 Docker 差異以及優點，那當我們在管理容器時，其中一個容器出現故障，則需要啟動另一個容器，如果要用手動，會十分麻煩，所以這時就是 Kubernetes 的厲害的地方了，Kubernetes 提供：\n服務發現和負載平衡：K8s 可以使用 DNS 名稱或是自己的 IP 位址來公開容器。如果容器流量過高，Kubernetes 能夠使用負載平衡和分配網路流量，能使部署更穩定。 編排儲存：Kubernetes 允許使用自動掛載來選擇儲存系統，例如使用本地儲存，或是公共雲等。 自動部署、刪除：可以使用 Kubernetes 來幫我們自動化部屬新的容器、刪除現有的容器並將其資源用於新容器。 自動打包：當我們為 Kubernetes 提供一個節點叢集，它可以用來運行容器化的任務，告訴 Kubernetes 每個容器需要多少 CPU 和 RAM。Kubernetes 可以將容器安裝到節點上，充分利用資源。 自動修復：Kubernetes 會重新啟動失敗的容器、替換容器、刪除不回應用戶的不健康容器，並且在容器準備好服務之前不會通知客戶端。 機密和配置管理：Kubernetes 允許儲存和管理敏感訊息，例如密碼、OAuth token 和 SSH 金鑰。可以部署和更新機密的應用程序配置。 kubernetes 官網\nKubernetes 很常被拿來與 Docker Swarm 做比較，兩者不同的是，Docker Swarm 必須建構在 Docker 的架構下，功能侷限、無法跳脫。\nKubernetes 則因為功能較為廣泛，而逐漸取代 Docker Swarm 在市場上的地位。下方有簡易的比較表格：\n比較 Kubernetes Docker Swarm 說明 Kubernetes 是一個開源容器的編排平台，Kubernetes 的叢集結構比 Docker Swarm 更為複雜。\nKubernetes 通常有建構器和工作節點，還可進一步分為 Pod、命名空間、配置映射等。 Docker Swarm 是一個由 Docker 構建和維護的開源容器編排平台。\n一個 Docker Swarm 叢集通常包含三個項目：Nodes、Services and tasks、Load balancers。 優點 它有龐大的開源社群，由 Google 支持。\n它可以維持和管理大型架構和複雜的工作負載。\n它是自動化，並支持自動化擴展的自我修護能力。\n它有內建監控和廣泛的可用集成。 Docker Swarm 安裝簡單，它輕量化且容易學習使用。\nDocker Swarm 與 Docker CLI 一起運作，因此不需要多運行或是安裝新的 CLI 缺點 它複雜的安裝過程以及較難學習\n它需要安裝單獨的 CLI 工具並且學習每一項功能 它是輕量級且與 Docker API 相關聯，與 Kubernetes 相比，Docker Swarm 被限制很多功能，且自動化也沒有 Kubernetes 強大。 ","什麼是-label-#什麼是 Label ?":"Label 顧名思義就是標籤，可以為每一個 Pod 貼上標籤，讓 Kubernetes 更方便的管控這些 Pod。\nLabel 的寫法很簡單，可以自己自訂一對具有辨識度的 key/value，舉我們上面的例子來說：我們可以在 labels 內加入 app: demo，那 Label 有什麼好處呢?\n這邊要稍微提一下 Selector，它的功用是選取對應的物件。為了要方便選取到我們設定好的 Pod，這時候 Label 就派上用場了！\nSelector 的寫法也很簡單，只要把我們在 Label 定義的 key/value 直接完整的貼過來就可以了～\n就像這樣：\nselectors: app: demo 那選取後有什麼功用呢！請看 Kubernetes - 進階 - Service\n講完 Label 後，順邊提一下跟 Label 有相似的：","參考資料#參考資料":"kubernetes 官網：https://kubernetes.io/\nKubernetes（K8s）是什麼？基礎介紹+3 大優點解析：https://www.sysage.com.tw/news/technology/293\nDocker Swarm vs Kubernetes: how to choose a container orchestration tool：https://circleci.com/blog/docker-swarm-vs-kubernetes/\nKubernetes 基礎教學（一）原理介紹：https://cwhu.medium.com/kubernetes-basic-concept-tutorial-e033e3504ec0\nKubernetes 那些事 — 基礎觀念與操作：https://medium.com/andy-blog/kubernetes%E9%82%A3%E4%BA%9B%E4%BA%8B-%E5%9F%BA%E7%A4%8E%E8%A7%80%E5%BF%B5%E8%88%87%E6%93%8D%E4%BD%9C-97cc203a2660\nKubernetes 那些事 — Pod 篇：https://medium.com/andy-blog/kubernetes-%E9%82%A3%E4%BA%9B%E4%BA%8B-pod-%E7%AF%87-57475cec22f3\nKubernetes 那些事 — Label 篇：https://medium.com/andy-blog/kubernetes-%E9%82%A3%E4%BA%9B%E4%BA%8B-label-%E7%AF%87-4186af2af556","基本運作#基本運作":" kubernetes 組件 Kubernetes 基礎教學（一）原理介紹\n接下來我們用 「Kuberntes 是如何建立一個 Pod ？」來複習一下整個 Kubernetes 的架構。\n(上圖是一個簡易的 Kubernetes Cluster ，通常 Cluster 為了高穩定性都會有多個 Master 作為備援，但為了簡化我們只顯示一個。)\n當使用者要部署一個新的 Pod 到 Kubernetes Cluster 時，使用者要先透過 User Command (kubectl) 輸入建立 Pod 對應的指令 (後面會說明要如何實際的動手建立一個 Pod)。此時指令會經過一層確認使用者身份後，傳遞到 Master Node 中的 API Server，API Server 會把指令備份到 etcd。 controller-manager 會從 API Server 收到需要創建一個新的 Pod 的訊息，並檢查如果資源許可，就會建立一個新的 Pod。最後 Scheduler 在定期訪問 API Server 時，會詢問 controller-manager 是否有建置新的 Pod，如果發現新建立的 Pod 時，Scheduler 就會負責把 Pod 配送到最適合的 Node 上面。 雖然上面基本的運作看起來十分複雜，但其實我們在實際操作時，只是需入一行指令後，剩下的都是 Kubernetes 會自動幫我們完成後續的動作。","如何建立一個-pod#如何建立一個 Pod":"版本資訊\nMinikube：v1.25.2 hyperkit：0.20200908 Kubectl：Client Version：v1.22.5、Server Version：v1.23.3 下載完 Minikube 後，我們可以先透過 Minikube 來查詢全部的指令，由於我們前面有安裝 Hyperkit 這個驅動程式，啟動 Minikube 預設是使用 Docker，我們這邊要利用 Hyperkit 來啟動，所以使用 Minikube start --vm-driver=hyperkit 來啟動 Minikube。\nMinikube 其他指令介紹：\n顯示 minikube 狀態 minikube status\n停止 minikube 運行 minikube stop\nssh 進入 minikube 中 minikube ssh\n查詢 minikube 對外的 ip minikube ip\n使用 minikube 所提供的瀏覽器 GUI minikube dashboard (可以加 - - url 看網址歐)\n我們啟動 minikube 後，我們要打做一個可以在 Pod 運行的小程式。這個小程式是一個 Node.js 的 Web 程式，他會建立一個 Server 來監聽 3000 Port，收到 request 進來後會渲染 index.html 這個檔案，這個檔案裡面會有一隻可愛的小柴犬。\n因為本文章是在介紹 kubernetes 所以在程式部分就不多做說明，我把程式碼放在 Github，以及附上 Dockerhub 的 Repository 可以直接使用包好的 image 來做測試！\nPod yaml 檔案說明 接下來我們要先撰寫一個 Pod 的定義文件 (.yaml) 檔，這個 .yaml 檔就可以建立出 Pod 了！\nkubernetes-demo.yaml (程式縮排要正確，不然會無法執行歐！) apiVersion: v1 kind: Pod metadata: name: kubernetes-demo-pod labels: app: demo spec: containers: - name: kubernetes-demo-container image: 880831ian/kubernetes-demo ports: - containerPort: 3000 apiVersion\n該元件的版本號，必須依照 Server 上 K8s 版本來做設定 (想要知道 k8s 版本，可以使用 kubectl version 指令來查詢，會顯示 client 跟 server 的版本訊息，client 代表 kubectl 版本訊息，server 代表的是 master node 的 k8s 版本訊息)，目前 k8s 都使用 1.23 版本以上，所以 apiVersion 直接寫 v1 即可。\nkind\n該元件的屬性，用來決定此設定檔的類型，常見的有 Pod、Node、Service、Namespace、ReplicationController 等等\nmetadata\n用來擺放描述性資料的地方，像是 Pod 名稱或是標籤等等都會放在此處。\nname：指定該 Pod 的名稱 labels：指定該 Pod 的標籤 spec\n用來描述物件生成的細節，像是 Pod 內其實是跑 Dokcer container，所以在 Pod 的 spec 內就會描述 container 的細節。\ncontainer.name：指定運行的 Container 的名稱 container.image：指定 Container 要使用哪個 Image，這裡會從 DockerHub 上搜尋 container.ports：指定該 Container 有哪些 Port number 是允許外部存取的 使用 kubectl 建立 Pod 當我們有了定義文件後，我們就可以使用 kubectl 的指令來建立 Pod\nkubectl create -f kubernetes-demo.yaml kubectl apply -f kubernetes-demo.yaml 可以使用 create or apply 來建立 Pod ，那這兩個的差異是什麼呢？\nkubectl create kubectl create 是所謂的 “命令式管理” (Imperative Management)。通過這種方式，可以告訴 Kubernets API 你要建立、更新、刪除的內容。\nkubectl create 命令是先刪除所有現有的東西，重新根據 YAML 文件生成新的 Pod。所以要求 YAML 文件中的配置必須完整。\nkubectl create 命令，使用同一個 YAML 文件重複建立會失敗。\nkubectl apply kubectl apply 是 “聲明式管理” (Declarative Management)方法的一部分。在該方法中，即使對目標用了其他更新，也可以保持你對目標應用的更新。\nkubectl apply 命令，根據配置文件裡面出來的內容，生成就有的。所以 YAML 文件的內容可以只寫需要升級的欄位。\n如果看到 pod/kubernetes-demo-pod created 就代表我們成功建立第一個 Pod 了，接下來我們可以使用：\nkubectl get pods 可以查看我們運行中的 Pod：\nNAME READY STATUS RESTARTS AGE kubernetes-demo-pod 1/1 Running 0 3m5s Pod 指令介紹：\n查詢現有 Pod 狀態 kubectl get po/pod/pods\n查看該 Pod 詳細資訊 kubectl describe pods \u003cpod-name\u003e\n刪除 Pod kubectl delete pods\n查看 Pod log kubectl logs \u003cpod-name\u003e\n也可以使用 minikube 圖形化頁面來看一下是否成功！\nminikube dashboard --url 🤔 Verifying dashboard health ... 🚀 Launching proxy ... 🤔 Verifying proxy health ... http://127.0.0.1:55991/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/ minikube dashboard\n連線到 Pod 的服務 當我們建立好 Pod 之後，打開瀏覽器 localhost:3000 會發現，什麼都沒有，是因為我們剛剛在 .yaml 裡面設定的是 Pod 的 Port ，它與本機的 Port 是不相通的，因此我們需要使用 kubectl port-forward \u003cpod\u003e \u003cexternal-port\u003e:\u003cpod-port\u003e ，來將 Pod 與本機端做 mapping。\nkubectl port-forward kubernetes-demo-pod 3000:3000 Forwarding from 127.0.0.1:3000 -\u003e 3000 Forwarding from [::1]:3000 -\u003e 3000 Handling connection for 3000 我們在此瀏覽 localhost:3000，就會看到可愛的柴犬囉！\n成功顯示柴犬\n前面我們已經創建屬於我們第一個 Pod 了，但當我們 Pod 越建越多時，要怎麼快速的得知每個 Pod 在做什麼事情？除了用 Pod 的 metadata name 來命名外，還有另一種方式：","安裝-kubernetes#安裝 Kubernetes":"在我們開始操作 Kubernetes 之前，需要先下載 Minikube、Hyperkit、Kubectl 套件：\nMinikube 一個 Google 發佈的輕量級工具，讓開發者可以輕鬆體驗一個 Kubernetes Cluster。(僅限開發測試環境)\n安裝 minikube\n(Mac 專用) Hyperkit Hyperkit 是 MacOS 系統細部設定的驅動程式。\n安裝 hyperkit\nKubectl Kubectl 是 Kubernetes 的 Command Line 工具，我們之後會透過 Kubectl 去操作我們的 Kubernetes Cluster。\n安裝 kubectl"},"title":"Kubernetes (K8s) 介紹 - 基本"},"/blog/kubernetes/kube-dns-vs-core-dns/":{"data":{"":"前情提要：由於上一篇筆記 Pod 出現 cURL error 6: Could not resolve host 的後續規劃中，有提出我們想研究看看 Core DNS 來取代 Kube DNS，因此，這篇筆記要來比較一下 Kube DNS 以及 Core DNS 的差異以及兩者的優缺點，另外還會說明一下 DNS 的最佳實踐做法。\n前面概論介紹 K8s 內的 DNS 以及 CoreDNS 主要都是參考 探秘K8S DNS：解密IP查詢與服務發現、CoreDNS簡單除錯：解決你遇到的一般問題，詳細可以查看原文。","dns-的最佳實踐#DNS 的最佳實踐":"(代補….)","gke-上的-dns可以改用-coredns-嗎#GKE 上的 DNS，可以改用 CoreDNS 嗎？":"目前 GKE 上的 DNS 預設是 kube-dns，或是使用 GCP 提供的另一個 Cloud DNS 服務，但是上次我們有提到，因為公司之後想走多雲架構，會開始使用 AWS，而 AWS 的 EKS 預設的 DNS 服務是 CoreDNS (K8s 現在預設也是 CoreDNS 😿)\n所以我們想說，是不是可以在 GKE 上也改使用 CoreDNS，這樣之後維護可以更一致。\n在爬文的過程中有發現，Google 有一篇 knowledge 文章 How to run CoreDNS on Kubernetes Engine? 有提到 Google 預設就是使用 kube-dns，沒辦法把 kube-dns 縮小到 0 並完全替換，如果只是單純想要使用 CoreDNS 的快取功能，可以啟用 GKE 上的 NodeLocal DNSCache\n如果想要把 CronDNS 解析功能加到 GKE 上，要先部署一個 core-dns Pod，並透過 service 公開它，並為 kube-dns 配置一個 stub domain，將其指向 core-dns 服務 IP。\n所有與 stub domain 後綴相符的流量都會路由到 core-dns Pod。不符合的流量將繼續由 kube-dns 解析。","kube-dns-vs-coredns-優缺點#kube-dns vs CoreDNS 優缺點":"kube-dns 優點：有 dnsmesq，在性能上有一定的保障。 缺點： dnsmesq 如果重啟，會先刪掉 process 才會重新起服務，中間可能會出現查詢失敗。在確認內部檔案時，如果數量過多或是太頻繁更新 DNS 有可能反而導致 dnsmasq 不穩，這時候就需要重啟 dnsmasq 服務。 CoreDNS 優點：可以自行根據需求使用自訂的 Plugins，記憶體的佔用情況也比 kube-dns 好。 缺點：Cache 功能不如 dnsmasq，內部解析速度可能會比 kube-dns 慢。 ","參考資料#參考資料":"探秘K8S DNS：解密IP查詢與服務發現：https://weng-albert.medium.com/%E6%8E%A2%E7%A7%98k8s-dns-%E8%A7%A3%E5%AF%86ip%E6%9F%A5%E8%A9%A2%E8%88%87%E6%9C%8D%E5%8B%99%E7%99%BC%E7%8F%BE-034de2e72abe\nCoreDNS簡單除錯：解決你遇到的一般問題：https://weng-albert.medium.com/coredns%E7%B0%A1%E5%96%AE%E9%99%A4%E9%8C%AF-%E8%A7%A3%E6%B1%BA%E4%BD%A0%E9%81%87%E5%88%B0%E7%9A%84%E4%B8%80%E8%88%AC%E5%95%8F%E9%A1%8C-71d255e39548\nConnecting the Dots: Understanding How Pods Talk in Kubernetes Networks：https://medium.com/@seifeddinerajhi/connecting-the-dots-understanding-how-pods-talk-in-kubernetes-networks-992fa69fbbeb\nHow to run CoreDNS on Kubernetes Engine?：https://cloud.google.com/knowledge/kb/how-to-run-coredns-on-kubernetes-engine-000004698","應用程式-dns-查詢流程#應用程式 DNS 查詢流程":"\n應用程式 DNS 查詢流程 圖片：Connecting the Dots: Understanding How Pods Talk in Kubernetes Networks\n下面列出一下應用程式在 K8s 內的 DNS 查詢流程：\n當 Pod 執行 DNS 查詢時，會先查詢 Pod 裡面的 resolv.conf 檔案，再來查詢 Pod 所在的 node 上的 resolv.conf 檔案，這個檔案有設定 nodelocaldns 伺服器被設定為預設遞迴 DNS 解析器，充當快取 (在 GKE 需要另外開啟)。 如果此快取不包含所請求主機名稱的 IP 位址，則查詢將轉送至叢集 DNS 伺服器 (CoreDNS)，目前 GKE 是 kube-dns。 此 DNS 伺服器透過查詢 Kubernetes 服務註冊表來決定 IP 位址。此註冊表包含服務名稱到對應 IP 位址的對應。這允許叢集 DNS 伺服器將正確的 IP 位址傳回給請求的 pod。 任何被查詢但不在 Kubernetes 服務註冊表中的網域都會轉送到上游 DNS 伺服器。 ","概論-coredns#概論 (CoreDNS)":"這邊就說明一下我們想要研究的 CoreDNS 吧，其實早就應該用 CoreDNS 來取代 kube-dns，只是 GKE 不知道為什麼還不支援\nCoreDNS 是一個用 Go 語言寫的 DNS 伺服器，它是一個 CNCF 的專案，目前已經成為 K8s 的預設 DNS 伺服器。\n跟其他 DNS 比較不一樣的是他非常靈活彈性，並且所有功能都是透過 Plugins 來實現，這樣就可以根據需求來自訂自己的 DNS 伺服器。\n它的特點是：\nPlugins (插件化) Service Discovery (服務發現) Fast and Flexible (快速且靈活) Simplicity (簡單) 這邊來說一下 常見的 Plugins 如下：\nloadbalance：提供基於 DNS 的負載均衡功能 loop：檢測在 DNS 解析過程中出現的簡單循環問題 cache：提供前端快取功能 health：對 Endpoint 進行健康檢查 kubernetes：提供對 Kubernetes API 的訪問，並將其轉換為 DNS 記錄 etcd：提供對 etcd 的訪問，並將其轉換為 DNS 記錄 reload：定時自動重新加載 Corefile 配置的內容 forward：轉發域名查詢到上游 DNS 伺服器 proxy：轉發特定的域名查詢到多個其他 DNS 伺服器，同時提供到多個 DNS 服務器的負載均衡功能 prometheus：爲 Prometheus 系統提供採集性能指摽數據的 URL log：對 DNS 查詢進行日誌記錄 errors：對 DNS 查詢錯誤進行日誌記錄 CoreDNS 使用的 Plugins/架構 圖片：A Brief Look at CoreDNS\nCoreDNS 的 helm：https://github.com/coredns/helm/tree/master/charts/coredns","概論-k8s-dns#概論 (K8s DNS)":"K8s 會有 DNS 服務，主要是因為 K8s 連線是透過 Pod IP 來連線，但每當 Pod 重啟後 IP 會變動，所以就需要透過 Service 這個 Type 先建立持久性的名稱來讓其他 Pod 連線到後端服務上。\n當我們建立 Service 時，K8s DNS 就會自動產生一個對應的 A record 將 service dns name 與 IP 位址配對。之後，Pod 就可以透過 DNS 名稱來進行連線。\n而 DNS 負責動態更新 A record 與 Service IP 的異動。\n就如上面有提到，目前 K8s 的 DNS 已經從 kube-dns 改成 CoreDNS，那兩者都有些相同的功能，如下：\nkube-dns svc 可以建立一個或多個 Pod kube-dns svc 監控 Kubernetes API 的 service 和 endpoint 事件，並根據需要變更其 DNS 項目。當透過建立、編輯或刪除操作修改這些 Kubernetes 服務及其相關 pod 時，這些事件會自動觸發。 Kubelet 會將 kube-dns svc 的叢集 IP 指派給每個新 Pod 的 /etc/resolv.conf 名稱伺服器檔案內，以及適當的搜尋設定以允許更短的主機名稱，如下：。 可以看到 /etc/resolv.conf 與 kube-dns 的 cluster IP ㄧ致\nK8s 服務的整個 DNS A record 如下：\nService DNS name： [service].[namespace].svc.cluster.local service、namespace 可以自行替換，這也是我們最常使用的方式。\n當然 Pod 也有自己的 DNS name，只是因爲 Pod 每次重啟 IP 會變動，所以我們通常不會使用 Pod DNS name：\nPod DNS name： [pod cluster IP].[namespace].pod.cluster.local pod cluster IP 這邊要將 IP 從 . 的分隔符號換成 -，例如：10-166-65-136.default.pod.cluster.local\n可以看到 curl 的 pod cluster IP 與 svc 的 endpoint ㄧ致\n所以，程式可以透過簡單且一致的主機名稱來存取 cluster 其他服務或是 Pod。\n也不需要使用完整的 hostname 來存取服務，因為在 resolv.conf 會設定好 domain 的 suffixes，如果同一個 namespace 底下服務溝通，只要設定以下即可：\n[service] 跨到其他 nampesace 就要改成：\n[service].[namespace] ","概論-kube-dns#概論 (kube-dns)":"這邊就來說一下目前 GKE 在使用的 kube-dns 架構，主要由這三個 container 組成：\nkube-dns 架構 圖片：DNS\n目前 GKE 1.29 kube-dns\nkubedns：DNS 服務核心元件，主要由 KubeDNS 以及 SkyDNS 兩個元件組成： KubeDNS 負責監聽 Kubernetes API 的 service 和 endpoint 事件，並將相關訊息更新到 SkyDNS 中。 SkyDNS 負責 DNS 解析，監聽在 10053 Port (tcp/udp)，也同時監聽在 10055 Port 提供 metrics。 SkyDNS Metrics Port 以及監聽 10053 Port\ndnsmasq (有換名稱)：負責啟動 dnsmasq，並在變化時重啟 dnsmasq：\ndnsmasq 的 Upstream DNS Server 是 SkyDNS，cluster 內部的 DNS 查詢都會由 SkyDNS 負責。 sidecar：負責健康檢查和提供 DNS metrics 監聽在 10054 Port。\nsidecar 健康檢查以及監聽 10054 Port"},"title":"kube-dns vs CoreDNS 比較，DNS 的最佳實踐"},"/blog/kubernetes/pod-curl-error-6-could-not-resolve-host/":{"data":{"":"","參考資料#參考資料":"k8s dns 故障 Pod 无法解析主机名 Couldn‘t resolve host：https://blog.csdn.net/hknaruto/article/details/109361342\nSetting up NodeLocal DNSCache/Scaling up kube-dns：https://cloud.google.com/kubernetes-engine/docs/how-to/nodelocal-dns-cache#scaling_up_kube-dns\nDNS on GKE: Everything you need to know：https://medium.com/google-cloud/dns-on-gke-everything-you-need-to-know-b961303f9153\nMonitoring kube-dns pods in GKE：https://medium.com/@prageeshag/monitoring-kube-dns-pods-in-gke-83b76a0ef3d9","問題發生#問題發生":"","解決辦法--後續規劃#解決辦法 \u0026amp; 後續規劃":"此篇主要是來記錄一下在正式環境服務出現 cURL error 6: Could not resolve host 的問題，以及如何解決。\n問題發生 當天 RD 在公窗反應說，API 程式去打其他單位或是 Google BigQuery 的 API 時，會出現 cURL error 6: Could not resolve host 的錯誤，但這個錯誤是偶發性錯誤，且出現錯誤的 Pod 也不是固定，也會出現在不同的 node 上。\n當下檢查服務是正常的，也沒有任何異常，因此我們馬上開了 Google Support Ticket 來詢問。\n解決辦法 \u0026 後續規劃 自行測試 在等待 Google Support 回覆同時，也先在網路上搜尋出現 cURL error 6: Could not resolve host 的相關錯誤，主要都是因為 DNS 導致，因此我們也先檢查了一下目前 DNS 的設定。\n參考了 Google 的 kube-dns 文件後 Setting up NodeLocal DNSCache/Scaling up kube-dns，我們嘗試調整 kube-dns-autoscaler configmap 的設定：\nkube-dns-autoscaler configmap\n並觀察一陣子後發現，出現 cURL error 6: Could not resolve host 的錯誤 Log 已經沒有再出現，因此我們認為是 DNS 的問題，並回報給 Google Support。\nKube-dns 的 Pod 是由 kube-dns-autoscaler 的 configmap 來控制 Pod 數量，如圖：\nkube-dns-autoscaler configmap\nkube-dns-autoscaler configmap 設定 我們來說明一下這個設定內容是什麼 (下面會先以 GCP 的 kube-dns 官網文件來補充說明，而不是上面圖片)：\nconfigmap kube-dns-autoscalerlinear: '{ \"coresPerReplica\": 256, \"includeUnschedulableNodes\": true, \"nodesPerReplica\": 16, \"min\": 1, \"max\": 5, \"preventSinglePointFailure\": true }' coresPerReplica：多少個 cores 啟動一個 kube-dns pod (預設 256，也就是每 256 core 才會啟動一個 kube-dns pod) nodesPerReplica：多少個 node 啟動一個 kube-dns pod (預設 16，也就是每 16 node 才會啟動一個 kube-dns pod) min：最小 kube-dns pod 數量 max：最大 kube-dns pod 數量 GCP kube-dns 官網文件建議：\n要對 nodesPerReplica 設定比較低的值，確保在叢集節點擴充時，kube-dns pod 數量也會跟著增加。 也強烈建議設定一個明確的 max 值，以確保 GKE control plane 不會因大量的 kube-dns pod 去使用 Kubernetes API 而過載。 kube-dns 計算方式 kube-dns 的 pod 數量計算方式如下：\nreplicas = max( ceil( cores × 1/coresPerReplica ) , ceil( nodes × 1/nodesPerReplica ), maxValue ) 情境一：假設我現在有 16 個 node，core 數量會是 50，那麼 kube-dns pod 數量就會是：\nreplicas = max( ceil(50 × 1/256) , ceil(16 × 1/16)) \u003e replicas = max( 1 , 1) \u003e replicas = 1 情境二：假設我現在有 32 個 node，core 數量會是 100，那麼 kube-dns pod 數量就會是：\nreplicas = max( ceil(100 × 1/256) , ceil(32 × 1/16)) \u003e replicas = max( 1 , 2) \u003e replicas = 2 情境三：假設我現在有 96 個 node，core 數量會是 300，有多設定 max 是 5，那麼 kube-dns pod 數量就會是：\nreplicas = max( ceil(300 × 1/256) , ceil(96 × 1/16)) \u003e replicas = max( 2 , 6) \u003e replicas = 6 但是我們有設定 max 是 5，所以最後 kube-dns pod 數量就會是 5，不會超過 5。\n為了確保叢集 DNS 可用性的水平，也建議幫 kube-dns 設定 mix 值。\n從上面計算方法也可以知道，當叢集具有多個核心的節點時，coresPerReplica 會佔主導地位。當叢集使用核心數較少的節點數，nodesPerReplica 佔主導地位。通常應該會以 nodesPerReplica 來設定。\n調整完 kube-dns-autoscaler 的 configmap 設定，不需要重啟服務，kube-dns 就會自動 scale up or scale down pod 數\n最後等待 Google Support 回覆，並告知已經調整 kube-dns-autoscaler configmap 的設定，並觀察一陣子後，問題已經解決。\nGoogle Support 回覆 Google Support 回覆：\n簡單解釋一下為什麼 scale up kube-dns pods 數量會有幫助：\n當 Pod 做 DNS 查詢時，因為你們有啟用 NodeLocal DNSCache，所以他會先找 cache，如果沒有 cache 就會回 kube-dns 查詢。\n我們在表象上先觀察到 NodeLocal DNSCache latency 很高，而這有兩個可能，一個當然可能是自身問題，但更大的可能性是當 cache 要回源跟 kube-dns 查找時，kube-dns 已經沒辦法再承接更多 request，所以 NodeLocal DNSCache 會一直嘗試 retry，我們在指標上也看到 latency 拉高並持平在一個水位顯示出某個瓶頸。\n在經過 kube-dns 增加 pod 操作之後，看起來後續 DNS 查詢可以順利被處理跟 cache，所以問題就大幅緩解。\nGoogle Support 建議 架構優化：未來可以評估用 CloudDNS 取代 kube-dns，CloudDNS 為託管服務，可以自動 scale up 來處理效能問題。參考文件 監控：如果短期暫時無法替換 CloudDNS，建議可在 kube-dns 安裝 agent 做監控，當接近效能瓶頸時，可以提前 scale up kube-dns pod。非官方參考文件 後續規劃 但由於我們公司最近也在考慮 multi-cloud 的方案，所以暫時不考慮用 GCP 雲供應商的 CloudDNS 取代 kube-dns，我們想研究看看另一套 coreDNS 是否能夠解決這個問題。\n有興趣可以參考下一篇文章：Kube DNS vs Core DNS。"},"title":"Pod 出現 cURL error 6: Could not resolve host"},"/blog/kubernetes/pod-veth-name-provided-eth0-already-exists/":{"data":{"":"此文章要來記錄一下公司同事在正式服務上遇到的問題，會詳細說明遇到事情的經過，以及開單詢問 google support 最後討論出的暫時解決的辦法：\n簡單列出正式站當下服務環境：\ngke master version：1.25.10-gke.2700 gke node version：1.25.8-gke.1000 該問題發生的 node pool 有設定 taint 發生問題的 Pod 是用 Statefulset 建立的服務 ","事情發生的經過#事情發生的經過":" RD 同仁反應，發現使用 Statefulset 建立的排程服務有問題，下 kubectl delete 指令想要刪除 Pod，讓 Pod 重新長，卻卡在 Terminating，等待一段時間後，決定下 kubectl delete --force --grace-period=0 來強制刪除 Pod，這時候狀態會卡在 ContainerCreating，使用 Describe 查看，會出現以下錯誤： Warning (combined from similar events): Failed to create pod sandbox: rpo error: code = Unknown desc = failed to setup network for sandbox \"14fe0cd3d688aed4ffed4c36ffab1a145230449881bcbe4cac6478a63412b0c*: plugin type=*gke\" failed (add): container veth name provided (etho) already exists 我們 SRE 協助查看後，也有嘗試去下 kubectl delete --force --grace-period=0 來刪除 Pod，但還是一樣卡在 ContainerCreating，最後是先開一個新的 Node 並讓該 Pod 建立到新的 Node 上，才解決問題。為了方便 google support 協助檢查出問題的 Node，先將 Node 設定成 cordon，避免其他 Pod 被調度到該問題 node 上。 Node 設定成 cordon\nNode 可以設定 cordon、drain 和 delete 三個指定都會使 Node 停止被調度，只是每個的操作暴力程度不同：\ncordon：影響最小，只會將 Node 標示為 SchedulingDisabled 不可調度狀態，但不會影響到已經在該 Node 上的 Pod，使用 kubectl cordon [node name] 來停止調度，使用 kubectl uncordon [node name] 來恢復調度。\ndrain：會先驅逐在 Node 上的 Pod，再將 Node 標示為 SchedulingDisabled 不可調度狀態，使用 kubectl drain [node name] --ignore-daemonsets --delete-local-data 來停止調度，使用 kubectl uncordon [node name] 來恢復調度。\ndelete：會先驅逐 Node 上的 Pod，再刪除 Node 節點，它是一種暴力刪除 Node 的作法，在驅逐 Pod 時，會強制 Kill 容器進程，沒辦法優雅的終止 Pod。\n我們隨後開單詢問 goolge support。 ","參考資料#參考資料":"Node 節點禁止調度（平滑維護）方式- cordon，drain，delete：https://www.cnblogs.com/kevingrace/p/14412254.html","與-google-support-討論內容#與 Google Support 討論內容":"Google Support 經過查詢後，回覆說：這個問題是因為 Pod 被強制刪除導致，強制刪除是一種危險的操作，不建議這樣處理，下面有詳細討論。\n一開始卡在 Terminating 狀態，我們也有請 RD 說明一下當下遇到的問題以及處理動作：RD 當時想要刪除 Pod 是因為該程式當下有 Bug，將 redis 與 db 連線給關閉，程式找不到就會一直 retry，導致相關進程無法結束，再加上 terminationGracePeriodSeconds 我們設定 14400，也就是 4 小時，才會卡在 Terminating 狀態。 (terminationGracePeriodSeconds 設定這麼久是希望如果有被 on call，工程師上來時，可以查看該 Pod 的錯誤原因)\n因為卡在 Terminating 太久，RD 有執行 kubectl delete --force，就是因為下了 --force 才造成相關資源問題 (例如 container proccess, sandbox, 以及網路資源)沒有刪乾淨。所以引起了此次的報錯 “container veth name provided (eth0) already exists”。 (因為我們服務使用 Statefulset，Pod 名稱相同，導致 eth0 這個網路資源名稱重複，所以造成錯誤，可以用 deployment 來改善這個問題，只是資源如果沒有清理乾淨會佔用 IP，所以單純調整成 deployment 也不是最佳解)\nGoogle 產品團隊建議，如果 Pod 處於 Running 狀態時，想要快速刪除 Pod 時，一開始就先使用 kubectl delete pod --grace-period=number[秒數] 來刪除，如果已經是 Terminating 狀態則無效。(SRE 同仁已測試過，與 Google Support 說明相同)\n那如果已經處於 Terminating 狀態，要怎麽讓 Pod 被順利刪除，這部分 Google Support 後續會在測試並給出建議，目前測試是：進去卡住的 Pod Container，手動刪除主進程 (pkill) 就可以了。\nGoogle Support 回覆"},"title":"部署 Pod 遇到 container veth name provided (eth0) already exists 錯誤"},"/blog/nginx/":{"data":{"":"此分類包含 Nginx 相關的文章。\nNginx Proxy Set Header 注意事項 想使用 Nginx Upstream Proxy 到外部服務，並帶入對應的 header 該怎麼做？ Soketi WebSocket Server LOG 不定時出現 502 error 以及 connect() failed (111: Connection refused) "},"title":"Nginx"},"/blog/nginx/nginx-upstream-set-host-header/":{"data":{"":"此文章要來記錄一下最近在公司服務入口遇到的一些小問題，以及解決的方法。簡單說明一下，我們的服務入口是用 Nginx 來當作 proxy server，將不同路徑或是 servername 導到對應的後端程式，或是外部的服務上(例如 AWS cloudfront.net)，本篇要測試的是如果使用要同時使用 upstream 到外部服務，且需要帶 host header 該怎麼做。\nNginx 的 upstream 是什麼？\n通常我們 proxy_pass 的寫法會是這樣：\nlocation /aaa { proxy_pass http://aaa.example.com; } 當 Nginx 收到的 request 是 /aaa 時，就會將 request 轉發到 http://aaa.example.com。\n但假如後端有多台機器或是服務，可以處理同一種 request，這時候就可以使用 upstream 來處理：\nupstream backend_hosts { server aaa.example.com; server bbb.example.com; server ccc.example.com; } location /aaa { proxy_pass http://backend_hosts; } 這樣子的好處是可以有多個機器或是後端服務可以分散請求，做到負載平衡的效果。","參考資料#參考資料":"Make nginx to pass hostname of the upstream when reverseproxying：https://serverfault.com/questions/598202/make-nginx-to-pass-hostname-of-the-upstream-when-reverseproxying","問題#問題":"那如果我們使用 Nginx upstream 時，還想要同時帶 host 的 header 到後端該怎麼做呢？我們先來看一下目前的寫法：\n( 測試範例是使用 docker 來模擬，可以參考程式碼 \u003e 點我前往 github，會有三個 nginx，其中一個是負責 proxy 的 nginx 名為 proxy，另外兩台是 upstream 後的服務，名為 upstream_server1、upstream_server2 )\nnginx-old.conf upstream upstream_server { server upstream_server1; server upstream_server2; } server { listen 80; server_name localhost; location /upstream_server/ { proxy_pass http://upstream_server; proxy_set_header Host \"upstream_server1\"; proxy_set_header Host \"upstream_server2\"; access_log /var/log/nginx/access.log upstream_log; } } } 可以看到我們希望 Nginx 收到 request 是 /upstream_server 時，將 request 轉發到 http://upstream_server，而 upstream_server 後面有兩個 server，並且在 proxy 時，帶入兩個不同的 host header。但如果真的這樣寫，可以達到我們想要得效果嗎？我們實際跑看看程式 (範例可以使用 nginx-old.conf)：\nnginx 原本寫法\n從上面的 LOG 可以發現，我們 call /upstream_server 時，後端的 upstream_server1、upstream_server2 收到的 host 只會收到第一個設定的 Host，且服務會出現 400 Bad Request，查了一下網路文章，發現出現 400 Bad Request，可能跟 header 送太多資訊過去，詳細可以參考 解決網站出現 400 Bad Request 狀態的方法。\n這邊推測應該是後端如果也是用 nginx 直接接收才會遇到 400 的問題，還好目前公司服務還是正常的 xDD，檢查一下後發現，其實後端根本沒有要求對應 header 才能接收(應該是對方忘記加上此限制)。","解決#解決":"好，不管是否需要對應 header，我們還是找看看有沒有辦法同時使用 upstream，並帶入對應 host 的方法呢？\n最後參考網路上的文章，似乎只能使用兩層的 proxy，才能完成這兩個需求，我們來看看要怎麼寫吧 (範例可以使用 nginx.conf)：\nnginx.conf server { listen 777; server_name localhost; location / { proxy_pass http://upstream_server1; proxy_set_header Host \"upstream_server1\"; access_log /var/log/nginx/access.log upstream_log; } } server { listen 888; server_name localhost; location / { proxy_pass http://upstream_server2; proxy_set_header Host \"upstream_server2\"; access_log /var/log/nginx/access.log upstream_log; } } upstream upstream_server { server 127.0.0.1:777; server 127.0.0.1:888; } server { listen 80; server_name localhost; location /upstream_server/ { proxy_pass http://upstream_server; access_log /var/log/nginx/access.log upstream_log; } } 可以看到上面的程式碼，我們透過兩層的 proxy，來達到我們想要的效果，這樣子就可以同時使用 upstream，並且帶入對應的 host header。\n首先在 28 ~ 36 行，我們一樣如果 Nginx 收到 request 是 /upstream_server 時，會 proxy 到 upstream_server 這個 upstream 中，而 upstream_server 有兩個 server，分別是 127.0.0.1:777、127.0.0.1:888，但實際上沒有這兩個 port，所以我們需要再寫一層一般的 proxy 設定，分別是 1 ~ 10 行、12 ~ 21 行，這樣子就可以達到我們想要的效果。\n但這個方法比較適用於 upstream 後端沒有太多個服務或是機器的情況，如果有很多個服務或是機器，就需要寫很多的 proxy，這樣子會變得很麻煩，所以如果有更好的方法，也歡迎留言跟我分享 🤣。\n最後我們來看一下實際執行的結果：\n使用多層的 nginx proxy 處理"},"title":"想使用 Nginx Upstream Proxy 到外部服務，並帶入對應的 header 該怎麼做？"},"/blog/nginx/proxy-set-header/":{"data":{"":"","參考資料#參考資料":"关于 proxy_set_header 的注意点：https://keepmoving.ren/nginx/about-proxy-set-header/\nModule ngx_http_proxy_module#proxy_set_header：https://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_set_header","注意事項#注意事項":"那假設我有很多個 path 都需要帶同樣的 header，例如：\nserver { listen 80; server_name localhost; location /api { proxy_pass http://api-svc; proxy_set_header Host $host; } location /blog { proxy_pass http://blog-svc; proxy_set_header Host $host; } location /ian { proxy_pass http://ian-svc; proxy_set_header Host $host; } } 那我們可以將 proxy_set_header 改寫到 server block 中，這樣就可以避免重複寫很多次。\nserver { listen 80; server_name localhost; proxy_set_header Host $host; location /api { proxy_pass http://api-svc; } location /blog { proxy_pass http://blog-svc; } location /ian { proxy_pass http://ian-svc; } } 官方文件說明：\nAllows redefining or appending fields to the request header passed to the proxied server. The value can contain text, variables, and their combinations.\n因爲 proxy_set_header 是一個全域設定，它可以寫在 http、server、location block 中，所以如果有多台 server 都需要帶同樣的 header，可以寫在 http block 中。\n官方文件介紹 proxy_set_header\n但是 (っ・Д・)っ 如果原本的 location 有設定 proxy_set_header，那麼在 server block 中的 proxy_set_header 是沒有辦法同時使用，後端只會收到 location 的 proxy_set_header，所以要注意這個地方。\nserver { listen 80; server_name localhost; proxy_set_header Host $host; location /api { proxy_pass http://api-svc; } location /blog { proxy_pass http://blog-svc; } location /ian { proxy_pass http://ian-svc; proxy_set_header Test helloworld; } } 上面這個例子，/api 跟 /blog 都會帶 Host header，但是 /ian 只會帶 Test header。\n官方文件說明：\nThese directives are inherited from the previous configuration level if and only if there are no proxy_set_header directives defined on the current level. By default, only two fields are redefined:\n只有當前層級沒有定義 proxy_set_header，才會繼承上一層級，所以如果當前層級有定義，就繼承不到上一層級的設定。","說明#說明":"在 Nginx 中，我們可以透過 proxy_set_header 來設定 header，這樣子可以讓我們在 proxy 的過程中，將一些 header 帶到後端的服務上，除了新增，也可以蓋掉前面原有的 header。\n通常都長得像：\nserver { listen 80; server_name localhost; location / { proxy_pass http://your_service; proxy_set_header Host $host; } } "},"title":"Nginx Proxy Set Header 注意事項"},"/blog/nginx/soketi-log-502-error/":{"data":{"":"此文章要來記錄一下 RD 同仁前陣子有反應使用 Soketi 這個 WebSocket Server 會不定時在 LOG 出現 502 error 錯誤訊息以及 connect() failed (111: Connection refused) while connecting to upstream，雖然說服務使用上不會影響很大，但還是希望我們可以協助找出 502 的原因。\n出錯的 LOG\n在開始找問題前，先簡單介紹一下 Soketi 是什麼東西好了，根據官網的說明，他是簡單、快速且有彈性的開源 WebSockets server，想要了解更多的可以到它官網去查看。\n另外會把程式碼相關放到 GitHub » 點我前往","參考資料#參考資料":"[Nginx] 解決 connect() failed (111: Connection refused) while connecting to upstream：https://wshs0713.github.io/posts/8c1276a7/\nWebSocket proxying：http://nginx.org/en/docs/http/websocket.html\nday 10 Pod(3)-生命週期, 容器探測：https://ithelp.ithome.com.tw/articles/10236314","壓測#壓測":"最後調整完，我們來測試看看是否在 Pod 自動重啟 or 更新 Deployment 的時候(並且有大量連線時)還會噴 502 error 或是 connect() failed (111: Connection refused)，我們這邊使用 k6 來做 websocket 服務的壓測，有簡單寫一個壓測程式如下：\nk6 壓測\nk6 是一個開源的壓測工具，可以用來測試 API、WebSocket、gRPC 等服務，可以到它的官網查看更多資訊。\nMacOS 安裝方式：brew install k6\nwebsocket.jsimport ws from \"k6/ws\"; import { check } from \"k6\"; export const options = { vus: 1000, duration: \"30s\", }; export default function () { const url = \"wss://socket.XXX.com/app/hex-ws?protocol=7\u0026client=js\u0026version=7.4.1\u0026flash=false\"; const res = ws.connect(url, function (socket) { socket.on(\"open\", () =\u003e console.log(\"connected\")); socket.on(\"message\", (data) =\u003e console.log(\"Message received: \", data)); socket.on(\"close\", () =\u003e console.log(\"disconnected\")); }); check(res, { \"status is 101\": (r) =\u003e r \u0026\u0026 r.status === 101 }); } 簡單說明一下上面程式在寫什麼，我們在 const 設定 vus 代表有 1000 個虛擬使用者，會在 duration 30s 內完成測試，下面的 default 就是測試連線 ws 以及 message 跟 close 等動作，最後需要回傳 101 (ws 交握)\n執行 k6 run websocket.js 後，就會開始壓測，可以看到會開始執行剛剛在上面提到 default 的動作：\nk6 壓測過程\n等到跑完，就會告訴你 1000 筆裡面有多少的 http 101，這邊顯示 status is 101，就代表都是 101，代表都有連線成功，沒有出現 502 error 或是 connect() failed (111: Connection refused) 的錯誤，這樣就代表我們的問題解決了。\nk6 壓測結果","解決過程#解決過程":"我們可以看到上方錯誤 LOG 中，發現有出現 502 error 以及 connect() failed (111: Connection refused) while connecting to upstream，這兩個錯誤都是由 Nginx 所產生的，那我們先來理解一下，Nginx 與 Soketi 之間的關係。\n在使用上，RD 的程式會打 Soketi 專用的 Subdomain 來使用這個 WebSocket 服務，而在我們的架構上，這個 Subdomain 會經過用 nginx proxy server，來轉發到 Soketi WebSocket Server (走 k8s svc)，設定檔如下圖：\n入口 nginx 設定\n然後會出現 connect() failed (111: Connection refused) while connecting to upstream 的錯誤訊息，代表我們的 Nginx 設定少了一個重要的一行設定，就是 proxy_http_version 1.1;，這個設定要讓 Nginx 作為 proxy 可以和 upstream 的後端服務也是用 keepalive，必須使用 http 1.1，但如果沒有設定預設是 1.0，也要記得設定 proxy_set_header Upgrade、proxy_set_header Connection。調整過後就變成：\nws.confserver { server_name socket.XXX.com; listen 80 ; listen [::]:80 ; listen 443 ssl; listen [::]:443 ssl; ssl_certificate /etc/nginx/ingress.gcp.cert; ssl_certificate_key /etc/nginx/ingress.gcp.key; access_log /var/log/nginx/access.log main; location / { proxy_pass http://soketi-ws-ci:6001; proxy_connect_timeout 10s; proxy_read_timeout 1800s; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \"Upgrade\"; proxy_set_header X-Real-IP $remote_addr; } } 解決完 connect() failed (111: Connection refused) 這個問題後，接下來就是要解決 502 error 這個問題，會導致 502 代表 Nginx 這個 proxy server 連不上後端的 Soketi WebSocket Server，再觀察 LOG 以及測試後發現，當 Pod 自動重啟，或是手動重啟 Deployment 的時候，就會有 502 的錯誤，代表 Nginx 在 proxy 到後面的 Soketi svc 再到 Pod 的時候，有一段時間是連不上的，所以就會出現 502 的錯誤，可以推測是流量進到正在關閉的 Pod 或是進到還沒有啟動好的 Pod 才導致的。\n那我們先來看一下 Soketi WebSocket Server 的服務 yaml 檔案：\ndeployment.yaml deployment.yaml spec: terminationGracePeriodSeconds: 30 securityContext: {} containers: - name: soketi securityContext: {} image: \"quay.io/soketi/soketi::1.6.0-16-alpine\" ... 省略 (可以到 github 看 code)... livenessProbe: failureThreshold: 3 httpGet: httpHeaders: - name: X-Kube-Healthcheck value: \"Yes\" path: / port: 6001 initialDelaySeconds: 5 periodSeconds: 2 successThreshold: 1 可以看到原來的設定只有 livenessProbe 而已，因此我們為了要避免流量進到正在關閉的 Pod 或是進到還沒有啟動好的 Pod，所以我們需要加上 readinessProbe 以及 preStop，讓 Pod 確定啟動完畢，或是等待 Service 的 endpoint list 中移除 Pod，才開始接收流量，這樣就可以避免出現 502 的錯誤。\ndeployment.yaml spec: terminationGracePeriodSeconds: 30 securityContext: {} containers: - name: soketi securityContext: {} image: \"quay.io/soketi/soketi::1.6.0-16-alpine\" ... 省略 (可以到 github 看 code)... livenessProbe: failureThreshold: 3 httpGet: httpHeaders: - name: X-Kube-Healthcheck value: \"Yes\" path: / port: 6001 initialDelaySeconds: 5 periodSeconds: 2 successThreshold: 1 readinessProbe: failureThreshold: 3 httpGet: httpHeaders: - name: X-Kube-Healthcheck value: \"Yes\" path: /ready port: 6001 initialDelaySeconds: 5 periodSeconds: 2 successThreshold: 1 lifecycle: preStop: exec: command: [\"/bin/sh\", \"-c\", \"sleep 20\"] Pod 終止的過程"},"title":"Soketi WebSocket Server LOG 不定時出現 502 error 以及 connect() failed (111: Connection refused)"},"/blog/opentelemetry/":{"data":{"":"此分類包含 Opentelemetry 相關的文章。\n什麼是 Opentelemetry？可觀測性 (Observability) 又是什麼？ 如何透過 OpenTelemetry 來收集 Ingress Nginx Controller 的 Metrics 與 Traces 並送到 Datadog 上 "},"title":"Opentelemetry"},"/blog/opentelemetry/opentelemetry-ingress-nginx-controller/":{"data":{"":"由於最近公司想要導入 Datadog，在測試過程中順便導入 OpenTelemetry 來收集 Metrics 與 Traces 並送到 Datadog 上 ～\n🔥 這個範例比較特別，因為 Datadog 有提供 Ingress Nginx Controller 的 integrations，可以透過 Datadog Agent 來收集 Metrics，不需要透過 OpenTelemetry Collector 來收集。 ( Datadog Agent 請參考：https://docs.datadoghq.com/containers/kubernetes/ )\n程式部分也同步上傳到 github 上，可以點我前往","參考資料#參考資料":"Configure Nginx Ingress Controller to use JSON log format：https://dev.to/bzon/send-gke-nginx-ingress-controller-logs-to-stackdriver-2ih4\n淺談 OpenTelemetry - Collector Compoents：https://ithelp.ithome.com.tw/articles/10290703","執行步驟#執行步驟":" 先 clone 這個 repo (廢話 xD)\n先建立 OpenTelemetry Collector，執行以下指令：\nhelm upgrade collector \\ opentelemetry-collector \\ --repo https://open-telemetry.github.io/opentelemetry-helm-charts \\ --install \\ --create-namespace \\ --namespace opentelemetry \\ -f \"otel-collector.yaml\" 再建立 Ingress Nginx Controller，執行以下指令：\nhelm upgrade ingress-nginx \\ ingress-nginx \\ --repo https://kubernetes.github.io/ingress-nginx \\ --install \\ --create-namespace \\ --namespace ingress-nginx \\ -f \"ingress-nginx-values.yaml\" 接著建立測試用 Nginx 服務，執行以下指令：\nkubectl apply -f nginx.yaml ","檔案說明#檔案說明":" otel-collector.yaml： OpenTelemetry Collector 的設定檔，主要是設定要收集哪些 metrics、traces，並且要送到哪個 exporter，要注意的是 exporters 的 datadog 需要設定 site、api_key，以及 image 要記得用 otel/opentelemetry-collector-contrib，才會有 datadog 的 exporter。\ningress-nginx-values.yaml： Ingress Nginx Controller 的設定檔，這邊的 podAnnotations 是為了讓 Ingress Nginx Controller 的 Pod 能夠透過 Datadog agent 收集 metrics 到 Datadog 才加上的。\nconfig 裡面的設定有很多，主要都是 openTelemetry 的設定，要注意的是 enable-opentelemetry 要設為 true，另外 otlp-collector-host 以及 otlp-collector-port 要送到哪個 collector 等等也要記得設定。 另外如果想要將 LOG 與 Trace 串再一起，記得要把 log-format 設為 json，並且帶入，trace_id 與 span_id ( 這邊有多帶 dd.trace_id 是為了讓 datadog 可以自動串接 LOG \u0026 Trace )。\nnginx.yaml： 一個簡單的 Nginx 整套服務 (Deployment、Service、Ingress)，要注意的是 Ingress 需要設定 annotations kubernetes.io/ingress.class: nginx (這個是 Ingress Nginx Controller 的預設 class name)，才會被 Ingress Nginx Controller 接管 (才會有 Load Balancer 的 IP)","測試#測試":"當你執行完上面的步驟後，你會發現有產生兩個 namespace，一個是 ingress-nginx，另一個是 opentelemetry，並且會有 OpenTelemetry Collector、Ingress Nginx Controller、Nginx 等服務，如下：\n啟動服務\n我們試著打 http://nginx.example.com/ (測試網址，需要先在 /etc/hosts 綁定 Ingress Nginx Controller 咬住的 Load Balancer IP)，查看一下 Datadog 的 LOG，看看是否有收到 Nginx 的 LOG (此收集 LOG 的方式是透過在 cluster 上安裝 Datadog 的 agent)，如下：\nDatadog LOG\n接著查看 Datadog APM 的 trace，如下：\nDatadog APM\n由於我們在後面目前沒有串其他服務，所以只有一個 span，之後還有另外兩篇文章是介紹如何串其他服務 (會增加服務以及部分設定)，可以參考看看：opentelemetry-roadrunner、opentelemetry-nodejs\n順便看一下透過 Datadog Agent 收集的 Ingress Nginx Controller 的 Metrics，如下：\nDatadog Ingress Nginx Controller 的 Metrics\n可以用這些 Metrics 來做 Dashboard，如下：\nDatadog Dashboard","結論#結論":"透過 OpenTelemetry Collector 來收集 Ingress Nginx Controller 的 Metrics 與 Traces 並送到 Datadog 上，這樣就可以透過 Ingress Nginx Controller 的 Metrics 來做監控了，對於 RD 再開發上，有 Traces 也更方便 RD 他們找到程式的瓶頸 (有可能是服務導致的)。"},"title":"如何透過 OpenTelemetry 來收集 Ingress Nginx Controller 的 Metrics 與 Traces 並送到 Datadog 上"},"/blog/opentelemetry/opentelemetry-observability/":{"data":{"":"在介紹 Opentelemetry 之前，我們要先了解一下目前軟體架構以及基礎設施的演進：\n軟體架構以及基礎設施的演進\n第一階段在軟體架構設計上較為簡單，不會有什麼特別需要拆分出來的程式，所以都是一整包的程式，再測試以及除錯也比較不會有什麼問題。基礎設施都是使用 VM 或是使用放在 IDC 的機房來當 Server。\n第二階段隨著雲端技術的推出，會開始將服務搬上雲供應商提供的 IaaS 服務，或是使用私有雲給企業放置較機密的內容，其他則放置公有雲上，達成混合雲的模式。\n第三階段隨著雲端技術越來越成熟，有更多的雲端 IaC 以及功能推出，會開始考慮使用分散式的系統架構，將 DB 等服務也改用 Cloud SQL 的方式。在基礎設施上也隨著容器化的技術成熟而進入新的時代。\n第四階段已經使用 docker 來管理好一陣子，但發現虛擬容器技術在管理上十分不方便，因此 K8s 逐漸盛行，將架構從分散式改成微服務的方式進行，在讓開發團隊使用上可以更靈活且容易。\n雖然使用 K8s 可以讓我們的服務更靈活方便，但也會將服務切的越來越細，這時會讓開發變的十分複雜，我們在架構上從一開始的單體式架構，變成分散式架構，再到最後的微服務，讓開發人員需要處理的事情會越來越多。服務要如何連線？Log 要如何記錄？以及當一個請求會經過多個服務時，相對的延遲也會增加，這時要怎麼去處理等。在監控上，因為服務切分得很細，當線上有一個服務有問題時，要如何快速的找到問題點也是一大挑戰。\n當我們使用分散式系統或是微服務時發生故障時，會很難快速的恢復服務，因為每個服務都互相依賴，在以往都是透過經驗以及對系統的了解來得以解決。那有什麼其他的方式，能夠讓我們更快掌握每個服務呢？我們先來了解一個名詞：可觀測性(Observability)","opentelemetry#Opentelemetry":"那我們這次要介紹的可觀測性(Observability)工具就是 Opentelemetry，縮寫 OTel，它是由 CNCF (Cloud Native Computing Foundation) 組織孵化的開源專案，在 2021 年 5 月由 OpenTracing 與 OpenCensus 兩個框架合併，結合兩項分散式追蹤框架最重要的特性成為下一代收集遙測數據的新標準。\ntelemetry 又叫做遙測，是指能夠跨越不同系統來收集資料 (包含 LOG、Metric、trace) 的能力。\n我們可以看一下官網的說明：\nOpentelemetry 是雲原生的可觀測性(Observability)框架，提供標準化的 API、SDK 與協議自動檢測、蒐集、導出遙測數據資料 (Metrics、Log、Trace)，並支援 W3C 定義的 Http trace-context 規範，降低開發者在搜集遙測數據上的困難度，以及方便進行後續分析以及性能的優化。\nOpentelemetry\n在 OpenTelemetry 核心元件如下：\nAPI：開發人員可以透過 OpenTelemetry API 自動生成、蒐集應用程式(Application)的遙測數據資料(Metrics, Log, Trace)，每個程式語言都需實作 OpenTelemetry 規範所定義的 API 方法簽章。\nSDK：是 OpenTelemetry API 的實現。\nOTLP：規範定義了遙測數據的編碼與客戶端及服務器之間如何交換的協議 (gRPC、HTTP)。\nCollector：OpenTelemetry 中儲存庫，用於接收、處理、導出遙測數據到各種後端平台。","參考資料#參考資料":"Observability：https://linkedin.github.io/school-of-sre/level101/metrics_and_monitoring/observability/\n[OpenTelemetry] 現代化監控使用 OpenTelemetry 實現 : 可觀測性(Observability)：https://marcus116.blogspot.com/2022/01/modern-monitoring-using-openTelemetry-with-Observability.html\n[OpenTelemetry] 現代化監控使用 OpenTelemetry 實現 : OpenTelemetry 開放遙測：https://marcus116.blogspot.com/2022/01/opentelemetry-opentelemetry.html\n淺談 Observability(下)：https://ithelp.ithome.com.tw/m/articles/10287598\nManage services, spans, and traces in Splunk APM：https://docs.splunk.com/Observability/apm/apm-spans-traces/traces-spans.html","可觀測性observability#可觀測性(Observability)":"可觀測性有三個重要的特性，分別是：\nMetrics 負責監控系統有什麼狀況，當要發生服務故障前可以透過設定閥值搭配告警提早得知。\nLogs 當問題發生時，可以用來查看故障時正在執行哪些服務，以及產生的錯誤資訊。\nTraces （後面詳細介紹）\n可觀測性三大支柱\n我們對於 Metrics 跟 Logs 有基本的了解，所以我這邊會注重在 Traces 的部分：\n當有多個微服務的複雜分散式系統，用戶的請求會由系統中的多個微服務進行處理。Metrics 跟 Logs 可以提供我們有關系統如何去處理這些請求的一些資訊，但沒有辦法提供微服務的詳細訊息以及他是如何影響客戶端的請求。這時候就需要透過 Trace 來協助我們追蹤。\nTrace 可以在連續的時間維度上，透過 Trace 以及 Span 關聯，把空間給排列展示出來，並且有 Trace-Context 規範，能夠直觀的看到請求在分散式系統中經過所有服務的紀錄。\n什麼是 Trace、Span 、Trace-Context 呢？\n我們先說 Span，Span 又可以叫跨度，是系統中最小的單位，可以看下方圖片，SpanA 的資料是來源 SpanB，SpanB 來源是 SpanC 等等，每一個 Span 可以把它想成一個請求後面所有經過服務的工作流程，例如：nginx_module、db、redis 等等。\n請求的整個過程叫做 Trace，那他要怎麼知道 SpanA ~ SpanE 是同一個請求呢？\n就需要透過 TraceID 以及 SpanID 來記錄：\nTraceID：是唯一的 ID，用於識別整個分散式追蹤的一條請求路徑。在下方圖片中，當請求進入時，就會被賦予一個 TraceID，所有有經過的 Span 都會記錄此 TraceID，這樣才可以把不同服務依據 TraceID 關聯成同一個請求。\nSpanID：是一條請求路徑中單個操作唯一的 ID。追蹤路徑是由多個 Span 組成，每個 Span 都代表一個操作或特定的時間段。當請求進入時，每個服務就會產生一個 Span 來代表它處理請求的的時間。這些 Span 使用 TraceID 來連接再一起，形成完整的請求追蹤。\nTrace 示意圖\n那要怎麼查看每個 Span 的紀錄內容呢，就需要 Trace-Context：\n會放置一些用於追蹤和識別請求的上下文信息，例如 Trace ID、Span ID 和其他相關的數據。這些上下文信息可以是一些關鍵的數據，可以幫助我們在整個分佈式系統中追蹤請求的路徑，並將相關請求和操作關聯起來。\nECK Trace 示意圖\n上面的圖片中，可以看到 call /product/XXXX 後，會經過需多的 Span，隨便點擊一個 Span 可以看到它記錄的 Trace-Context，以及都會包含 TraceID 及 SpanID\nECK Trace 示意圖\nTrace 優點可以看到跨維度看到中間的資訊，對於找到問題以及瓶頸十分方便，但缺點就是因為需要在 Span 中產生 ID 以及內容，需要在程式裡面加入一定的套件以及調整程式碼。\n所以我們在可觀測性(Observability)最終的目的是希望可以透過可觀測性工具讓我們知道：\n請求通過哪些服務 每個服務在處理請求時做了些什麼 如果請求很慢，瓶頸在哪邊 如果請求失敗，錯誤點在哪 請求的路徑是什麼 為什麼花這麼長的時間 "},"title":"什麼是 Opentelemetry？可觀測性 (Observability) 又是什麼？"},"/blog/other/":{"data":{"":"先放置還沒有想到的分類 Blog 文章。\nLinux 常用指令 清除 Linux 機器上的 Swap (Buff、Cache、Swap 比較) Bookstack 開源知識庫筆記平台安裝 (K8s + docker) 找出程式碼、開源套件、容器的安全漏洞工具 - Snyk "},"title":"其他(還想不到分類)"},"/blog/other/bookstack/":{"data":{"":"最近剛好有公司同事離職，想要把交接的資料給整理整理，雖然部門之前有架設專用的 wiki 給 RD 使用，但覺得介面沒有到很好用，於是就在網路上尋找，可以多人編輯的筆記系統，一開始有想過用 CodiMD (HackMD)，但考量到需要多層的架構來區分文件，最後選擇 Bookstack 這個開源知識庫筆平台來作為組內的筆記系統，以下會簡單說一下 Bookstack 的特色以及使用 K8s 跟 docker 的安裝教學。","bookstack-介紹#Bookstack 介紹":"介紹部分主要參考 Bookstack 簡介，以下列出會選擇它的三個特色：\n簡潔的書本列表模式 書架分類\n書本分類\n頁面章節分類\n最主要是因為有以上幾個不同的分層架構，在資料整理上會更方便、更好彙整，可以自訂書架以及書本、頁面或是章節的封面以及內容。\n強大的搜尋功能 搜尋功能\n當我們的筆記內容越來越多，雖然有上面提到的分類模式，想要找到內容還是需要花一段時間，但 Bookstack 有強大的搜尋功能，可以針對書架、書本、章節或是書面個別搜尋，可以利用時間、標籤、標題或是內容來快速找到想要的內容。\n畫圖功能 在討論系統或是程式的架構，最好的方式就是用畫圖的來表示。在以往都是使用 Draw.io 來畫圖，當畫完圖後需要匯出畫好的圖以外，如果怕畫圖的原檔消失，還需要再另外下載原檔來保留，很不方便，又怕忘記下載，而 Bookstack 內建可直接編輯的功能，當畫完圖後有問題，可以直接點擊圖片來編輯，而這些功能還會搭配內建的版控，若有問題還可以還原到正確的圖片版本。\n畫圖功能","參考資料#參考資料":"BookStack 簡介：https://docs.ossii.com.tw/books/bookstack/page/bookstack\nBookStack Installation：https://www.bookstackapp.com/docs/admin/installation/","安裝說明#安裝說明":"那簡單說明為什麼會選擇 Bookstack 我們就來安裝它，這邊有使用 K8s + docker 來測試安裝，那我們就一起來看程式碼吧，程式碼放在這 👈：\nK8s namespace.yamlapiVersion: v1 kind: Namespace metadata: name: bookstack 我習慣會將不同的服務切 namespace 來部署，大家可以依照習慣來使用，下方的 yaml 都是建在此 namespace 上。\ndeployment.yamlapiVersion: apps/v1 kind: Deployment metadata: name: bookstack namespace: bookstack labels: app: bookstack spec: replicas: 1 selector: matchLabels: app: bookstack template: metadata: labels: app: bookstack spec: containers: - name: bookstack image: linuxserver/bookstack ports: - name: http containerPort: 80 env: - name: DB_DATABASE value: bookstack - name: DB_HOST value: \u003c\u003c更換此處\u003e\u003e - name: DB_PORT value: \"3306\" - name: DB_PASSWORD value: \u003c\u003c更換此處\u003e\u003e - name: DB_USERNAME value: \u003c\u003c更換此處\u003e\u003e - name: MAIL_USERNAME value: example@test.com - name: MAIL_PASSWORD value: mailpass - name: MAIL_HOST value: smtp.server.com - name: MAIL_PORT value: \"465\" - name: MAIL_ENCRYPTION value: SSL - name: MAIL_DRIVER value: smtp - name: MAIL_FROM value: no-reply@test.com - name: APP_URL value: https://\u003c\u003c更換此處\u003e\u003e - name: APP_LANG value: zh_TW - name: APP_TIMEZONE value: Asia/Taipei resources: limits: cpu: \"0.5\" memory: \"512Mi\" 必要更換的參數有標示 «更換此處»，其餘可以依照各組織來自行配置，Bookstack 會將內容存在 db，圖片等存在 pod 中，需要永久保存請使用 pvc + pv 或是另外掛 nas。\nsvc.yamlapiVersion: v1 kind: Service metadata: name: bookstack namespace: bookstack spec: type: NodePort selector: app: bookstack ports: - name: http protocol: TCP port: 80 targetPort: 80 ingress.yamlapiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: bookstack-ingress namespace: bookstack annotations: kubernetes.io/ingress.class: nginx service.beta.kubernetes.io/do-loadbalancer-enable-proxy-protocol: \"true\" spec: rules: - host: \u003c\u003c更換此處\u003e\u003e http: paths: - path: / pathType: Prefix backend: service: name: bookstack port: number: 80 最後透過 ingress 的 domain 去訪問 svc \u003e pod 上，就完成部署拉～第一次登入要使用預設帳號及密碼 admin@admin.com/password\ndocker docker-compose.yamlversion: \"3.8\" services: bookstack: image: lscr.io/linuxserver/bookstack container_name: bookstack environment: - PUID=\u003c\u003c更換此處\u003e\u003e - PGID=\u003c\u003c更換此處\u003e\u003e - DB_HOST=bookstack_db - DB_PORT=3306 - DB_USER=bookstack - DB_PASS=bookstack - DB_DATABASE=bookstackapp - APP_LANG=zh_TW - APP_TIMEZONE=Asia/Taipei - APP_URL=\u003c\u003c更換此處\u003e\u003e - GOOGLE_APP_ID=\u003c\u003c更換此處\u003e\u003e - GOOGLE_APP_SECRET=\u003c\u003c更換此處\u003e\u003e volumes: - /bookstack/config:/config ports: - 80:80 restart: unless-stopped depends_on: - bookstack_db bookstack_db: image: lscr.io/linuxserver/mariadb container_name: bookstack_db environment: - PUID=\u003c\u003c更換此處\u003e\u003e - PGID=\u003c\u003c更換此處\u003e\u003e - MYSQL_ROOT_PASSWORD=bookstack - TZ=Asia/Taipei - MYSQL_DATABASE=bookstackapp - MYSQL_USER=bookstack - MYSQL_PASSWORD=bookstack volumes: - /bookstack/config:/config restart: unless-stopped docker 的部分就更簡單了，一樣是把 «更換此處» 換成對應的內容即可，env 的部分可以參考官方文件，這邊比較特別的是我們有多使用 Google 來 Oauth 登入，Bookstack 支援多種的 Oauth 登入方式，可以參考 Third Party Authentication。\n架設好 Bookstack 就可以開始寫部門或是組織內的筆記拉～ 如果有開放外網連線，要記得修改預設的管理員帳號，以及用管理員帳號登入，到功能與安全的公開存取給取消，這樣就必須要登入才可以瀏覽筆記了！那就交給大家自己去玩這個好用的筆記工具囉～～ 😍\n功能與安全設定"},"title":"Bookstack 開源知識庫筆記平台安裝 (K8s + docker)"},"/blog/other/linux-clear-swap/":{"data":{"":"今天在工作時，遇到機器的 Swap 超過預警值，需要手動去清除 Swap，那剛好就由這次機會來介紹要如何清除 Linux 機器上的 Swap，以及查詢 Linux 記憶體的使用狀況！","buff-跟-cache-以及-swap-的比較#buff 跟 cache 以及 Swap 的比較":" 比較 buff cache Swap 功用 記憶體寫完資料會先暫存起來，等之後再定期將資料存到硬碟上 記憶體讀完資料後暫存起來，可以在下此查詢時快速的顯示 硬碟的交換分區，當 buff/cache 記憶體已經用完後，又有新的讀寫請求時，就會將部份內存的資料存入硬碟，也就是把內存的部分空間當成虛擬的記憶體來做使用 ","free-與-available-的比較#free 與 available 的比較":" free：是真正尚未被使用的實體記憶體數量 available：是應用程式認為可用的記憶體數量，可以理解成 available = free + buff/cache 接下來我們就要進入主題，如何清除 Swap ：","參考資料#參考資料":"Linux 內存、Swap、Cache、Buffer 詳細解析：https://os.51cto.com/article/636622.html\nlinux free 命令下 free/available 區別：https://www.796t.com/content/1545715382.html\n釋放 linux 的 swap 記憶體：https://www.796t.com/article.php?id=207781","如何查詢-linux-記憶體#如何查詢 Linux 記憶體":"在講 Swap 之前，我們先來說一下怎麼查詢 Linux 記憶體，可以使用以下指令來顯示：\nfree 下完後，格式會長這樣：\ntotal used free shared buff/cache available Mem: 32765564 8499252 1825132 1857720 22441180 19693100 Swap: 16776188 0 16776188 但這樣子不是很好觀察，所以我們可以加上 -h 來顯示大小的單位，讓我更清楚的知道每一個的大小：\ntotal used free shared buff/cache available Mem: 31G 8.1G 1.6G 1.8G 21G 18G Swap: 15G 0B 15G 那我們接著先來說說使用 free 查詢後，所有欄位的意思吧！\nfree 第一列 Mem：記憶體的使用資訊 Swap：交換空間的使用資訊 free 第一行 total：系統總共的可用實體記憶體大小 used：已被使用的實體記憶體大小 free：還剩下多少可用的實體記憶體 shared：被共享使用的實體記憶體大小 buff/cache：被 buffer 和 cache 使用的實體記憶體大小 available：可被 應用程式 使用的實體記憶體大小 ","清除-swap#清除 Swap":"首先第一步我們先使用 free 來查看目前的 Swap 使用狀態：\nSwap 使用 (尚未清除)\n可以看到我們的 Swap used 是 797M，我們設定它不能超過 5%，超過就會通知，所以我們要把它手動清除。\n先檢查記憶體 available 為什麼要先檢查 available，是因為一開始會使用到 Swap 的原因就是因為應用程式的可用記憶體空間不足，所以現在要清除 Swap 條件就是：Mem 的 available 必須要大於 Swap 的 used 才可以，否則會導致記憶體爆炸 💥\n將記憶體資料暫存到硬碟 接下來因為我們要清除 Swap ，所以不能讓資料在寫入記憶體中，所以我們先使用下方指令，讓記憶體的資料暫存到硬碟。\nsync 這個指令就是將存於暫存的資料強制寫入到硬碟中，來確保清除時導致資料遺失。\n關閉 Swap，再打開 Swap 沒錯，Swap 的清除就是把他先關掉，再重新打開，他就會自己清除 Swap 的資料了！使用的指令如下：(清除過程需要稍等，讓他進行刪除動作)\nswapoff -a \u0026\u0026 swapon -a 確認都沒問題後，我們就使用 free 來重新查看記憶體狀態：\nSwap 使用 (已清除)\n可以看到我們清除完 Swap 後，Swap 的 used 已經從 797M 變成 0B。\n如果碰到執行 swapoff -a \u0026\u0026 swapon -a 出現 swapoff: Not superuser.，只需要在指令前面加上 sudo 就可以了！ "},"title":"清除 Linux 機器上的 Swap (Buff、Cache、Swap 比較)"},"/blog/other/linux-command/":{"data":{"":"因為最近在管理機器時，常常會使用各式各樣的指令來協助管理，所以把常用的指令依照不同類別整理在底下呦 😘","參考資料#參考資料":"3 種檢查遠端埠號是否開啟的方法：https://www.ltsplus.com/linux/3-ways-check-remote-server-open-port","系統類#系統類":"顯示當前進程狀態 ps ps [參數] -A 列出所有的進程 -w 可以加寬顯示較多的訊息 -au 顯示更詳細的訊息 -aux 顯示所有包含其他用戶的進程 au(x) 輸出的格式：\nUSER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND USER：行程的擁有者 PID：pid %CPU：佔用的 CPU 使用率 %MEM：佔用的記憶體使用率 VSZ：佔用的虛擬記憶體大小 RSS：佔用的記憶體大小 TTY：終端的次要裝置號碼 STAT：該行程的狀態： D：無法中斷的休眠狀態 (通常都是 IO 進程) R：正在執行中 S：靜止狀態 T：暫停執行 Z：不存在但暫時無法消除 W：沒有足夠的記憶體可以分配 \u003c：高優先序的進程 N：低優先序的進程 START：進程開始時間 TIME：執行的時間 COMMAND：所執行的指令 刪除執行中的進程 kill kill [-s \u003c訊息名稱或編號\u003e] [程序] or [-l \u003c訊息編號\u003e] -l \u003c訊息編號\u003e：若不加 \u003c訊息編號\u003e選項，-l 會列出全部的訊息名稱 -s \u003c訊息名稱或編號\u003e：指定要送出的訊息 最常用的訊息是\n1 (HUP)：重新加載進程 9 (KILL)：刪除一個進程 15 (TERM)：正常停止一個進程 顯示系統開機紀錄 who who [參數] -b：查看最後一次(上次)系統啟動時間 -r：查看最後一次(上次)系統啟動時間及運行級別 top up 後面代表系統到目前運行的時間，所以反推就可以知道啟動時間\nuptime uptime 17:44 up 2 days, 8:48, 3 users, load averages: 3.48 3.37 3.56 一樣會顯示到目前已經啟動多久，跟是幾點啟動等資訊","網路類#網路類":"查詢遠端 Port 是否開放 nc (netcat) nc (netcat)，可以讀取經過 TCP 及 UDP 的網路連線資料，是一套很實用的網路除錯工具。\n安裝指令：\nyum install nc 檢查 Port 是否有開放，可以用以下指令來查詢：\nnc -zvw3 \u003cIP:Port\u003e -z 只進行掃描，不進行任何的資料傳輸\n-v 顯示掃描訊息\n-w3 等待 3 秒\n如果 Port 有開放，會回傳以下內容：\nNcat: Version 7.50 ( https://nmap.org/ncat ) Ncat: Connected to \u003cIP:Port\u003e. Ncat: 0 bytes sent, 0 bytes received in 0.01 seconds. 如果沒有開放，會回傳以下內容：\nNcat: No route to host. nmap nmap (Network Mapper) 是另一個可以檢查 Port 的工具，安裝語法是這樣：\nyum install nmap 檢查 Port 是否有開放，可以用以下指令來查詢：\nnmap \u003cIP\u003e -p \u003cPort\u003e 如果 Port 有開放，會回傳以下內容：\nStarting Nmap 7.92 ( https://nmap.org ) at 2022-06-17 16:06 CST Nmap scan report for\u003cDomain\u003e (\u003cIP\u003e) Host is up (0.0039s latency). PORT STATE SERVICE 80/tcp open http Nmap done: 1 IP address (1 host up) scanned in 1.09 seconds 如果沒有開放，會回傳以下內容：\nStarting Nmap 7.92 ( https://nmap.org ) at 2022-06-17 16:06 CST Nmap scan report for\u003cDomain\u003e (\u003cIP\u003e) Host is up (0.0039s latency). PORT STATE SERVICE 80/tcp filtered http Nmap done: 1 IP address (1 host up) scanned in 1.09 seconds Telnet Telnet 也是一個可以檢查 Port 的工具，安裝語法是這樣：\nyum install telnet 檢查 Port 是否有開放，可以用以下指令來查詢：\ntelnet \u003cIP\u003e \u003cPort\u003e 如果 Port 有開放，會回傳以下內容：\nTrying \u003cIP\u003e… Connected to \u003cIP\u003e. Escape character is ‘^]’. ^CConnection closed by foreign host. 如果沒有開放，會回傳以下內容：\nTrying \u003cIP\u003e… telnet: Unable to connect to remote host: Connection refused "},"title":"Linux 常用指令"},"/blog/other/snyk/":{"data":{"":"前幾天有機會可以去參加 DevOpsDays Taipei 2022 活動，聽到了一門有關於如何將 Security 加入 DevOps - GitLab CI/CD with Snyk 的講座，覺得蠻有趣的，想說順便紀錄一下這幾天去聽的內容，文章底下有附上本次大會的共筆以及該講者於自己部落格所發的文章，歡迎大家先行查看歐 😆\nDevOps 圖片 (醫院 DevOps 如何落地(1) ─ DevOps 與醫院)\n我們平常熟知的 DevOps 文化應該是像上面這張圖一樣，可以透過自動化「軟體交付」和「架構變更」的流程，來使得構建、測試、發布軟體能夠更加地快捷、頻繁和可靠。\n通常在 Development 跟 Operations 之外還有一個專門負責檢查程式有無漏洞或是安全性問題的單位，那我們試想一下，如果在 Dev 或是 Ops 中間先加上了檢查安全的步驟，是不是可以讓整體流程效率更加提升呢！？ 可以免去程式寫完才發現有安全性問題的漏洞，需要重新修改，而是在部署前，或是程式開發中就先被檢查出來。\n對於資訊安全方面，開發人員進行開發時，必須關注目前的寫法是不是安全的，除了開發人員的程式碼外，還要注意使用的開源程式或是套件是不是已經有被掃出漏洞，加上現在越來越多人使用容器化，對於 Container Images 裡面所包含的套件安全也是一大問題。\n所以單純的自動化整合與部署已經不能滿足需求，就像下圖一樣，開始有了 DevSecOps 這個詞產生，在 DevOps 的 CI/CD 流程中加入資安解決方案，藉由資安解決方案來減少開發人員去檢查程式漏洞的時間，也讓 DevOps 快速迭代更加的完整。\nDevSecOps 圖片 (Apps Built Better: Why DevSecOps is Your Security Team’s Silver Bullet)","snyk#Snyk":"那我們這次要使用的工具是 - Snyk，我先簡單介紹一下這個公司以及工具，它是位於波士頓的網絡安全公司，專門從事雲計算，提供增輕鬆集成，可以將安全掃描結合到 IDE、Repository、CI/CD 等，連續掃描以及一鍵修復等功能。他本身是一個 SaaS 的服務，所以需要到官網註冊一個帳號才能使用，也支援不同的 SSO 登入，詳細可以參考註冊畫面。\n這個工具主要提供了 4 種掃描的功能， 分別是：\nCode Open Source Container IaC 那我們會依照這４種功能依序介紹，詳細的可以參考 Snyk User Documentation # Explore Snyk products。\nCode 能夠直接掃描目前開發 Project 的程式碼有沒有安全性的問題，可以大幅減少開發人員去檢查程式漏洞的時間，目前支援了以下三種掃描方式：\nIDE Plugins Web CLI IDE Plugins IDE 我們這邊以 VS Code 為例來說明，如果有在用 VS Code 的大家應該知道 VS Code 有一個 Marketplace，可以安裝很多不同的套件 Plugins，之後有時間也會寫一篇好用的 Plugins 文章，大家可以在持續關注我歐 😍\n首先先開啟左側的 Marketplace，搜尋 Snyk，選擇下載量最高的準沒錯，選擇 Snyk Security - Code and Open Source Dependencies。 VS Code Marketplace 安裝 Snyk\n安裝完後，在左側欄位應該就會看到 Snyk 的狗狗 Logo，如果沒有登入過，會先要求你登入 Snyk 並授權給 Plugins 使用。最後當你開啟專案時，會開始掃描程式碼，並且會依照開源套件安全性、程式碼安全性來分類，會顯示套件遇到什麼漏洞需要升級以及程式碼哪一段會有安全性的問題，能夠讓開發人員在第一時間就修復安全問題。 IDE Plugins 掃描\nWeb 登入 Snyk 網站，直接加入對應的 Project，會直接於網頁上掃描漏洞，並提示錯誤內容。\n選擇加入 project\n掃描漏洞並顯示錯誤內容\nCLI 要使用 CLI 需要先安裝 Snyk CLI，我們一樣使用 Homebrew 來安裝：\nbrew tap snyk/tap brew install snyk 當然除了 Homebrew 以外還有其他的安裝方式，例如：curl 下載執行檔、npm、yarn、docker 執行等等，詳細可以參考以下安裝連結。\n可以直接使用以下指令才進行測試 sync test，還有其他 CLI 指令，詳細可以參考 Snyk CLI：\nCLI 掃描 (官網)\nOpen Source 能夠掃描目前開發的 Project 中有沒有使用到有漏洞的 library，目前支援以下圖片語言，詳細可以參考 Open Source - Supported languages and package managers：\nOpen Source library 掃描 (官網)\nContainer 在 Container image 中會存放著不同的 base image ，在 base image 中又會使用不同的 library。所以這些 container 的安全性也是需要被關注的。\n可以直接使用 web 來選擇相對的 Container 倉庫去做掃描，如果有安裝上面說的 Snyk CLI，也可以使用該指令執行 snyk container test \u003ccontainer image\u003e\nContainer 掃描\nIac Snyk 的基礎設施即代碼 (IaC) 可幫助開發人員編寫安全的基礎設施配置，預防錯誤的設定產生，並且支援多種格式：\nK8s YAML HashiCorp Terraform AWS CloudFormation Azure Resource Manager (ARM) ","參考資料#參考資料":"DevOpsDays Taipei 2022 共同筆記 - 在你的 DevOps 中加入一點 Security — GitLab CI/CD with Snyk - 鄭荃樺 (Barry.Cheng) ：https://hackmd.io/@DevOpsDay/2022/%2F%40DevOpsDay%2FHkm1iY6xi\n身為 DevOps 工程師，使用 Snyk 掃描漏洞也是很正常的：https://barry-cheng.medium.com/%E8%BA%AB%E7%82%BAdevops%E5%B7%A5%E7%A8%8B%E5%B8%AB-%E4%BD%BF%E7%94%A8snyk%E6%8E%83%E6%8F%8F%E6%BC%8F%E6%B4%9E%E4%B9%9F%E6%98%AF%E5%BE%88%E6%AD%A3%E5%B8%B8%E7%9A%84-d7d8f2ad2304\n在你的 DevOps 中加入一點 Security — GitLab CI/CD with Snyk (講師簡報)：https://s.itho.me/ccms_slides/2022/9/26/3a2ac1ef-4143-4328-9fef-066782ac7dc6.pdf"},"title":"找出程式碼、開源套件、容器的安全漏洞工具 - Snyk"},"/blog/rd/":{"data":{"":"此分類包含 RD 使用的相關工具文章。\n用 EFK 收集容器日誌 (HAProxy、Redis Sentinel、Docker-compose) Fluentd-Server 出現 Fluent::Plugin::Elasticsearch Error 400 - Rejected by Elasticsearch 錯誤解決 Kibana 新增 index 索引時一直轉圈圈以及顯示 HTTP 403 Forbidden "},"title":"RD 使用的相關工具"},"/blog/rd/fluentd-server-show-fluent-plugin-elasticsearch-error400/":{"data":{"":"先說結論 … EFK 真的好多雷 😆 我們今天又再度踩雷拉，這次又是炸的分身碎骨，花了 4 個多小時才找到原因，也有可能是小弟我跟 EFK 不是很熟 XD。就如標題所說，我在抽 Log 的時候發現有部分的 Log 送不到 Kibana，檢查 Fluentd-Server 的 Log 發現裡面有 Fluent::Plugin::Elasticsearch Error 400 - Rejected by Elasticsearch 的錯誤訊息，到底這個 Error 400 是什麼咧，就跟著我一起重溫找問題的苦難吧 XD","efk-結構#EFK 結構":"在開始之前我先簡單說一下我現在的 EFK 結構：\nEFK 結構\n現在的 EFK 結構如上圖，我在要抽 Log 的 Pod 中多塞一個 fluentd-bit container，將 fluentd-bit 與服務(例如 nginx) 的 log 掛相同路徑，讓 fluentd-bit 可以抽到服務 log，接著就透過 fluentd-server-svc 打到 fluentd-server pod，後面就是常見的 EFK 結構，就不多做說明。","參考資料#參考資料":"Elasticsearch 400 error #467：https://github.com/uken/fluent-plugin-elasticsearch/issues/467","問題的出現#問題的出現":"我們就像平常一樣寫好整個 EFK 的 yaml，run 的時候也正常，可以收到 log，但很奇怪的是我們先送 log_false 的 log 可以正常收到，但再接著送 log_true 就收不到，我們也嘗試反過來送，先送 log_true，再送 log_false，發現換收不到 log_false 的 log\nlog_false { \"result\": false, \"message\": \"我是馬賽克\", \"code\": 11223344, \"data\": { \"end_at\": \"2022-12-19 04:09:00\" }, \"response_code\": \"326dgf241geh1596489\", \"job_name\": \"叮叮噹噹聖誕節\", \"method\": \"GET\", \"url\": \"url.com\" } log_true { \"result\": true, \"data\": true, \"response_code\": \"wkCJl1dfr45671607965\" } 接著在 Fluentd-Server Log 中發現 Fluent::Plugin::Elasticsearch Error 400 - Rejected by Elasticsearch 錯誤\nfluentd-server 跳出錯誤 LOG\n我到網路上搜尋，看看有沒有人有遇到跟我一樣的問題，發現在 uken/fluent-plugin-elasticsearch 也有其他人有遇到同樣的問題\nuken/fluent-plugin-elasticsearch issus 詢問\n有人說可能是 Elasticsearch 的 disk 滿了，也有人說是裝在 Fluentd-Server 的套件 fluent-plugin-elasticsearch 版本有問題，或是將 Elasticsearch 跟 Kibana index 刪除就可以，我有砍掉 index 再重新倒入 Log，還是會有問題。\n一開始以為是 log_false 跟 log_true 的欄位不同，所以才會有這個問題發生（你看就知道我跟 EFK 不熟吧ＸＤ），所以有另外倒一些其他服務(欄位也不同)的 log 進去測試，發現是可以正常抽到，且 kibana 那邊會依照 es 提供的欄位自動新增，所以也不是欄位的問題，那最後的問題是什麼呢？","如何解決問題#如何解決問題":"最後在我們仔細檢查後發現，log_false 跟 log_true 的欄位其中一樣存的資料型態不太一樣，發現其中的 data 欄位在 log_false 的資料型態是 array，在 log_true 存的時候是字串，所以導致當其中一個 log 寫入後，另一個 log 就會因為同欄位的資料型態有所不同，而無法寫入 log，且在 Fluentd-Server 會出現標題的 Error 400 原因拉～\n最後重新調整欄位的資料型態，就順利解決此次問題 ✌️✌️✌️"},"title":"Fluentd-Server 出現 Fluent::Plugin::Elasticsearch Error 400 - Rejected by Elasticsearch 錯誤解決"},"/blog/rd/kibana-create-index-forbidden/":{"data":{"":"前幾天在工作使用 Kibana 時，想要新增一個新的索引，發現選擇索引並按下新增的按鈕，會一直轉圈圈，等了一陣子，使用開發工具 F12 查看，跳出了 HTTP 403 Forbidden，到底是什麼原因導致的呢！？我們一起看下去吧，會從問題的出現到問題原因再到如何解決問題，來仔細介紹，希望大家不要像我一樣踩到雷 🤣","參考資料#參考資料":"Kibana 创建索引 POST 403 (forbidden) on create index：https://www.cnblogs.com/caoweixiong/p/10972120.html","問題原因#問題原因":"文章的說明是索引變成只允許讀取的狀態，其原因是因為出現這個 HTTP 403 Forbidden 前，ElasticSearch 的空間滿了，導致 Kibana 會自動的將索引改成只允許讀取的狀態，我們來看一下剛剛的 Index 狀態是不是像他說的一樣變成只允許讀取的狀態呢\n可以到 kibana 的 Dev Tools 下指令來查詢歐，輸入 GET _settings，就會顯示以下圖片的內容囉 索引只允許讀取的設定變成\n發現正如文章所說，是因為 read_only_allow_delete 的狀態變成了 true，所以才沒辦法新增索引～","問題的出現#問題的出現":"那天是個變冷的 12 月，要幫 RD 同仁新增 Kibana 的索引時，照往常一樣輸入 Index pattern，按下一步，選擇 Configure settings ：\n輸入 Index pattern (圖片為範例，正常都是用 -* )\n按下新增索引，他就開始無限的轉圈圈，有去查看 ElasticSearch 的 log，發現也沒有特別的錯誤訊息\n新增索引後，一直轉圈圈\n接著想說打開開發者工具 F12 來看看，是卡在哪一個點，卻發現有幾個紅字寫著 HTTP 403 Forbidden\n開發者工具網頁內容顯示 HTTP 403 Forbidden\n想說為什麼會有 HTTP 403 Forbidden，之前也沒有看過類似的錯誤訊息，於是就開始在網路上亂晃，最後在同事的幫助下找到了一個跟我們情況很相似的文章 Kibana 创建索引 POST 403 (forbidden) on create index","如何解決問題#如何解決問題":"我們查看文章的解決辦法，有兩種辦法，一個是擴大 ElasticSearch 的空間，以及使用 kibana 的 Dev Tools 下指令修改，那我們兩種都有做，這邊就直接介紹下指令需要輸入什麼～\n需要再 Dev Tools 輸入以下指令，來修改 Index 的狀態：\nPUT _settings { \"index\": { \"blocks\": { \"read_only_allow_delete\": \"false\" } } } Dev Tools 修改 Index 的狀態\n最後我們用 GET _settings 檢查一下索引狀態是不是已經變回原本的了：\n索引只允許讀取的設定變成 false\n最後就可以順利新增索引拉 👍👍👍\n順利新增索引"},"title":"Kibana 新增 index 索引時一直轉圈圈以及顯示 HTTP 403 Forbidden"},"/blog/rd/redis-sentinel-docker-compose-haproxy-efk/":{"data":{"":"前情提要：本篇是 用 HAProxy 對 Redis 做負載平衡 (Redis Sentinel、Docker-compose) 以及 Redis 哨兵模式 (Sentinel) 搭配 Docker-compose 實作 的後續文章，主要會優化原本的程式碼，並使用 EFK 來收集 LOG！","什麼是-efk-#什麼是 EFK ?":"隨著現在各種程式系統複雜度越來越高，特別是現在都往雲上開始作部署，當我們想要查看 log 的時候，不可能一個一個去登入各節點去查看 log，不僅效率低，也會有安全性的問題，所以不可能讓工程師直接去訪問每一個節點。\n而且現在大規模的系統基本上都採用叢集的部署方式，意味著對每個 service，會啟動多個完全一樣的 POD 對外提供服務，每個 container 都會產生自己的 log，從產生 log 來看，你根本不知道是由哪個 POD 產生的，這樣對查看分佈式的日誌更加困難。\n所以在雲時代，需要一個收集並分析 log 的解決方案。首先需要將分佈在各個角落的 log 統一收集到一個集中的地方，方便查看。收集之後，還可以進行各種統計以及分析，甚至用流行的大數據或機器學習的方法來進行分析。\n所以誕生了 ELK 或是 EFK 的解決方式：\nELK 是由 Elasticsearch、Logstash、Kibana 所組成， EFK 是由 Elasticsearch、(Filebeats or Fluentd)、Kibana 所組成，兩者的差異在中間使用的開源程式，我會分別介紹一下每一個程式主要的用途：\nElasticsearch：它是一個集中儲存 log 的地方，更重要的是它是一個全文檢索以及分析的引擎，它能讓用戶以近乎實時的方式來查看、分析海量的數據。 Logstash、Filebeats、Fluentd：它們主要是收集分佈在各處的 log 並進行處理(Filebeats 僅收集)。 Kibana：它是為 Elasticsearch 開發的前端 GUI，可以讓用戶很方便的以圖形化進行查詢 Elasticsearch 中儲存的數據，同時也提供各式各樣的模組可以使用。 Logstash、Filebeats、Fluentd 關係：\nFilebeats 是一個輕量級收集本地 log 數據的方案，它僅能收集本地的 log，不能對 log 做處理，所以 Filebeats 通常會將 log 送到 Logstash 做進一步的處理。\n那為什麼不直接使用 Logstash 來收集收集並處理 log 呢？\n因為 Logstash 會消耗許多的記憶體，所以才會先透過 Filebeats 收集資料再傳給 Logstash 做處理。\n另外 Filebeats、Logstash、Elasticsearch 和 Kibana 都是屬於同一家公司的開源項目：https://www.elastic.co/guide/\n而 Fluentd 則是另一家公司的開源項目：https://docs.fluentd.org/\n那我們在這邊就使用 Fluentd 來做我們的 EFK 示範：\nEFK 示意圖 EFK Stack: Elasticsearch, Fluentd and Kibana on Docker\n上面這張圖代表我們會有很多 Docker 容器的 log (也就是之前的 redis 以及其他 nginx 等的容器)，會先透過 Fluentd 收集並處理各容器的 log 在傳送到 Elasticsearch 集中儲存，再使用 Kibana 圖形化介面來查詢或檢索儲在 Elasticsearch 的 log。","參考資料#參考資料":"elastic 官網：https://www.elastic.co/\nfluentd 官網：https://www.fluentd.org/\nBuild the EFK system used for simulating logging server on Docker：https://stackoverflow.com/questions/71155142/build-the-efk-system-used-for-simulating-logging-server-on-docker\n開源日誌管理方案ELK 和EFK 的區別：https://wsgzao.github.io/post/efk/","實作#實作":"那接下來會使用 Docker-compose 實作 EFK 的 LOG 分析，範例程式連結 點我 😘\n版本資訊\nmacOS：11.6 Docker：Docker version 20.10.12, build e91ed57 Nginx：1.20 PHP：7.4-fpm Redis：6.2.6 HAProxy：HAProxy version 2.5.5-384c5c5 2022/03/14 - https://haproxy.org/ Elasticsearch：8.1.3 Fluentd：v1.14 Kibana：8.1.3 檔案結構 . ├── docker-volume │ ├── fluentd │ │ └── fluent.conf │ ├── haproxy │ │ └── haproxy.cfg │ ├── nginx │ │ └── nginx.conf │ ├── php │ │ ├── info.php │ │ ├── r.php │ │ └── rw.php │ └── redis │ ├── redis.conf │ ├── redis1 │ ├── redis2 │ └── redis3 ├── docker.sh ├── efk │ └── Docker-compose.yaml ├── fluentd │ └── Dockerfile ├── haproxy_sentinel │ ├── Docker-compose.yaml │ ├── sentinel1 │ │ └── sentinel.conf │ ├── sentinel2 │ │ └── sentinel.conf │ └── sentinel3 │ └── sentinel.conf ├── nginx_php_redis │ └── Docker-compose.yaml └── php └── Dockerfile 這是主要的結構，簡單說明一下：(檔案越來越多了XD，這次把每一個都有分類，所以結構會與之前不太相同)\ndocker-volume/fluentd/fluent.conf：fluentd 的設定檔。 docker-volume/haproxy/haproxy.cfg：haproxy 的設定檔。 docker-volume/nginx/nginx.conf：nginx 的設定檔。 docker-volume/php/(r.php、rw.php)：測試用檔案。 docker-volume/redis/redis.conf：redis 的設定檔。 docker-volume/redis/(redis1、redis2、redis3)：放 redis 的資料。 docker.sh：是我另外多寫的腳本，可以查看相對應的角色。 efk/Docker-compose.yaml：會放置要產生的 elasticsearch、kibana、fluentd 的容器設定檔。 fluentd/Dockerfile：因為 fluentd 需要另外安裝 fluent-plugin-elasticsearch 才能使用，所以用 Dockerfile 另外寫 fluent 的映像檔。 haproxy_sentinel/Docker-compose.yaml：會放置要產生的 haproxy、sentinel1、sentinel2、sentinel3 的容器設定檔。 haproxy_sentinel/(sentinel1、sentinel2、sentinel3)/.conf：哨兵的設定檔。 nginx_php_redis/Docker-compose.yaml：會放置要產生的 nginx、php、redis1、redis2、redis3 的容器設定檔。 php/Dokcerfile：因為在 php 要使用 redis 需要多安裝一些設定，所以用 Dockerfile 另外寫 PHP 的映像檔。 那我們就依照安裝的設定開始說明：(這邊只說明與上一篇不同地方，重複的就請大家回去看之前的文章) docker-volume/fluentd/fluent.conf \u003csource\u003e @type forward bind 0.0.0.0 port 24224 \u003c/source\u003e \u003cmatch *.**\u003e @type copy \u003cstore\u003e @type elasticsearch host elasticsearch port 9200 logstash_format true logstash_prefix fluentd logstash_dateformat %Y%m%d include_tag_key true type_name access_log tag_key @log_name flush_interval 1s \u003c/store\u003e \u003cstore\u003e @type stdout \u003c/store\u003e \u003c/match\u003e 這是 fluent 的設定檔，可以在這邊做自訂的設定，例如 host、port、預設日期、tag_key 等等。\nfluentd/Dockerfile FROM fluent/fluentd:v1.14 USER root RUN [\"gem\", \"install\", \"fluent-plugin-elasticsearch\"] 因為我們需要先安裝 fluent-plugin-elasticsearch 才可以讓 fluentd 來使用 elasticsearch，所以多寫一個 Dockerfile 來設定。\nefk/Docker-compose.yaml version: '3.8' services: elasticsearch: image: elasticsearch:8.1.3 container_name: elasticsearch environment: - discovery.type=single-node - xpack.security.enabled=false # elasticsearch 8.x版本後會自動開啟SSL networks: efk_network: ipv4_address: 172.20.0.3 ports: - 9200:9200 kibana: image: kibana:8.1.3 container_name: kibana environment: - ELASTICSEARCH_HOSTS=http://elasticsearch:9200 # - I18N_LOCALE=zh-CN networks: efk_network: ipv4_address: 172.20.0.4 ports: - 5601:5601 depends_on: - elasticsearch fluentd: build: ../fluentd container_name: fluentd volumes: - ../docker-volume/fluentd:/fluentd/etc depends_on: - elasticsearch ports: - \"24224:24224\" - \"24224:24224/udp\" networks: efk_network: ipv4_address: 172.20.0.5 networks: efk_network: driver: bridge name: efk_network ipam: config: - subnet: 172.20.0.0/16 gateway: 172.20.0.1 這邊是設定 EFK 三個容器的檔案，比較特別的是 elasticsearch 在 8.x 版本後會自動開啟 SSL 連線，所以沒有使用的或是在測試中的要先把它關掉，不然會連不上！，kibana 它支援 I18N 多語系，但目前沒有繁體中文，所以想要看中文的可以切成 zh-CH 的簡體中文來使用，其他就是基本的設定 Posts 以及我們全部設定好 IP，在後續測試會比較方便～\nnginx_php_redis/Docker-compose.yaml version: \"3.8\" services: nginx: image: nginx container_name: nginx networks: efk_network: ports: - 8080:80 volumes: - ../docker-volume/nginx/:/etc/nginx/conf.d/ environment: - TZ=Asia/Taipei logging: driver: \"fluentd\" options: fluentd-address: 172.20.0.5:24224 tag: nginx php: build: ../php container_name: php networks: efk_network: expose: - 9000 volumes: - ../docker-volume/php/:/var/www/html environment: - TZ=Asia/Taipei logging: driver: \"fluentd\" options: fluentd-address: 172.20.0.5:24224 tag: php redis1: image: redis container_name: redis1 command: redis-server /usr/local/etc/redis/redis.conf --appendonly yes volumes: - ../docker-volume/redis/redis1/:/data - ../docker-volume/redis/:/usr/local/etc/redis/ environment: - TZ=Asia/Taipei networks: efk_network: ipv4_address: 172.20.0.11 ports: - 6379:6379 logging: driver: \"fluentd\" options: fluentd-address: 172.20.0.5:24224 tag: redis1 redis2: image: redis container_name: redis2 command: redis-server /usr/local/etc/redis/redis.conf --slaveof redis1 6379 --appendonly yes volumes: - ../docker-volume/redis/redis2/:/data - ../docker-volume/redis/:/usr/local/etc/redis/ environment: - TZ=Asia/Taipei networks: efk_network: ipv4_address: 172.20.0.12 ports: - 6380:6379 depends_on: - redis1 logging: driver: \"fluentd\" options: fluentd-address: 172.20.0.5:24224 tag: redis2 redis3: image: redis container_name: redis3 command: redis-server /usr/local/etc/redis/redis.conf --slaveof redis1 6379 --appendonly yes volumes: - ../docker-volume/redis/redis3/:/data - ../docker-volume/redis/:/usr/local/etc/redis/ environment: - TZ=Asia/Taipei networks: efk_network: ipv4_address: 172.20.0.13 ports: - 6381:6379 depends_on: - redis1 - redis2 logging: driver: \"fluentd\" options: fluentd-address: 172.20.0.5:24224 tag: redis3 networks: efk_network: external: name: efk_network 那這邊基本上都與上一篇一樣，沒有修改特別的地方，只有修改網路名稱以及：\nlogging: driver: \"fluentd\" options: fluentd-address: 172.20.0.5:24224 tag: nginx 我們要把每一個容器的 log 進行收集與處理，所以我們使用 logging 然後 driver 選擇 fluentd，並且要設定 fluentd 的 IP 位置以及每一個容器可設定不同的 tag 方便我們查詢。\nhaproxy_sentinel/Docker-compose.yaml version: '3.8' services: haproxy: image: haproxy container_name: haproxy volumes: - ../docker-volume/haproxy/:/usr/local/etc/haproxy environment: - TZ=Asia/Taipei networks: efk_network: ipv4_address: 172.20.0.20 ports: - 16379:6379 - 8404:8404 logging: driver: \"fluentd\" options: fluentd-address: 172.20.0.5:24224 tag: haproxy sentinel1: image: redis container_name: redis-sentinel-1 networks: efk_network: ports: - 26379:26379 command: redis-server /usr/local/etc/redis/sentinel.conf --sentinel volumes: - ./sentinel1:/usr/local/etc/redis/ environment: - TZ=Asia/Taipei logging: driver: \"fluentd\" options: fluentd-address: 172.20.0.5:24224 tag: sentinel1 sentinel2: image: redis container_name: redis-sentinel-2 networks: efk_network: ports: - 26380:26379 command: redis-server /usr/local/etc/redis/sentinel.conf --sentinel volumes: - ./sentinel2:/usr/local/etc/redis/ environment: - TZ=Asia/Taipei logging: driver: \"fluentd\" options: fluentd-address: 172.20.0.5:24224 tag: sentinel2 sentinel3: image: redis container_name: redis-sentinel-3 networks: efk_network: ports: - 26381:26379 command: redis-server /usr/local/etc/redis/sentinel.conf --sentinel volumes: - ./sentinel3:/usr/local/etc/redis/ environment: - TZ=Asia/Taipei logging: driver: \"fluentd\" options: fluentd-address: 172.20.0.5:24224 tag: sentinel3 networks: efk_network: external: name: efk_network 與前面一樣，多了一個 logging 來設定 fluentd 位置以及 tag。","測試#測試":"我們先用 docker-compose up 來啟動 efk/Docker-compose.yaml，接著再啟動 nginx_php_redis/Docker-compose.yaml，最後啟動 haproxy_sentinel/Docker-compose.yaml：\n啟動 efk/Docker-compose.yaml\n啟動 nginx_php_redis/Docker-compose.yaml\n啟動 efk/Docker-compose.yaml\n(啟動 efk 時要等他跑完，因為他需要啟動一陣子，如果還沒等他啟動完畢就啟動下一個，會導致 fluentd 的 fluentd-address 尚未設定好，導致啟動錯誤)\n這時候可以使用瀏覽器搜尋以下網址：\ntest.com:5601：Kibana GUI 頁面。 kibana 設定\n如果連線成功進來，就代表我們有安裝好 Elasticsearch 以及 kibana，那我們來做一些設定，讓我們可以在 kibana 上面看到 log。\n先點選左邊的欄位，點選 “Stack Management” ，可以看到目前的 kibana 的版本，再點選左邊的 “Kibana \u003e Date Views”：\nkibana 設定\n然後會跳出一個視窗點選 “Create data view”，在 name 欄位輸入 fluentd* (右側有 fluentd 加時間，代表我們接 fluentd 有成功)，按下 “Create data view”\nkibana 設定\n接著點選左側的欄位，點選 “Analytics \u003e Discover” ，就可以看到我們目前所有的 log 囉！\nkibana 設定\nkibana Analytics \u003e Discover"},"title":"用 EFK 收集容器日誌 (HAProxy、Redis Sentinel、Docker-compose)"},"/blog/redis/":{"data":{"":"此分類包含 Redis 相關的文章。\n用 HAProxy 對 Redis 做負載平衡 (Redis Sentinel、Docker-compose) Redis 哨兵模式 (Sentinel) 搭配 Docker-compose 實作 Redis 介紹 "},"title":"Redis"},"/blog/redis/redis-introduce/":{"data":{"":"","什麼是-redis-#什麼是 Redis ?":"Redis 全名是 Remote Dictionary Server ，是快速的開源記憶體鍵值資料庫 (keys-value database)。\n由於 Redis 的回應時間極短，低於一毫秒，可以讓遊戲、金融服務、醫療保健等即時應用服務每秒處理幾百萬個請求。\nRedis 的優勢 效能 所有的 Redis 資料都是存放在記憶體中，進而實現低延遲和高傳輸量的資料存取。\n彈性的資料結構 一般的鍵值資料庫提供的資料結構有限，而 Redis 提供多樣化的資料結構來滿足服務的需求，包含字串(Strings)、哈希(Hashes)、列表(Lists)、集合(Sets)、有序集合(Zset)。(後續會有詳細介紹)\n簡單易用 Redis 可以用更少、更精簡的指令來取代傳統複雜的程式碼，可以存取應用程式的資料。並支援 Java、Python、PHP、C/C++、C#、JavaScript、Node.js、Ruby、R、GO。\n複寫和持久性 Redis 採主要-複本架構，支援非同步複寫，可以將資料複寫到多個複本伺服器。不但可以提升讀取效能(因為請求可分割到多部伺服器)，還可以再主服務器發生故障時快速恢復。至於持久性，Redis 支援時間點備份，會將資料複製到磁碟中。","參考資料#參考資料":"Redis 官網：https://redis.io\nRedis：https://aws.amazon.com/tw/redis/\nRedis 基本資料形態：https://blog.judysocute.com/2020/10/04/redis-%E5%9F%BA%E6%9C%AC%E8%B3%87%E6%96%99%E5%BD%A2%E6%85%8B/\nRedis 發布訂閱：https://www.runoob.com/redis/redis-pub-sub.html","實際操作#實際操作":"安裝 Redis 使用 Homebrew 來安裝 Redis (Mac OS：11.6)\n$ brew install redis 安裝好用 -v 檢查版本\n$ redis-server -v Redis server v=6.2.6 sha=00000000:0 malloc=libc bits=64 build=c6f3693d1aced7d9 $ redis-cli -v redis-cli 6.2.6 一個是 Server、另一個是 Cli，所以在稍後測試時，需要開啟兩個 Terminal 來執行歐！\n執行 Redis Server $ redis-server 39403:C 08 Mar 2022 11:13:17.500 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo 39403:C 08 Mar 2022 11:13:17.500 # Redis version=6.2.6, bits=64, commit=00000000, modified=0, pid=39403, just started 39403:C 08 Mar 2022 11:13:17.500 39403:M 08 Mar 2022 11:13:17.501 * Increased maximum number of open files to 10032 (it was originally set to 256). 39403:M 08 Mar 2022 11:13:17.501 * monotonic clock: POSIX clock_gettime _._ _.-``__ ''-._ _.-`` `. `_. ''-._ Redis 6.2.6 (00000000/0) 64 bit .-`` .-```. ```\\/ _.,_ ''-._ ( ' , .-` | `, ) Running in standalone mode |`-._`-...-` __...-.``-._|'` _.-'| Port: 6379 | `-._ `._ / _.-' | PID: 39403 `-._ `-._ `-./ _.-' _.-' |`-._`-._ `-.__.-' _.-'_.-'| | `-._`-._ _.-'_.-' | https://redis.io `-._ `-._`-.__.-'_.-' _.-' |`-._`-._ `-.__.-' _.-'_.-'| | `-._`-._ _.-'_.-' | `-._ `-._`-.__.-'_.-' _.-' `-._ `-.__.-' _.-' `-._ _.-' `-.__.-' 39403:M 08 Mar 2022 11:13:17.503 # Server initialized 39403:M 08 Mar 2022 11:13:17.503 * Ready to accept connections 如果出現上面符號，就代表 Server 已經啟動，接下來再開另一個 Terminal 來執行 Cli。\nCli 開啟後，在下一個 ping 指令，該指令用於檢測 redis 服務是否啟動，正常會顯示 pong。\n$ redis-cli 127.0.0.1:6379\u003e ping PONG 接下來，我們都會在 Cli 視窗做測試，會詳細介紹每一個資料型態以及其適合情境。\n後續會使用 Cli 畫面來做示範，但也有不錯的圖形化工具可以用於 redis 上 AnotherRedisDesktopManager 大家可以去試用看看~\n資料型態 字串 (Strings) set、get 這是最基本的型態，可以存放 binary, string, integer, float資料，一個 Strings 的欄位，最高可儲存 512 Megabytes，這裡會用到的指令是 set、get ，分別用來儲存以及讀取字串，我們來看一下範例吧。\n127.0.0.1:6379\u003e set string hello-world OK 127.0.0.1:6379\u003e get string \"hello-world\" 我們先用 set 將 hello-world 字串存到 string 這個 key，再用 get 顯示 string 裡面的 value。\nincr、decr Redis 還有一些方便的指令，如果存入的 value 是 integer 型態，就可以使用 incr 、decr ，來累加與累減。分別代表累加，像是我們的 ++ ，以及累減，像是我們的 - -。\n127.0.0.1:6379\u003e set num 10 OK 127.0.0.1:6379\u003e incr num (integer) 11 127.0.0.1:6379\u003e incr num (integer) 12 127.0.0.1:6379\u003e decr num (integer) 11 127.0.0.1:6379\u003e decr num (integer) 10 append 如果 key 已經存在並且它是字串，可以使用 append 指令，會從字串最後面附加進去，如果不存在，則會直接建立一個，並把值存進去。\n127.0.0.1:6379\u003e exists str (integer) 0 127.0.0.1:6379\u003e append str \"Hello\" (integer) 5 127.0.0.1:6379\u003e append str \" World~\" (integer) 12 127.0.0.1:6379\u003e get str \"Hello World~\" getrange 可以輸入字串的開始位元與結束位元，會依照你輸入的去顯示字串。我把它理解成是陣列的 key 與 value 的關係。\n127.0.0.1:6379\u003e set a \"This is a string\" OK 127.0.0.1:6379\u003e getrange a 0 3 \"This\" 127.0.0.1:6379\u003e getrange a -3 -1 \"ing\" 127.0.0.1:6379\u003e getrange a 0 -1 \"This is a string\" 127.0.0.1:6379\u003e getrange a 10 100 \"string\" mset 我們也可以設定的時候，把要設定的值都一起設定，只需要使用 mset 就可以達成。\n127.0.0.1:6379\u003e mset 1 1 2 2 OK 127.0.0.1:6379\u003e get 1 \"1\" 127.0.0.1:6379\u003e get 2 \"2\" 127.0.0.1:6379\u003e get 3 (nil) 如果用 get 顯示資料，若沒有對應的 key ，會顯示 (nil)。\n字串型態適合場景 字串(strings) 型態適合用於圖片快取 （使用binary）、累計次數、觀看累計次數\nString 適用場景圖\n哈希 (Hashes) 可以把他想像成二維陣列，應該會比較好理解，我網路上找了一張圖，應該會比較清楚！\nHashes 示意圖 (Redis 基本資料形態)\nHashes 是用來存放一組相同性質的資料，這些資料 Hashes 或是物件的某一屬性，與 String 較為不同的是他可以取回單一個欄位資料，但 String 必須取回所有資料，單一個 Key 可以存放2^32 - 1的資料欄位，\n他的資料型態有像是，一個 user001 裡面是一個 Hashes，Hashes 裡面又會存放 name、phone、gender ，我們來實際操作看看。\nhset、hget 127.0.0.1:6379\u003e hset student name ian phone 0980123456 gender M (integer) 3 127.0.0.1:6379\u003e hget student name \"ian\" 127.0.0.1:6379\u003e hget student phone \"0980123456\" 127.0.0.1:6379\u003e hget student gender \"M\" Hashes 的話要使用 hset、hget 來對 Hashes 做儲存以及讀取。\nhgetall 想要一次顯示 Hashes 裡面的 key 跟 value ，可以使用 hgetall 來顯示全部資料。\n127.0.0.1:6379\u003e hset student name ian phone 0980123456 gender M (integer) 3 127.0.0.1:6379\u003e hgetall student 1) \"name\" 2) \"ian\" 3) \"phone\" 4) \"0980123456\" 5) \"gender\" 6) \"M\" hkeys 想要單獨顯示 Hashes 裡面的 key ，可以使用 hkeys 來顯示。\n127.0.0.1:6379\u003e hset student name ian phone 0980123456 gender M (integer) 3 127.0.0.1:6379\u003e hkeys student 1) \"name\" 2) \"phone\" 3) \"gender\" hvals 想要單獨顯示 Hashes 裡面的 value ，可以使用 hvals 來顯示。\n127.0.0.1:6379\u003e hset student name ian phone 0980123456 gender M (integer) 3 127.0.0.1:6379\u003e hvals student 1) \"ian\" 2) \"0980123456\" 3) \"M\" hlen 想要顯示 Hashes 裡面的 key 長度，可以使用 hlen 來顯示。\n127.0.0.1:6379\u003e hgetall student 1) \"name\" 2) \"ian\" 3) \"gender\" 4) \"M\" 127.0.0.1:6379\u003e hlen student (integer) 2 hincrby 想要增加 Hashes 裡面的 value 整數，可以使用 hincrby 來新增。\n127.0.0.1:6379\u003e hset test a 10 b 20 (integer) 2 127.0.0.1:6379\u003e hincrby test a 2 (integer) 12 hdel 想要刪除 Hashes 裡面的 key，可以使用 hdel 來刪除。\n127.0.0.1:6379\u003e hset student name ian phone 0980123456 gender M (integer) 3 127.0.0.1:6379\u003e hdel student phone (integer) 1 127.0.0.1:6379\u003e hgetall student 1) \"name\" 2) \"ian\" 3) \"gender\" 4) \"M\" 哈希型態適合場景 哈希(Hashes) 型態適合用於每次只需要取用一部分的資料\nHashes 適用場景圖\n列表 (Lists) lpush、lrange Lists 資料型態可以想像成程式語言中的Array物件。Lists 單一個Key可以存放2^32 - 1，這邊會使用到 lpush、lrange 來對 Lists 做儲存以及讀取。\n127.0.0.1:6379\u003e lpush list2 a a b b c d e (integer) 7 127.0.0.1:6379\u003e lrange list2 0 10 1) \"e\" 2) \"d\" 3) \"c\" 4) \"b\" 5) \"b\" 6) \"a\" 7) \"a\" rpush 除了從隊伍頭放入資料，也可以用 rpush 從隊伍尾放入資料，如果使用 lrange 來顯示方向也會相反歐。\n127.0.0.1:6379\u003e rpush list3 a a b b c d e (integer) 7 127.0.0.1:6379\u003e lrange list3 0 10 1) \"a\" 2) \"a\" 3) \"b\" 4) \"b\" 5) \"c\" 6) \"d\" 7) \"e\" lpop、rpop 也可以使用 lpop、rpop 分別從隊伍頭或尾彈出一筆資料。\n127.0.0.1:6379\u003e rpush list3 a a b b c d e (integer) 7 127.0.0.1:6379\u003e lpop list3 \"a\" 127.0.0.1:6379\u003e rpop list3 \"e\" lset 可以使用 lset 來設定指定位置的資料。\n127.0.0.1:6379\u003e lrange list3 0 2 1) \"a\" 2) \"b\" 3) \"c\" 127.0.0.1:6379\u003e lset list3 1 w OK 127.0.0.1:6379\u003e lrange list3 0 2 1) \"a\" 2) \"w\" 3) \"c\" 列表型態適合場景 列表(Lists) 型態適合用於文章列表或者資料分頁展示的應用\nLists 適用場景圖\n集合 (Sets) 其實跟 Lists 一樣，就是資料的集合，只是 Sets 多了一層限制，就是集合中的值不能重複，這邊會使用到 sadd、smembers 來對 Sets 做儲存以及讀取。\n127.0.0.1:6379\u003e sadd set1 a b c d a a b b (integer) 4 127.0.0.1:6379\u003e smembers set1 1) \"c\" 2) \"a\" 3) \"b\" 4) \"d\" 可以看到他實際寫入的只有 4 筆資料。\nspop 從 set 集合中隨機跳出一定數量的資料。\n127.0.0.1:6379\u003e sadd set1 a a b b c d (integer) 4 127.0.0.1:6379\u003e spop set1 2 1) \"c\" 2) \"d\" 127.0.0.1:6379\u003e spop set1 2 1) \"a\" 2) \"b\" sismember 可以使用 sismember 來檢查輸入值是否為 Set 集合的成員。\n127.0.0.1:6379\u003e sadd set a b b c d (integer) 4 127.0.0.1:6379\u003e sismember set d (integer) 1 127.0.0.1:6379\u003e sismember set f (integer) 0 srem 可以使用 srem 來刪除 Set 集合中的成員。\n127.0.0.1:6379\u003e sadd set a b b c d (integer) 4 127.0.0.1:6379\u003e srem set a d (integer) 2 127.0.0.1:6379\u003e smembers set 1) \"b\" 2) \"c\" sdiff 會顯示第一個集合與其他集合不同的值。\n127.0.0.1:6379\u003e sadd set1 a b c d (integer) 4 127.0.0.1:6379\u003e sadd set2 a b (integer) 2 127.0.0.1:6379\u003e sadd set3 b (integer) 1 127.0.0.1:6379\u003e sdiff set1 set2 set3 1) \"c\" 2) \"d\" 集合型態適合場景 集合(Sets) 型態適合用於文章中的Tag標籤、或是要排除相同資料\nSets 適用場景圖\n有序集合 (Zset) 故名思義，有序集合就是有排序的集合，Sorted Sets 結構會 多一個數字值去為排序的權重 來決定先後順序，這邊會使用到 zadd 、 zrangebyscore 來對 Zset 做儲存以及讀取。\n127.0.0.1:6379\u003e zadd sortset 10 '10' (integer) 1 127.0.0.1:6379\u003e zadd sortset 6 '6' (integer) 1 127.0.0.1:6379\u003e zadd sortset 2 '2' (integer) 1 127.0.0.1:6379\u003e zadd sortset 99 '99' (integer) 1 127.0.0.1:6379\u003e zrangebyscore sortset 0 100 1) \"2\" 2) \"6\" 3) \"10\" 4) \"99\" 就可以看出在顯示的時候並不是依照寫入順序，而是依照我們所設定的權重去排序的。\nwithscores 如果想要顯示 value 與 score ，可以使用 withscores 來顯示。\n127.0.0.1:6379\u003e zadd money 2000 tom (integer) 1 127.0.0.1:6379\u003e zadd money 3500 peter (integer) 1 127.0.0.1:6379\u003e zadd money 5000 jack (integer) 1 127.0.0.1:6379\u003e zrange money 0 -1 withscores 1) \"tom\" 2) \"2000\" 3) \"peter\" 4) \"3500\" 5) \"jack\" 6) \"5000\" zremrangebyscore 移除有序集合中，指定分數內區間的所有成員。\n127.0.0.1:6379\u003e zadd money 2000 tom (integer) 1 127.0.0.1:6379\u003e zadd money 3500 peter (integer) 1 127.0.0.1:6379\u003e zadd money 5000 jack (integer) 1 127.0.0.1:6379\u003e zremrangebyscore money 1500 3500 (integer) 2 127.0.0.1:6379\u003e zrange money 0 -1 withscores 1) \"jack\" 2) \"5000\" zcard 可以使用 zcard 來計算集合中元素的數量。\n127.0.0.1:6379\u003e zadd sort 10 10 (integer) 1 127.0.0.1:6379\u003e zadd sort 3 3 (integer) 1 127.0.0.1:6379\u003e zadd sort 66 66 (integer) 1 127.0.0.1:6379\u003e zadd sort 33 33 (integer) 1 127.0.0.1:6379\u003e zcard sort (integer) 4 有序集合型態適合場景 有序集合(Zset) 型態適合用於需要有序排列的資料\nZset 適用場景圖\n發布訂閱 (PUB/SUB) Redis 發布訂閱 (pub/sub) 是一種消息通信模式，發送者 (pub) 發送消息，訂閱者 (sub) 接收消息。\nRedis 客戶端可以使用 subscribe 來訂閱任意數量的頻道。\n下面這張圖是頻道1，以及訂閱這個頻道的三個用戶端分別是客戶端2、客戶端7、客戶端5\nSubscribe 示意圖\n當我們有新消息通過 publish 指令發送給頻道1 ，這個消息就會被發送給有訂閱頻道1的三個客戶端。\nPublish 示意圖\n我們來模擬一下吧 ! 先開兩個 Terminal 來執行 redis-cli 一個當作發送(pub)，另一個當作接收(sub)。\n我們先用第一個 Terminal 訂閱一個頻道 channel_1\n127.0.0.1:6379\u003e subscribe channel_1 Reading messages... (press Ctrl-C to quit) 1) \"subscribe\" 2) \"channel_1\" 3) (integer) 1 開啟另一個 Terminal 發送訊息到 channel_1\n127.0.0.1:6379\u003e publish channel_1 \"Hello World~\" (integer) 1 127.0.0.1:6379\u003e publish channel_1 \"ian~\" (integer) 1 127.0.0.1:6379\u003e publish channel_1 \"test~\" (integer) 1 這時候再切換回來第一個 Terminal ，就可以看到他接收到我們傳送的訊息 127.0.0.1:6379\u003e subscribe channel_1 Reading messages... (press Ctrl-C to quit) 1) \"subscribe\" 2) \"channel_1\" 3) (integer) 1 1) \"message\" 2) \"channel_1\" 3) \"Hello World~\" 1) \"message\" 2) \"channel_1\" 3) \"ian~\" 1) \"message\" 2) \"channel_1\" 3) \"test~\" "},"title":"Redis 介紹"},"/blog/redis/redis-sentinel-docker-compose-haproxy/":{"data":{"":"前情提要：本篇是 Redis 哨兵模式 (Sentinel) 搭配 Docker-compose 實作 的後續文章，主要會優化原本的程式碼，並加上 HAProxy 來做負載平衡！","什麼是-haproxy-以及負載均衡-#什麼是 HAProxy 以及負載均衡 ?":"HAProxy 是一個使用 C 語言編寫的自由及開放原始碼軟體，其提供高可用性、負載均衡，以及基於 TCP 和 HTTP 的應用程式代理。\n負載平衡 (Load Balance)：\n現在很多網路服務都需要服務大量使用者，以前可以砸錢擴充機器硬體設施，但隨著網路服務的用量暴增，增加伺服器硬體設備已經無法解決問題。\n為了可以擴充服務，負載平衡成為主流的技術，這幾年雖然雲端與分散式儲存運算技術火紅，除非有特別的使用需求，不然在技術上負載均衡算是比較容易達成與掌握的技術。\n負載平衡除了分流能力之外，有另一個很大的好處就是可以提供 High Availability，也就是傳說中的 HA 架構，好讓你一台機器掛了其他伺服器可以繼續服務，降低斷線率。\nHAProxy 與 Reids Sentinel 示意圖 selcukusta/redis-sentinel-with-haproxy","參考資料#參考資料":"HAProxy 首頁：http://www.haproxy.org/\nHAproxy的安裝設定及範例：https://tw511.com/a/01/6959.html\nredis sentinel集群配置及haproxy配置：https://www.cnblogs.com/tzm7614/p/5691912.html\n富人用 L4 Switch，窮人用 Linux HAProxy！：https://blog.toright.com/posts/3967/%E5%AF%8C%E4%BA%BA%E7%94%A8-l4-switch%EF%BC%8C%E7%AA%AE%E4%BA%BA%E7%94%A8-linux-haproxy%EF%BC%81.html\nselcukusta/redis-sentinel-with-haproxy：https://github.com/selcukusta/redis-sentinel-with-haproxy\nHow to Enable Health Checks in HAProxy：https://www.haproxy.com/blog/how-to-enable-health-checks-in-haproxy/","實作#實作":"那接下來會使用 Docker-compose 實作 Redis 哨兵模式 + HAProxy，範例程式連結 點我 😘\n版本資訊\nmacOS：11.6 Docker：Docker version 20.10.12, build e91ed57 Nginx：1.20 PHP：7.4-fpm Redis：6.2.6 HAProxy：HAProxy version 2.5.5-384c5c5 2022/03/14 - https://haproxy.org/ 檔案結構 . ├── Docker-compose.yaml ├── docker-volume │ ├── haproxy │ │ └── haproxy.cfg │ ├── nginx │ │ └── nginx.conf │ ├── php │ │ ├── info.php │ │ ├── r.php │ │ └── rw.php │ └── redis │ ├── redis.conf │ ├── redis1 │ ├── redis2 │ └── redis3 ├── php │ └── Dockerfile ├── redis.sh └── sentinel ├── Docker-compose.yaml ├── sentinel1 │ └── sentinel.conf ├── sentinel2 │ └── sentinel.conf └── sentinel3 └── sentinel.conf 這是主要的結構，簡單說明一下：\nDocker-compose.yaml：會放置要產生的 Nginx、PHP、redis1、redis2、redis3 容器設定檔。 docker-volume/haproxy/haproxy.cfg：haproxy 的設定檔。 docker-volume/nginx/nginx.conf：nginx 的設定檔。 docker-volume/php/(r.php、rw.php)：測試用檔案。 docker-volume/redis/redis.conf：redis 的設定檔。 docker-volume/redis/(redis1、redis2、redis3)：放 redis 的資料。 php/Dokcerfile：因為在 php 要使用 redis 需要多安裝一些設定，所以用 Dockerfile 另外寫 PHP 的映像檔。 redis.sh：是我另外多寫的腳本，可以查看相對應的角色。 sentinel/Docker-compose.yaml：會放置要產生的 haproxy、sentinel1、sentinel2、sentinel3 的容器設定檔。 sentinel/(sentinel1、sentinel2、sentinel3)/.conf：哨兵的設定檔。 那我們就依照安裝的設定開始說明：\nDocker-compose.yaml version: '3.8' services: nginx: image: nginx:1.20 container_name: nginx networks: HAProxy_Redis: ports: - \"8888:80\" volumes: - ./docker-volume/nginx/:/etc/nginx/conf.d/ - ./log/nginx/:/var/log/nginx/ environment: - TZ=Asia/Taipei php: build: ./php container_name: php networks: HAProxy_Redis: expose: - 9000 volumes: - ./docker-volume/php/:/var/www/html redis1: image: redis container_name: redis1 command: redis-server /usr/local/etc/redis/redis.conf --appendonly yes volumes: - ./docker-volume/redis/redis1/:/data - ./docker-volume/redis/:/usr/local/etc/redis/ - ./log/redis1:/var/log/redis/ environment: - TZ=Asia/Taipei networks: HAProxy_Redis: ipv4_address: 172.20.0.11 ports: - 6379:6379 redis2: image: redis container_name: redis2 command: redis-server /usr/local/etc/redis/redis.conf --slaveof redis1 6379 --appendonly yes volumes: - ./docker-volume/redis/redis2/:/data - ./docker-volume/redis/:/usr/local/etc/redis/ - ./log/redis2:/var/log/redis/ environment: - TZ=Asia/Taipei networks: HAProxy_Redis: ipv4_address: 172.20.0.12 ports: - 6380:6379 depends_on: - redis1 redis3: image: redis container_name: redis3 command: redis-server /usr/local/etc/redis/redis.conf --slaveof redis1 6379 --appendonly yes volumes: - ./docker-volume/redis/redis3/:/data - ./docker-volume/redis/:/usr/local/etc/redis/ - ./log/redis3:/var/log/redis/ environment: - TZ=Asia/Taipei networks: HAProxy_Redis: ipv4_address: 172.20.0.13 ports: - 6381:6379 depends_on: - redis1 - redis2 networks: HAProxy_Redis: driver: bridge name: HAProxy_Redis ipam: config: - subnet: 172.20.0.0/16 gateway: 172.20.0.1 一樣詳細的 Docker 設定說明，可以參考 Docker 介紹 內有詳細設定說明。其他比較特別的地方是：\n幫每一個容器都設定好 IP ，方便後續測試使用。 有掛載 log 目錄，可以將我們設定好的 log 做收集。 呈上，有加入 environment 時區，這樣在看 log 的時候才知道正確時間。 docker-volume/haproxy/haproxy.cfg global log stdout format raw local0 info defaults mode http # 默認模式 { tcp | http | health }，tcp 是4層，http 是7層，health 只會返回 OK timeout client 10s # 客戶端超時 timeout connect 5s # 連接超時 timeout server 10s # 伺服器超時 timeout http-request 10s log global listen admin_status bind 0.0.0.0:8404 mode http stats enable stats uri /redis stats realm Global\\ statistics stats refresh 1s listen rw-redis # 判斷是否為 master 並可讀可寫 bind 0.0.0.0:16379 mode tcp balance roundrobin option tcp-check # redis 健康检查，確保是 master tcp-check connect tcp-check send PING\\r\\n tcp-check expect string +PONG tcp-check send info\\ replication\\r\\n tcp-check expect string role:master tcp-check send QUIT\\r\\n tcp-check expect string +OK server redis1 redis1:6379 check inter 2000 server redis2 redis2:6379 check inter 2000 server redis3 redis3:6379 check inter 2000 listen r-redis # 判斷是否為 master、slave 並可讀 bind 0.0.0.0:16380 mode tcp balance roundrobin server redis1 redis1:6379 check inter 2000 server redis2 redis2:6379 check inter 2000 server redis3 redis3:6379 check inter 2000 這裡是本章的重點，我們會在這邊設定好 haproxy，詳細說明請看：\ndefaults： 一些初始值，像是 mode 我們預設 http，它主要有三種模式 { tcp | http | health }，tcp 是4層，http 是7層，health 只會返回 OK，以及客戶端、連接、伺服器、http 請求超時時間設定。\nlisten admin_status：\nbind：我們要開啟 HAProxy 監控平台的 port。 mode：模式，我們使用 http 模式。 stats ：是否要啟動平台。 stats uri：平台網址，我們使用 redis。 stats refresh：平台自動更新時間，我們設定 1 秒。 listen rw-redis：\nbind ：rw 使用 16379 Port 來當輸出。 balance：使用負載平衡。 option tcp-check # redis 健康检查，確保是 master tcp-check connect tcp-check send PING\\r\\n tcp-check expect string +PONG tcp-check send info\\ replication\\r\\n tcp-check expect string role:master tcp-check send QUIT\\r\\n tcp-check expect string +OK 上面這些是用來判斷角色是不是 master。\n最後放我們 3 個 redis 服務：\nserver redis1 redis1:6379 check inter 2000 server redis2 redis2:6379 check inter 2000 server redis3 redis3:6379 check inter 2000 check：開啟健康偵測。 inter：參數更改檢查間隔，預設是 2 秒。 docker-volume/nginx/nginx.conf server { listen 80; server_name default_server; return 404; } server { listen 80; server_name test.com; index index.php index.html; error_log /var/log/nginx/error.log warn; access_log /var/log/nginx/access.log; root /var/www/html; location / { try_files $uri $uri/ /index.php?$query_string; } location ~ \\.php$ { fastcgi_pass php:9000; fastcgi_index index.php; include fastcgi_params; fastcgi_param SCRIPT_FILENAME /var/www/html$fastcgi_script_name; } } Nginx 設定檔案。\ndocker-volume/php/rw.php \u003c?php $redis = new Redis(); $redis-\u003econnect('172.20.0.20', 16379); $r = $redis-\u003einfo(); echo $r['run_id'] . '\u003cbr\u003e' . $r['role'] . '\u003cbr\u003e\u003cbr\u003e'; echo '\u003cpre\u003e', print_r($r), '\u003c/pre\u003e'; 跟 r.php 比較不同的是，使用 16379 Port，我們在 haproxy.cfg 有設定 rw-redis，來判斷是不是 master 並且是可讀可寫。\ndocker-volume/php/r.php \u003c?php $redis = new Redis(); $redis-\u003econnect('172.20.0.20', 16380); $r = $redis-\u003einfo(); echo $r['run_id'] . '\u003cbr\u003e' . $r['role'] . '\u003cbr\u003e\u003cbr\u003e'; echo '\u003cpre\u003e', print_r($r), '\u003c/pre\u003e'; 使用 16380 Port，在 haproxy.cfg 有設定 r-redis，來顯示是不是 master、slave 且可讀。\nphp/Dockerfile FROM php:7.4-fpm RUN pecl install -o -f redis \\ \u0026\u0026 rm -rf /tmp/pear \\ \u0026\u0026 echo \"extension=redis.so\" \u003e /usr/local/etc/php/conf.d/redis.ini \\ \u0026\u0026 echo \"session.save_handler = redis\" \u003e\u003e /usr/local/etc/php/conf.d/redis.ini \\ \u0026\u0026 echo \"session.save_path = tcp://redis:6379\" \u003e\u003e /usr/local/etc/php/conf.d/redis.ini 因為 PHP 要使用 Redis，會需要安裝一些套件，所以我們將 PHP 分開來，使用 Dockerfile 來設定映像檔。\nredis.sh #!/bin/bash green=\"\\033[1;32m\";white=\"\\033[1;0m\";red=\"\\033[1;31m\"; echo \"redis1 IPAddress:\" redis1_ip=`docker inspect redis1 | grep \"IPv4\" | egrep -o \"[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\"` echo $redis1_ip; echo \"------------------------------\" echo \"redis2 IPAddress:\" redis2_ip=`docker inspect redis2 | grep \"IPv4\" | egrep -o \"[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\"` echo $redis2_ip; echo \"------------------------------\" echo \"redis3 IPAddress:\" redis3_ip=`docker inspect redis3 | grep \"IPv4\" | egrep -o \"[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\"` echo $redis3_ip; echo \"------------------------------\" echo \"haproxy IPAddress:\" haproxy_ip=`docker inspect haproxy | grep \"IPv4\" | egrep -o \"[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\"` echo $haproxy_ip; echo \"------------------------------\" echo \"redis1:\" docker exec -it redis1 redis-cli info Replication | grep role echo \"redis2:\" docker exec -it redis2 redis-cli info Replication | grep role echo \"redis3:\" docker exec -it redis3 redis-cli info Replication | grep role 這個是我自己所寫的腳本，可以詳細知道目前服務的角色轉移狀況。\nsentinel/Docker-compose.yaml version: '3.8' services: haproxy: image: haproxy container_name: haproxy volumes: - ../docker-volume/haproxy/:/usr/local/etc/haproxy environment: - TZ=Asia/Taipei networks: HAProxy_Redis: ipv4_address: 172.20.0.20 ports: - 16379:6379 - 8404:8404 sentinel1: image: redis container_name: redis-sentinel-1 networks: HAProxy_Redis: ports: - 26379:26379 command: redis-server /usr/local/etc/redis/sentinel.conf --sentinel volumes: - ./sentinel1:/usr/local/etc/redis/ - ../log/sentinel1:/var/log/redis/ environment: - TZ=Asia/Taipei sentinel2: image: redis container_name: redis-sentinel-2 networks: HAProxy_Redis: ports: - 26380:26379 command: redis-server /usr/local/etc/redis/sentinel.conf --sentinel volumes: - ./sentinel2:/usr/local/etc/redis/ - ../log/sentinel2:/var/log/redis/ environment: - TZ=Asia/Taipei sentinel3: image: redis container_name: redis-sentinel-3 networks: HAProxy_Redis: ports: - 26381:26379 command: redis-server /usr/local/etc/redis/sentinel.conf --sentinel volumes: - ./sentinel3:/usr/local/etc/redis/ - ../log/sentinel3:/var/log/redis/ environment: - TZ=Asia/Taipei networks: HAProxy_Redis: external: name: HAProxy_Redis sentinel/sentine.conf 因為 sentine 內容都基本上相同，所以舉一個來說明：\nport 26379 logfile \"/var/log/redis/redis-sentinel.log\" protected-mode no #設定要監控的 Master，最後的 2 代表判定客觀下線所需的哨兵數 sentinel monitor mymaster 172.20.0.11 6379 2 #哨兵 Ping 不到 Master 超過此毫秒數會認定主觀下線 sentinel down-after-milliseconds mymaster 5000 要設定指定的 Port sentine1 是 26379、sentine2 是 26380、sentine3 是 26381。接下來要設定要監控的 Master，最後的數字代表我們前面有提到客觀下線需要達到的哨兵數。以及主觀下線的時間跟 failover 超過的時間。","測試#測試":"我們先用 docker-compose up 來啟動 Docker-compose.yaml，接著再啟動 sentinel/Docker-compose.yaml：\n啟動 Docker-compose.yaml\n啟動 sentinel/Docker-compose.yaml\n由於為了測試，有先將所有容器設定好 IP，就不會像上一篇文章一樣要去抓 IP ，才能啟動 Sentinel。\n這時候可以使用瀏覽器搜尋以下網址：\ntest.com:8404/redis：HAProxy 監看平台(只取片段)。 HAProxy 監控平台\ntest.com:8888/rw.php：只會顯示 master，並且可讀可寫。 master (redis1)\ntest.com:8888/r.php：會顯示 master、slave，且可讀。 master (redis1)\nslave (redis2)\nslave (redis3)\n接下來可以執行：\n$ sh redis.sh redis1 IPAddress: 172.20.0.11 ------------------------------ redis2 IPAddress: 172.20.0.12 ------------------------------ redis3 IPAddress: 172.20.0.13 ------------------------------ haproxy IPAddress: 172.20.0.20 ------------------------------ redis1: role:master redis2: role:slave redis3: role:slave 就會顯示三個 redis 的 IP 以及 haproxy 的 IP，這些都是已經寫在 Docker-compose.yaml 檔案內的，如果忘記的可以再往前看 ↑\n接下來我們可以先一直 F5 test.com:8888/r.php，來模擬大量的讀取請求，如果發現網站內容一直在更換，就代表我們成功透過 HAProxy 做到負載平衡了，可以將讀取的需求分給三個服務做處理！那因為 test.com:8888/rw.php 他只會抓 master，所以刷新還是同一個 master。\n還記得我們上次用 Redis 的哨兵模式嗎？那我們用它來搭配 Haproxy 會有什麼結果呢？\n我們先使用 docker stop 來模擬服務中斷：\n$ docker stop redis1 redis1 可以看到 test.com:8404/redis 原本綠色的 redis1 開始變成黃色，最後變成紅色：\n模擬中斷服務 HAProxy 監控平台\n最後可以看到 Redis Sentinel 作動，將 master 轉移到 redis3：\n模擬中斷服務 HAProxy 監控平台\n這時候我們再去看 test.com:8888/rw.php ，就會發現與剛剛的 master 不太一樣囉，因為已經變成 redis2 了！\nmaster (redis2)\n代表我們 HAProxy 也有成功將 master 給顯示出來！\n我們再去看 test.com:8888/r.php ，就可以發現剩下 redis2 以及 redis 3 了，因為 redis1 被我們給暫停服務了，而且 redis2 變成 master！\nmaster (redis2)\nslave (redis3)"},"title":"用 HAProxy 對 Redis 做負載平衡 (Redis Sentinel、Docker-compose)"},"/blog/redis/redis-sentinel-docker-compose/":{"data":{"":"","什麼是-redis-哨兵模式-sentinel-#什麼是 Redis 哨兵模式 (Sentinel) ?":"有關 Redis 之前有寫一篇 Redis 的介紹文，有興趣可以去看看！\nRedis 提供非常實用的功能來讓我們實現多機的 in-memory 資料庫：\n主從複製模式 (Master-Slave Replication) 哨兵模式 (Sentinel) 叢集模式 (Cluster) 我們這邊主要介紹哨兵模式 (Sentinel)，但主要也是由主從複製模式 (Master-Slave Replication) 修改而來：\n哨兵模式就是用來監視 Redis 系統，哨兵會監控 Master 是否正常運作。如果遇到 Master 出現故障或是離線時，哨兵之間會開始判斷，直到我們所設定需達到的判斷數量後，哨兵會將其所屬的 Slave 變成 Master，並再將其他的 Slave 指向新的 Master。\n哨兵模式 (Sentinel)\n監控 哨兵會和要監控的 Master 建立兩條連接，Cmd 和 Pub/Sub：\nCmd 是哨兵用來定期向 Master 發送 Info 命令以取得 Master 的訊息，訊息中會包含 Master 有哪些 Slave。當與 Master 獲得 Slave 訊息後，哨兵也會和 Slave 建立連接。 哨兵也會定期透過 Cmd 向 Master、Slave 和其他哨兵發送 Ping 指令來檢查是否存在，確認節點狀態等。 Pub/Sub 讓哨兵可以透過訂閱 Master 和 Slave 的 __Sentinel__:hello 這個頻道來和其他哨兵定期的進行資訊交換。 主觀下線 (SDOWN) 主觀下線是指單個哨兵認為 Master 已經停止服務了，有可能是網路不通或是接收不到訂閱等，而哨兵的判斷是依據傳送 Ping 指令之後一定時間內是否收到回覆或是錯誤訊息，如果有哨兵就會主觀認為這個 Master 已經下線停止服務了。\n客觀下線 (ODOWN) 客觀下線是指由多個哨兵對同一個 Master 各自進行主觀下線的判斷後，再綜合所有哨兵的判斷。若是認為主觀下線的哨兵到達我們所配置的數量後，即為客觀下線。\n故障轉移 (Failover) 當 Master 已經被標記為客觀下線時，起初發現 Master 下線的哨兵會發起一個選舉 (採用的是 Raft 演算法)，並要求其他哨兵選他做為領頭哨兵，領頭哨兵會負責進行故障的恢復。當選的標準是要有超過一半的哨兵同意，所以哨兵的數量建議設定成奇數個。\n此時若有多個哨兵同時參選領頭哨兵，則有可能會發生一輪後沒有產生勝選者，則所有的哨兵會再等隨機一個時間再次發起參選的請求，進行下一輪的選舉，一直到選出領頭為止。所以若哨兵數量為偶數就很有可能一直無法選出領頭哨兵。\n選出領頭哨兵後，領頭哨兵會開始從下線的 Master 所屬 Slave 中跳選出一個來變成新的 Master，挑選的依據如下：\n所有在線的 Slave 擁有最高優先權的，優先權可以透過 slave-priority 來做設定。 如果有多個同為最高優先權的 Slave，則會選擇複製最完整的。 若還是有多個 Slave 皆符合上述條件，則選擇 id 最小的。 接著領頭哨兵會將舊的 Master 更新成新的 Master 的 Slave ，讓其恢復服務後以 Slave 的身份繼續運作。","參考資料#參考資料":"Redis (六) - 主從複製、哨兵與叢集模式：https://blog.tienyulin.com/redis-master-slave-replication-sentinel-cluster/","測試#測試":"我們先用 docker-compose up 來啟動 Docker-compose.yaml ，接著下 sh redis.sh 指令來查看各服務的 IP 位置以及目前角色：\n啟動 Docker-compose.yaml\n使用 redis.sh 檢查目前 IP 及角色\n接下來先修改 docker-volume/php/index.php\n$sentinel = array( array( 'host' =\u003e '192.168.208.4', 'port' =\u003e 6379, 'role' =\u003e 'master', ), array( 'host' =\u003e '192.168.208.5', 'port' =\u003e 6379, 'role' =\u003e 'slave1', ), array( 'host' =\u003e '192.168.208.6', 'port' =\u003e 6379, 'role' =\u003e 'slave2', ), ); 將各自的 IP 帶入測試的網站中。\n可以瀏覽 test.com:8888 是否有正常抓到 redis 的 master\ntest.com:8888 測試網站\n再來修改 sentinel 內的 sentinel(1、2、3).conf 檔案\nport 26379 #設定要監控的 Master，最後的 2 代表判定客觀下線所需的哨兵數 sentinel monitor mymaster 192.168.208.4 6379 2 #哨兵 Ping 不到 Master 超過此毫秒數會認定主觀下線 sentinel down-after-milliseconds mymaster 5000 #failover 超過次毫秒數即代表 failover 失敗 sentinel failover-timeout mymaster 180000 將要監控的 Master IP 帶入 sentinel monitor mymaster。\n再啟動 sentinel/Docker-compose.yaml\n啟動 Docker-compose.yaml\n接下來我們來模擬假設 Master 服務中斷後，sentinel 會發生什麼事情：\ndocker stop redis-master 下完指令後，再使用 sh redis.sh 來看看目前的 role 狀態：\n使用 redis.sh 檢查目前 IP 及角色\n發現已經抓不到 master IP 以及他的角色。\n等待一下子後，重新下 sh redis.sh 來看目前的 role 狀態：\n使用 redis.sh 檢查目前 IP 及角色\n就會發現已經將 master 轉移到原 slave1。\n那我們來看一下 sentinel 在背後做了哪些事情：\n判斷是否主觀下線及客觀下線，並發起投票\n可以看到三個哨兵都認為 master 為 主觀下線 (sdown)，這時 sentinel-2 就認定為 客觀下線 (odown)，並發起投票要求成為領頭哨兵。\n進行透票，確認誰當選\n我們可以看到 Sentinel2 和 Sentinel3 都投給 Sentinel2，所以最後 Sentinel2 當選。\n領頭哨兵選定新 master\n接著 sentinel2 選出 redis-slave1 (192.168.208.5:6379) 作為 Master ，並且使用 failover-state-send-slaveof-noone 來將 redis-slave1 解除 Slave 狀態變成獨立的 master，隨後將 redis-slave1 升成 master。\n設定 master 並修改原 master 變成 slave\n設定完新的 Master 後，Sentinel2 讓原本的 Master 轉為 Slave，並且讓 redis-slave2(192.168.208.6:6379) 指向新的 Master。最後 Sentinel1 和 Sentinel3 開始從 Sentinel2 取得設定然後更新自己的設定，至此整個故障轉移就完成了。\n最後我們來看一下我們用 PHP 連線的測試：\n連線 master\n就會發現，已經 slave1 變成現在的 master。\n那我們最後把原本的 master 恢復，看看會發生什麼事情：\n連線 master\n會發現因為該啟動 master，所以他還認為他是 master，但過一下下，在查看就正常顯示 slave1 為 master，舊的 master 就變成 slave。\nRedis 常見錯誤問題 在學習的時候，有發現啟動 sentinel 的時候，會跳出 WARNING: Sentinel was not able to save the new configuration on disk!!!: Permission denied 錯誤訊息，後來再去翻 redis github 的 issus 才發現作者也有發現這個問題，且已經修復了，主要是權限的問題，以及要掛載目錄而非 conf 檔案。\nredis WARNING 錯誤訊息 github issus","用-docker-實作-redis-哨兵模式#用 Docker 實作 Redis 哨兵模式":"那接下來會使用 Docker 來實作 Redis 的哨兵模式，範例程式連結 點我 😘\n版本資訊\nmacOS：11.6 Docker：Docker version 20.10.12, build e91ed57 Nginx：1.20 PHP：7.4-fpm Redis：latest 檔案結構 . ├── Docker-compose.yaml ├── docker-volume │ ├── log │ │ └── nginx │ │ ├── access.log │ │ └── error.log │ ├── nginx │ │ └── nginx.conf │ ├── php │ │ └── index.php │ ├── redis-master │ ├── redis-slave1 │ └── redis-slave2 ├── redis.sh ├── php │ └── Dockerfile └── sentinel ├── Docker-compose.yaml ├── sentinel1.conf ├── sentinel2.conf └── sentinel3.conf 這是主要的結構，簡單說明一下：\nDocker-compose.yaml：會放置要產生的 Nginx、PHP、redis-master、redis-slave1、redis-slave2 容器設定檔。 docker-volume：是我們要掛載進去到容器內的檔案，包含像是 nginx.conf 或是 log/nginx 以及 redis 記憶體儲存內容。 redis.sh：是我另外多寫的腳本，可以在最後方面我們測試 Redis sentinel 是否成功。 php/Dokcerfile：因為在 php 要使用 redis 需要多安裝一些設定，所以用 Dockerfile 另外寫 PHP 的映像檔。 sentinel/Docker-compose.yaml：會放置要產生的 sentinel1、sentinel2、sentinel3 的容器設定檔。 sentinel(1、2、3).conf：哨兵的設定檔。 那我們就依照安裝的設定開始說明：\nDocker-compose.yaml version: '3.8' services: nginx: image: nginx:1.20 container_name: nginx ports: - \"8888:80\" volumes: - ./docker-volume/nginx/:/etc/nginx/conf.d/ - ./docker-volume/log/nginx/:/var/log/nginx/ php: build: ./php container_name: php expose: - 9000 volumes: - ./docker-volume/php/:/var/www/html redis-master: image: redis container_name: redis-master volumes: - ./docker-volume/redis-master/:/data ports: - 6379:6379 command: redis-server --appendonly yes redis-slave1: image: redis container_name: redis-slave1 volumes: - ./docker-volume/redis-slave1/:/data ports: - 6380:6379 command: redis-server --slaveof redis-master 6379 --appendonly yes depends_on: - redis-master redis-slave2: image: redis container_name: redis-slave2 volumes: - ./docker-volume/redis-slave2/:/data ports: - 6381:6379 command: redis-server --slaveof redis-master 6379 --appendonly yes depends_on: - redis-master - redis-slave1 詳細的 Docker 設定說明，可以參考 Docker 介紹 內有詳細設定說明。比較特別的地方是：\nredis-(master、slave1、slave2)\nvolumes：將 redis 的資料掛載到 docker-volume/redis-(master、slave1、slave2)。 command：使用 redis-server 啟動，並且將該服務器轉變成指定服務器的從屬服務器 (slave server)。(如果想要保存 redis 的資料，要記得在 後面加上 –appendonly yes) docker-volume/nginx/nginx.conf server { listen 80; server_name default_server; return 404; } server { listen 80; server_name test.com; index index.php index.html; error_log /var/log/nginx/error.log warn; access_log /var/log/nginx/access.log; root /var/www/html; location / { try_files $uri $uri/ /index.php?$query_string; } location ~ \\.php$ { fastcgi_pass php:9000; fastcgi_index index.php; include fastcgi_params; fastcgi_param SCRIPT_FILENAME /var/www/html$fastcgi_script_name; } } Nginx 設定檔案。\nphp/Dockerfile FROM php:7.4-fpm RUN pecl install -o -f redis \\ \u0026\u0026 rm -rf /tmp/pear \\ \u0026\u0026 echo \"extension=redis.so\" \u003e /usr/local/etc/php/conf.d/redis.ini \\ \u0026\u0026 echo \"session.save_handler = redis\" \u003e\u003e /usr/local/etc/php/conf.d/redis.ini \\ \u0026\u0026 echo \"session.save_path = tcp://redis:6379\" \u003e\u003e /usr/local/etc/php/conf.d/redis.ini 因為 PHP 要使用 Redis，會需要安裝一些套件，所以我們將 PHP 分開來，使用 Dockerfile 來設定映像檔。\nsentinel/sentine.conf 因為 sentine 內容都基本上相同，所以舉一個來說明：\nport 26379 #設定要監控的 Master，最後的 2 代表判定客觀下線所需的哨兵數 sentinel monitor mymaster 192.168.176.4 6379 2 #哨兵 Ping 不到 Master 超過此毫秒數會認定主觀下線 sentinel down-after-milliseconds mymaster 5000 #failover 超過次毫秒數即代表 failover 失敗 sentinel failover-timeout mymaster 180000 要設定指定的 Port sentine1 是 26379、sentine2 是 26380、sentine3 是 26381。接下來要設定要監控的 Master，最後的數字代表我們前面有提到客觀下線需要達到的哨兵數。以及主觀下線的時間跟 failover 超過的時間。\nsentinel/Docker-compose.yaml version: '3.8' services: sentinel1: image: redis container_name: redis-sentinel-1 ports: - 26379:26379 command: redis-sentinel /usr/local/etc/redis/sentinel.conf volumes: - ./sentinel1.conf:/usr/local/etc/redis/sentinel.conf sentinel2: image: redis container_name: redis-sentinel-2 ports: - 26380:26379 command: redis-sentinel /usr/local/etc/redis/sentinel.conf volumes: - ./sentinel2.conf:/usr/local/etc/redis/sentinel.conf sentinel3: image: redis container_name: redis-sentinel-3 ports: - 26381:26379 command: redis-sentinel /usr/local/etc/redis/sentinel.conf volumes: - ./sentinel3.conf:/usr/local/etc/redis/sentinel.conf networks: default: external: name: redis_default docker-volume/php/index.php \u003c?php $redis = new Redis(); $sentinel = array( array( 'host' =\u003e '192.168.176.4', 'port' =\u003e 6379, 'role' =\u003e 'master', ), array( 'host' =\u003e '192.168.176.5', 'port' =\u003e 6379, 'role' =\u003e 'slave1', ), array( 'host' =\u003e '192.168.176.6', 'port' =\u003e 6379, 'role' =\u003e 'slave2', ), ); foreach ($sentinel as $value) { try { $redis-\u003econnect($value['host'], $value['port']); $redis-\u003eset('foo', 'bar'); echo \"連線成功 \" . $value['host'] . \"\u003cbr\u003e目前 master：\" . $value['role'] . \"\u003cbr\u003e\"; } catch (\\Exception $e) { continue; } } 為了要讓 PHP 可以知道目前的 Master 是哪一個服務器，所以寫了一個 try…catch 來做判斷，並且把3個服務內容都放到陣列中，後續再測試中會再說明。\nredis.sh #!/bin/bash green=\"\\033[1;32m\";white=\"\\033[1;0m\";red=\"\\033[1;31m\"; echo \"master IPAddress:\" master_ip=`docker inspect redis-master | grep \"IP\" | egrep -o \"[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\"` echo $master_ip; echo \"------------------------------\" echo \"slave1 IPAddress:\" slave1_ip=`docker inspect redis-slave1 | grep \"IP\" | egrep -o \"[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\"` echo $slave1_ip; echo \"------------------------------\" echo \"slave2 IPAddress:\" slave2_ip=`docker inspect redis-slave2 | grep \"IP\" | egrep -o \"[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\"` echo $slave2_ip; echo \"------------------------------\" echo \"master:\" docker exec -it redis-master redis-cli info Replication | grep role echo \"slave1:\" docker exec -it redis-slave1 redis-cli info Replication | grep role echo \"slave2:\" docker exec -it redis-slave2 redis-cli info Replication | grep role 這個是我自己所寫的腳本，再等等測試時，可以詳細知道目前服務的角色轉移狀況。"},"title":"Redis 哨兵模式 (Sentinel) 搭配 Docker-compose 實作"},"/blog/terraform/":{"data":{"":"此分類包含 Terraform 相關的文章。\n如何導入 Terragrunt，Terragrunt 好處是什麼？ 如何將 Terraform 改寫成 module ? Terraform 如何多人共同開發 (將 tfstate 存在後端) 使用 Terraform 建立 Google Kubernetes Engine 使用 Terraform 建立 Google Compute Engine 什麼是 IaC ? Terraform 又是什麼？ "},"title":"Terraform"},"/blog/terraform/terraform-gce/":{"data":{"":"嗨嗨大家好，距離上一篇筆記又隔了 3 個月，最近公司有專案在忙，沒時間把上次提到的 Terraform 應用筆記寫完，現在他來拉～～～ 😂 我們這次的主題是使用 Terraform 來建立 Google Compute Engine 的機器，想知道要怎麼用一段程式碼就可以建立、修改、刪除 Google Compute Engine 的機器一定要來看這一篇～我們開始囉 🧑‍💻","修改-google-compute-engine#修改 Google Compute Engine":"當我們發現我們建立的 Google Compute Engine 參數有錯，想要修改時，我們只需要修改程式碼部分，並重新下一次 terraform apply 來修改 Google Compute Engine，就會看到以下畫面 (有些設定檔是不能修改的，若修改他會重新創建一個新的機器，像是 name 之類的，使用時要小心一點 😉)\n我們拿剛剛提到的 nat_ip，我們先把它註解掉，再下 terraform apply 看看機器有什麼變化～\nterraform changed\n可以看到他會提示說，他會改變 network_interface，也移除 access_config 的設定，執行後的 Resources 也會從剛剛的 added 變成 changed，我們看一下 GCP 有沒有改變：\nterraform changed\n可以看到原本的外部 IP 位置被改掉了～ 最後提醒：如果有使用 terraform 來修改資源設定，不能動到特定的項目，不然他的流程是先把原本的給刪掉，再重新建立一個新的，原本的機器沒有備份，東西就會不見歐～","刪除-google-compute-engine#刪除 Google Compute Engine":"最後假如我們要刪除 Google Compute Engine，也可以使用 terraform 的指令來刪除，我們順便來測試一下上面有設定的 deletion_protection 刪除保護機制是不是正常～\n目前 deletion_protection 還是 true，我們直接下 terraform destroy，看看是否可以刪除 Google Compute Engine\nterraform destroy\n可以看到他會提醒你說 Deletion Protection is enabled，必須先把他改成 false terraform apply 後才可以刪除～\n改成 false 在下 terraform apply\n現在 deletion_protection 是 false，我們就可以下 terraform destroy 來刪除 Google Compute Engine 🔪\nterraform destroy\nGoogle Compute Engine 刪除中…\n以上就是簡單的使用 Terraform 建立 Google Compute Engine 介紹囉～歡迎大家留言指教，明天的文章是介紹如何使用 Terraform 建立 GKE 💕","參考資料#參考資料":"registry.terraform.io/providers (google_compute_instance)：https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/compute_instance","建立-google-compute-engine#建立 Google Compute Engine":"當我們寫好 Google Compute Engine tf 檔案後，我們接著把他建立，建立前要先使用 terraform init 來做初始化\nterraform init\n接著可以先使用 terraform plan 來查看我們的設定是否是我們想要的，或是直接用 terraform apply 來建立 Google Compute Engine\nterraform apply\n可以看到成功建立我們的 test Google Compute Engine（也可以看到因為我們有開 nat_ip 所以有外部 IP）\nGCP Google Compute Engine","撰寫-google-compute-engine-tf-檔案#撰寫 Google Compute Engine tf 檔案":"相信大家有先看完上一篇 什麼是 IaC ? Terraform 又是什麼？ 才來看這一篇的對吧 😎，對於 Terraform 的程式架構及指令，我們這邊就不多做介紹，我們直接來看程式要怎麼寫～(程式碼主要是參考官方文件，加上一些其他的設定來做介紹，程式碼也會同步到 Github ，需要的也可以去 Clone 來使用歐！ Github 程式碼連結 )\n小提醒：由於程式碼較長，我將他拆開來說明 💖\n選擇供應者以及對應的專案 provider \"google\" { project = \"gcp-20210526-001\" } 由於我們要建立的 Google Compute Engine 是由 Google 所提供的 api 來建立的，所以一開始要先設定好提供者的名稱 google 以及我們要在哪一個 GCP 的專案 ID\nresource 設定 接下來的設定都會放在以下的 google_compute_instance resource 內，為了方便介紹，就不會標明 google_compute_instance，詳細完整程式碼請參考 GitLab Github 程式碼連結\nresource \"google_compute_instance\" \"default\" { } 基本設定 name = \"test\" description = \"我是 test 機器\" machine_type = \"n2-standard-8\" zone = \"asia-east1-b\" tags = [\"test\"] labels = { env = \"test\" } deletion_protection = \"true\" name：GCE 要求資源的唯一名稱。如果有更改此項會直接強制創建的新資源 (必填) description：對此資源的簡單說明 (選填) machine_type：要創建的機器類型 (必填) zone：創建機器的所在區域，若沒有填寫，則會使用提供者的區域 (選填) tags：附加到實體的網路標籤列表 (選填) labels：一組分配給 disk 的 key/value 標籤 (選填) deletion_protection：刪除保護，預設是 false，當我們使用 terraform destroy 刪除 GCE 時，必須先改成 false，才可以刪除，否則會無法刪除且 Terraform 運行也會失敗，算是一個保護機制，後面再刪除 Google Compute Engine 時會測試畫面 (選填) 啟動 disk 設定 boot_disk { initialize_params { image = \"debian-cloud/debian-10-buster-v20210512\" type = \"pd-balanced\" size = \"50\" } } image：初始化此 disk 的 image (選填) type：GCE disk 類型 (選填) size：image 大小，已 GB 為單位，如果未指定，將會繼承其初始化 disk 的 image 大小 (選填) 網路設定 network_interface { network = \"projects/rd-gateway/global/networks/rd-common\" subnetwork = \"projects/rd-gateway/regions/asia-east1/subnetworks/rd-common-asia-east1-pid-cicd\" access_config { nat_ip = \"\" } } network：設定附加到的網路名稱或是 self_link (選填) subnetwork：設定附加到的子網路名稱或是 self_link (選填) nat_ip：如果想要有外網的 ip，必須加上此參數，才會產生一組外網 ip (選填) 權限設定 service_account { email = \"676962704505-compute@developer.gserviceaccount.com\" scopes = [\"storage-rw\", \"logging-write\", \"monitoring-write\", \"service-control\", \"service-management\", \"trace\"] } email：服務帳戶電子郵件地址。如果未提供，則使用預設的 Google Compute Engine 服務帳戶 (選填) scopes：服務範圍列表，可以點我查看範圍的完整列表 (必填) 以上只是我在建立 Google Compute Engine 最簡單的設定，當然還有很多其他的設定，可以參考 registry.terraform.io/providers (google_compute_instance) 裡面有更多的 resource 設定，有需要的就自己來看看吧 🧐"},"title":"使用 Terraform 建立 Google Compute Engine"},"/blog/terraform/terraform-gke/":{"data":{"":"我們接續昨天的建立 Google Kubernetes Engine 文章，今天要來介紹的是如何用 Terraform 建立 Google Kubernetes Engine，由於使用 terraform 去建立、修改、刪除的指令大家應該都清楚了，那我今天的文章就不在多說，直接來介紹一下要怎麼撰寫 Google Kubernetes Engine tf 檔案 😏","參考資料#參考資料":"registry.terraform.io/providers (google_container_cluster)：https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/container_cluster\nregistry.terraform.io/providers (google_container_node_pool)：https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/container_node_pool","撰寫-google-kubernetes-engine-tf-檔案#撰寫 Google Kubernetes Engine tf 檔案":"程式碼會同步到 Github ，需要的也可以去 Clone 來使用歐！ Github 程式碼連結，小提醒：由於程式碼較長，我將他拆開來說明 💖\n由於等等程式碼較長，所以我在前面這邊先做說明，GKE 的結構是 叢集(cluster) \u003e 節點池(node_pool) \u003e 節點(node)，本次的介紹範例，會有一個叢集裡面有一個節點池，節點池裡面有 6 個節點數量，範例裡面會加上我比較常用到的一些設定，以及一些文件裡面的用法，大家可以依照自己的需求來使用參數：\n限制使用的版本 在上一篇 使用 Terraform 建立 Google Compute Engine，我們知道 Terraform 其實就是對應的提供商，提供對應的 api 來讓我們可以用 terraform 去建置很多 IaC，但供應商提供的 api 會隨著版本而有所更動，可能換了一個版本，原本可以使用的 resource 參數就會有所不同，所以我們可以在一開始，先設定好這隻 tf 要使用的供應商及對應的版本，可以參考以下程式碼：\nterraform { required_providers { google = { source = \"hashicorp/google\" version = \"~\u003e 4.38.0\" } } } 可以看到我們把 google 這個供應商裡面設定好他的 source 以及 version，這樣就算之後 goolge 有更新 terraform 的 api，我們也不需要去更換參數就可以使用了～\n選擇供應者以及對應的專案 provider \"google\" { project = \"project\" } 除了可以使用專案 ID 以外，當然也可以使用專案的名稱拉 🥳\nresource 設定 google_container_cluster resource \"google_container_cluster\" \"cluster\" { name = \"test\" location = \"asia-southeast1-b\" min_master_version = \"1.22.12-gke.300\" network = \"projects/gcp-202011216-001/global/networks/XXXX\" subnetwork = \"projects/gcp-202011216-001/regions/asia-southeast1/subnetworks/XXXX\" default_max_pods_per_node = 64 remove_default_node_pool = true initial_node_count = 1 enable_intranode_visibility = false ip_allocation_policy { cluster_secondary_range_name = \"gke-pods\" services_secondary_range_name = \"gke-service\" } resource_labels = { \"env\" = \"test\" } addons_config { http_load_balancing { disabled = false } horizontal_pod_autoscaling { disabled = false } network_policy_config { disabled = false } } master_auth { client_certificate_config { issue_client_certificate = false } } private_cluster_config { enable_private_endpoint = false enable_private_nodes = true master_ipv4_cidr_block = \"172.16.0.0/28\" } logging_config { enable_components = [\"SYSTEM_COMPONENTS\", \"WORKLOADS\"] } monitoring_config { enable_components = [\"SYSTEM_COMPONENTS\"] } node_config { machine_type = \"e2-medium\" disk_size_gb = 100 disk_type = \"pd-standard\" image_type = \"COS_CONTAINERD\" oauth_scopes = [ \"https://www.googleapis.com/auth/devstorage.read_only\", \"https://www.googleapis.com/auth/logging.write\", \"https://www.googleapis.com/auth/monitoring\", \"https://www.googleapis.com/auth/servicecontrol\", \"https://www.googleapis.com/auth/service.management.readonly\", \"https://www.googleapis.com/auth/trace.append\" ] metadata = { disable-legacy-endpoints = \"true\" } } } name：叢集的名稱，在這個專案及區域唯一名稱 (必填) location：要將此叢集建立在哪一個區域 (選填) min_master_version：master 的最低版本 (選填) network：叢集連接到的 Google Compute Engine 網絡的名稱或 self_link (選填) subnetwork：啟動叢集的 Google Compute Engine 子網的名稱或 self_link (選填) default_max_pods_per_node：此叢集中每個節點的預設最大 pod 數 (選填) remove_default_node_pool：如果設定為 true，則在創建叢集時會幫我們刪除預設的節點池。會使用到這個的原因是因為 terraform 沒辦法修改預設節點池的名稱，所以我的做法是，會新增要的節點池，在使用這個參數把預設的給刪掉(選填) initial_node_count：要在此叢集的預設節點池中創建的節點數 (選填) enable_intranode_visibility：是否為此叢集啟用了節點內可見性 (選填) ip_allocation_policy：為 VPC 原生叢集分配叢集 IP (選填) resource_labels：應用於叢集的 GCE 資源標籤 key/value (選填) addons_config http_load_balancing：是否要啟用 HTTP (L7) 的負載平衡 (選填) horizontal_pod_autoscaling：是否要啟用 HPA 水平 Pod 自動擴展 (選填) network_policy_config：是否要啟用網路策略 (選填) master_auth client_certificate_config：是否要啟用該叢集客戶端證書授權 (選填) private_cluster_config enable_private_endpoint：是否要啟用叢集專用的私有端點，禁止公共端點的訪問 (選填) enable_private_nodes：是否要啟用私有叢集功能，在叢集創建私有端點 (選填) master_ipv4_cidr_block：私有端點 IP 範圍 (選填) logging_config：叢集的日誌記錄配置 enable_components (公開日誌的 GKE 組件) 設定，包含：SYSTEM_COMPONENTS、APISERVER、CONTROLLER_MANAGER、SCHEDULER、WORKLOADS (必填) monitoring_config：叢集的監控配置 enable_components (GKE 組件公開指標) 設定，包含：SYSTEM_COMPONENTS、APISERVER、CONTROLLER_MANAGER、SCHEDULER (選填) node_config：創建預設節點池參數 machine_type：Google Compute Engine 機器類型，預設為 e2-medium (選填) disk_size_gb：每個節點的 disk 大小，以 GB 為單位。允許最小為 10 GB，預設為 100 GB (選填) disk_type：連接到每個節點的 disk 類型，有 pd-standard、pd-balanced 或 pd-ssd，預設為 pd-standard (選填) image_type：創建新節點池後 NAP 使用的預設 image 類型。該值必須是 [COS_CONTAINERD、COS、UBUNTU_CONTAINERD、UBUNTU] 之一。COS 和 UBUNTU 已於 GKE 1.24 棄用 (選填) oauth_scopes：在預設服務帳戶下的所有節點虛擬機上可用的一組 Google API 範圍。 (選填) metadata：分配給叢集中實例的 key/value (選填) google_container_node_pool resource \"google_container_node_pool\" \"aaa\" { name = \"aaa\" project = \"project\" location = google_container_cluster.cluster.location cluster = google_container_cluster.cluster.name node_count = 6 node_locations = [ google_container_cluster.cluster.location ] node_config { # 省略 ... 與上面的 google_container_cluster 相同 } management { auto_repair = true auto_upgrade = false } upgrade_settings { max_surge = 1 max_unavailable = 0 } } name：節點池名稱 (選填) project：創建節點池的項目 ID (選填) location：叢集所在位置，可以使用資源名稱 (google_container_node_pool) +命名 (cluster) + 參數 (location) 來代表 (選填) cluster：叢集名稱，可以使用資源名稱 (google_container_node_pool) +命名 (cluster) + 參數 (name) 來代表 (選填) node_count：節點數量 (選填) node_locations：節點區域 (選填) management：節點管理配置 auto_repair：是否要自動修復 (選填) auto_upgrade：是否要自動升級 (選填) upgrade_settings：指定節點升級設定及方式 max_surge：升級期間可以添加到節點池的額外節點數 (選填) max_unavailable：升級期間可以同時不可用的節點數 (選填) 可以看到有很多設定都是選填的，所以不需要像我範例一樣，把所有的都打出來，可以參考官方文件，將自己想要的設定寫出來，並注意其他參數的預設值是多少，就可以打造屬於自己的 Terraform 建立 Google Kubernetes Engine IaC 程式囉～"},"title":"使用 Terraform 建立 Google Kubernetes Engine"},"/blog/terraform/terraform-module/":{"data":{"":"當我們要管理的資源越來越多後，會產生很多的 tf 檔案，假設我們現在有三個 gce 服務，會在以下三個不同環境上面運作，每個環境都會有我們之前學會的基本 tf 檔案(包含 provider.tf 、main.tf、backend.tf)，且其中的 main.tf 檔案內有些設定會不太一樣，如下：\ndev backend.tf main.tf provider.tf prod backend.tf main.tf provider.tf qa backend.tf main.tf provider.tf dev/main.tfresource \"google_compute_instance\" \"instance\" { project = \"馬賽克\" name = \"test-dev\" machine_type = \"e2-small\" zone = \"asia-east1-b\" boot_disk { initialize_params { image = \"debian-cloud/debian-10\" size = 50 } } .... 其他省略不寫 .... } qa/main.tf (多了 tags)resource \"google_compute_instance\" \"instance\" { project = \"馬賽克\" name = \"test-qa\" machine_type = \"e2-small\" zone = \"asia-east1-b\" tags = [\"for-qa\"] boot_disk { initialize_params { image = \"debian-cloud/debian-10\" size = 50 } } .... 其他省略不寫 .... } prod/main.tf (多了 labels)resource \"google_compute_instance\" \"instance\" { project = \"馬賽克\" name = \"test-prod\" machine_type = \"e2-small\" zone = \"asia-east1-b\" labels = { aaa = \"test1\" bbb = \"test2\" ccc = \"test3\" } boot_disk { initialize_params { image = \"debian-cloud/debian-10\" size = 50 } } .... 其他省略不寫 .... } 可以看到三個 main.tf 檔案除了 name 以外，在 qa 還多了 tags、prod 多了 labels 等設定，等於我們會依照每個不同環境不同服務去客製化他的 tf 資源設定，雖然非常直覺，但往後的維護以及調整卻非常不方便 ( 假設我們現在要全部都加上 labels，就必須一個一個檢查並調整 )。\n為了方便我們維護以及重複使用，因此有了 module，可以先將全部會使用到的設定寫成模板，透過參數的方式帶入即可，module 有以下幾個優點：\n重複使用性： module 讓程式碼更易於重複使用。當我們需要在多個項目中使用相同的基礎架構或配置時，可以先將其封裝為一個 module。這樣，我們只需要在不同的項目中引用並調整模組的參數，而不需要重新寫整個 tf 檔。\n抽象化：將 Terraform 代碼轉換為 module 可以將詳細的實現細節抽象化，僅寫必要的參數。這樣做可以提高程式碼的可讀性和可維護性，並降低使用者學習和使用的門檻。\n參數化配置：module 可以使用輸入參數來接收不同的配置值。這意味著您可以根據需要動態更改模組的行為，而不需要直接修改模組的內部程式。這使得配置更靈活並支持不同環境的部署。\nmodule 版本控制：將 Terraform 程式封裝為 module 後，可以使用 git 對其進行版本控制。可以更輕鬆地協作和共享 module (可以將 module 與 Terraform 分別存放，並使用對應 tag or 分支來做開發 )。","參考資料#參考資料":"Types and Values：https://developer.hashicorp.com/terraform/language/expressions/types","實作#實作":"當我們完成上面的架構後，我們進入 projects/prod/main.tf 路徑下，開始用 module 的方式建立資源，建立資源的流程與原本的相同，一樣是 init \u003e plan \u003e apply 這三個步驟，那我們一個一個來看，與原本的建立方式有哪些不同之處吧～\ninit 我們使用 terraform init 來看看原本 init 與使用模組 init 後差在哪裡：\n原先 terraform init 結果\n使用 module init 結果\n可以看到有使用 module 在初始化的時候，會連同 module 也一併初始化，接著我們進到 .terraform 資料夾內，可以看到有 moduels 資料夾。\n.terraform 檔案差異\n在進去看會看有一個 modules.json 檔案，會紀錄 module 使用的路徑，因此當我們使用的 module 有改變時，要記得重新 init 才可以確保使用的 module 是正確的。\n使用 module 會多一個 modules.json 檔案\nplan 我們一樣下 terraform plan 指令，來看看兩者顯示的差異：\n原先 terraform plan 結果\n使用 module plan 結果\n可以看到使用 module 在 plan 時，預覽創建的資源格式不同，也就代表他存在 tfstate 檔案的格式也會不同 (這個後面會在提到，與 import 也有關係)\napply 使用 terraform apply 來看建立資源後的結果有什麼不同：\n原先 terraform apply 結果\n使用 module apply 結果\napply 看到的與 plan 顯示的一樣，使用 module 建立的資料格式會不太一樣，所以我們來看看兩者 tfstate 檔案的差異：\n原先 terraform 建立的 tfstate 檔案\n使用 module 建立的 tfstate 檔案\nimport import 的功用是可以從雲上服務轉成 tf，在之前原本的 terraform 是要先建立一個空的 resource：\nresource \"google_compute_instance\" \"instance\" { } 再使用 terraform import google_compute_instance.instance 專案ID/機器地區/機器名稱 來匯入雲上服務的狀態到後端存到 tfstate 的位子。\n原先 terraform import 線上服務\n那我們現在改成 module，會比較麻煩一點，因為我們有在 variables.tf 設定我們的變數，若是沒有設定預設值，就必須一定要輸入，所以我們在建立時，要先把變數的空值也補上，如下：\nmodule \"ian-test\" { source = \"../../module/google_compute_instance\" project_id = \"\" instance_name = \"\" machine_type = \"\" instance_zone = \"\" instance_tags = [] instance_labels = {} boot_disk_image_name = \"\" boot_disk_size = 50 attached_disk_enabled = false network_name = \"\" subnetwork_name = \"\" nat_ip_enabled = false metadata = {} resource_policies = [] service_account_email = \"\" service_account_scopes = [] } \" \" 是 string 格式的空值，[ ] 是 list 格式的空值，{ } 是 map 格式的空值，其他的 bool 我預設會給他 false，number 我會隨便給他一個數字 xD。這邊帶入的內容不是很重要，主要是讓他可以去抓到他的架構，我們也可以在 variables.tf 設定時都補上預設值。\n再使用 terraform import module.ian-test.google_compute_instance.instance 專案ID/機器地區/機器名稱 來匯入狀態檔案。(這邊要記得依照你 module 設定的名稱帶入)\n使用 module import 線上服務","檔案說明#檔案說明":"\n首先我們要先定義我們的 module，我們先建立以下資料夾結構以及對應檔案：(同步到 GitHub 需要程式碼的可以前往查看)\n(再次提醒，會區分檔案名稱是因為方便調整跟維護，也可以把它全部寫在同一個 tf 檔案內歐)\nmodule google_compute_instance main.tf outputs.tf variables.tf projects dev backend.tf main.tf provider.tf prod backend.tf main.tf provider.tf qa backend.tf main.tf provider.tf module 資料夾：放我們 module 設定 (這邊範例是放 gce)\nprojects 資料夾：放我們不同服務、不同環境設定 (這邊為了簡化，範例只以不同環境為例)\nmodule/google_compute_instance/main.tfprovider \"google\" { project = var.project_id zone = var.instance_zone } resource \"google_compute_instance\" \"instance\" { name = var.instance_name machine_type = var.machine_type zone = var.instance_zone tags = var.instance_tags labels = var.instance_labels boot_disk { auto_delete = var.boot_disk_auto_delete initialize_params { image = var.boot_disk_image_name size = var.boot_disk_size } } dynamic \"attached_disk\" { for_each = var.attached_disk_enabled ? [1] : [] content { device_name = var.attached_disk_name mode = var.attached_disk_mode source = var.attached_disk_source } } network_interface { network = var.network_name subnetwork = var.subnetwork_name dynamic \"access_config\" { for_each = var.nat_ip_enabled ? [1] : [] content { } } } metadata = var.metadata enable_display = var.enable_display resource_policies = var.resource_policies service_account { email = var.service_account_email scopes = var.service_account_scopes } timeouts {} deletion_protection = var.deletion_protection allow_stopping_for_update = var.allow_stopping_for_update } 我們需要把所有設定的值都挖洞，使用 var 的方式來帶入參數，這邊要注意的是等號前面的值或是 block 名稱都是不能修改的，他是 google 定義的 api 變數，但 var 後的參數名稱我們可以自訂 (後面 variable.tf 會在詳細說明)，那這邊比較特別的用法是 dynamic，以下說明：\ndynamic \"attached_disk\" { for_each = var.attached_disk_enabled ? [1] : [] content { device_name = var.attached_disk_name mode = var.attached_disk_mode source = var.attached_disk_source } } 我們有些 block 只有在特定服務時才需使用，例如上面的 attached_disk 他是 gce 另外掛載其他磁碟的設定，如果有需要我們才會多設定這個 block，沒有則不需要加，因此須使用 dynamic 來動態產生 block，這邊的設定是我們要在參數要帶入 attached_disk_enabled 用 for_each 來判斷是否需要這個 block，如果是 true，就會產生 attached_disk block，且需要輸入 attached_disk_name、attached_disk_mode、attached_disk_source。\nmodule/google_compute_instance/variables.tfvariable \"project_id\" { type = string description = \"GCP 專案 ID\" } variable \"instance_name\" { type = string description = \"GCE 名稱\" } variable \"machine_type\" { type = string description = \"GCE 類型\" } variable \"instance_zone\" { type = string description = \"GCE 所在區域\" } variable \"instance_tags\" { type = list(string) description = \"GCE 網路標記\" } variable \"instance_labels\" { type = map(string) description = \"GCE 標籤\" } variable \"boot_disk_auto_delete\" { type = bool description = \"是否刪除 instance 時，自動刪除開機磁碟\" default = true } variable \"boot_disk_image_name\" { type = string description = \"GCE 映像檔名稱\" } variable \"boot_disk_size\" { type = number description = \"GCE 開機磁碟大小 (單位: GB)\" } variable \"attached_disk_enabled\" { type = bool description = \"是否啟用附加磁碟\" default = false } variable \"attached_disk_name\" { type = string description = \"GCE 附加磁碟名稱\" default = \"\" } variable \"attached_disk_mode\" { type = string description = \"GCE 附加磁碟模式\" default = \"READ_ONLY\" validation { condition = contains([\"READ_WRITE\", \"READ_ONLY\"], var.attached_disk_mode) error_message = \"不符合附加磁碟模式的值，請輸入 READ_WRITE 或 READ_ONLY\" } } variable \"attached_disk_source\" { type = string description = \"GCE 附加磁碟來源\" default = \"\" } variable \"network_name\" { type = string description = \"GCE 網路名稱\" } variable \"subnetwork_name\" { type = string description = \"GCE 子網路名稱\" } variable \"nat_ip_enabled\" { type = bool description = \"是否啟用 NAT IP\" default = false } variable \"metadata\" { type = map(string) description = \"GCE 中繼資料\" } variable \"enable_display\" { type = bool description = \"是否啟用虛擬顯示\" default = false } variable \"resource_policies\" { type = list(string) description = \"GCE 資源原則\" } variable \"service_account_email\" { type = string description = \"GCE 服務帳戶電子郵件\" } variable \"service_account_scopes\" { type = list(string) description = \"GCE 服務帳戶範圍\" } variable \"deletion_protection\" { type = bool description = \"是否啟用刪除保護\" default = false } variable \"allow_stopping_for_update\" { type = bool description = \"是否允許自動停止後更新\" default = false } 這個檔案會定義每個變數的名稱以及資料型態，也可以寫說明以及預設的值，這邊比較特別的是 validation ，他可以驗證帶入的參數是否符合 condition 內容，也可以自定義錯誤的訊息，如下：\nvariable \"attached_disk_mode\" { type = string description = \"GCE 附加磁碟模式\" default = \"READ_ONLY\" validation { condition = contains([\"READ_WRITE\", \"READ_ONLY\"], var.attached_disk_mode) error_message = \"不符合附加磁碟模式的值，請輸入 READ_WRITE 或 READ_ONLY\" } } 這邊限制 attached_disk_mode 輸入必須符合 READ_WRITE or READ_ONLY 的值，如果輸入其他不符合的會顯示 error_message 內容。\n另外 variable 這邊有幾個資料型態可以選擇，如下：\nstring：字串，不知道要選什麼就選他沒錯 xD\nbool：布林值，只有 true、false 兩種選項，適用於判斷的內容，例如剛剛上面說的 attached_disk_enabled 就是使用 bool\nnumber：數字，只能輸入數字\nlist (tuple)：清單，內容可以放置類似 [\"us-west-1a\", \"us-west-1c\"] 的資料\nmap (object)： key value 存放模式，例如：\n{ \"aaa\": \"test1\", \"bbb\": \"test2\", \"ccc\": \"test3\" } module/google_compute_instance/outputs.tfoutput \"instance_id\" { value = google_compute_instance.instance.instance_id } 這邊主要放置要輸出的內容，像我們這邊就會把 instance_id 給顯示出來。\nprojects 我這邊只示範 prod 的部分\nprojects/prod/main.tfmodule \"ian-test\" { source = \"../../module/google_compute_instance\" project_id = \"馬賽克\" instance_name = \"test-prod\" machine_type = \"e2-small\" instance_zone = \"asia-east1-b\" instance_tags = [] instance_labels = { \"aaa\" = \"test1\" \"bbb\" = \"test2\" \"ccc\" = \"test3\" } boot_disk_image_name = \"debian-cloud/debian-10\" boot_disk_size = \"50\" attached_disk_enabled = false network_name = \"馬賽克\" subnetwork_name = \"馬賽克\" nat_ip_enabled = false metadata = {} resource_policies = [] service_account_email = \"馬賽克\" service_account_scopes = [\"storage-ro\", \"logging-write\", \"monitoring-write\", \"service-control\", \"service-management\", \"trace\"] } 這邊我們可以定義要使用 module 的叫什麼，這邊我就取名 google_compute_instance，然後他會去 source \"../../module/ian-test\"，也就是我們剛剛在上面先挖洞的模板，底下就開始帶入我們在 variables.tf 有設定的參數。這邊比較要注意的是，在 main.tf、variables.tf 有使用的變數設定，都必須要寫在個別資源 tf 的檔案裡面，沒有的就帶入對應資料型態的空值，例如 instance_tags、metadata、resource_policies 等等。"},"title":"如何將 Terraform 改寫成 module ?"},"/blog/terraform/terraform-tfstate/":{"data":{"":"此篇是接續上一篇 什麼是 IaC ? Terraform 又是什麼？的 Terraform 文章，我們在上一篇有提到 terraform apply 完後，會多一個檔案 *.tfstate，這個檔案是用來存放服務狀態的檔案，它包含基礎架構的狀態和資源的詳細信息。假設大家都在自己的本地去 apply 同一個服務，會導致每個人的 tfstate 檔案內容不同，有可能去覆蓋掉其他人已經調整的內容，因此我們必須將此 tfstate 檔案存放在一個地方，讓大家都去使用同一份檔案來調整資源。\n我們常用的儲存方式會將 tfstate 存在 gitlab 或 gcs (gcp 架構為例)，以下會簡單說明要如何把 tfstate 存到後端以及各頁面的功能：","gcs#gcs":"gcs 儲存比較簡單一點，因為他就是一個 bucket，所以頁面就跟一般的 gcs 一樣，會顯示檔案名稱、大小、類型等，如果需要查看 tfstate，可以點擊最後的下載按鈕來查看\n那我們接著使用上面 gitlab 的範例檔案，只是要將 backend.tf 內容改為以下：\nbackend.tfterraform { backend \"gcs\" { bucket = \"pid-terraform-state\" prefix = \"/aaa\" } } 上面的設定是指，我們將 backend 後端設定改成 gcs，並且選擇名為 pid-terraform-state 的 bucket，此 bucket 需要先手動建立(因為 bucket 名稱是全域不重複，所以不需要特別設定其 project_id，只要有權限正確都可以跨專案使用)，以及我們要將此 tfstate 存在 aaa 資料夾內。\n接著我們重新刪除剛剛 gitlab 已產生的 .terraform/ 跟 .terraform.lock.hcl 檔案，重新下 init，就可以到 gcs 對應資料夾下，新增了 defaulte.tfstate 檔案。\n產生 defaulte.tfstate\nLock 我們一樣來看一下 gcs 的 lock 會長什麼樣子，gcs lock 會產生一個 default.tflock 檔案，由他去判斷現在是否是 Lock 狀態\ngcs Lock 會出現 .tflock 檔案\n當有其他人也執行 plan or apply 後，就會顯示以下：\ngcs Lock 其他人不能操作","gitlab#gitlab":"那我們要怎麼把 tfstate 存到 gitlab 呢？首先跟之前一樣，先新增 provider.tf 來放供應商的來源以及版本，以及 main.tf 來放 gce 相關設定，最後還要多一個 backend.tf 來放我們要儲存 tfstate 的位置設定，如下：(同步到 GitHub 需要程式碼的可以前往查看)\n(這次範例會使用 gce，此項會需要 gitlab 先啟用 Infrastructure 功能以及建立自己的 gitlab token)\nprovider.tfterraform { required_providers { google = { source = \"hashicorp/google\" version = \"~\u003e 4.48.0\" } } } main.tf provider \"google\" { project = \"XXXXX\" zone = \"asia-east1-b\" } resource \"google_compute_instance\" \"instance\" { name = \"test\" machine_type = \"e2-small\" zone = \"asia-east1-b\" labels = { env = \"11\" } boot_disk { initialize_params { image = \"debian-cloud/debian-10\" } } network_interface { network = \"projects/XXXX/global/networks/test\" subnetwork = \"projects/XXXX/regions/asia-east1/subnetworks/testtest\" } } backend.tf « 新的 專案 ID 要寫我們想要放 terraform state 的 GitLab Project ID，服務名稱是指顯示在 GitLab terraform state 的名稱\ngitlab 個人 token 是指個人存取權杖，大家再依照自己的來做設定\nbackend.tfterraform { backend \"http\" { address = \"[Gitlab 網址]/api/v4/projects/[專案ID]/terraform/state/[服務名稱]\" lock_address = \"[Gitlab 網址]/api/v4/projects/[專案ID]/terraform/state/[服務名稱]/lock\" unlock_address = \"[Gitlab 網址]/api/v4/projects/[專案ID]/terraform/state/[服務名稱]/lock\" username = \"[Gitlab 帳號]\" password = \"[Gitlab 個人 token]\" lock_method = \"POST\" unlock_method = \"DELETE\" retry_wait_min = 5 } } 當我們新增好後，就跟之前步驟一樣，先 init \u003e plan \u003e apply 來做測試，在 init 時會發現，與之前不太一樣的是，在 Initializing the backend 的下方有多了綠色的成功設定後端字樣，代表他也會將後端的相關資訊存進 .terraform 資料夾中，所以有變更後端儲存位置，要記得重新 init 歐\ninit 初始化後，會將後端資訊也存到 .terraform 資料夾\n當我們 plan \u003e apply 完成後，可以觀察一下，發現原本會產生的 terraform.tfstate 檔案沒有出現在該目錄下：\napply 完，沒有在本地產生 .tfstate 檔案\n這時候，我們可以到剛剛在上面設定的專案 ID 內的有一個 Infrastructure / Terraform，裡面就會存放 Terraform state 檔案，如下：\ngitlab/Infrastructure/Terraform\n會顯示狀態名稱、更新資訊、以及 Actions 等欄位：\ngitlab terraform tfstate 網頁\n功能部分可以看後面的 Actions 欄位底下有三個點點，可以下載對應的 tfstate 檔案、Lock 讓其他人不能對此進行 apply，或是刪除此 tfstate 檔案等\ngitlab terraform tfstate 功能說明\n這樣我們就可以透過同一份的 tfstate 檔案來做管理，但有個前提是，之後對該資源的變更都只能使用 tf，如果還有用 WEB UI 去調整，就會遇到線上服務與 tfstate 儲存狀態不同的問題。\nLock 那當我們已經有了共同儲存的地方，也溝通好，不會使用 WEB UI 去調整，但如果有兩個人同時去下 apply 的話，第一個人的 apply 還在執行，後面那個人的 apply 是不是就會蓋掉前一個人的設定呢？\n所以 Terraform 在 0.14 版本推出了 Lock 功能，當有人在 plan or apply 的時候，我們去查看 gitlab Terraform state，會看到我們的 tfstate 檔案會被 Lock 起來\nGitLab Lock 鎖住\n此時除了第一個操作者，其他人再去 plan or apply 就會出現錯誤，可以看到是誰正在使用，以及操作的動作是 plan or apply\n在 Lock 下，其他人沒辦法去 plan or apply\n在 CI 時，需要在 plan 時就將它給 lock，避免第一個人 plan 完，沒有及時的去執行 apply，後來有其他人比第一個人先調整了資源，第一個人再來執行 apply，就會導致第一個人 apply 的內容與自己原先看 plan 的內容會不同，也有可能會將上一個人調整的設定給覆蓋，當第一個操作者結束動作後，該 Lock 才會被解鎖。\n其他 gitlab terraform state 詳細內容可以參考：https://docs.gitlab.com/ee/user/infrastructure/iac/terraform_state.html"},"title":"Terraform 如何多人共同開發 (將 tfstate 存在後端)"},"/blog/terraform/terraform/":{"data":{"":"跟上一篇 Snyk 一樣，本系列也是去參加 DevOpsDay Taipei 2022 活動聽到各位產業大佬目前在使用的名詞以及技術，想說回家也充實一下自己，了解一下在 DevOpsDay 最常出現的 IaC 是什麼？以及聽說很方便的 Terraform 又是什麼，將學習的過程打成此篇筆記，歡迎大家多交流，那我們就開始囉。\n目前打算寫本篇介紹 IaC 以及 Terraform 以外，之後還想寫另外兩篇說明 Terraform 如何共同維護開發(將 .tfstate 檔案存在 gitlab or gcs 上)、怎麼把 Terraform 轉成 module，最後導入 Terragrunt 達到 DRY 等等，也會有用 Terraform 建立 GCE 以及 GKE。大家可以持續關注此篇文章，最後會在文章後附上連結 😍","terraform-又是什麼#Terraform 又是什麼？":"IaC 的工具有很多種，接著我們就來介紹其中一個工具 - Terraform，Terraform 是什麼呢？根據官網的說明可以知道，Terraform 是 HashiCorp 所開發的基礎設施即代碼工具。它可以使用人類方便讀的配置文件來定義資源和基礎設施，以下有使用 Terraform 幾個優點：\nTerraform 可以管理多個雲平台上的基礎架構 使用人類可讀的配置語言來幫助我們快速編寫基礎設施代碼 可以將配置提交給版本控制，安全地在基礎架構上進行協作 管理任何基礎設施 Terraform 提供插件讓 Terraform 可以通過其 API 與雲平台和其他服務進行交互。HashiCorp 和 Terraform 社區編寫了 3193 多個提供商來管理像 AWS、Azure、GCP、Kubernetes、Helm、GitHub 等資源，可以到 Terraform Registry 查看更多平台或服務的提供者，當然如果沒有找到想要的提供者，也可以自己編寫自己的套件。\n標準化部署工作流程 提供商會將基礎設施的每個單元 (例如建立 VM 或是 VPC) 定義為資源。你可以將來自不同提供者的資源組合，變成模組，讓我們可以用一致的語言和工作流程去管理他們。\nTerraform 什麼是 Terraform 的基礎設施即代碼？","什麼是-iac-#什麼是 IaC ?":"IaC 全名是 Infrastructure as Code (基礎設施即代碼)，從字面意思就可以略知一二，也就是把基礎設施變成程式碼，在還沒有這些 IaC 工具之前，大家都是開啟 WEB UI 畫面來進行建置或設定，雖然使用 UI 點一點就建好了，但這些步驟都沒有被紀錄下來 (git)，也沒有辦法透過其他人一起 Review 的方式來避免人為操作錯誤。因此有了 IaC 這些工具，可以將實際的操作流程，轉換成程式碼或是其他像是 JSON、Yaml 的方式給紀錄下來，以下是導入 IaC 帶來的好處：\n建置 CI/CD 自動化 (不需要再仰賴 UI 進行操作) 版本控制 (大家可以透過 MR 規定 code review，避免出現人為錯誤) 重複使用 (可以將常用的變成參數代入，減少建置時間) 環境一致性 (以上 IaC 說明來自 小惡魔 - 初探 Infrastructure as Code 工具 Terraform vs Pulumi 文章，寫得真的很好，推推)\nInfrastructure as Code 初心企服行研07：认识「基础设施即代码」(Infrastructure as Code) — 初心内参","參考資料#參考資料":"初探 Infrastructure as Code 工具 Terraform vs Pulumi：https://blog.wu-boy.com/2021/02/introduction-to-infrastructure-as-code-terraform-vs-pulumi/\n今晚我想認識 Terraform：https://ithelp.ithome.com.tw/articles/10233759","安裝-terraform#安裝 Terraform":"安裝 Terraform 的方式有很多種，我就以我在使用的 Mac OS 為例，其他可以參考 Install Terraform：\n安裝步驟 先安裝 HashiCorp tap，這是 HashiCorp 在 Homebrew 的儲存庫： brew tap hashicorp/tap 使用 hashicorp/tap/terraform brew install hashicorp/tap/terraform 如何驗證是否安裝成功 打開一個新的 Terminal，使用 terraform -help 檢查是否有安裝成功，也可以在 -help 後面加入參數來查看該參數的功能與更多訊息\n驗證 Terraform 安裝成功 Install Terraform\n自動補全指令 可以啟動終端機上的 Tab 自動補全功能，執行以下指令，再重開終端機，就會出現了：\nterraform -install-autocomplete 想要解除自動補全 (雖然應該不會拉)，執行以下指令：\nterraform -uninstall-autocomplete 放上成果圖片\n快速入門 當我們安裝好，想要最快的了解 Terraform ，當然是自己動手做一次，我們依照官網的教學，可以在一分鐘內使用 Docker 配置好 NGINX 伺服器，那我們開始囉！\n首先，我們必須要先安裝好 Docker，下載 Mac 版 Docker 桌面 建立一個資料夾，並進入該資料夾內 將以下 Terraform 配置文件貼到檔案中，並取名 main.tf：(同步到 GitHub 需要程式碼的可以前往查看) provider \"docker\" {} resource \"docker_image\" \"nginx\" { name = \"nginx:1.23\" keep_locally = false } resource \"docker_container\" \"nginx\" { image = docker_image.nginx.name name = \"nginx\" ports { internal = 80 external = 8000 } } 再開一個檔案取名為 provider.tf，將以下配置文件貼到檔案中： terraform { required_providers { docker = { source = \"kreuzwerker/docker\" version = \"~\u003e 2.13.0\" } } } (以上程式碼來自官網 安裝 Terraform#快速入門 加上小修改)\n先來簡單說明一下 Terraform 程式碼格式，Terraform 的檔案副檔名是 *.tf，採用名為 HCL (HashiCorp Configuration Language) 的組態語言來描述基礎架構。\n(補充說明：只要是同一個目錄下有 .tf 檔案結尾的，Terraform 都會去執行，所以檔案名稱可以自己取名，但為了方便管理都會使用 main、provider、backend、output 的檔案來放置對應的內容)\nHCL 是一種宣告式的語言，讓你直接寫下期望的基礎架構，而不是寫下過程的每一個步驟。\n檔案介紹 我們先看 provider.tf 檔案，檔案內會先寫好需要的供應商來源以及版本 (版本有點像是對應供應商提供的 api 版本)\nterraform { required_providers { 供應商名稱 = { source = \"供應商來源\" version = \"~\u003e 所使用的版本\" } } } main.tf 檔案內的 provider 區塊會寫供應商的相關設定，假設我們使用 google 就會在裡面先設定好 project id 等。\nresource 區塊會需要寫雲端資源名稱以及自定義的名稱，雲端資源名稱這項是不可以更改的，假設我們要使用 docker 的 container 服務，這邊就需要填寫 docker_container。自定義的名稱可以是你想要為使用這個雲端資源去定義的名稱。\nprovider \"供應商名稱\" {} resource \"雲端資源名稱\" \"自定義的名稱\" { 屬性 = 值 } 所以我們已上面的 Docker 配置好 NGINX 伺服器為例，provider 我們這次使用的是 docker， resource 我們可以拆開來寫，\n像是第一個 resource docker_image 我們幫他取叫 nginx，裡面就是放有關 image 的設定，詳細的 image 設定可以參考 Resource (docker_image)，\n第二個 resource docker_container 一樣叫 nginx，裡面用的 image 是拿前面的 docker_image resource name 來使用，一樣詳細可以參考 Resource (docker_container)。\n小提醒，如果不知道要怎麼寫 provider 供應商設定，可以打開 terraform 官網找到該供應商，點選右邊的 USE PROVIDER\n官方教學\n可以看到官方教學要怎麼使用這個供應商。\n指令說明 接著有幾個指令要帶大家認識：\nterraform init：初始化項目，下載 tf 檔案中所需要的外掛套件 terraform plan：會產生一份執行計劃。上面會寫著它將會做哪些事，你可以驗證是否符合你預期的設計 terraform apply：實際運作，把基礎架構建置完成。在完成之後，會把目前的狀態儲存到一份檔案中 (*.tfstate) terraform destroy：會銷毀用 Terraform 起的服務 terraform fmt：幫你整理好 tf 文件 terraform validate：靜態檢查 tf 文件 附上懶人指令\nalias ti='terraform init' alias ta='terraform apply' alias tp='terraform plan' alias td='terraform destroy' 由於在 apply 的時候會跳出詢問視窗，如果是要寫成腳本，可以把指令改成 terraform apply -auto-approve 就不需要輸入 yes 了！\n實際操作 有上面的指令後，我們來實際操作看看：\n首先到該 main.tf 檔案目錄下，先使用 terraform apply 來測試看看： 無法直接執行 apply\n會發現沒有辦法直接用 terraform apply 指令來建置服務，我們看一下他提示的說明，他說他找不到 lock file，需要先進行初始化才可以執行，所以我們的建置流程是先 init –\u003e apply\nterraform init 我們先執行 terraform init，可以看到他會下載 tf 檔案中所需要的外掛套件 (docker) terraform init\n當我們初始化後，資料夾會多一個檔案 (.terraform.lock.hcl) 以及資料夾 (.terraform)\ninit 前後檔案差異\n.terraform.lock.hcl：是 Terraform 中用於鎖定和管理外部提供者（providers）版本的檔案。它的主要功能是確保在不同的環境中使用相同的外部提供者版本，以避免在團隊合作或不同環境中引入不一致性和問題。\n.terraform/：資料夾主要用於存儲初始化和管理基礎架構相關的臨時文件。\nterraform plan 接著我們使用 terraform plan 來查看我們的計劃，可以看到他會列出我們所寫的 tf 裡面有用到的 resource，除了我們有設定的屬性，其他的屬性也會顯示出來，可以更方便地讓我們知道這個 resource 有哪些屬性可以設定 terraform plan\nterraform apply 最後我們檢查都沒有問題，就可以使用 terraform apply 來建置，apply 其實跟 plan 一樣都會先讓我們看一下計劃，但會跳出詢問是否要執行，除非你輸入 yes，否則就跟 plan 單純顯示計劃內容，最後我們就可以看到他成功在 docker 上面建立 nginx 服務 terraform apply\n查看 docker nginx 以及檢查其服務\n當我們 apply 完，服務也建立後，查看一下資料夾後會發現，又多了一個檔案 terraform.tfstate：\n多了一個檔案 terraform.tfstate\nterraform.tfstate： 是 Terraform 的狀態檔案，它包含了基礎架構的狀態和資源的詳細信息。預設情況下，這個檔案是本地的並且只存在於 Terraform 初始化和操作的目錄中。(但要如何實現共同維護同一個 IaC 呢，敬待後續分享 🤣)\nterraform destory 另外，當你想要移除服務時，可以使用 terraform destroy 來將服務給移除 terraform destroy\nterraform import 最後還有一個蠻重要的，就是我們已經有很多服務都是使用 WEB UI 方式建立的服務，那我們要怎麼把它變成 tf 檔案呢？ 跟我們剛剛說的 terraform.tfstate 檔案有關，他會儲存我們 IaC 的狀態，所以我們才可以透過他知道現在是對資源做新增、修改、刪除哪個操作\n那當這個檔案不見時，如果再重新下 terraform apply，他會認為你是新增狀態，但實際上 docker 服務還是啟動的狀態，所以就會錯誤，會跟你說他已經存在。\n測試沒有 terraform.tfstate 直接下 terraform apply\n所以這時候我們要把線上服務的資源轉成 tf ，第一步要先把 resource 的框架給寫出來，其他可以先留空白，如下 main.tf：\nprovider \"docker\" {} resource \"docker_image\" \"nginx\" { name = \"nginx:1.23\" keep_locally = false } resource \"docker_container\" \"nginx\" { } 接著使用 terraform import 來將線上服務的資源套用到我們 main.tf 裡面的 resource，所以會長得像：\nterraform import docker_container.nginx 7f363ea3f6a64b5432ae3627f490b3e297abf80f196bce9c028ec2eb82706f12 import 後面會加上 main.tf resource 名稱 docker_container，以及我們取名的 nginx，最後帶 container id (docker ps 查詢)，就可以匯出 terraform.tfstate 檔案囉，詳細的 import 可以參考每個供應商資源的網頁(這邊以 docker 為例)\nterraform import 匯出 terraform.tfstate\nterraform show 當我們匯出後，可以看一下 terraform.tfstate，它會是一個 json 格式，如果要轉成 tf 格式，還需要使用 terraform show 來將 terraform.tfstate 檔案轉成 tf 格式，如下：\nterraform show 將 terraform.tfstate 轉成 tf\n如果透過上述的方式來轉成 tf，會發現在重新 apply 時，會出現錯誤，這邊以 gce 的當範例，轉完的 tf 設定，有些是 tf 不支援的參數，只會顯示在 tfstate，所以還需要手動刪除。\n轉換後還需將不用的設定給移除"},"title":"什麼是 IaC ? Terraform 又是什麼？"},"/blog/terraform/terragrunt/":{"data":{"":"我們接續上一篇的 如何將 Terraform 改寫成 module ? ，我們已經將 Terraform 改成 module 的方式來進行管理，但當我們要管理的資源越來越多，且有分不同的專案時，整個服務架構會長的像以下：\nmodules google_compute_instance main.tf outputs.tf variables.tf projects gcp-1234 aaa backend.tf main.tf provider.tf bbb backend.tf main.tf provider.tf ccc backend.tf main.tf provider.tf gcp-2345 aaa backend.tf main.tf provider.tf bbb backend.tf main.tf provider.tf ccc backend.tf main.tf provider.tf gcp-3456 aaa backend.tf main.tf provider.tf bbb backend.tf main.tf provider.tf ccc backend.tf main.tf provider.tf 這邊的範例是以不同專案來分，再分區不同的服務，每個服務裡面都會有 backend.tf、main.tf、provider.tf 檔案所組成，我們可以來比較一下 gcp-1234 的 aaa 服務以及 gcp-2345 的 aaa 服務差異：\n檔案差異\n可以看到在 backend.tf 除了 prefix 路徑以外，其他設定也都一樣，但因為 Terraform 本身沒辦法透過帶入參數的方式來設定 backend.tf 後端部分，所以必須要先寫好每個服務所存放的後端位置，十分的不方便。\nbackend.tfterraform { backend \"gcs\" { bucket = \"terragrunt-tfstate\" prefix = \"/gcp-1234-aaa\" } } 為了減少上述這些需要一直重複寫差不多檔案的工作內容，因此有了 Terragrunt 這個工具，Terragrunt 是 Terraform 的包裝器，可以彌補 Terraform 上的一些缺陷，並且讓我們的 IaC 更貼近 DRY 原則。\n這邊說明一下 DRY 原則\nDRY 全名是 Don’t repeat yourself，也就是不要做重複的事情，能夠一次做完的就不要重複的去做。","terragrunt-好處#Terragrunt 好處":"\n接著介紹一下 Terragrunt 的好處：\n方便管理後端狀態設定\n將後端存儲桶納入管理\n使用 generate 自動生成檔案\n使用 include 檔案來達到 DRY 原則\n管理 Module 之間的依賴性\n產生依賴關聯圖\n方便管理後端狀態設定 首先第一個方便管理後端狀態設定，也就是我們上面提到的 backend.tf 設定。在 Terraform 原生為了區別不同專案不同服務的狀態檔，就必須先寫好每個儲存的路徑，但使用 Terragrunt，可以先在該目錄下，也就是 gcp-3456 目錄下先寫一個設定檔案 (我們以 gcp-3456 專案為例)，讓底下的 aaa、bbb、ccc 服務可以去 include 它，我們就不需要每個服務都寫幾乎差不多的設定檔，接著我們在 gcp-3456 資料夾下新增 terragrunt.hcl 檔案來說明：\nterragrunt.hclremote_state { backend = \"gcs\" generate = { path = \"backend.tf\" if_exists = \"overwrite\" } config = { bucket = \"terragrunt-tfstate\" prefix = \"${path_relative_to_include()}\" } } 這邊的設定其實跟之前的 backend.tf 差不多，只是後端現在儲存的 block 改叫做 remote_state，可以看到 backend 設定我們一樣是存在 gcs 上，generate 這個 block 它會判斷 backend.tf 檔案是否存在，如果沒有它就會幫我們建立，其中設定檔案內容是將狀態檔案存在 terragrunt-tfstate 這個 bucket，並透過${path_relative_to_include()} 這個變數來自動帶入有 include 這份檔案的路徑，並在相對路徑產生 backend.tf 檔案。\n有點抽象，所以我畫一個比較簡單的架構圖來做說明一下，假設現在有三個服務，如下：\naaa terragrunt.hcl bbb terragrunt.hcl ccc terragrunt.hcl terragrunt.hcl 我們剛剛的後端設定是寫在此根目錄的 terragrunt.hcl 檔案(第 8 行)，然後 ccc 這個服務去 include 根目錄的 terragrunt.hcl 檔案， Terragrunt 就會自動幫你產生以下的 backend.tf 檔案：\nbackend.tfterraform { backend \"gcs\" { bucket = \"terragrunt-tfstate\" prefix = \"/ccc\" } } 這樣就可以省下我們要重複寫 backend.tf 的時間，在維護上也會更加的方便。\n將後端存儲桶納入管理 接著，大家有沒有想過，我們都已經使用 Terraform 來管理 IaC ，並把狀態檔案放到 gcs 上面來保存，但一開始還沒有用 Terraform 管理 gcs 的資源，我們還需要在設定 backend.tf 前，先手動去新增一個 gcs ，才能來存放 tfstate 狀態檔案呢？\n因此在 Terragrunt remote_state 的 config 時，可以多設定 gcs 的 project id 以及 location，Terragrunt 在初始化後端時，會檢查是否有該 gcs bucket，如果沒有就會自動建立，設定檔如下：\nterragrunt.hclremote_state { backend = \"gcs\" generate = { path = \"backend.tf\" if_exists = \"overwrite\" } config = { project = \"gcp-xxxxxx\" location = \"asia\" bucket = \"terragrunt-tfstate\" prefix = \"${path_relative_to_include()}\" } } 使用 generate 自動生成檔案 在剛剛我們可以使用 generate 來自動 backend.tf 檔案，那代表我們也可以把每個 provider.tf 的內容，也透過 generate 來生成，如下：\nterragrunt.hclgenerate \"provider\" { path = \"provider.tf\" if_exists = \"overwrite\" contents = \u003c\u003cEOF terraform { required_providers { google = { source = \"hashicorp/google\" version = \"~\u003e 4.48.0\" } } } EOF } 這樣子每個 include 這份設定檔的服務除了 backend.tf 檔案以外，還會自動產生 provider.tf 檔案。\n使用 include 檔案來達到 DRY 原則 我們搞定了 backend.tf 跟 provider.tf 後，還剩下 main.tf，所以我們也將它改成 Terragrunt 的格式如下：\nterragrunt.hclterraform { source = \"${get_path_to_repo_root()}/modules/google_compute_instance\" } include \"root\" { path = find_in_parent_folders() } inputs = { instance_name = \"gcp-3456-ccc\" machine_type = \"e2-small\" instance_zone = \"asia-east1-b\" instance_tags = [] instance_labels = {} boot_disk_auto_delete = true boot_disk_image_name = \"debian-cloud/debian-10\" ... 設定部分省略 ... } 這邊可以看到 terraform source 它就是我們使用 module 的路徑，也可以用 ${get_path_to_repo_root()} 這個變數他會自動抓該專案的根目錄，我們就不需要去特別設定。\n此外也可以將 module 獨立成一個專案，或是使用其他人寫好的 module，在 source 的時候可以使用 git::https://[gitlab-網址]/sre/terraform/module.git//google_compute_address 的方式來取得 module，可以設定要使用哪個分支或是 tag，只需要在網址後面加上，?ref=[分之 or tag 名稱] 即可，這樣可以讓開發中的 module 不會影響到線上其他正在使用中的 module 設定。\n(module.git 後面的 // 是 Terraform module source 的規則，如果不加會跳警告訊息)\n接著我們可以看到 include \"root\" {} 這段，裡面有使用 find_in_parent_folders 這邊變數，他就是上面的提到會自動去抓放在父資料夾的 remote_state 跟 generate terragrunt.hcl 檔案。\n後面的 input 就跟使用 module 時一樣，將 module 的參數帶入即可。\n補充：所以我們也可以把一些通用的設定寫在根目錄的 terragrunt.hcl 檔案，例如專案的 id，可以寫以下內容來讓 include 它的檔案吃到同一個參數設定：\nterragrunt.hclinputs = { project_id = \"gcp-3456\" } 管理 Module 之間的依賴性 由於 Terragrunt 是把每個服務拆分成最小化，沒辦法把使用不同 module 的資源放在一起(單純使用 module 的話，可以一次 source 多了 module，並把他放在同一個 tf 檔案中)，那像是我們建立 k8s 會使用到 google_container_cluster、google_container_node_pool 兩種不同的 module 該怎麼辦呢？\n首先我們先在 modules 資料夾放上 google_container_cluster、google_container_node_pool 兩個 module 的設定檔案，詳細程式可以點我前往\nmodules google_container_cluster main.tf outputs.tf variables.tf google_container_node_pool main.tf variables.tf 在 projects 底下新增 gke 資料夾，新增 terragrunt.hcl 來放 remote_state provider 的設定，並區分兩個資料夾，分別是 cluster 資料夾來存放 cluster 資訊，以及 test 資料夾來存放 test node-pool 資訊：\nprojects gke cluster terragrunt.hcl terragrunt.hcl test terragrunt.hcl cluster 的 terragrunt.hcl 檔案如下：\nterragrunt.hclterraform { source = \"${get_path_to_repo_root()}/modules/google_container_cluster\" } include { path = find_in_parent_folders() } inputs = { cluster_name = \"tf-test\" cluster_location = \"asia-east1-b\" node_locations = [] cluster_version = \"1.24.12-gke.500\" network_name = \"projects/gcp-202011216-001/global/networks/bbin-testdev\" subnetwork_name = \"projects/gcp-202011216-001/regions/asia-east1/subnetworks/bbin-testdev-dev-platform\" node_max_pods = 64 remove_default_node_pool = true initial_node_count = 1 enable_shielded_nodes = false resource_labels = { \"dept\" : \"pid\", \"env\" : \"dev\", \"product\" : \"bbin\" } dns_enabled = false cluster_dns = \"PROVIDER_UNSPECIFIED\" cluster_dns_scope = \"DNS_SCOPE_UNSPECIFIED\" private_cluster_ipv4_cidr = \"172.16.0.176/28\" binary_authorization_enabled = true binary_authorization = \"DISABLED\" } test node-pool 檔案如下：\nterragrunt.hclterraform { source = \"${get_path_to_repo_root()}/modules/google_container_node_pool\" } include { path = find_in_parent_folders() } dependency \"cluster\" { config_path = \"../cluster\" } inputs = { cluster_name = dependency.cluster.outputs.cluster_name cluster_location = dependency.cluster.outputs.cluster_location cluster_version = dependency.cluster.outputs.cluster_version node_pool_name = \"test\" node_count = 1 node_machine_type = \"e2-small\" node_disk_size = 100 node_disk_type = \"pd-standard\" node_image_type = \"COS_CONTAINERD\" node_oauth_scopes = [ \"https://www.googleapis.com/auth/devstorage.read_only\", \"https://www.googleapis.com/auth/logging.write\", \"https://www.googleapis.com/auth/monitoring\", \"https://www.googleapis.com/auth/service.management.readonly\", \"https://www.googleapis.com/auth/servicecontrol\", \"https://www.googleapis.com/auth/trace.append\" ] node_tags = [] node_taint_enabled = false node_taint_key = \"\" node_taint_value = \"\" node_taint_effect = \"\" auto_repair = true auto_upgrade = true upgrade_max_surge = 1 upgrade_max_unavailable = 0 upgrade_strategy = \"SURGE\" autoscaling_enabled = true autoscaling_max_node_count = 2 autoscaling_min_node_count = 1 autoscaling_total_max_node_count = 0 autoscaling_total_min_node_count = 0 } 上面兩個檔案分別是 cluster 的設定，以及 test node-pool 的設定，裡面的設定，上面基本都有提過，這邊要提的是 dependency，dependency 他是 Terragrunt 提供讓我們可以方便地去管理 IaC 之間的相依性，像是我們這邊，需要先建立好 cluster 才能建立 node_pool，此時就可以依靠 dependency block 來完成需求。\n( 靠 dependency 來取得 cluster 的資訊，並帶入 node_pool 中 )\n此時的執行指令是在 gke 目錄下，使用 terragrunt run-all [參數] 來跑整個相依的 module，我們這邊就建立一個名為 tf-test 的 cluster，並且有一個名為 test 的 node_pool ，其他設定請參考上面程式：\n測試 terragrunt run-all\n(黃色的 WARN 是因為 gcs 上還沒有存過該狀態檔案，所以會跳出提示)\n等到 cluster 建立完成後，會將 cluster 的資訊帶入 node_pool，才開始建立 node_pool 的資源：\n測試 terragrunt run-all\n產生依賴關聯圖 當我們服務使用到很多依賴關係，想要釐清是誰依賴誰，如果單純看程式會比較麻煩，在 Terragrunt 還有一個好用的指令，可以使用以下指令，產生對應的依賴關係圖，在檢視時可以更清楚知道關係：\nterragrunt graph-dependencies | dot -Tpng \u003e graph.png ( 這個 dot 指令是另外的套件，會將關係圖程式碼轉成圖檔，請先安裝 brew install graphviz )\n關聯圖","terragrunt-安裝方式#Terragrunt 安裝方式":"那要怎麼使用 Terragrunt 呢！？\n第一步當然是安裝它囉，我們系統是 macOS，所以我們安裝方式是 Homebrew 來進行安裝：\nbrew install terragrunt 接著我們的指令會從 terraform XXX 變成以下：\nterragrunt plan terragrunt apply terragrunt output terragrunt destroy Terragrunt 會將所有命令、參數和選項直接轉發到 Terraform。(所以我們也需要下載 Terraform)\nTerragrunt 的預設檔案名稱是 terragrunt.hcl ，Terragrunt 的設定檔案基本上與 Module 差不多，只是有更多更方便的變數可以使用。","參考資料#參考資料":"事半功倍 — 使用 Terragrunt 搭配 Terraform 管理基礎設置：https://medium.com/act-as-a-software-engineer/%E4%BA%8B%E5%8D%8A%E5%8A%9F%E5%80%8D-%E4%BD%BF%E7%94%A8-terragrunt-%E6%90%AD%E9%85%8D-terraform-%E7%AE%A1%E7%90%86%E5%9F%BA%E7%A4%8E%E8%A8%AD%E6%96%BD-f70c30166639"},"title":"如何導入 Terragrunt，Terragrunt 好處是什麼？"},"/projects/":{"data":{"":" 活動申請系統 (Web)幫學校開發的大型校務系統，方便系統社團使用電子填單方式申請活動，也讓之後的學弟妹，可以更快速的查看到歷屆辦理的活動。 巔峰極速 兌換虛寶網站 (Web)該遊戲有大量虛寶可以兌換，但官方提供的網頁，需要重複輸入 ID 以及驗證碼還有序號，因此寫了一個小網頁，可以只輸入一次 ID 以及驗證碼，就兌換完所有序號。 京緯工程有限公司官網 (Web)協助京緯工程有限公司建立公司官方網頁。 京緯工程有限公司管理後台 (Web)協助京緯工程有限公司建立公司後台管理系統，自動計算薪資、工程管理、發票管理、材料管理、員工管理等功能。 臺灣行事曆 (API)臺灣行事曆，資料來源中華民國政府行政機關辦公日曆表，可以透過 API 取得資料，並新增其他欄位，例如：民國年、星期英文、星期縮寫等等，方便大家使用。 臺中市北區國民運動中心 (API)用爬蟲抓取取得游泳池、健身房人數 API。 "},"title":"專案成就"}}