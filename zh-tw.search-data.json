{"/about/":{"data":{"":" 哈囉大家好，我叫莊品毅，也可以叫我 Ian，目前是一位 Site Reliability Engineering (SRE) 工程師，主要負責維護公司的 Google Cloud Platform (GCP) 雲端相關服務，包含 GKE、GCE、GCS、GLB 等等，除此之外也熟悉使用 Terraform + Terragrunt 來管理雲端大量的 IaC 資源，配合 Prometheus、Datadog、EFK 等監控工具來確保服務的穩定性，協助 RD 建立 CICD 部署流程。\n在下班空閒時間，我喜歡閱讀技術相關的文件、部落格，也會參加一些線下技術社群的活動，例如 DevOpsDay，希望能夠透過這些活動來學習更多的知識，歡迎大家使用下方 giscus 留言系統留言交流。","工作經驗#工作經驗":" 凡谷興業有限公司 - SRE 工程師 (2022/02 - 現在)"},"title":"關於我"},"/blog/":{"data":{"":"","介紹#介紹":"👋 你好、妳好、大家好，歡迎來到我的秘密花園，這邊主要會記錄我研究一個新的技術或工具、以及處理一些 SRE 遇到的靈異事件問題的小天地。\n會開始寫 Blog 的初衷主要是我的小腦袋瓜，如果不寫下來，過陣子很容易就忘記 (´_ゝ`)，當然也希望可以幫助到有相同問題的人 (可以使用搜尋功能來快速找到相關文檔喔)，如果有任何問題或建議，歡迎在下方留言。","聲明#聲明":"由於在學習新的技術或工具時，會參考網路上許多的文件和照片，雖然會加上自己的見解與實作內容改寫而成，且會於文章後附上相關資料來源，如有侵犯到您的權益，請於下方告知，我會立即刪除相關內容，謝謝 (๑•́ ₃ •̀๑)。"},"title":"Blog"},"/blog/gcp/":{"data":{"":"此分類包含 Google Cloud Platform 相關的文章。\n如何過濾 GCP LOG，減少 Cloud Logging API 的花費 Google Kubernetes Engine CronJob 會有短暫時間沒有執行 Job "},"title":"Google Cloud Platform"},"/blog/gcp/gcp-log-reduce-cloud-logging-api/":{"data":{"":"當我們使用 GCP 的 Cloud Logging 服務來查看 Log 時，有時候會有一些我們不需要顯示出來的，或是從來都不會去查詢的 Log，再者是 GCP 本身的錯誤導致大量噴錯的 Log ，這些 Log 都會導致 Cloud Logging 的費用增加。","介紹-cloud-logging#介紹 Cloud Logging":"先來簡單說一下 Cloud Logging 這項服務的基本架構，請看圖：\nCloud Logging 基本架構\n可以看到 Logs Data 會透過 API 再經過 _Default log sink (router) 存到相應命名的 log bucket (預設配置)，圖中 _Required 以及 _Default 的 log sink 都是 GCP 自動創建的接收器，下面簡述一下它們的區別：\n_Required 日誌儲存桶 Cloud Logging 會將以下類型的 Log 存到 _Required 儲存桶\n管理員活動審核 Log\n系統事件審核 Log\nAccess Transparency Log\nCloud Logging 會將 _Required 儲存桶 Log 保留 400 天，無法調整該期限，且無法修改或刪除 _Required 儲存桶，也沒辦法停用 _Required log sink 接受器路由到 _Required 儲存桶的設定。\n_Default 日誌儲存桶 只要不是存在 _Required 日誌儲存桶的 Log 就會透過 _Default log sink 接受器路由到 _Default 儲存桶。\n除非有另外配置自定義設定， 否則 _Default 日誌儲存桶 Log 只會保留 30 天，也一樣無法刪除 _Default 日誌儲存桶。此外 Cloud Logging 的費用是以存在 _Default 日誌儲存桶來計算。\n功能 價格 每月免費額度 Logging 提取 提取的 Log $0.5/GiB 每個項目前 50 GiB Logging 儲存 保留超過 30 天的 Log，每月每 GiB $0.01 在默認保留期限的 Log 不會有額外儲存費用 查看該專案使用的 Cloud Logging API 費用：https://console.cloud.google.com/apis/api/logging.googleapis.com/cost","參考資料#參考資料":"Routing and storage overview https://cloud.google.com/logging/docs/routing/overview","過濾-log#過濾 Log":"那現在知道 Cloud Logging 的架構，那當我們遇到需要過濾 Log 時，我們可以使用以下步驟來過濾以節省 Cloud Logging API 費用：\n範例說明 這次範例是 google 在 2023/07/06 所發佈的 Service Health，當 GKE 版本大於 1.24 以上，就會噴\nFailed to get record: decoder: failed to decode payload: EOF\ncannot parse ‘0609 05:31:59.491654’ after %L\ninvalid time format %m%d %H:%M:%S.%L%z for ‘0609 05:31:59.490647’\n這三種類型的錯誤 Log，在等待官方修復前，官方的建議是先將他給過濾掉，避免一直刷噴錢 ┐(´д`)┌\n附上當時的 Service Health 連結：https://status.cloud.google.com/incidents/y5XvpyBXFhsphSt4DfiE\n我們在上面架構圖有說到，Log 會透過 log sink 路由到 bucket，所以我們要將過濾條件加在 log router 上：\n選擇 Log Router 選擇 Log Router\n選擇 Log Router Sinks 選擇 _Default 的 Log Router Sinks，點選右邊按鈕的 Edit Sink\n選擇 _Default 的 Log Router Sinks\n設定 Sink details 第一個是 details，可以輸入說明，這邊輸入：google 有 bug 會噴大量的意外 LOG，怕費用飆高，先用官方建議的來過濾 LOG，詳細可以看： https://status.cloud.google.com/incidents/y5XvpyBXFhsphSt4DfiE\n輸入 Sink details 說明\n選擇 Sink Service 跟 Bucket 接著 sink 服務選擇 Logging bucket，以及對應儲存的 log bucket (這邊基本上都是預設)\n選擇 Logging bucket\n設定 include Log 選擇那些可以被 include 到 sink 接收器的 LOG 格式 (這邊基本上都是預設)\n預設的 LOG 格式\n設定 filter Log 這邊就是我們要輸入過濾的地方，先點擊 ADD EXCLUSION，輸入過濾的名稱，以及過濾的內容格式，我們輸入 google 在 Service Health 所提供的格式，最後按下 UPDATE SINK\n新增要過濾的 LOG 格式\n設定完成 等待更新完成，就可以看到我們已經在接收器上設定好過濾條件囉～\n查看詳細接收器設定\n檢查 Log 是否過濾成功 最後再檢查一下 Log 是不是沒有收到該錯誤的 Log 內容\n檢查 LOG 是否不會再出現"},"title":"如何過濾 GCP LOG，減少 Cloud Logging API 的花費"},"/blog/gcp/gke-cronjob-not-working/":{"data":{"":"前陣子公司建立在 Google Kubernetes Engine 叢集上的 CronJob 服務會有短暫時間沒有執行 Job。先前情提要一下，此 CronJob 的設定是每分鐘都會執行 (圖一)，所以理當來說 Log 應該要可以看到每分鐘都有此 CronJob 的紀錄，但有時候會發生 CronJob 短暫時間都沒有執行的狀況，找了一陣子都沒有找到原因，最後開支援單請 Google 那邊協助查看，終於找到原因拉 😍。那就跟我一起看一下發生的過程，以及 Google 幫我們找到的原因，以及要如何解決等等～\n(圖一) CronJob schedule 時間為每分鐘執行","參考資料#參考資料":"[1] Standard cluster upgrades：https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-upgrades\n[2] maintenance-windows-and-exclusions：https://cloud.google.com/kubernetes-engine/docs/concepts/maintenance-windows-and-exclusions","問題發生以及問題原因#問題發生以及問題原因":"我們使用 Google Cloud Platform 裡面的記錄功能，可以看到 (圖二)，在每分鐘執行的 Log 中，有短暫時間沒有執行 Job，但這個時間除了 CronJob 以外，其他的服務都是好的。\n(圖二) Google Cloud Platform 記錄有短暫沒有執行\n找了一陣子都沒有找到原因，於是我們開支援單請 Google 那邊協助查看，Google 那邊找了一陣子後終於找到原因拉！！！我們一起來看看吧 (圖三)\n(圖三) Google 支援單回覆\n就如同 Google 所說，使用提供的指令參數來查詢，叢集在該時段有發生 master control plane 的升級，如 (圖四)，再加上我們建的這個 cluster 是使用 zonal cluster (圖五)，所以叢集只有一個 control plane，當 control plane 在更新時，會無法部署新的 workload，導致該 CronJob 沒有執行 Job。參考資料 [1]\n(圖四) 發生 master control plane 的升級\n(圖五) 有問題的叢集位置類型\nGoogle 的建議是可以考慮使用另一個 regional cluster，讓 master node 在更新時不會只在單一地區，或是一樣使用舊的 zonal cluster，透過設定 Maintenance window 或者 Maintenance exclusions 來降低服務受到 workload 的影響。參考資料 [2]\n就算把 node_pool 裡面的自動升級給停掉，也沒有辦法解決此問題！因為此 master control plane (也就是 master node) 的升級，不是 worker node 的 node pool 升級，是由 GKE 負責維護的，所以他們會定期升級 control plane，也沒辦法停止此類的升級。\n若已經建立好 zonal cluster 後，想要改成 regional cluster ，是沒有辦法使用修改的方式，一定只能重建 cluster，所以大家在建立時要注意～","解決問題#解決問題":"最後我們選擇將叢集給整個重建，來確保 CronJob 不會有沒有執行到的狀況發生，重建叢集跟搬服務的過程很辛苦的 😰 希望大家不要發生 QQ，最後我們來看一下重建完後，在 master control plane 更新的時候，還會不會有 CronJob 沒有執行的情況發生。\n新叢集使用 regional cluster 來建立，在 2/1 也有 master control plane 的升級。 (圖六)\n(圖六) 發生 master control plane 的升級\n查看 CronJob 執行的紀錄可以發現並沒有 Job 沒有執行的情況發生。 (圖七)\n(圖七) 叢集位置類型\n(圖八) 新叢集位置類型"},"title":"Google Kubernetes Engine CronJob 會有短暫時間沒有執行 Job"},"/blog/kubernetes/":{"data":{"":"此分類包含 Kubernetes 相關的文章。\n在正式環境上踩到 StatefulSet 的雷，拿到 P1 的教訓 部署 Pod 遇到 container veth name provided (eth0) already exists 錯誤 "},"title":"Kubernetes"},"/blog/kubernetes/k8s-statefulset-podmanagementpolicy/":{"data":{"":"此文章要來記錄一下前陣子在公司的正式環境踩到 StatefulSet 的雷，事情是這樣的，我們有些服務，是使用 StatefulSet 來建置，至於為什麼不用 Deployment，這個說來話長 (也不是因為需要特定的 Pod 名稱、或是網路標記等等)，我們這邊先不討論，這個 StatefulSet 服務是 Nginx + PHP-FPM，為了避免流量進入到 processes 已經被用光的 Pod 中，我們在 StatefulSet 的 PHP Container 上有設定 readiness，readiness 的設定長得像以下：\nreadinessProbe: exec: command: - \"/bin/bash\" - \"-c\" - | CHECK_INFO=$(curl -s -w 'http code:\\t%{http_code}\\n' 127.0.0.1/status) HTTP_CODE=$(echo -e \"${CHECK_INFO}\" | awk '/http code:/ {print $3}') IDLE_PROCESSES=$(echo -e \"${CHECK_INFO}\" | awk '/idle processes:/ {print $3}') [[ $HTTP_CODE -eq 200 \u0026\u0026 $IDLE_PROCESSES -ge 10 ]] || exit 1 我們會用 curl 來打 /status，檢查回傳的 http code 是否為 200，以及 idle processes 是否大於等於 10，如果不符合，就會回傳 1，讓他被標記不健康，讓 Kubernetes 停止流量到不健康的容器，以確保流量被路由到其他健康的副本。","參考資料#參考資料":"Kubernetes — 健康檢查：https://medium.com/learn-or-die/kubernetes-%E5%81%A5%E5%BA%B7%E6%AA%A2%E6%9F%A5-59ee2a798115\nPod Management Policies：https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#pod-management-policies","問題#問題":"當天遇到的情況是，我們上程式後，Pod 都一切正常，當流量開始進來後，發現 10 個 Pod 會開始偶發的噴 Readiness probe failed，查看監控發現 processes 越來越低，最後被反應服務有問題，我們查看 Hpa 的紀錄的確有觸發到 40 個 Pod，只是查看 Pod 數還是依樣卡在 10 個，當下我們有嘗試使用調整 yaml 在 apply，發現 StatefulSet 的 yaml 也已經更新了，但 Pod 還是一樣卡在 10 個，也有使用 kubectl 下 kubectl scale sts [服務名稱] --replicas=0，想要切換 Pod 數也沒有辦法。\n當下我們有先 Call Google 的 Support 一起找原因，Google 是建議我們 readiness 的條件不要設的太嚴格，可以加上 timeoutSeconds: 秒數，但對於 Pod 卡住，還是沒有找到原因，後來我們查了一下 StatefulSet 的文件發現，StatefulSet 有一個設定 podManagementPolicy，預設是 OrderedReady，他必須等待前面的 Pod 是 Ready 狀態，才會再繼續建立新的，也就是說我們的 StatefulSet 已經卡住，導致就算 Hpa 觸發要長到 40 個 Pod 也沒有用。","測試結果#測試結果":"最後我們就使用兩種模式來測試看看，已下是測試結果(透過 P1 才知道的設定ＱＱ)：\n有將測試的 StatefulSet 放在 Github，可以點我查看 (可以調整 readinessProbe 的 httpGet.Path 故意把他用壞)\n使用 OrderedReady 模式 StatefulSet 在 podManagementPolicy 預設 OrderedReady 的模式，故意讓 readiness 卡住時 (Pod 卡住時)：\n當下的 StatefulSet 設定： StatefulSet 設定\nPod 狀態： Pod 狀態\n使用指令調整 Pod 數量 我們這時候下指令調整 Pod 數量，看看會發生什麼事：\nkubectl scale sts my-statefulset --replicas=5 我們先看 StatefulSet 的 yaml 可以看到 Pod replicas 已經改變，也可以看 generation 有更新，代表 StatefulSet 本身有接收到調整設定的請求。\n下指令調整後的 StatefulSet 設定\n看了一下 Pod 數量，也是一樣卡住，且 Pod 數量也沒有變化。\n下指令調整後的 Pod 狀態\n使用 yaml 調整 Pod 數量 我們直接調整 StatefulSet yaml 的 Pod 數量，看看會發生什麼事：\n一樣我們先看 StatefulSet 的 yaml 可以看到 Pod replicas 已經改變(這裡應該切別的 Pod 數量，切回 3 個好像沒有意義 xD)，也可以看 generation 有更新。\n使用 yaml 調整後的 StatefulSet 設定\n看了一下 Pod 數量，也是一樣卡住，且 Pod 數量也沒有變化。\n使用 yaml 調整後的 Pod 狀態\n所以代表在 OrderedReady 的模式下，Pod 卡住時，無法對 Pod 進行任何操作，必須要手動刪除卡住的 Pod 才吃得到最新的設定。\n使用 Parallel 模式 StatefulSet 在 podManagementPolicy Parallel 的模式，故意讓 readiness 卡住時 (Pod 卡住時)：\n當下的 StatefulSet 設定： StatefulSet 設定\nPod 狀態： Pod 狀態\n使用指令調整 Pod 數量 我們這時候下指令調整 Pod 數量，看看會發生什麼事：\nkubectl scale sts my-statefulset --replicas=5 我們先看 StatefulSet 的 yaml 可以看到 Pod replicas 已經改變，也可以看 generation 有更新，代表 StatefulSet 本身有接收到調整設定的請求。\n下指令調整後的 StatefulSet 設定\n看了一下 Pod 數量，就算 my-statefulset-2 卡住，還是可以擴到 5 個 Pod。\n下指令調整後的 Pod 狀態\n使用 yaml 調整 Pod 數量 我們直接調整 StatefulSet yaml 的 Pod 數量，看看會發生什麼事：\n一樣我們先看 StatefulSet 的 yaml 可以看到 Pod replicas 已經改變，也可以看 generation 有更新。\n使用 yaml 調整後的 StatefulSet 設定\n看了一下 Pod 數量，也不會管其他 Pod 是否 Ready，一樣可以縮小成 2 個 Pod。\n使用 yaml 調整後的 Pod 狀態","結論#結論":"後來我們重新檢查了一下為什麼 processes 會用完，結果發現是 RD 的程式邏輯，導致每筆 Request 必須等待前一筆 Request 做完，才會開始動作，讓 processes 一直被占用，沒辦法即時消化，導致 processes 用完，又加上服務是使用 StatefulSet，預設模式的 OrderedReady，必須等待前一個 Pod 是 Ready 才可以自動擴縮，所以當我們 Hpa 想要擴縮，來增加可用的 processes 數量，也因為沒辦法擴縮，最後導致這一連串的問題 😕。\n另外，如果想要從 OrderedReady 模式切成 Parallel 模式 (反正過來也是)，必須先將原本的 StatefulSet 給刪除，才可以調整：\nOrderedReady 模式切成 Parallel 模式","解決辦法#解決辦法":"當下想趕快解決 readiness 這個問題，調整 timeoutSeconds 後，單純 apply 是沒有用的，要記得刪掉卡住的 Pod，讓他重新建立，才會套用新的設定 (但我們當下太在意為甚麼 Pod 會卡住，沒有想到要先把 readiness 問題修掉 xD，我們當下的解法是先將流量導到地端正常的服務上)。\n另外 Google 也說，假如我們還是必須使用 StatefulSet 來建立服務，建議我們把 podManagementPolicy 改成 Parallel，它會有點像是 Deployment 的感覺，不會等待其他 Pod 變成 Ready 狀態，所以可以讓我們就算在 readiness 卡住的情況下，也可以自動擴縮服務。\nℹ️ StatefulSet podManagementPolicy 參數說明\nOrderedReady (預設) Pods 會按照順序一個接一個地被創建。即，n+1 號 Pod 不會在 n 號 Pod 成功創建且 Ready 之前開始創建。 在縮小 StatefulSet 的大小時，Pods 會按照反向順序一個接一個地被終止。即，n 號 Pod 不會在 n+1 號 Pod 完全終止之前開始終止。 這確保了 Pods 的啟動和終止的順序性。\nParallel 所有 Pods 會同時地被創建或終止。 當 StatefulSet 擴展時，新的 Pods 會立即開始創建，不用等待其他 Pods 成為 Ready 狀態。 當縮小 StatefulSet 的大小時，要終止的 Pods 會立即開始終止，不用等待其他 Pods 先終止。 這種策略提供了快速的擴展和縮小操作，但缺乏順序性保證。"},"title":"正式環境上踩到 StatefulSet 的雷，拿到 P1 的教訓"},"/blog/kubernetes/pod-veth-name-provided-eth0-already-exists/":{"data":{"":"此文章要來記錄一下公司同事在正式服務上遇到的問題，會詳細說明遇到事情的經過，以及開單詢問 google support 最後討論出的暫時解決的辦法：\n簡單列出正式站當下服務環境：\ngke master version：1.25.10-gke.2700 gke node version：1.25.8-gke.1000 該問題發生的 node pool 有設定 taint 發生問題的 Pod 是用 Statefulset 建立的服務 ","事情發生的經過#事情發生的經過":" RD 同仁反應，發現使用 Statefulset 建立的排程服務有問題，下 kubectl delete 指令想要刪除 Pod，讓 Pod 重新長，卻卡在 Terminating，等待一段時間後，決定下 kubectl delete --force --grace-period=0 來強制刪除 Pod，這時候狀態會卡在 ContainerCreating，使用 Describe 查看，會出現以下錯誤： Warning (combined from similar events): Failed to create pod sandbox: rpo error: code = Unknown desc = failed to setup network for sandbox \"14fe0cd3d688aed4ffed4c36ffab1a145230449881bcbe4cac6478a63412b0c*: plugin type=*gke\" failed (add): container veth name provided (etho) already exists 我們 SRE 協助查看後，也有嘗試去下 kubectl delete --force --grace-period=0 來刪除 Pod，但還是一樣卡在 ContainerCreating，最後是先開一個新的 Node 並讓該 Pod 建立到新的 Node 上，才解決問題。為了方便 google support 協助檢查出問題的 Node，先將 Node 設定成 cordon，避免其他 Pod 被調度到該問題 node 上。 Node 設定成 cordon\nNode 可以設定 cordon、drain 和 delete 三個指定都會使 Node 停止被調度，只是每個的操作暴力程度不同：\ncordon：影響最小，只會將 Node 標示為 SchedulingDisabled 不可調度狀態，但不會影響到已經在該 Node 上的 Pod，使用 kubectl cordon [node name] 來停止調度，使用 kubectl uncordon [node name] 來恢復調度。\ndrain：會先驅逐在 Node 上的 Pod，再將 Node 標示為 SchedulingDisabled 不可調度狀態，使用 kubectl drain [node name] --ignore-daemonsets --delete-local-data 來停止調度，使用 kubectl uncordon [node name] 來恢復調度。\ndelete：會先驅逐 Node 上的 Pod，再刪除 Node 節點，它是一種暴力刪除 Node 的作法，在驅逐 Pod 時，會強制 Kill 容器進程，沒辦法優雅的終止 Pod。\n我們隨後開單詢問 goolge support。 ","參考資料#參考資料":"Node 節點禁止調度（平滑維護）方式- cordon，drain，delete：https://www.cnblogs.com/kevingrace/p/14412254.html","與-google-support-討論內容#與 Google Support 討論內容":"Google Support 經過查詢後，回覆說：這個問題是因為 Pod 被強制刪除導致，強制刪除是一種危險的操作，不建議這樣處理，下面有詳細討論。\n一開始卡在 Terminating 狀態，我們也有請 RD 說明一下當下遇到的問題以及處理動作：RD 當時想要刪除 Pod 是因為該程式當下有 Bug，將 redis 與 db 連線給關閉，程式找不到就會一直 retry，導致相關進程無法結束，再加上 terminationGracePeriodSeconds 我們設定 14400，也就是 4 小時，才會卡在 Terminating 狀態。 (terminationGracePeriodSeconds 設定這麼久是希望如果有被 on call，工程師上來時，可以查看該 Pod 的錯誤原因)\n因為卡在 Terminating 太久，RD 有執行 kubectl delete --force，就是因為下了 --force 才造成相關資源問題 (例如 container proccess, sandbox, 以及網路資源)沒有刪乾淨。所以引起了此次的報錯 “container veth name provided (eth0) already exists”。 (因為我們服務使用 Statefulset，Pod 名稱相同，導致 eth0 這個網路資源名稱重複，所以造成錯誤，可以用 deployment 來改善這個問題，只是資源如果沒有清理乾淨會佔用 IP，所以單純調整成 deployment 也不是最佳解)\nGoogle 產品團隊建議，如果 Pod 處於 Running 狀態時，想要快速刪除 Pod 時，一開始就先使用 kubectl delete pod --grace-period=number[秒數] 來刪除，如果已經是 Terminating 狀態則無效。(SRE 同仁已測試過，與 Google Support 說明相同)\n那如果已經處於 Terminating 狀態，要怎麽讓 Pod 被順利刪除，這部分 Google Support 後續會在測試並給出建議，目前測試是：進去卡住的 Pod Container，手動刪除主進程 (pkill) 就可以了。\nGoogle Support 回覆"},"title":"部署 Pod 遇到 container veth name provided (eth0) already exists 錯誤"},"/blog/nginx/":{"data":{"":"此分類包含 Nginx 相關的文章。\n想使用 Nginx Upstream Proxy 到外部服務，並帶入對應的 header 該怎麼做？ Soketi WebSocket Server LOG 不定時出現 502 error 以及 connect() failed (111: Connection refused) "},"title":"Nginx"},"/blog/nginx/nginx-upstream-set-host-header/":{"data":{"":"此文章要來記錄一下最近在公司服務入口遇到的一些小問題，以及解決的方法。簡單說明一下，我們的服務入口是用 Nginx 來當作 proxy server，將不同路徑或是 servername 導到對應的後端程式，或是外部的服務上(例如 AWS cloudfront.net)，本篇要測試的是如果使用要同時使用 upstream 到外部服務，且需要帶 host header 該怎麼做。\nNginx 的 upstream 是什麼？\n通常我們 proxy_pass 的寫法會是這樣：\nlocation /aaa { proxy_pass http://aaa.example.com; } 當 Nginx 收到的 request 是 /aaa 時，就會將 request 轉發到 http://aaa.example.com。\n但假如後端有多台機器或是服務，可以處理同一種 request，這時候就可以使用 upstream 來處理：\nupstream backend_hosts { server aaa.example.com; server bbb.example.com; server ccc.example.com; } location /aaa { proxy_pass http://backend_hosts; } 這樣子的好處是可以有多個機器或是後端服務可以分散請求，做到負載平衡的效果。","參考資料#參考資料":"Make nginx to pass hostname of the upstream when reverseproxying：https://serverfault.com/questions/598202/make-nginx-to-pass-hostname-of-the-upstream-when-reverseproxying","問題#問題":"那如果我們使用 Nginx upstream 時，還想要同時帶 host 的 header 到後端該怎麼做呢？我們先來看一下目前的寫法：\n( 測試範例是使用 docker 來模擬，可以參考程式碼 \u003e 點我前往 github，會有三個 nginx，其中一個是負責 proxy 的 nginx 名為 proxy，另外兩台是 upstream 後的服務，名為 upstream_server1、upstream_server2 )\nnginx-old.conf upstream upstream_server { server upstream_server1; server upstream_server2; } server { listen 80; server_name localhost; location /upstream_server/ { proxy_pass http://upstream_server; proxy_set_header Host \"upstream_server1\"; proxy_set_header Host \"upstream_server2\"; access_log /var/log/nginx/access.log upstream_log; } } } 可以看到我們希望 Nginx 收到 request 是 /upstream_server 時，將 request 轉發到 http://upstream_server，而 upstream_server 後面有兩個 server，並且在 proxy 時，帶入兩個不同的 host header。但如果真的這樣寫，可以達到我們想要得效果嗎？我們實際跑看看程式 (範例可以使用 nginx-old.conf)：\nnginx 原本寫法\n從上面的 LOG 可以發現，我們 call /upstream_server 時，後端的 upstream_server1、upstream_server2 收到的 host 只會收到第一個設定的 Host，且服務會出現 400 Bad Request，查了一下網路文章，發現出現 400 Bad Request，可能跟 header 送太多資訊過去，詳細可以參考 解決網站出現 400 Bad Request 狀態的方法。\n這邊推測應該是後端如果也是用 nginx 直接接收才會遇到 400 的問題，還好目前公司服務還是正常的 xDD，檢查一下後發現，其實後端根本沒有要求對應 header 才能接收(應該是對方忘記加上此限制)。","解決#解決":"好，不管是否需要對應 header，我們還是找看看有沒有辦法同時使用 upstream，並帶入對應 host 的方法呢？\n最後參考網路上的文章，似乎只能使用兩層的 proxy，才能完成這兩個需求，我們來看看要怎麼寫吧 (範例可以使用 nginx.conf)：\nnginx.conf server { listen 777; server_name localhost; location / { proxy_pass http://upstream_server1; proxy_set_header Host \"upstream_server1\"; access_log /var/log/nginx/access.log upstream_log; } } server { listen 888; server_name localhost; location / { proxy_pass http://upstream_server2; proxy_set_header Host \"upstream_server2\"; access_log /var/log/nginx/access.log upstream_log; } } upstream upstream_server { server 127.0.0.1:777; server 127.0.0.1:888; } server { listen 80; server_name localhost; location /upstream_server/ { proxy_pass http://upstream_server; access_log /var/log/nginx/access.log upstream_log; } } 可以看到上面的程式碼，我們透過兩層的 proxy，來達到我們想要的效果，這樣子就可以同時使用 upstream，並且帶入對應的 host header。\n首先在 28 ~ 36 行，我們一樣如果 Nginx 收到 request 是 /upstream_server 時，會 proxy 到 upstream_server 這個 upstream 中，而 upstream_server 有兩個 server，分別是 127.0.0.1:777、127.0.0.1:888，但實際上沒有這兩個 port，所以我們需要再寫一層一般的 proxy 設定，分別是 1 ~ 10 行、12 ~ 21 行，這樣子就可以達到我們想要的效果。\n但這個方法比較適用於 upstream 後端沒有太多個服務或是機器的情況，如果有很多個服務或是機器，就需要寫很多的 proxy，這樣子會變得很麻煩，所以如果有更好的方法，也歡迎留言跟我分享 🤣。\n最後我們來看一下實際執行的結果：\n使用多層的 nginx proxy 處理"},"title":"想使用 Nginx Upstream Proxy 到外部服務，並帶入對應的 header 該怎麼做？"},"/blog/nginx/soketi-log-502-error/":{"data":{"":"此文章要來記錄一下 RD 同仁前陣子有反應使用 Soketi 這個 WebSocket Server 會不定時在 LOG 出現 502 error 錯誤訊息以及 connect() failed (111: Connection refused) while connecting to upstream，雖然說服務使用上不會影響很大，但還是希望我們可以協助找出 502 的原因。\n出錯的 LOG\n在開始找問題前，先簡單介紹一下 Soketi 是什麼東西好了，根據官網的說明，他是簡單、快速且有彈性的開源 WebSockets server，想要了解更多的可以到它官網去查看。\n另外會把程式碼相關放到 GitHub » 點我前往","參考資料#參考資料":"[Nginx] 解決 connect() failed (111: Connection refused) while connecting to upstream：https://wshs0713.github.io/posts/8c1276a7/\nWebSocket proxying：http://nginx.org/en/docs/http/websocket.html\nday 10 Pod(3)-生命週期, 容器探測：https://ithelp.ithome.com.tw/articles/10236314","壓測#壓測":"最後調整完，我們來測試看看是否在 Pod 自動重啟 or 更新 Deployment 的時候(並且有大量連線時)還會噴 502 error 或是 connect() failed (111: Connection refused)，我們這邊使用 k6 來做 websocket 服務的壓測，有簡單寫一個壓測程式如下：\nk6 壓測\nk6 是一個開源的壓測工具，可以用來測試 API、WebSocket、gRPC 等服務，可以到它的官網查看更多資訊。\nMacOS 安裝方式：brew install k6\nwebsocket.jsimport ws from \"k6/ws\"; import { check } from \"k6\"; export const options = { vus: 1000, duration: \"30s\", }; export default function () { const url = \"wss://socket.XXX.com/app/hex-ws?protocol=7\u0026client=js\u0026version=7.4.1\u0026flash=false\"; const res = ws.connect(url, function (socket) { socket.on(\"open\", () =\u003e console.log(\"connected\")); socket.on(\"message\", (data) =\u003e console.log(\"Message received: \", data)); socket.on(\"close\", () =\u003e console.log(\"disconnected\")); }); check(res, { \"status is 101\": (r) =\u003e r \u0026\u0026 r.status === 101 }); } 簡單說明一下上面程式在寫什麼，我們在 const 設定 vus 代表有 1000 個虛擬使用者，會在 duration 30s 內完成測試，下面的 default 就是測試連線 ws 以及 message 跟 close 等動作，最後需要回傳 101 (ws 交握)\n執行 k6 run websocket.js 後，就會開始壓測，可以看到會開始執行剛剛在上面提到 default 的動作：\nk6 壓測過程\n等到跑完，就會告訴你 1000 筆裡面有多少的 http 101，這邊顯示 status is 101，就代表都是 101，代表都有連線成功，沒有出現 502 error 或是 connect() failed (111: Connection refused) 的錯誤，這樣就代表我們的問題解決了。\nk6 壓測結果","解決過程#解決過程":"我們可以看到上方錯誤 LOG 中，發現有出現 502 error 以及 connect() failed (111: Connection refused) while connecting to upstream，這兩個錯誤都是由 Nginx 所產生的，那我們先來理解一下，Nginx 與 Soketi 之間的關係。\n在使用上，RD 的程式會打 Soketi 專用的 Subdomain 來使用這個 WebSocket 服務，而在我們的架構上，這個 Subdomain 會經過用 nginx proxy server，來轉發到 Soketi WebSocket Server (走 k8s svc)，設定檔如下圖：\n入口 nginx 設定\n然後會出現 connect() failed (111: Connection refused) while connecting to upstream 的錯誤訊息，代表我們的 Nginx 設定少了一個重要的一行設定，就是 proxy_http_version 1.1;，這個設定要讓 Nginx 作為 proxy 可以和 upstream 的後端服務也是用 keepalive，必須使用 http 1.1，但如果沒有設定預設是 1.0，也要記得設定 proxy_set_header Upgrade、proxy_set_header Connection。調整過後就變成：\nws.confserver { server_name socket.XXX.com; listen 80 ; listen [::]:80 ; listen 443 ssl; listen [::]:443 ssl; ssl_certificate /etc/nginx/ingress.gcp.cert; ssl_certificate_key /etc/nginx/ingress.gcp.key; access_log /var/log/nginx/access.log main; location / { proxy_pass http://soketi-ws-ci:6001; proxy_connect_timeout 10s; proxy_read_timeout 1800s; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \"Upgrade\"; proxy_set_header X-Real-IP $remote_addr; } } 解決完 connect() failed (111: Connection refused) 這個問題後，接下來就是要解決 502 error 這個問題，會導致 502 代表 Nginx 這個 proxy server 連不上後端的 Soketi WebSocket Server，再觀察 LOG 以及測試後發現，當 Pod 自動重啟，或是手動重啟 Deployment 的時候，就會有 502 的錯誤，代表 Nginx 在 proxy 到後面的 Soketi svc 再到 Pod 的時候，有一段時間是連不上的，所以就會出現 502 的錯誤，可以推測是流量進到正在關閉的 Pod 或是進到還沒有啟動好的 Pod 才導致的。\n那我們先來看一下 Soketi WebSocket Server 的服務 yaml 檔案：\ndeployment.yaml deployment.yaml spec: terminationGracePeriodSeconds: 30 securityContext: {} containers: - name: soketi securityContext: {} image: \"quay.io/soketi/soketi::1.6.0-16-alpine\" ... 省略 (可以到 github 看 code)... livenessProbe: failureThreshold: 3 httpGet: httpHeaders: - name: X-Kube-Healthcheck value: \"Yes\" path: / port: 6001 initialDelaySeconds: 5 periodSeconds: 2 successThreshold: 1 可以看到原來的設定只有 livenessProbe 而已，因此我們為了要避免流量進到正在關閉的 Pod 或是進到還沒有啟動好的 Pod，所以我們需要加上 readinessProbe 以及 preStop，讓 Pod 確定啟動完畢，或是等待 Service 的 endpoint list 中移除 Pod，才開始接收流量，這樣就可以避免出現 502 的錯誤。\ndeployment.yaml spec: terminationGracePeriodSeconds: 30 securityContext: {} containers: - name: soketi securityContext: {} image: \"quay.io/soketi/soketi::1.6.0-16-alpine\" ... 省略 (可以到 github 看 code)... livenessProbe: failureThreshold: 3 httpGet: httpHeaders: - name: X-Kube-Healthcheck value: \"Yes\" path: / port: 6001 initialDelaySeconds: 5 periodSeconds: 2 successThreshold: 1 readinessProbe: failureThreshold: 3 httpGet: httpHeaders: - name: X-Kube-Healthcheck value: \"Yes\" path: /ready port: 6001 initialDelaySeconds: 5 periodSeconds: 2 successThreshold: 1 lifecycle: preStop: exec: command: [\"/bin/sh\", \"-c\", \"sleep 20\"] Pod 終止的過程"},"title":"Soketi WebSocket Server LOG 不定時出現 502 error 以及 connect() failed (111: Connection refused)"},"/blog/opentelemetry/":{"data":{"":"此分類包含 Opentelemetry 相關的文章。\n什麼是 Opentelemetry？可觀測性 (Observability) 又是什麼？ 如何透過 OpenTelemetry 來收集 Ingress Nginx Controller 的 Metrics 與 Traces 並送到 Datadog 上 "},"title":"Opentelemetry"},"/blog/opentelemetry/opentelemetry-ingress-nginx-controller/":{"data":{"":"由於最近公司想要導入 Datadog，在測試過程中順便導入 OpenTelemetry 來收集 Metrics 與 Traces 並送到 Datadog 上 ～\n🔥 這個範例比較特別，因為 Datadog 有提供 Ingress Nginx Controller 的 integrations，可以透過 Datadog Agent 來收集 Metrics，不需要透過 OpenTelemetry Collector 來收集。 ( Datadog Agent 請參考：https://docs.datadoghq.com/containers/kubernetes/ )\n程式部分也同步上傳到 github 上，可以點我前往","參考資料#參考資料":"Configure Nginx Ingress Controller to use JSON log format：https://dev.to/bzon/send-gke-nginx-ingress-controller-logs-to-stackdriver-2ih4\n淺談 OpenTelemetry - Collector Compoents：https://ithelp.ithome.com.tw/articles/10290703","執行步驟#執行步驟":" 先 clone 這個 repo (廢話 xD)\n先建立 OpenTelemetry Collector，執行以下指令：\nhelm upgrade collector \\ opentelemetry-collector \\ --repo https://open-telemetry.github.io/opentelemetry-helm-charts \\ --install \\ --create-namespace \\ --namespace opentelemetry \\ -f \"otel-collector.yaml\" 再建立 Ingress Nginx Controller，執行以下指令：\nhelm upgrade ingress-nginx \\ ingress-nginx \\ --repo https://kubernetes.github.io/ingress-nginx \\ --install \\ --create-namespace \\ --namespace ingress-nginx \\ -f \"ingress-nginx-values.yaml\" 接著建立測試用 Nginx 服務，執行以下指令：\nkubectl apply -f nginx.yaml ","檔案說明#檔案說明":" otel-collector.yaml： OpenTelemetry Collector 的設定檔，主要是設定要收集哪些 metrics、traces，並且要送到哪個 exporter，要注意的是 exporters 的 datadog 需要設定 site、api_key，以及 image 要記得用 otel/opentelemetry-collector-contrib，才會有 datadog 的 exporter。\ningress-nginx-values.yaml： Ingress Nginx Controller 的設定檔，這邊的 podAnnotations 是為了讓 Ingress Nginx Controller 的 Pod 能夠透過 Datadog agent 收集 metrics 到 Datadog 才加上的。\nconfig 裡面的設定有很多，主要都是 openTelemetry 的設定，要注意的是 enable-opentelemetry 要設為 true，另外 otlp-collector-host 以及 otlp-collector-port 要送到哪個 collector 等等也要記得設定。 另外如果想要將 LOG 與 Trace 串再一起，記得要把 log-format 設為 json，並且帶入，trace_id 與 span_id ( 這邊有多帶 dd.trace_id 是為了讓 datadog 可以自動串接 LOG \u0026 Trace )。\nnginx.yaml： 一個簡單的 Nginx 整套服務 (Deployment、Service、Ingress)，要注意的是 Ingress 需要設定 annotations kubernetes.io/ingress.class: nginx (這個是 Ingress Nginx Controller 的預設 class name)，才會被 Ingress Nginx Controller 接管 (才會有 Load Balancer 的 IP)","測試#測試":"當你執行完上面的步驟後，你會發現有產生兩個 namespace，一個是 ingress-nginx，另一個是 opentelemetry，並且會有 OpenTelemetry Collector、Ingress Nginx Controller、Nginx 等服務，如下：\n啟動服務\n我們試著打 http://nginx.example.com/ (測試網址，需要先在 /etc/hosts 綁定 Ingress Nginx Controller 咬住的 Load Balancer IP)，查看一下 Datadog 的 LOG，看看是否有收到 Nginx 的 LOG (此收集 LOG 的方式是透過在 cluster 上安裝 Datadog 的 agent)，如下：\nDatadog LOG\n接著查看 Datadog APM 的 trace，如下：\nDatadog APM\n由於我們在後面目前沒有串其他服務，所以只有一個 span，之後還有另外兩篇文章是介紹如何串其他服務 (會增加服務以及部分設定)，可以參考看看：opentelemetry-roadrunner、opentelemetry-nodejs\n順便看一下透過 Datadog Agent 收集的 Ingress Nginx Controller 的 Metrics，如下：\nDatadog Ingress Nginx Controller 的 Metrics\n可以用這些 Metrics 來做 Dashboard，如下：\nDatadog Dashboard","結論#結論":"透過 OpenTelemetry Collector 來收集 Ingress Nginx Controller 的 Metrics 與 Traces 並送到 Datadog 上，這樣就可以透過 Ingress Nginx Controller 的 Metrics 來做監控了，對於 RD 再開發上，有 Traces 也更方便 RD 他們找到程式的瓶頸 (有可能是服務導致的)。"},"title":"如何透過 OpenTelemetry 來收集 Ingress Nginx Controller 的 Metrics 與 Traces 並送到 Datadog 上"},"/blog/opentelemetry/opentelemetry-observability/":{"data":{"":"在介紹 Opentelemetry 之前，我們要先了解一下目前軟體架構以及基礎設施的演進：\n軟體架構以及基礎設施的演進\n第一階段在軟體架構設計上較為簡單，不會有什麼特別需要拆分出來的程式，所以都是一整包的程式，再測試以及除錯也比較不會有什麼問題。基礎設施都是使用 VM 或是使用放在 IDC 的機房來當 Server。\n第二階段隨著雲端技術的推出，會開始將服務搬上雲供應商提供的 IaaS 服務，或是使用私有雲給企業放置較機密的內容，其他則放置公有雲上，達成混合雲的模式。\n第三階段隨著雲端技術越來越成熟，有更多的雲端 IaC 以及功能推出，會開始考慮使用分散式的系統架構，將 DB 等服務也改用 Cloud SQL 的方式。在基礎設施上也隨著容器化的技術成熟而進入新的時代。\n第四階段已經使用 docker 來管理好一陣子，但發現虛擬容器技術在管理上十分不方便，因此 K8s 逐漸盛行，將架構從分散式改成微服務的方式進行，在讓開發團隊使用上可以更靈活且容易。\n雖然使用 K8s 可以讓我們的服務更靈活方便，但也會將服務切的越來越細，這時會讓開發變的十分複雜，我們在架構上從一開始的單體式架構，變成分散式架構，再到最後的微服務，讓開發人員需要處理的事情會越來越多。服務要如何連線？Log 要如何記錄？以及當一個請求會經過多個服務時，相對的延遲也會增加，這時要怎麼去處理等。在監控上，因為服務切分得很細，當線上有一個服務有問題時，要如何快速的找到問題點也是一大挑戰。\n當我們使用分散式系統或是微服務時發生故障時，會很難快速的恢復服務，因為每個服務都互相依賴，在以往都是透過經驗以及對系統的了解來得以解決。那有什麼其他的方式，能夠讓我們更快掌握每個服務呢？我們先來了解一個名詞：可觀測性(Observability)","opentelemetry#Opentelemetry":"那我們這次要介紹的可觀測性(Observability)工具就是 Opentelemetry，縮寫 OTel，它是由 CNCF (Cloud Native Computing Foundation) 組織孵化的開源專案，在 2021 年 5 月由 OpenTracing 與 OpenCensus 兩個框架合併，結合兩項分散式追蹤框架最重要的特性成為下一代收集遙測數據的新標準。\ntelemetry 又叫做遙測，是指能夠跨越不同系統來收集資料 (包含 LOG、Metric、trace) 的能力。\n我們可以看一下官網的說明：\nOpentelemetry 是雲原生的可觀測性(Observability)框架，提供標準化的 API、SDK 與協議自動檢測、蒐集、導出遙測數據資料 (Metrics、Log、Trace)，並支援 W3C 定義的 Http trace-context 規範，降低開發者在搜集遙測數據上的困難度，以及方便進行後續分析以及性能的優化。\nOpentelemetry\n在 OpenTelemetry 核心元件如下：\nAPI：開發人員可以透過 OpenTelemetry API 自動生成、蒐集應用程式(Application)的遙測數據資料(Metrics, Log, Trace)，每個程式語言都需實作 OpenTelemetry 規範所定義的 API 方法簽章。\nSDK：是 OpenTelemetry API 的實現。\nOTLP：規範定義了遙測數據的編碼與客戶端及服務器之間如何交換的協議 (gRPC、HTTP)。\nCollector：OpenTelemetry 中儲存庫，用於接收、處理、導出遙測數據到各種後端平台。","參考資料#參考資料":"Observability：https://linkedin.github.io/school-of-sre/level101/metrics_and_monitoring/observability/\n[OpenTelemetry] 現代化監控使用 OpenTelemetry 實現 : 可觀測性(Observability)：https://marcus116.blogspot.com/2022/01/modern-monitoring-using-openTelemetry-with-Observability.html\n[OpenTelemetry] 現代化監控使用 OpenTelemetry 實現 : OpenTelemetry 開放遙測：https://marcus116.blogspot.com/2022/01/opentelemetry-opentelemetry.html\n淺談 Observability(下)：https://ithelp.ithome.com.tw/m/articles/10287598\nManage services, spans, and traces in Splunk APM：https://docs.splunk.com/Observability/apm/apm-spans-traces/traces-spans.html","可觀測性observability#可觀測性(Observability)":"可觀測性有三個重要的特性，分別是：\nMetrics 負責監控系統有什麼狀況，當要發生服務故障前可以透過設定閥值搭配告警提早得知。\nLogs 當問題發生時，可以用來查看故障時正在執行哪些服務，以及產生的錯誤資訊。\nTraces （後面詳細介紹）\n可觀測性三大支柱\n我們對於 Metrics 跟 Logs 有基本的了解，所以我這邊會注重在 Traces 的部分：\n當有多個微服務的複雜分散式系統，用戶的請求會由系統中的多個微服務進行處理。Metrics 跟 Logs 可以提供我們有關系統如何去處理這些請求的一些資訊，但沒有辦法提供微服務的詳細訊息以及他是如何影響客戶端的請求。這時候就需要透過 Trace 來協助我們追蹤。\nTrace 可以在連續的時間維度上，透過 Trace 以及 Span 關聯，把空間給排列展示出來，並且有 Trace-Context 規範，能夠直觀的看到請求在分散式系統中經過所有服務的紀錄。\n什麼是 Trace、Span 、Trace-Context 呢？\n我們先說 Span，Span 又可以叫跨度，是系統中最小的單位，可以看下方圖片，SpanA 的資料是來源 SpanB，SpanB 來源是 SpanC 等等，每一個 Span 可以把它想成一個請求後面所有經過服務的工作流程，例如：nginx_module、db、redis 等等。\n請求的整個過程叫做 Trace，那他要怎麼知道 SpanA ~ SpanE 是同一個請求呢？\n就需要透過 TraceID 以及 SpanID 來記錄：\nTraceID：是唯一的 ID，用於識別整個分散式追蹤的一條請求路徑。在下方圖片中，當請求進入時，就會被賦予一個 TraceID，所有有經過的 Span 都會記錄此 TraceID，這樣才可以把不同服務依據 TraceID 關聯成同一個請求。\nSpanID：是一條請求路徑中單個操作唯一的 ID。追蹤路徑是由多個 Span 組成，每個 Span 都代表一個操作或特定的時間段。當請求進入時，每個服務就會產生一個 Span 來代表它處理請求的的時間。這些 Span 使用 TraceID 來連接再一起，形成完整的請求追蹤。\nTrace 示意圖\n那要怎麼查看每個 Span 的紀錄內容呢，就需要 Trace-Context：\n會放置一些用於追蹤和識別請求的上下文信息，例如 Trace ID、Span ID 和其他相關的數據。這些上下文信息可以是一些關鍵的數據，可以幫助我們在整個分佈式系統中追蹤請求的路徑，並將相關請求和操作關聯起來。\nECK Trace 示意圖\n上面的圖片中，可以看到 call /product/XXXX 後，會經過需多的 Span，隨便點擊一個 Span 可以看到它記錄的 Trace-Context，以及都會包含 TraceID 及 SpanID\nECK Trace 示意圖\nTrace 優點可以看到跨維度看到中間的資訊，對於找到問題以及瓶頸十分方便，但缺點就是因為需要在 Span 中產生 ID 以及內容，需要在程式裡面加入一定的套件以及調整程式碼。\n所以我們在可觀測性(Observability)最終的目的是希望可以透過可觀測性工具讓我們知道：\n請求通過哪些服務 每個服務在處理請求時做了些什麼 如果請求很慢，瓶頸在哪邊 如果請求失敗，錯誤點在哪 請求的路徑是什麼 為什麼花這麼長的時間 "},"title":"什麼是 Opentelemetry？可觀測性 (Observability) 又是什麼？"},"/blog/other/":{"data":{"":"先放置還沒有想到的分類 Blog 文章。\n如何啟用 GitLab 的 Package Registry 以及將儲存位置從伺服器改到 GCS 上 Bookstack 開源知識庫筆記平台安裝 (K8s + docker) "},"title":"其他(還想不到分類)"},"/blog/other/bookstack/":{"data":{"":"最近剛好有公司同事離職，想要把交接的資料給整理整理，雖然部門之前有架設專用的 wiki 給 RD 使用，但覺得介面沒有到很好用，於是就在網路上尋找，可以多人編輯的筆記系統，一開始有想過用 CodiMD (HackMD)，但考量到需要多層的架構來區分文件，最後選擇 Bookstack 這個開源知識庫筆平台來作為組內的筆記系統，以下會簡單說一下 Bookstack 的特色以及使用 K8s 跟 docker 的安裝教學。","bookstack-介紹#Bookstack 介紹":"介紹部分主要參考 Bookstack 簡介，以下列出會選擇它的三個特色：\n簡潔的書本列表模式 書架分類\n書本分類\n頁面章節分類\n最主要是因為有以上幾個不同的分層架構，在資料整理上會更方便、更好彙整，可以自訂書架以及書本、頁面或是章節的封面以及內容。\n強大的搜尋功能 搜尋功能\n當我們的筆記內容越來越多，雖然有上面提到的分類模式，想要找到內容還是需要花一段時間，但 Bookstack 有強大的搜尋功能，可以針對書架、書本、章節或是書面個別搜尋，可以利用時間、標籤、標題或是內容來快速找到想要的內容。\n畫圖功能 在討論系統或是程式的架構，最好的方式就是用畫圖的來表示。在以往都是使用 Draw.io 來畫圖，當畫完圖後需要匯出畫好的圖以外，如果怕畫圖的原檔消失，還需要再另外下載原檔來保留，很不方便，又怕忘記下載，而 Bookstack 內建可直接編輯的功能，當畫完圖後有問題，可以直接點擊圖片來編輯，而這些功能還會搭配內建的版控，若有問題還可以還原到正確的圖片版本。\n畫圖功能","參考資料#參考資料":"BookStack 簡介：https://docs.ossii.com.tw/books/bookstack/page/bookstack\nBookStack Installation：https://www.bookstackapp.com/docs/admin/installation/","安裝說明#安裝說明":"那簡單說明為什麼會選擇 Bookstack 我們就來安裝它，這邊有使用 K8s + docker 來測試安裝，那我們就一起來看程式碼吧，程式碼放在這 👈：\nK8s namespace.yamlapiVersion: v1 kind: Namespace metadata: name: bookstack 我習慣會將不同的服務切 namespace 來部署，大家可以依照習慣來使用，下方的 yaml 都是建在此 namespace 上。\ndeployment.yamlapiVersion: apps/v1 kind: Deployment metadata: name: bookstack namespace: bookstack labels: app: bookstack spec: replicas: 1 selector: matchLabels: app: bookstack template: metadata: labels: app: bookstack spec: containers: - name: bookstack image: linuxserver/bookstack ports: - name: http containerPort: 80 env: - name: DB_DATABASE value: bookstack - name: DB_HOST value: \u003c\u003c更換此處\u003e\u003e - name: DB_PORT value: \"3306\" - name: DB_PASSWORD value: \u003c\u003c更換此處\u003e\u003e - name: DB_USERNAME value: \u003c\u003c更換此處\u003e\u003e - name: MAIL_USERNAME value: example@test.com - name: MAIL_PASSWORD value: mailpass - name: MAIL_HOST value: smtp.server.com - name: MAIL_PORT value: \"465\" - name: MAIL_ENCRYPTION value: SSL - name: MAIL_DRIVER value: smtp - name: MAIL_FROM value: no-reply@test.com - name: APP_URL value: https://\u003c\u003c更換此處\u003e\u003e - name: APP_LANG value: zh_TW - name: APP_TIMEZONE value: Asia/Taipei resources: limits: cpu: \"0.5\" memory: \"512Mi\" 必要更換的參數有標示 «更換此處»，其餘可以依照各組織來自行配置，Bookstack 會將內容存在 db，圖片等存在 pod 中，需要永久保存請使用 pvc + pv 或是另外掛 nas。\nsvc.yamlapiVersion: v1 kind: Service metadata: name: bookstack namespace: bookstack spec: type: NodePort selector: app: bookstack ports: - name: http protocol: TCP port: 80 targetPort: 80 ingress.yamlapiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: bookstack-ingress namespace: bookstack annotations: kubernetes.io/ingress.class: nginx service.beta.kubernetes.io/do-loadbalancer-enable-proxy-protocol: \"true\" spec: rules: - host: \u003c\u003c更換此處\u003e\u003e http: paths: - path: / pathType: Prefix backend: service: name: bookstack port: number: 80 最後透過 ingress 的 domain 去訪問 svc \u003e pod 上，就完成部署拉～第一次登入要使用預設帳號及密碼 admin@admin.com/password\ndocker docker-compose.yamlversion: \"3.8\" services: bookstack: image: lscr.io/linuxserver/bookstack container_name: bookstack environment: - PUID=\u003c\u003c更換此處\u003e\u003e - PGID=\u003c\u003c更換此處\u003e\u003e - DB_HOST=bookstack_db - DB_PORT=3306 - DB_USER=bookstack - DB_PASS=bookstack - DB_DATABASE=bookstackapp - APP_LANG=zh_TW - APP_TIMEZONE=Asia/Taipei - APP_URL=\u003c\u003c更換此處\u003e\u003e - GOOGLE_APP_ID=\u003c\u003c更換此處\u003e\u003e - GOOGLE_APP_SECRET=\u003c\u003c更換此處\u003e\u003e volumes: - /bookstack/config:/config ports: - 80:80 restart: unless-stopped depends_on: - bookstack_db bookstack_db: image: lscr.io/linuxserver/mariadb container_name: bookstack_db environment: - PUID=\u003c\u003c更換此處\u003e\u003e - PGID=\u003c\u003c更換此處\u003e\u003e - MYSQL_ROOT_PASSWORD=bookstack - TZ=Asia/Taipei - MYSQL_DATABASE=bookstackapp - MYSQL_USER=bookstack - MYSQL_PASSWORD=bookstack volumes: - /bookstack/config:/config restart: unless-stopped docker 的部分就更簡單了，一樣是把 «更換此處» 換成對應的內容即可，env 的部分可以參考官方文件，這邊比較特別的是我們有多使用 Google 來 Oauth 登入，Bookstack 支援多種的 Oauth 登入方式，可以參考 Third Party Authentication。\n架設好 Bookstack 就可以開始寫部門或是組織內的筆記拉～ 如果有開放外網連線，要記得修改預設的管理員帳號，以及用管理員帳號登入，到功能與安全的公開存取給取消，這樣就必須要登入才可以瀏覽筆記了！那就交給大家自己去玩這個好用的筆記工具囉～～ 😍\n功能與安全設定"},"title":"Bookstack 開源知識庫筆記平台安裝 (K8s + docker)"},"/blog/other/gitlab-package-registry-to-gcs/":{"data":{"":"今天接到一個案子，RD 部門之後想要使用 GitLab 的 Package Registry 功能來發布套件，且不想把它存在 GitLab 伺服器上，希望可以直接存到 GCP 的 Google Cloud Storage 上，所以才會有了此篇筆記來記錄一下整個過程。\n版本資訊\nGitLab 14.10 (有部分設定會於新版本棄用，請記得確認好自己的版本是否支援) 先說一下，我們的 GitLab 是使用 docker-compose 來建置，所以後續的實作內容都會以 docker-compose 的方式來介紹。","先查看尚未重啟的-gitlab-package#先查看尚未重啟的 GitLab Package":"由於公司 GitLab 預設有先開啟 packages_enabled，所以我就拿同事用 Helm 寫的 CI，來做測試。當更新 value.yaml 後會自動打包 Package 放到 Package Registry 中，我們直接進入到預設 Package Registry 的儲存位置，是在 /var/opt/gitlab/gitlab-rails/shared/packages/，用指令發現打包的 Package 的確存放於此 ，如下：\n檢查是否還有 Package 在預設儲存位置 (尚未遷移)","參考資料#參考資料":"GitLab Package Registry administration：https://docs.gitlab.com/14.10/ee/administration/packages/","啟動-gitlab-的-package-registry#啟動 GitLab 的 Package Registry":"首先，我們當然要先啟動這項 Package Registry 功能，才可以再之後使用它，我們先看一下 GitLab 啟動的 docker-compose.yml 檔案：\nversion: '3' services: gitlab: image: 'gitlab/gitlab-ee:14.10.5-ee.0' restart: always container_name: gitlab hostname: gitlab-pid logging: driver: \"json-file\" options: max-size: \"100m\" max-file: \"50\" environment: GITLAB_OMNIBUS_CONFIG: | external_url '${GITLAB_DOMAIN}' letsencrypt['enable'] = false gitlab_rails['initial_root_password'] = '${GITLAB_ROOT_PASSWORD}' gitlab_rails['gitlab_shell_ssh_port'] = '${GITLAB_HOST_SSH_PORT}' gitlab_rails['backup_keep_time'] = 79200 gitlab_rails['omniauth_allow_single_sign_on'] = ['google_oauth2'] gitlab_rails['omniauth_block_auto_created_users'] = false gitlab_rails['omniauth_sync_profile_from_provider'] = ['google_oauth2'] gitlab_rails['omniauth_sync_profile_attributes'] = ['name', 'email'] gitlab_rails['omniauth_providers'] = [ { 略過．．． } ] ports: - '${GITLAB_HOST_SSH_PORT}:22' - '${GITLAB_HOST_HTTP_PORT}:80' - '${GITLAB_HOST_HTTPS_PORT}:443' volumes: - './config:/etc/gitlab' - './logs:/var/log/gitlab' - './data:/var/opt/gitlab' 有些設定有略過或是省略不寫，大家就依照自己的設定來看就好～","新增-package-registry-設定#新增 Package Registry 設定":"我們在上方的 gitlab_rails['omniauth_providers'] = [ ... 略 ... ] 之後加上新增 Package Registry 設定內容：\ngitlab_rails['packages_enabled'] = true gitlab_rails['packages_object_store_enabled'] = true gitlab_rails['packages_object_store_remote_directory'] = \"GCS 名稱\" gitlab_rails['packages_object_store_direct_upload'] = true gitlab_rails['packages_object_store_background_upload'] = true gitlab_rails['packages_object_store_proxy_download'] = true gitlab_rails['packages_object_store_connection'] = { 'provider' =\u003e 'Google', 'google_project' =\u003e '專案 ID', 'google_json_key_location' =\u003e '/etc/gitlab/google_key.json' } packages_enabled：啟動 packages packages_object_store_enabled：啟動 packages 對象存儲 packages_object_store_remote_directory：設定 packages 對象存儲位置，這邊要輸入 GCS 的名稱 packages_object_store_direct_upload：設定是否可以直接上傳到對象存儲位置 packages_object_store_background_upload：設定是否以後台方式上傳到對象存儲位置 packages_object_store_proxy_download：設定是否可以透過代理伺服器進行套件下載 packages_object_store_connection：設定連接到對象存儲，由於我們要存到 GCS 上面，需要有這三項 provider、google_project、google_json_key_location 才可以將 packages 存到 GCS 上。如果想用其他的儲存位置，例如 Amazon S3、Azure Blob storage 可以參考 Object storage 詳細設定 ( 其中的 google_json_key_location 是要放可以讀寫 GCS 的 SA SECRET 檔案 ) ","重啟設定後再次檢查-gitlab-package#重啟設定後再次檢查 GitLab Package":"當我們重啟設定後，也有建立好可供我們權限 SA 的 GCS 後，會發現原本存在預設 /var/opt/gitlab/gitlab-rails/shared/packages/ 沒有自動跑到 GCS 上，是因為我們還需要手動下指令將他遷移過去，指令是 gitlab-rake \"gitlab:packages:migrate\"，最後等他跑完我們在檢查一下預設儲存位置就發現已經沒有 Package 了\n檢查是否還有 Package 在預設儲存位置 (已遷移)\n開 GCS 網站來看會發現原先在預設儲存位置的 Package 都可以跑到 GCS 上：\n查看已遷移到 Google Cloud Storage 的 Package"},"title":"如何啟用 GitLab 的 Package Registry 以及將儲存位置從伺服器改到 GCS 上"},"/blog/terraform/":{"data":{"":"此分類包含 Terraform 相關的文章。\nTerraform 如何多人共同開發 (將 tfstate 存在後端) 如何將 Terraform 改寫成 module ? 如何導入 Terragrunt，Terragrunt 好處是什麼？ "},"title":"Terraform"},"/blog/terraform/terraform-module/":{"data":{"":"當我們要管理的資源越來越多後，會產生很多的 tf 檔案，假設我們現在有三個 gce 服務，會在以下三個不同環境上面運作，每個環境都會有我們之前學會的基本 tf 檔案(包含 provider.tf 、main.tf、backend.tf)，且其中的 main.tf 檔案內有些設定會不太一樣，如下：\ndev backend.tf main.tf provider.tf prod backend.tf main.tf provider.tf qa backend.tf main.tf provider.tf dev/main.tfresource \"google_compute_instance\" \"instance\" { project = \"馬賽克\" name = \"test-dev\" machine_type = \"e2-small\" zone = \"asia-east1-b\" boot_disk { initialize_params { image = \"debian-cloud/debian-10\" size = 50 } } .... 其他省略不寫 .... } qa/main.tf (多了 tags)resource \"google_compute_instance\" \"instance\" { project = \"馬賽克\" name = \"test-qa\" machine_type = \"e2-small\" zone = \"asia-east1-b\" tags = [\"for-qa\"] boot_disk { initialize_params { image = \"debian-cloud/debian-10\" size = 50 } } .... 其他省略不寫 .... } prod/main.tf (多了 labels)resource \"google_compute_instance\" \"instance\" { project = \"馬賽克\" name = \"test-prod\" machine_type = \"e2-small\" zone = \"asia-east1-b\" labels = { aaa = \"test1\" bbb = \"test2\" ccc = \"test3\" } boot_disk { initialize_params { image = \"debian-cloud/debian-10\" size = 50 } } .... 其他省略不寫 .... } 可以看到三個 main.tf 檔案除了 name 以外，在 qa 還多了 tags、prod 多了 labels 等設定，等於我們會依照每個不同環境不同服務去客製化他的 tf 資源設定，雖然非常直覺，但往後的維護以及調整卻非常不方便 ( 假設我們現在要全部都加上 labels，就必須一個一個檢查並調整 )。\n為了方便我們維護以及重複使用，因此有了 module，可以先將全部會使用到的設定寫成模板，透過參數的方式帶入即可，module 有以下幾個優點：\n重複使用性： module 讓程式碼更易於重複使用。當我們需要在多個項目中使用相同的基礎架構或配置時，可以先將其封裝為一個 module。這樣，我們只需要在不同的項目中引用並調整模組的參數，而不需要重新寫整個 tf 檔。\n抽象化：將 Terraform 代碼轉換為 module 可以將詳細的實現細節抽象化，僅寫必要的參數。這樣做可以提高程式碼的可讀性和可維護性，並降低使用者學習和使用的門檻。\n參數化配置：module 可以使用輸入參數來接收不同的配置值。這意味著您可以根據需要動態更改模組的行為，而不需要直接修改模組的內部程式。這使得配置更靈活並支持不同環境的部署。\nmodule 版本控制：將 Terraform 程式封裝為 module 後，可以使用 git 對其進行版本控制。可以更輕鬆地協作和共享 module (可以將 module 與 Terraform 分別存放，並使用對應 tag or 分支來做開發 )。","參考資料#參考資料":"Types and Values：https://developer.hashicorp.com/terraform/language/expressions/types","實作#實作":"當我們完成上面的架構後，我們進入 projects/prod/main.tf 路徑下，開始用 module 的方式建立資源，建立資源的流程與原本的相同，一樣是 init \u003e plan \u003e apply 這三個步驟，那我們一個一個來看，與原本的建立方式有哪些不同之處吧～\ninit 我們使用 terraform init 來看看原本 init 與使用模組 init 後差在哪裡：\n原先 terraform init 結果\n使用 module init 結果\n可以看到有使用 module 在初始化的時候，會連同 module 也一併初始化，接著我們進到 .terraform 資料夾內，可以看到有 moduels 資料夾。\n.terraform 檔案差異\n在進去看會看有一個 modules.json 檔案，會紀錄 module 使用的路徑，因此當我們使用的 module 有改變時，要記得重新 init 才可以確保使用的 module 是正確的。\n使用 module 會多一個 modules.json 檔案\nplan 我們一樣下 terraform plan 指令，來看看兩者顯示的差異：\n原先 terraform plan 結果\n使用 module plan 結果\n可以看到使用 module 在 plan 時，預覽創建的資源格式不同，也就代表他存在 tfstate 檔案的格式也會不同 (這個後面會在提到，與 import 也有關係)\napply 使用 terraform apply 來看建立資源後的結果有什麼不同：\n原先 terraform apply 結果\n使用 module apply 結果\napply 看到的與 plan 顯示的一樣，使用 module 建立的資料格式會不太一樣，所以我們來看看兩者 tfstate 檔案的差異：\n原先 terraform 建立的 tfstate 檔案\n使用 module 建立的 tfstate 檔案\nimport import 的功用是可以從雲上服務轉成 tf，在之前原本的 terraform 是要先建立一個空的 resource：\nresource \"google_compute_instance\" \"instance\" { } 再使用 terraform import google_compute_instance.instance 專案ID/機器地區/機器名稱 來匯入雲上服務的狀態到後端存到 tfstate 的位子。\n原先 terraform import 線上服務\n那我們現在改成 module，會比較麻煩一點，因為我們有在 variables.tf 設定我們的變數，若是沒有設定預設值，就必須一定要輸入，所以我們在建立時，要先把變數的空值也補上，如下：\nmodule \"ian-test\" { source = \"../../module/google_compute_instance\" project_id = \"\" instance_name = \"\" machine_type = \"\" instance_zone = \"\" instance_tags = [] instance_labels = {} boot_disk_image_name = \"\" boot_disk_size = 50 attached_disk_enabled = false network_name = \"\" subnetwork_name = \"\" nat_ip_enabled = false metadata = {} resource_policies = [] service_account_email = \"\" service_account_scopes = [] } \" \" 是 string 格式的空值，[ ] 是 list 格式的空值，{ } 是 map 格式的空值，其他的 bool 我預設會給他 false，number 我會隨便給他一個數字 xD。這邊帶入的內容不是很重要，主要是讓他可以去抓到他的架構，我們也可以在 variables.tf 設定時都補上預設值。\n再使用 terraform import module.ian-test.google_compute_instance.instance 專案ID/機器地區/機器名稱 來匯入狀態檔案。(這邊要記得依照你 module 設定的名稱帶入)\n使用 module import 線上服務","檔案說明#檔案說明":"\n首先我們要先定義我們的 module，我們先建立以下資料夾結構以及對應檔案：(同步到 GitHub 需要程式碼的可以前往查看)\n(再次提醒，會區分檔案名稱是因為方便調整跟維護，也可以把它全部寫在同一個 tf 檔案內歐)\nmodule google_compute_instance main.tf outputs.tf variables.tf projects dev backend.tf main.tf provider.tf prod backend.tf main.tf provider.tf qa backend.tf main.tf provider.tf module 資料夾：放我們 module 設定 (這邊範例是放 gce)\nprojects 資料夾：放我們不同服務、不同環境設定 (這邊為了簡化，範例只以不同環境為例)\nmodule/google_compute_instance/main.tfprovider \"google\" { project = var.project_id zone = var.instance_zone } resource \"google_compute_instance\" \"instance\" { name = var.instance_name machine_type = var.machine_type zone = var.instance_zone tags = var.instance_tags labels = var.instance_labels boot_disk { auto_delete = var.boot_disk_auto_delete initialize_params { image = var.boot_disk_image_name size = var.boot_disk_size } } dynamic \"attached_disk\" { for_each = var.attached_disk_enabled ? [1] : [] content { device_name = var.attached_disk_name mode = var.attached_disk_mode source = var.attached_disk_source } } network_interface { network = var.network_name subnetwork = var.subnetwork_name dynamic \"access_config\" { for_each = var.nat_ip_enabled ? [1] : [] content { } } } metadata = var.metadata enable_display = var.enable_display resource_policies = var.resource_policies service_account { email = var.service_account_email scopes = var.service_account_scopes } timeouts {} deletion_protection = var.deletion_protection allow_stopping_for_update = var.allow_stopping_for_update } 我們需要把所有設定的值都挖洞，使用 var 的方式來帶入參數，這邊要注意的是等號前面的值或是 block 名稱都是不能修改的，他是 google 定義的 api 變數，但 var 後的參數名稱我們可以自訂 (後面 variable.tf 會在詳細說明)，那這邊比較特別的用法是 dynamic，以下說明：\ndynamic \"attached_disk\" { for_each = var.attached_disk_enabled ? [1] : [] content { device_name = var.attached_disk_name mode = var.attached_disk_mode source = var.attached_disk_source } } 我們有些 block 只有在特定服務時才需使用，例如上面的 attached_disk 他是 gce 另外掛載其他磁碟的設定，如果有需要我們才會多設定這個 block，沒有則不需要加，因此須使用 dynamic 來動態產生 block，這邊的設定是我們要在參數要帶入 attached_disk_enabled 用 for_each 來判斷是否需要這個 block，如果是 true，就會產生 attached_disk block，且需要輸入 attached_disk_name、attached_disk_mode、attached_disk_source。\nmodule/google_compute_instance/variables.tfvariable \"project_id\" { type = string description = \"GCP 專案 ID\" } variable \"instance_name\" { type = string description = \"GCE 名稱\" } variable \"machine_type\" { type = string description = \"GCE 類型\" } variable \"instance_zone\" { type = string description = \"GCE 所在區域\" } variable \"instance_tags\" { type = list(string) description = \"GCE 網路標記\" } variable \"instance_labels\" { type = map(string) description = \"GCE 標籤\" } variable \"boot_disk_auto_delete\" { type = bool description = \"是否刪除 instance 時，自動刪除開機磁碟\" default = true } variable \"boot_disk_image_name\" { type = string description = \"GCE 映像檔名稱\" } variable \"boot_disk_size\" { type = number description = \"GCE 開機磁碟大小 (單位: GB)\" } variable \"attached_disk_enabled\" { type = bool description = \"是否啟用附加磁碟\" default = false } variable \"attached_disk_name\" { type = string description = \"GCE 附加磁碟名稱\" default = \"\" } variable \"attached_disk_mode\" { type = string description = \"GCE 附加磁碟模式\" default = \"READ_ONLY\" validation { condition = contains([\"READ_WRITE\", \"READ_ONLY\"], var.attached_disk_mode) error_message = \"不符合附加磁碟模式的值，請輸入 READ_WRITE 或 READ_ONLY\" } } variable \"attached_disk_source\" { type = string description = \"GCE 附加磁碟來源\" default = \"\" } variable \"network_name\" { type = string description = \"GCE 網路名稱\" } variable \"subnetwork_name\" { type = string description = \"GCE 子網路名稱\" } variable \"nat_ip_enabled\" { type = bool description = \"是否啟用 NAT IP\" default = false } variable \"metadata\" { type = map(string) description = \"GCE 中繼資料\" } variable \"enable_display\" { type = bool description = \"是否啟用虛擬顯示\" default = false } variable \"resource_policies\" { type = list(string) description = \"GCE 資源原則\" } variable \"service_account_email\" { type = string description = \"GCE 服務帳戶電子郵件\" } variable \"service_account_scopes\" { type = list(string) description = \"GCE 服務帳戶範圍\" } variable \"deletion_protection\" { type = bool description = \"是否啟用刪除保護\" default = false } variable \"allow_stopping_for_update\" { type = bool description = \"是否允許自動停止後更新\" default = false } 這個檔案會定義每個變數的名稱以及資料型態，也可以寫說明以及預設的值，這邊比較特別的是 validation ，他可以驗證帶入的參數是否符合 condition 內容，也可以自定義錯誤的訊息，如下：\nvariable \"attached_disk_mode\" { type = string description = \"GCE 附加磁碟模式\" default = \"READ_ONLY\" validation { condition = contains([\"READ_WRITE\", \"READ_ONLY\"], var.attached_disk_mode) error_message = \"不符合附加磁碟模式的值，請輸入 READ_WRITE 或 READ_ONLY\" } } 這邊限制 attached_disk_mode 輸入必須符合 READ_WRITE or READ_ONLY 的值，如果輸入其他不符合的會顯示 error_message 內容。\n另外 variable 這邊有幾個資料型態可以選擇，如下：\nstring：字串，不知道要選什麼就選他沒錯 xD\nbool：布林值，只有 true、false 兩種選項，適用於判斷的內容，例如剛剛上面說的 attached_disk_enabled 就是使用 bool\nnumber：數字，只能輸入數字\nlist (tuple)：清單，內容可以放置類似 [\"us-west-1a\", \"us-west-1c\"] 的資料\nmap (object)： key value 存放模式，例如：\n{ \"aaa\": \"test1\", \"bbb\": \"test2\", \"ccc\": \"test3\" } module/google_compute_instance/outputs.tfoutput \"instance_id\" { value = google_compute_instance.instance.instance_id } 這邊主要放置要輸出的內容，像我們這邊就會把 instance_id 給顯示出來。\nprojects 我這邊只示範 prod 的部分\nprojects/prod/main.tfmodule \"ian-test\" { source = \"../../module/google_compute_instance\" project_id = \"馬賽克\" instance_name = \"test-prod\" machine_type = \"e2-small\" instance_zone = \"asia-east1-b\" instance_tags = [] instance_labels = { \"aaa\" = \"test1\" \"bbb\" = \"test2\" \"ccc\" = \"test3\" } boot_disk_image_name = \"debian-cloud/debian-10\" boot_disk_size = \"50\" attached_disk_enabled = false network_name = \"馬賽克\" subnetwork_name = \"馬賽克\" nat_ip_enabled = false metadata = {} resource_policies = [] service_account_email = \"馬賽克\" service_account_scopes = [\"storage-ro\", \"logging-write\", \"monitoring-write\", \"service-control\", \"service-management\", \"trace\"] } 這邊我們可以定義要使用 module 的叫什麼，這邊我就取名 google_compute_instance，然後他會去 source \"../../module/ian-test\"，也就是我們剛剛在上面先挖洞的模板，底下就開始帶入我們在 variables.tf 有設定的參數。這邊比較要注意的是，在 main.tf、variables.tf 有使用的變數設定，都必須要寫在個別資源 tf 的檔案裡面，沒有的就帶入對應資料型態的空值，例如 instance_tags、metadata、resource_policies 等等。"},"title":"如何將 Terraform 改寫成 module ?"},"/blog/terraform/terraform-tfstate/":{"data":{"":"此篇是接續上一篇 什麼是 IaC ? Terraform 又是什麼？的 Terraform 文章，我們在上一篇有提到 terraform apply 完後，會多一個檔案 *.tfstate，這個檔案是用來存放服務狀態的檔案，它包含基礎架構的狀態和資源的詳細信息。假設大家都在自己的本地去 apply 同一個服務，會導致每個人的 tfstate 檔案內容不同，有可能去覆蓋掉其他人已經調整的內容，因此我們必須將此 tfstate 檔案存放在一個地方，讓大家都去使用同一份檔案來調整資源。\n我們常用的儲存方式會將 tfstate 存在 gitlab 或 gcs (gcp 架構為例)，以下會簡單說明要如何把 tfstate 存到後端以及各頁面的功能：","gcs#gcs":"gcs 儲存比較簡單一點，因為他就是一個 bucket，所以頁面就跟一般的 gcs 一樣，會顯示檔案名稱、大小、類型等，如果需要查看 tfstate，可以點擊最後的下載按鈕來查看\n那我們接著使用上面 gitlab 的範例檔案，只是要將 backend.tf 內容改為以下：\nbackend.tfterraform { backend \"gcs\" { bucket = \"pid-terraform-state\" prefix = \"/aaa\" } } 上面的設定是指，我們將 backend 後端設定改成 gcs，並且選擇名為 pid-terraform-state 的 bucket，此 bucket 需要先手動建立(因為 bucket 名稱是全域不重複，所以不需要特別設定其 project_id，只要有權限正確都可以跨專案使用)，以及我們要將此 tfstate 存在 aaa 資料夾內。\n接著我們重新刪除剛剛 gitlab 已產生的 .terraform/ 跟 .terraform.lock.hcl 檔案，重新下 init，就可以到 gcs 對應資料夾下，新增了 defaulte.tfstate 檔案。\n產生 defaulte.tfstate\nLock 我們一樣來看一下 gcs 的 lock 會長什麼樣子，gcs lock 會產生一個 default.tflock 檔案，由他去判斷現在是否是 Lock 狀態\ngcs Lock 會出現 .tflock 檔案\n當有其他人也執行 plan or apply 後，就會顯示以下：\ngcs Lock 其他人不能操作","gitlab#gitlab":"那我們要怎麼把 tfstate 存到 gitlab 呢？首先跟之前一樣，先新增 provider.tf 來放供應商的來源以及版本，以及 main.tf 來放 gce 相關設定，最後還要多一個 backend.tf 來放我們要儲存 tfstate 的位置設定，如下：(同步到 GitHub 需要程式碼的可以前往查看)\n(這次範例會使用 gce，此項會需要 gitlab 先啟用 Infrastructure 功能以及建立自己的 gitlab token)\nprovider.tfterraform { required_providers { google = { source = \"hashicorp/google\" version = \"~\u003e 4.48.0\" } } } main.tf provider \"google\" { project = \"XXXXX\" zone = \"asia-east1-b\" } resource \"google_compute_instance\" \"instance\" { name = \"test\" machine_type = \"e2-small\" zone = \"asia-east1-b\" labels = { env = \"11\" } boot_disk { initialize_params { image = \"debian-cloud/debian-10\" } } network_interface { network = \"projects/XXXX/global/networks/test\" subnetwork = \"projects/XXXX/regions/asia-east1/subnetworks/testtest\" } } backend.tf « 新的 專案 ID 要寫我們想要放 terraform state 的 GitLab Project ID，服務名稱是指顯示在 GitLab terraform state 的名稱\ngitlab 個人 token 是指個人存取權杖，大家再依照自己的來做設定\nbackend.tfterraform { backend \"http\" { address = \"[Gitlab 網址]/api/v4/projects/[專案ID]/terraform/state/[服務名稱]\" lock_address = \"[Gitlab 網址]/api/v4/projects/[專案ID]/terraform/state/[服務名稱]/lock\" unlock_address = \"[Gitlab 網址]/api/v4/projects/[專案ID]/terraform/state/[服務名稱]/lock\" username = \"[Gitlab 帳號]\" password = \"[Gitlab 個人 token]\" lock_method = \"POST\" unlock_method = \"DELETE\" retry_wait_min = 5 } } 當我們新增好後，就跟之前步驟一樣，先 init \u003e plan \u003e apply 來做測試，在 init 時會發現，與之前不太一樣的是，在 Initializing the backend 的下方有多了綠色的成功設定後端字樣，代表他也會將後端的相關資訊存進 .terraform 資料夾中，所以有變更後端儲存位置，要記得重新 init 歐\ninit 初始化後，會將後端資訊也存到 .terraform 資料夾\n當我們 plan \u003e apply 完成後，可以觀察一下，發現原本會產生的 terraform.tfstate 檔案沒有出現在該目錄下：\napply 完，沒有在本地產生 .tfstate 檔案\n這時候，我們可以到剛剛在上面設定的專案 ID 內的有一個 Infrastructure / Terraform，裡面就會存放 Terraform state 檔案，如下：\ngitlab/Infrastructure/Terraform\n會顯示狀態名稱、更新資訊、以及 Actions 等欄位：\ngitlab terraform tfstate 網頁\n功能部分可以看後面的 Actions 欄位底下有三個點點，可以下載對應的 tfstate 檔案、Lock 讓其他人不能對此進行 apply，或是刪除此 tfstate 檔案等\ngitlab terraform tfstate 功能說明\n這樣我們就可以透過同一份的 tfstate 檔案來做管理，但有個前提是，之後對該資源的變更都只能使用 tf，如果還有用 WEB UI 去調整，就會遇到線上服務與 tfstate 儲存狀態不同的問題。\nLock 那當我們已經有了共同儲存的地方，也溝通好，不會使用 WEB UI 去調整，但如果有兩個人同時去下 apply 的話，第一個人的 apply 還在執行，後面那個人的 apply 是不是就會蓋掉前一個人的設定呢？\n所以 Terraform 在 0.14 版本推出了 Lock 功能，當有人在 plan or apply 的時候，我們去查看 gitlab Terraform state，會看到我們的 tfstate 檔案會被 Lock 起來\nGitLab Lock 鎖住\n此時除了第一個操作者，其他人再去 plan or apply 就會出現錯誤，可以看到是誰正在使用，以及操作的動作是 plan or apply\n在 Lock 下，其他人沒辦法去 plan or apply\n在 CI 時，需要在 plan 時就將它給 lock，避免第一個人 plan 完，沒有及時的去執行 apply，後來有其他人比第一個人先調整了資源，第一個人再來執行 apply，就會導致第一個人 apply 的內容與自己原先看 plan 的內容會不同，也有可能會將上一個人調整的設定給覆蓋，當第一個操作者結束動作後，該 Lock 才會被解鎖。\n其他 gitlab terraform state 詳細內容可以參考：https://docs.gitlab.com/ee/user/infrastructure/iac/terraform_state.html"},"title":"Terraform 如何多人共同開發 (將 tfstate 存在後端)"},"/blog/terraform/terragrunt/":{"data":{"":"我們接續上一篇的 如何將 Terraform 改寫成 module ? ，我們已經將 Terraform 改成 module 的方式來進行管理，但當我們要管理的資源越來越多，且有分不同的專案時，整個服務架構會長的像以下：\nmodules google_compute_instance main.tf outputs.tf variables.tf projects gcp-1234 aaa backend.tf main.tf provider.tf bbb backend.tf main.tf provider.tf ccc backend.tf main.tf provider.tf gcp-2345 aaa backend.tf main.tf provider.tf bbb backend.tf main.tf provider.tf ccc backend.tf main.tf provider.tf gcp-3456 aaa backend.tf main.tf provider.tf bbb backend.tf main.tf provider.tf ccc backend.tf main.tf provider.tf 這邊的範例是以不同專案來分，再分區不同的服務，每個服務裡面都會有 backend.tf、main.tf、provider.tf 檔案所組成，我們可以來比較一下 gcp-1234 的 aaa 服務以及 gcp-2345 的 aaa 服務差異：\n檔案差異\n可以看到在 backend.tf 除了 prefix 路徑以外，其他設定也都一樣，但因為 Terraform 本身沒辦法透過帶入參數的方式來設定 backend.tf 後端部分，所以必須要先寫好每個服務所存放的後端位置，十分的不方便。\nbackend.tfterraform { backend \"gcs\" { bucket = \"terragrunt-tfstate\" prefix = \"/gcp-1234-aaa\" } } 為了減少上述這些需要一直重複寫差不多檔案的工作內容，因此有了 Terragrunt 這個工具，Terragrunt 是 Terraform 的包裝器，可以彌補 Terraform 上的一些缺陷，並且讓我們的 IaC 更貼近 DRY 原則。\n這邊說明一下 DRY 原則\nDRY 全名是 Don’t repeat yourself，也就是不要做重複的事情，能夠一次做完的就不要重複的去做。","terragrunt-好處#Terragrunt 好處":"\n接著介紹一下 Terragrunt 的好處：\n方便管理後端狀態設定\n將後端存儲桶納入管理\n使用 generate 自動生成檔案\n使用 include 檔案來達到 DRY 原則\n管理 Module 之間的依賴性\n產生依賴關聯圖\n方便管理後端狀態設定 首先第一個方便管理後端狀態設定，也就是我們上面提到的 backend.tf 設定。在 Terraform 原生為了區別不同專案不同服務的狀態檔，就必須先寫好每個儲存的路徑，但使用 Terragrunt，可以先在該目錄下，也就是 gcp-3456 目錄下先寫一個設定檔案 (我們以 gcp-3456 專案為例)，讓底下的 aaa、bbb、ccc 服務可以去 include 它，我們就不需要每個服務都寫幾乎差不多的設定檔，接著我們在 gcp-3456 資料夾下新增 terragrunt.hcl 檔案來說明：\nterragrunt.hclremote_state { backend = \"gcs\" generate = { path = \"backend.tf\" if_exists = \"overwrite\" } config = { bucket = \"terragrunt-tfstate\" prefix = \"${path_relative_to_include()}\" } } 這邊的設定其實跟之前的 backend.tf 差不多，只是後端現在儲存的 block 改叫做 remote_state，可以看到 backend 設定我們一樣是存在 gcs 上，generate 這個 block 它會判斷 backend.tf 檔案是否存在，如果沒有它就會幫我們建立，其中設定檔案內容是將狀態檔案存在 terragrunt-tfstate 這個 bucket，並透過${path_relative_to_include()} 這個變數來自動帶入有 include 這份檔案的路徑，並在相對路徑產生 backend.tf 檔案。\n有點抽象，所以我畫一個比較簡單的架構圖來做說明一下，假設現在有三個服務，如下：\naaa terragrunt.hcl bbb terragrunt.hcl ccc terragrunt.hcl terragrunt.hcl 我們剛剛的後端設定是寫在此根目錄的 terragrunt.hcl 檔案(第 8 行)，然後 ccc 這個服務去 include 根目錄的 terragrunt.hcl 檔案， Terragrunt 就會自動幫你產生以下的 backend.tf 檔案：\nbackend.tfterraform { backend \"gcs\" { bucket = \"terragrunt-tfstate\" prefix = \"/ccc\" } } 這樣就可以省下我們要重複寫 backend.tf 的時間，在維護上也會更加的方便。\n將後端存儲桶納入管理 接著，大家有沒有想過，我們都已經使用 Terraform 來管理 IaC ，並把狀態檔案放到 gcs 上面來保存，但一開始還沒有用 Terraform 管理 gcs 的資源，我們還需要在設定 backend.tf 前，先手動去新增一個 gcs ，才能來存放 tfstate 狀態檔案呢？\n因此在 Terragrunt remote_state 的 config 時，可以多設定 gcs 的 project id 以及 location，Terragrunt 在初始化後端時，會檢查是否有該 gcs bucket，如果沒有就會自動建立，設定檔如下：\nterragrunt.hclremote_state { backend = \"gcs\" generate = { path = \"backend.tf\" if_exists = \"overwrite\" } config = { project = \"gcp-xxxxxx\" location = \"asia\" bucket = \"terragrunt-tfstate\" prefix = \"${path_relative_to_include()}\" } } 使用 generate 自動生成檔案 在剛剛我們可以使用 generate 來自動 backend.tf 檔案，那代表我們也可以把每個 provider.tf 的內容，也透過 generate 來生成，如下：\nterragrunt.hclgenerate \"provider\" { path = \"provider.tf\" if_exists = \"overwrite\" contents = \u003c\u003cEOF terraform { required_providers { google = { source = \"hashicorp/google\" version = \"~\u003e 4.48.0\" } } } EOF } 這樣子每個 include 這份設定檔的服務除了 backend.tf 檔案以外，還會自動產生 provider.tf 檔案。\n使用 include 檔案來達到 DRY 原則 我們搞定了 backend.tf 跟 provider.tf 後，還剩下 main.tf，所以我們也將它改成 Terragrunt 的格式如下：\nterragrunt.hclterraform { source = \"${get_path_to_repo_root()}/modules/google_compute_instance\" } include \"root\" { path = find_in_parent_folders() } inputs = { instance_name = \"gcp-3456-ccc\" machine_type = \"e2-small\" instance_zone = \"asia-east1-b\" instance_tags = [] instance_labels = {} boot_disk_auto_delete = true boot_disk_image_name = \"debian-cloud/debian-10\" ... 設定部分省略 ... } 這邊可以看到 terraform source 它就是我們使用 module 的路徑，也可以用 ${get_path_to_repo_root()} 這個變數他會自動抓該專案的根目錄，我們就不需要去特別設定。\n此外也可以將 module 獨立成一個專案，或是使用其他人寫好的 module，在 source 的時候可以使用 git::https://[gitlab-網址]/sre/terraform/module.git//google_compute_address 的方式來取得 module，可以設定要使用哪個分支或是 tag，只需要在網址後面加上，?ref=[分之 or tag 名稱] 即可，這樣可以讓開發中的 module 不會影響到線上其他正在使用中的 module 設定。\n(module.git 後面的 // 是 Terraform module source 的規則，如果不加會跳警告訊息)\n接著我們可以看到 include \"root\" {} 這段，裡面有使用 find_in_parent_folders 這邊變數，他就是上面的提到會自動去抓放在父資料夾的 remote_state 跟 generate terragrunt.hcl 檔案。\n後面的 input 就跟使用 module 時一樣，將 module 的參數帶入即可。\n補充：所以我們也可以把一些通用的設定寫在根目錄的 terragrunt.hcl 檔案，例如專案的 id，可以寫以下內容來讓 include 它的檔案吃到同一個參數設定：\nterragrunt.hclinputs = { project_id = \"gcp-3456\" } 管理 Module 之間的依賴性 由於 Terragrunt 是把每個服務拆分成最小化，沒辦法把使用不同 module 的資源放在一起(單純使用 module 的話，可以一次 source 多了 module，並把他放在同一個 tf 檔案中)，那像是我們建立 k8s 會使用到 google_container_cluster、google_container_node_pool 兩種不同的 module 該怎麼辦呢？\n首先我們先在 modules 資料夾放上 google_container_cluster、google_container_node_pool 兩個 module 的設定檔案，詳細程式可以點我前往\nmodules google_container_cluster main.tf outputs.tf variables.tf google_container_node_pool main.tf variables.tf 在 projects 底下新增 gke 資料夾，新增 terragrunt.hcl 來放 remote_state provider 的設定，並區分兩個資料夾，分別是 cluster 資料夾來存放 cluster 資訊，以及 test 資料夾來存放 test node-pool 資訊：\nprojects gke cluster terragrunt.hcl terragrunt.hcl test terragrunt.hcl cluster 的 terragrunt.hcl 檔案如下：\nterragrunt.hclterraform { source = \"${get_path_to_repo_root()}/modules/google_container_cluster\" } include { path = find_in_parent_folders() } inputs = { cluster_name = \"tf-test\" cluster_location = \"asia-east1-b\" node_locations = [] cluster_version = \"1.24.12-gke.500\" network_name = \"projects/gcp-202011216-001/global/networks/bbin-testdev\" subnetwork_name = \"projects/gcp-202011216-001/regions/asia-east1/subnetworks/bbin-testdev-dev-platform\" node_max_pods = 64 remove_default_node_pool = true initial_node_count = 1 enable_shielded_nodes = false resource_labels = { \"dept\" : \"pid\", \"env\" : \"dev\", \"product\" : \"bbin\" } dns_enabled = false cluster_dns = \"PROVIDER_UNSPECIFIED\" cluster_dns_scope = \"DNS_SCOPE_UNSPECIFIED\" private_cluster_ipv4_cidr = \"172.16.0.176/28\" binary_authorization_enabled = true binary_authorization = \"DISABLED\" } test node-pool 檔案如下：\nterragrunt.hclterraform { source = \"${get_path_to_repo_root()}/modules/google_container_node_pool\" } include { path = find_in_parent_folders() } dependency \"cluster\" { config_path = \"../cluster\" } inputs = { cluster_name = dependency.cluster.outputs.cluster_name cluster_location = dependency.cluster.outputs.cluster_location cluster_version = dependency.cluster.outputs.cluster_version node_pool_name = \"test\" node_count = 1 node_machine_type = \"e2-small\" node_disk_size = 100 node_disk_type = \"pd-standard\" node_image_type = \"COS_CONTAINERD\" node_oauth_scopes = [ \"https://www.googleapis.com/auth/devstorage.read_only\", \"https://www.googleapis.com/auth/logging.write\", \"https://www.googleapis.com/auth/monitoring\", \"https://www.googleapis.com/auth/service.management.readonly\", \"https://www.googleapis.com/auth/servicecontrol\", \"https://www.googleapis.com/auth/trace.append\" ] node_tags = [] node_taint_enabled = false node_taint_key = \"\" node_taint_value = \"\" node_taint_effect = \"\" auto_repair = true auto_upgrade = true upgrade_max_surge = 1 upgrade_max_unavailable = 0 upgrade_strategy = \"SURGE\" autoscaling_enabled = true autoscaling_max_node_count = 2 autoscaling_min_node_count = 1 autoscaling_total_max_node_count = 0 autoscaling_total_min_node_count = 0 } 上面兩個檔案分別是 cluster 的設定，以及 test node-pool 的設定，裡面的設定，上面基本都有提過，這邊要提的是 dependency，dependency 他是 Terragrunt 提供讓我們可以方便地去管理 IaC 之間的相依性，像是我們這邊，需要先建立好 cluster 才能建立 node_pool，此時就可以依靠 dependency block 來完成需求。\n( 靠 dependency 來取得 cluster 的資訊，並帶入 node_pool 中 )\n此時的執行指令是在 gke 目錄下，使用 terragrunt run-all [參數] 來跑整個相依的 module，我們這邊就建立一個名為 tf-test 的 cluster，並且有一個名為 test 的 node_pool ，其他設定請參考上面程式：\n測試 terragrunt run-all\n(黃色的 WARN 是因為 gcs 上還沒有存過該狀態檔案，所以會跳出提示)\n等到 cluster 建立完成後，會將 cluster 的資訊帶入 node_pool，才開始建立 node_pool 的資源：\n測試 terragrunt run-all\n產生依賴關聯圖 當我們服務使用到很多依賴關係，想要釐清是誰依賴誰，如果單純看程式會比較麻煩，在 Terragrunt 還有一個好用的指令，可以使用以下指令，產生對應的依賴關係圖，在檢視時可以更清楚知道關係：\nterragrunt graph-dependencies | dot -Tpng \u003e graph.png ( 這個 dot 指令是另外的套件，會將關係圖程式碼轉成圖檔，請先安裝 brew install graphviz )\n關聯圖","terragrunt-安裝方式#Terragrunt 安裝方式":"那要怎麼使用 Terragrunt 呢！？\n第一步當然是安裝它囉，我們系統是 macOS，所以我們安裝方式是 Homebrew 來進行安裝：\nbrew install terragrunt 接著我們的指令會從 terraform XXX 變成以下：\nterragrunt plan terragrunt apply terragrunt output terragrunt destroy Terragrunt 會將所有命令、參數和選項直接轉發到 Terraform。(所以我們也需要下載 Terraform)\nTerragrunt 的預設檔案名稱是 terragrunt.hcl ，Terragrunt 的設定檔案基本上與 Module 差不多，只是有更多更方便的變數可以使用。","參考資料#參考資料":"事半功倍 — 使用 Terragrunt 搭配 Terraform 管理基礎設置：https://medium.com/act-as-a-software-engineer/%E4%BA%8B%E5%8D%8A%E5%8A%9F%E5%80%8D-%E4%BD%BF%E7%94%A8-terragrunt-%E6%90%AD%E9%85%8D-terraform-%E7%AE%A1%E7%90%86%E5%9F%BA%E7%A4%8E%E8%A8%AD%E6%96%BD-f70c30166639"},"title":"如何導入 Terragrunt，Terragrunt 好處是什麼？"},"/projects/":{"data":{"":" 活動申請系統 (Web)幫學校開發的大型校務系統，方便系統社團使用電子填單方式申請活動，也讓之後的學弟妹，可以更快速的查看到歷屆辦理的活動。 巔峰極速 兌換虛寶網站 (Web)該遊戲有大量虛寶可以兌換，但官方提供的網頁，需要重複輸入 ID 以及驗證碼還有序號，因此寫了一個小網頁，可以只輸入一次 ID 以及驗證碼，就兌換完所有序號。 京緯工程有限公司官網 (Web)協助京緯工程有限公司建立公司官方網頁。 "},"title":"專案成就"}}