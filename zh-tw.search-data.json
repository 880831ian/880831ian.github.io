{"/about/":{"data":{"":" 哈囉大家好，我叫莊品毅，也可以叫我 Ian，目前是一位 Site Reliability Engineering (SRE) 工程師，主要負責維護公司的 Google Cloud Platform (GCP) 雲端相關服務，包含 GKE、GCE、GCS、GLB 等等，除此之外也熟悉使用 Terraform + Terragrunt 來管理雲端大量的 IaC 資源，配合 Prometheus、Datadog、EFK 等監控工具來確保服務的穩定性，協助 RD 建立 CICD 部署流程。\n在下班空閒時間，我喜歡閱讀技術相關的文件、部落格，也會參加一些線下技術社群的活動，例如 DevOpsDay，希望能夠透過這些活動來學習更多的知識，歡迎大家使用下方 giscus 留言系統留言交流。","工作經驗#工作經驗":" 凡谷興業有限公司 - SRE 工程師 (2022/02 - 現在)"},"title":"關於我"},"/blog/":{"data":{"":"","介紹#介紹":"👋 你好、妳好、大家好，歡迎來到我的秘密花園，這邊主要會記錄我研究一個新的技術或工具、以及處理一些 SRE 遇到的靈異事件問題的小天地。\n會開始寫 Blog 的初衷主要是我的小腦袋瓜，如果不寫下來，過陣子很容易就忘記 (´_ゝ`)，當然也希望可以幫助到有相同問題的人 (可以使用搜尋功能來快速找到相關文檔喔)，如果有任何問題或建議，歡迎在下方留言。","聲明#聲明":"由於在學習新的技術或工具時，會參考網路上許多的文件和照片，雖然會加上自己的見解與實作內容改寫而成，且會於文章後附上相關資料來源，如有侵犯到您的權益，請於下方告知，我會立即刪除相關內容，謝謝 (๑•́ ₃ •̀๑)。"},"title":"Blog"},"/blog/gcp/":{"data":{"":"此分類包含 Google Cloud Platform 相關的文章。\n如何過濾 GCP LOG，減少 Cloud Logging API 的花費 Google Kubernetes Engine CronJob 會有短暫時間沒有執行 Job Google Cloud Platform (GCP) - IAM 與管理 Google Cloud Platform (GCP) - Compute Engine Google Cloud Platform (GCP) - Cloud Source Repositories Google Cloud Platform (GCP) - Container Registry Google Cloud Platform (GCP) - Cloud Build "},"title":"Google Cloud Platform"},"/blog/gcp/cloud-build/":{"data":{"":"跟大家介紹一下今天的主題 Cloud Build，Cloud Build 可以幫我們做持續建構、測試和部署，我們可以把它想成簡易版的 Jenkins，從整個映像檔案打包到部署，也就幾分鐘的事情，且內建許多指令。\n我們今天文章，需要使用前幾天提到的 Cloud Source Repositories 、Compute Engine、Container Registry，我們需要先透過 GitLab 將程式鏡像到 Cloud Source Repositories，再透過 Cloud Build 觸發將 GitLab 上面的 Dockerfile 建置到 Container Registry 中，再部署到 Compute Engine VM 上。大家可以參考流程圖，會更清楚今天的流程！那我們就開始囉 🥸\n流程圖","cloud-source-repositories-測試#Cloud Source Repositories 測試":"前面 GitLab 鏡像設定，請先參考上上篇 Google Cloud Platform (GCP) - Cloud Source Repositories，上上篇會帶大家從 GitLab 鏡像到 Cloud Source Repositories，所以我們就接續之前的內容，繼續往下開始學習吧～\n開啟 GCP，選擇左側的 menu \u003e 點擊 Cloud Build \u003e 選擇 觸發條件，點擊 建立觸發條件 按鈕。 輸入觸發條件的名稱，事件可以設定我們要怎麼進行觸發，我們這邊先選擇 推送至分支的版本 來觸發，在來源選擇上上篇建立好的 Cloud Source Repositories 存放區，分支版本我們先使用預設的 master，也就是推程式到 master 他會就觸發 Cloud Build： 建立觸發條件 1\n設定類型我們選擇 Cloud Build 設定檔，他也是 Cloud Build 專用的設定檔，後面會帶大家寫一份 Cloud Build，位置當然是使用我們 Cloud Source Repositories 存放區，以及可以依照專案來修改 cloudbuild.yaml 放在專案的哪裡，最後都沒問題，就按下建立： 建立觸發條件 2\n撰寫 cloudbuild.yaml 設定檔 在開始建立檔案前，先來跟大家說說檔案內有哪些設定吧：\n首先 Cloud Build 建構器是裝有常用的程式語言和工具的容器映像。我們可以配置 Cloud Build，讓建構器中運行特定命令，我舉個例子讓大家了解：\n以下程式碼是來自 Docker Hub 的 ubnutu 映像檔中所執行的命令：\nsteps: - name: \"ubuntu\" args: [\"echo\", \"hello world\"] 可以看到我們配置文件中 steps 參數是指我們要建構的步驟， name 字段指定 Docker 映像檔的位置，以及 args 字段中是指定映像檔運行的命令。\n我們的 Cloud Build 一樣會需要用 name 來指定建構容器的映像檔，以及使用 args 來執行我們映像檔所要運行的命令。\n我們的 Cloud Build 設定檔中的 name 常用的建構器映像檔如下：\nBuilder 名稱 bazel gcr.io/cloud-builders/bazel docker gcr.io/cloud-builders/docker git gcr.io/cloud-builders/git gcloud gcr.io/cloud-builders/gcloud gke-deploy gcr.io/cloud-builders/gke-deploy 接著我們來試著寫一個 cloudbuild.yaml 來建構我們的 nginx 服務，並部署到 Compute Engine 上。\n我們先回到 Gitlab 該專案下的目錄，新增 cloudbuild.yaml 檔案，將複製以下內容：\nsteps: # Docker Build - name: \"gcr.io/cloud-builders/docker\" args: [\"build\", \"-t\", \"gcr.io/$PROJECT_ID/ian-test:ian-nginx-test\", \".\"] # Docker Push - name: \"gcr.io/cloud-builders/docker\" args: [\"push\", \"gcr.io/$PROJECT_ID/ian-test:ian-nginx-test\"] # Build VM - name: \"gcr.io/google.com/cloudsdktool/cloud-sdk\" entrypoint: \"gcloud\" args: [ \"compute\", \"instances\", \"create-with-container\", \"ian-test-vm\", \"--container-image\", \"gcr.io/$PROJECT_ID/ian-test:ian-nginx-test\", ] env: - \"CLOUDSDK_COMPUTE_REGION=asia-east1\" - \"CLOUDSDK_COMPUTE_ZONE=asia-east1-b\" 我們一個一個來說說的這個 cloudbuild.yaml 裡面的設定吧！(我以前面的註解來區分)\nDocker Build：這邊的 name 我們用 gcr.io/cloud-builders/docker，代表我們將使用 docker 建構器，args 這邊下的意思是要把與 cloudbuild.yaml 放在一起的 Dokcerfile 給 build 起來，並改名為 gcr.io/$PROJECT_ID/ian-test:ian-nginx-test。 Docker Push：這邊一樣使用 gcr.io/cloud-builders/docker，args 指令部分變成我們要把他 push 到 gcr.io/$PROJECT_ID/ian-test 這個 Cloud Source Repositories，其中這個映像檔案的 tag 為 ian-nginx-test。 Build VM：這邊我們使用 gcr.io/google.com/cloudsdktool/cloud-sdk，可以透過它來建立 VM，並且執行 tag 名為 ian-nginx-test 的映像檔，後面環境變數是來設定 VM 的區域等等。 ian-test 是 Container Registry 資料夾名稱，ian-nginx-test 是 Container Registry 映像檔的 tag，ian-test-vm 是我們建立 VM 的名字，所以要記得改成自己的命名歐！ 撰寫 Dockerfile 接下來剛剛有提到 Docker Build 會將我們放在一起的 Dockerfile 給 build 起來，所以我們也要先寫好要用的 Dockerfile：\nFROM nginx:latest COPY ./index.html /usr/share/nginx/html/ 我們的 Dockerfile 很簡單，簡單寫了要使用的映像檔，以及將我們等等要測試的 index 複製到裡面\n撰寫測試 index.html \u003c!DOCTYPE html\u003e \u003chtml lang=\"en\"\u003e \u003chead\u003e \u003cmeta charset=\"UTF-8\" /\u003e \u003cmeta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\" /\u003e \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" /\u003e \u003ctitle\u003e測試測試測試\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e 我是測試檔案 \u003c/body\u003e \u003c/html\u003e 這個測試檔案，會蓋過 nginx 的預設畫面，當我們成功將 VM 建立後，瀏覽 80 Port 時，應該會跳出這個測試的網頁。\nCommit 到 GitLab 當我們都新增好檔案後，我們就將程式推到我們前幾篇的 Cloud Source Repositories 已經鏡像的 GitLab 中，接著就是等待見證奇蹟的時候了ＸＤ\n當我們推送後，我們先檢查 Cloud Source Repositories 是否有成功從 GitLab 鏡像過來： Cloud Source Repositories\n檢查看看 Container Registry 是否多了名為 ian-test 的資料夾，且裡面有一個 tag 為 ian-nginx-test 的映像檔： Container Registry\n檢查一下 Cloud Build 的觸發條件是不是在運作，最後成功可以看到類似下方圖片內容： Cloud Build 的觸發條件\n檢查 Compute Engine 的 VM 是否有成功被建立： Compute Engine\n最後就是測試這個映像檔案，是不是我們所 Build 的，測試方法很簡單，我們剛剛在 Dockerfile 有複製我們自己寫的 index.html 檔案，去蓋掉原本 nginx 的預設檔，所以我們可以瀏覽上面圖片的外部 IP，就可以看到我們所改的頁面囉！ 測試用 index.html","參考資料#參考資料":"Cloud Builder：https://cloud.google.com/build/docs/cloud-builders\nDay27 - 用 Cloud Build 實作 CI 部分：https://ithelp.ithome.com.tw/articles/10224727"},"title":"Google Cloud Platform (GCP) - Cloud Build"},"/blog/gcp/cloud-source-repositories/":{"data":{"":"跟大家介紹一下今天的主題 Cloud Source Repositories，聽到 Source Repositories 是不是感覺跟什麼東西很像呀，沒錯，就跟我們的 GitHub or GitLab 一樣，可以用來存放我們的程式碼的儲存庫，我們來看看官方怎麼介紹他吧：\n官方介紹 Cloud Source Repositories\n很好歐，非常簡單明瞭 🤣，沒錯，Cloud Source Repositories 就是託管在 Google Cloud 上功能齊全(？)的私有 Git 儲存庫。為什麼會打一個問號呢？是因為他其實沒有那麼好用，所以我們通常的做法，還是會依靠 GitHab 或是 GitLab 來存放程式碼，再透過鏡像 (mirror) 的方式到 Google Cloud Source Repositories。 那我們就開始囉～","cloud-source-repositories-測試#Cloud Source Repositories 測試":"建立 GitLab Project 首先，我們用 GitLab 來當示範，如何透過鏡像 (mirror) 到 Cloud Source Repositories 上面，我們先在 GitLab 上建立一個 Project：\n建立 GitLab Project\n使用 gcloud 指令建立 Source Repo 首先，一定要先裝 gcloud 指令到本機，這個步驟，前面文章也有說過，這邊就不在說明，我們先使用一下指令來查看目前所在的 GCP 專案： gcloud config get-value project 正常來說，如果有先用 config 設定好，會直接跳出你目前的專案 ID，如果沒有跳出來，請使用下面指令來設定：\ngcloud config set project \u003cproject id\u003e 接著我們要啟動該專案的 Cloud Source Repositories API： gcloud services enable sourcerepo.googleapis.com 創建 Cloud Source Repositories gcloud source repos create \u003crepo name\u003e 完成後，開啟 GCP 檢查一下是否有建立成功～點擊左側 menu \u003e Source Repositories， 開啟 Source Repositories\n成功建立 Source Repositories\n將程式碼新增至存放區中 我們要在這一步來設定鏡像 (mirror)，首先我們看剛剛上面建立好的 Source Repositories，其中有一個手動產生的憑證，點選 產生及儲存 Git 憑證 產生及儲存 Git 憑證\n點完後會需要先登入你的 GCP 帳號，登入完後會出現以下內容： Configure Git\n接著把藍色框框內的輸入到終端機內 Configure Git\n接著請複製以下指令貼到終端機內，會生成憑證密碼： grep 'source.developers.google.com' ~/.gitcookies | tail -1 | cut -d= -f2 生成憑證密碼\n接著請複製以下指令貼到終端機內，將用戶名存儲在 CSR_USER 環境變量中： CSR_USER=$(grep 'source.developers.google.com' ~/.gitcookies | \\ tail -1 | cut -d$'\\t' -f7 | cut -d= -f1) 用戶名存儲在 CSR_USER 環境變量中\n接著請複製以下指令貼到終端機內，將 GCP 存儲庫的 URL 存儲在 CSR_REPO 環境變量中 (repo name 要改成你在 gcp 上面的 repo)： CSR_REPO=$(gcloud source repos describe \u003crepo name\u003e --format=\"value(url)\") 將 GCP 存儲庫的 URL 存儲在 CSR_REPO 環境變量中\n接著請複製以下指令貼到終端機內，將存儲庫的 URL（包括用戶名）印到終端機上： echo $CSR_REPO | sed \"s/:\\/\\//:\\/\\/${CSR_USER}@/\" 存儲庫的 URL（包括用戶名）印到終端機上\n經過上面操作，我們可以在第 4 步驟拿到密碼，以及在第 7 步驟拿到完整的 GCP URL，接著我們要到 GItLab Mirror 來設定鏡像。\n到 GitLab Mirror 設定鏡像 先從右側 muen \u003e 選擇 Settings \u003e 點選 Repository，找到 Mirroring repositories GitLab Mirror 設定鏡像\n將剛剛拿到的 URL 以及密碼各別輸入 Git repository URL 以及 Password，記得要選擇 Mirror direction，因為我們是要將 gitlab 的鏡像到 GCP 的 Cloud Source Repositories，所以我們要選擇 PUSH，最後按下 Mirror repository： GitLab Mirror 設定鏡像\n如果沒有跳出錯誤，基本上是沒有問題了！\nGitLab Mirror 檢查\n就可以試著在 gitlab 上面推程式，看看有沒有跑到 Cloud Source Repositories 上面囉！ GitLab 推程式測試","參考資料#參考資料":"Cloud Source Repositories documentation\nMirroring GitLab repositories to Cloud Source Repositories"},"title":"Google Cloud Platform (GCP) - Cloud Source Repositories"},"/blog/gcp/compute-engine/":{"data":{"":"跟大家介紹一下今天的主題 Google Compute Engine(GCE)，GCE 是 Google Cloud 上的基礎架構服務 (IaaS)，該平台可以提供大規模的虛擬機器以及相關的基礎建設 (包含硬碟、網路、附載平衡器… 等等)來建置及運作您的服務，那我們可以將 GCE 服務的主要功能劃分成以下幾點：","google-compute-engine-測試#Google Compute Engine 測試":"首先我們使用 cloudskillsboost 提供的 Creating a Virtual Machine 來做練習，打開後，請先登入自己的 Google 帳號，接著點選左上角的 Start Lab，會跳出與下面圖片類似的內容：\n測試用的帳號密碼\n新增新的 VM 實例 點選 Open Google Console 按鈕來開啟 GCP 主控台 登入帳號就使用上面圖片所提供的帳號密碼來進行登入 登入成功會進入 GCP 主控台，點選左側的 menu \u003e Compute Engine \u003e VM Instances，可以參考下方圖片 新增 VM 實例\n點選 CREATE INSTANCE，請依照下方表格來進行設定： 標題 設定值 說明 Name gcelab 虛擬機實例的名稱 Region us-central1（愛荷華州） 有關區域的更多信息，請參閱 Compute Engine 指南 Regions and zones Zone us-central1-f Series N1 Machine type n1-standard-2 這是一個 2 vCPU、7.5 GB RAM 實例 Boot disk New balanced persistent disk/10 GB/Debian GNU/Linux 10 Firewall Allow HTTP taffic VM 實例 (Name、Region、Zone、Series)\nVM 實例 (Machine type、Boot disk、Firewall)\n新增完後，大約需要等待一分鐘，新的虛擬機就會列在 VM Instances 頁面上。 VM 實例\n安裝 NGINX Web 服務器 點擊 VM instances 實例最後的 SSH，會開啟 SSH 用戶端 在 SSH 終端，要先獲得 root 訪問權限，才更方便的進行後續的動作，請先使用一下指令： sudo su - 利用 root 用戶，來更新操作系統： apt-get update 更新操作系統\n安裝 NGINX： apt-get nginx -y 安裝 NGINX\n最後確認 NGINX 是否運行： ps auwx | grep nginx 確認 NGINX 是否運行\n可以打開瀏覽器瀏覽 http://外部 IP/ 或是使用 curl 外部IP，外部 IP 會在跟剛剛 VM 實例的 External IP 欄位呦～ curl 外部 IP\n完成後，記得可以點選 Check my progress 來檢查進度吧！ Check my progress\n使用 gcloud 創建一個新實例 我們剛剛是使用網頁版來新增，當然也可以使用 gcloud 指令來新增，這個工具有預先裝在 Google Cloud Shell 中。Cloud Shell 是一個基於 Debian 的虛擬機，包含了常用的開發工具 (gcloud、git 等工具)，另外你也可以將 gcloud 下載至本機上來做使用，請閱讀 gcloud 命令行工具指南\n在 Cloud Shell 中，使用 gcloud 來新增新的虛擬機實例： gcloud compute instances create gcelab2 --machine-type n1-standard-2 --zone us-central1-f 就會跳出以下圖片的內容，過一陣子去查看 VM Instances 也可以看到我們所新增的虛擬機實例歐～\nCheck my progress\n到這邊就完成了我們 Google Compute Engine 測試囉～我們知道可以新增 GCE 將實體主機的內容，移植到雲端上囉！希望大家會喜歡今天的文章 🥰","google-compute-engine-特色#Google Compute Engine 特色":"穩健的網路功能 提供使用者擁有穩健的網路功能，以運行各項應用程式及服務。\n自訂網路與預設網路 GCE 包含內部與外部的網路連線能力，讓使用者可以透過自訂規劃來建置屬於自己服務適用的網路，而在 GCE 服務開通當下，也提供預設的 default network，內建常用的路由與防火牆設定 (例如：SSH、RDP、ICMP… 等)，供入門使用者直接使用。\n防火牆規則 除了預設防火牆規則外，使用者也可以透過自建防火牆來開放可以連入的 IP。\n各區域的 HTTP(S) 的負載平衡 為 Layer 7 的負載平衡設備，透過設定可以串連多台主機或是主機群組，讓服務不再只是依賴於單點存在的伺服器。Layer 7 的負載平衡更可以識別路由規劃，進一步可以提供不同路由的重導規則設定，也可以提供 CDN 的 Cache 功能。\n網路的負載平衡 為 Layer4 的負載平衡設備，可以透過 Protocol 與 Port 的方式來重導外部流量到 GCE 主機或主機群，並可以透過 Health Check 的方式，讓流量僅通過健康的主機，避免服務中斷。\n子網路 透過 CIDR 的方式設定 GCE Network 中的各子網路範圍，並可以透過路由的方式串接各子網路中間的通訊，讓 GCE 網路的設計規劃可以更有彈性，也可以讓子網路的規劃來實作更安全的雲端網路架構。","參考資料#參考資料":"Compute Engine 基本介紹：https://gdgcloud-taipei.gitbook.io/google-cloud-platform-in-practice/google-cloud-shang-de-yun-suan-fu-wu/compute-engine/compute-engine-ji-ben-jie-shao\nCreating a Virtual Machine：https://www.cloudskillsboost.google/focuses/3563?locale=zh_TW\u0026parent=catalog"},"title":"Google Cloud Platform (GCP) - Compute Engine"},"/blog/gcp/container-registry/":{"data":{"":"跟大家介紹一下今天的主題 Container Registry，Container Registry 是儲存、管理和保護 Docker 容器映像檔的存放區，可以讓團隊透過同一項服務服務集中管理 Docker 映像檔、也可以執行安全漏洞分析，還能透過精密的存取權管理機制，來決定誰可以存取哪些內容。簡單來說他就是一個讓我們存放 Docker 映像檔的地方，他有以下幾個特點：","container-registry-特點#Container Registry 特點":" 安全的私人 Docker 註冊資料庫：只需要幾分鐘的時間，即可開始在 Google Cloud Platform 中使用安全的私人 Docker 映像檔儲存空間，控管能夠存取、檢視或下載映像檔的人員，並在受到 Google 安全防護機制保護的基礎架構上穩定執行。 自動建立及部署映像檔：在您修訂 Cloud Source Repositories 中的程式碼時，系統會自動建立映像檔並推送至私人註冊資料庫。您可以輕鬆設定持續整合/持續推送軟體更新管道，並整合至 Cloud Build，或是直接將管道部署至 Google Kubernetes Engine、App Engine、Cloud Functions 或 Firebase。(這個就是我們在下一篇文章會使用到的功能) 深度安全漏洞掃描：在軟體部署週期的早期階段找出安全漏洞，藉此確保您可以安全地部署容器映像檔。資料庫會持續重新整理，讓您的安全漏洞掃描作業取得最新型惡意軟體的相關資訊。 鎖定有風險的映像檔：運用二進位授權的原生整合功能來定義政策，避免部署與所設定政策相衝突的映像檔。您可以觸發容器映像檔的自動鎖定功能，禁止將有風險的映像檔部署至 Google Kubernetes Engine。 原生支援 Docker：您不僅可以視需求定義多個註冊資料庫，也能使用標準的 Docker 指令列介面，將 Docker 映像檔推送和提取到您的私人 Container Registry。Docker 提供依據名稱和標記搜尋映像檔的功能，讓您可以順暢使用。 快速的高可用性存取能力：您可使用我們遍布全球的區域性私人存放區，於全世界皆能享有最快速的回應時間。您可以就近將映像檔儲存在位於歐洲、亞洲或美國的運算執行個體中，並透過 Google 的高效能全球網路快速完成部署作業。 上面有提到 Container Registry 其中一個特點就是掃描映像檔，會檢查有沒有奇怪的內容或是錯誤，讓我們在部署前可以先做檢查，底下是整個的流程圖：\nContainer Registry 掃描流程圖\n我們可以延續上一篇文章 Cloud Source Repositories 來說明上面圖片，一開始先 commit 到 GCP 上，也可以把它當成 Cloud Source Repositories，就著透過會透過下一篇要講的 Cloud Build 來建置，並且掃描，如果沒有問題就會 Pubish 到 Container Registry 存放囉～\n我們就簡單說明要怎麼查看我們的 Container Registry，點選則左側的 menu \u003e 選擇 Container Registry \u003e 點擊 映像檔：\nContainer Registry\n一開始可能沒有任何的資料夾，之後當我們 Build 時，可以新增特定的資料夾來放我們的映像檔，可以參考下方圖片：\nContainer Registry 資料夾\n進入該資料夾後，就可以看到裡面的映像檔案：\nContainer Registry 映像檔"},"title":"Google Cloud Platform (GCP) - Container Registry"},"/blog/gcp/gcp-log-reduce-cloud-logging-api/":{"data":{"":"當我們使用 GCP 的 Cloud Logging 服務來查看 Log 時，有時候會有一些我們不需要顯示出來的，或是從來都不會去查詢的 Log，再者是 GCP 本身的錯誤導致大量噴錯的 Log ，這些 Log 都會導致 Cloud Logging 的費用增加。","介紹-cloud-logging#介紹 Cloud Logging":"先來簡單說一下 Cloud Logging 這項服務的基本架構，請看圖：\nCloud Logging 基本架構\n可以看到 Logs Data 會透過 API 再經過 _Default log sink (router) 存到相應命名的 log bucket (預設配置)，圖中 _Required 以及 _Default 的 log sink 都是 GCP 自動創建的接收器，下面簡述一下它們的區別：\n_Required 日誌儲存桶 Cloud Logging 會將以下類型的 Log 存到 _Required 儲存桶\n管理員活動審核 Log\n系統事件審核 Log\nAccess Transparency Log\nCloud Logging 會將 _Required 儲存桶 Log 保留 400 天，無法調整該期限，且無法修改或刪除 _Required 儲存桶，也沒辦法停用 _Required log sink 接受器路由到 _Required 儲存桶的設定。\n_Default 日誌儲存桶 只要不是存在 _Required 日誌儲存桶的 Log 就會透過 _Default log sink 接受器路由到 _Default 儲存桶。\n除非有另外配置自定義設定， 否則 _Default 日誌儲存桶 Log 只會保留 30 天，也一樣無法刪除 _Default 日誌儲存桶。此外 Cloud Logging 的費用是以存在 _Default 日誌儲存桶來計算。\n功能 價格 每月免費額度 Logging 提取 提取的 Log $0.5/GiB 每個項目前 50 GiB Logging 儲存 保留超過 30 天的 Log，每月每 GiB $0.01 在默認保留期限的 Log 不會有額外儲存費用 查看該專案使用的 Cloud Logging API 費用：https://console.cloud.google.com/apis/api/logging.googleapis.com/cost","參考資料#參考資料":"Routing and storage overview https://cloud.google.com/logging/docs/routing/overview","過濾-log#過濾 Log":"那現在知道 Cloud Logging 的架構，那當我們遇到需要過濾 Log 時，我們可以使用以下步驟來過濾以節省 Cloud Logging API 費用：\n範例說明 這次範例是 google 在 2023/07/06 所發佈的 Service Health，當 GKE 版本大於 1.24 以上，就會噴\nFailed to get record: decoder: failed to decode payload: EOF\ncannot parse ‘0609 05:31:59.491654’ after %L\ninvalid time format %m%d %H:%M:%S.%L%z for ‘0609 05:31:59.490647’\n這三種類型的錯誤 Log，在等待官方修復前，官方的建議是先將他給過濾掉，避免一直刷噴錢 ┐(´д`)┌\n附上當時的 Service Health 連結：https://status.cloud.google.com/incidents/y5XvpyBXFhsphSt4DfiE\n我們在上面架構圖有說到，Log 會透過 log sink 路由到 bucket，所以我們要將過濾條件加在 log router 上：\n選擇 Log Router 選擇 Log Router\n選擇 Log Router Sinks 選擇 _Default 的 Log Router Sinks，點選右邊按鈕的 Edit Sink\n選擇 _Default 的 Log Router Sinks\n設定 Sink details 第一個是 details，可以輸入說明，這邊輸入：google 有 bug 會噴大量的意外 LOG，怕費用飆高，先用官方建議的來過濾 LOG，詳細可以看： https://status.cloud.google.com/incidents/y5XvpyBXFhsphSt4DfiE\n輸入 Sink details 說明\n選擇 Sink Service 跟 Bucket 接著 sink 服務選擇 Logging bucket，以及對應儲存的 log bucket (這邊基本上都是預設)\n選擇 Logging bucket\n設定 include Log 選擇那些可以被 include 到 sink 接收器的 LOG 格式 (這邊基本上都是預設)\n預設的 LOG 格式\n設定 filter Log 這邊就是我們要輸入過濾的地方，先點擊 ADD EXCLUSION，輸入過濾的名稱，以及過濾的內容格式，我們輸入 google 在 Service Health 所提供的格式，最後按下 UPDATE SINK\n新增要過濾的 LOG 格式\n設定完成 等待更新完成，就可以看到我們已經在接收器上設定好過濾條件囉～\n查看詳細接收器設定\n檢查 Log 是否過濾成功 最後再檢查一下 Log 是不是沒有收到該錯誤的 Log 內容\n檢查 LOG 是否不會再出現"},"title":"如何過濾 GCP LOG，減少 Cloud Logging API 的花費"},"/blog/gcp/gke-cronjob-not-working/":{"data":{"":"前陣子公司建立在 Google Kubernetes Engine 叢集上的 CronJob 服務會有短暫時間沒有執行 Job。先前情提要一下，此 CronJob 的設定是每分鐘都會執行 (圖一)，所以理當來說 Log 應該要可以看到每分鐘都有此 CronJob 的紀錄，但有時候會發生 CronJob 短暫時間都沒有執行的狀況，找了一陣子都沒有找到原因，最後開支援單請 Google 那邊協助查看，終於找到原因拉 😍。那就跟我一起看一下發生的過程，以及 Google 幫我們找到的原因，以及要如何解決等等～\n(圖一) CronJob schedule 時間為每分鐘執行","參考資料#參考資料":"[1] Standard cluster upgrades：https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-upgrades\n[2] maintenance-windows-and-exclusions：https://cloud.google.com/kubernetes-engine/docs/concepts/maintenance-windows-and-exclusions","問題發生以及問題原因#問題發生以及問題原因":"我們使用 Google Cloud Platform 裡面的記錄功能，可以看到 (圖二)，在每分鐘執行的 Log 中，有短暫時間沒有執行 Job，但這個時間除了 CronJob 以外，其他的服務都是好的。\n(圖二) Google Cloud Platform 記錄有短暫沒有執行\n找了一陣子都沒有找到原因，於是我們開支援單請 Google 那邊協助查看，Google 那邊找了一陣子後終於找到原因拉！！！我們一起來看看吧 (圖三)\n(圖三) Google 支援單回覆\n就如同 Google 所說，使用提供的指令參數來查詢，叢集在該時段有發生 master control plane 的升級，如 (圖四)，再加上我們建的這個 cluster 是使用 zonal cluster (圖五)，所以叢集只有一個 control plane，當 control plane 在更新時，會無法部署新的 workload，導致該 CronJob 沒有執行 Job。參考資料 [1]\n(圖四) 發生 master control plane 的升級\n(圖五) 有問題的叢集位置類型\nGoogle 的建議是可以考慮使用另一個 regional cluster，讓 master node 在更新時不會只在單一地區，或是一樣使用舊的 zonal cluster，透過設定 Maintenance window 或者 Maintenance exclusions 來降低服務受到 workload 的影響。參考資料 [2]\n就算把 node_pool 裡面的自動升級給停掉，也沒有辦法解決此問題！因為此 master control plane (也就是 master node) 的升級，不是 worker node 的 node pool 升級，是由 GKE 負責維護的，所以他們會定期升級 control plane，也沒辦法停止此類的升級。\n若已經建立好 zonal cluster 後，想要改成 regional cluster ，是沒有辦法使用修改的方式，一定只能重建 cluster，所以大家在建立時要注意～","解決問題#解決問題":"最後我們選擇將叢集給整個重建，來確保 CronJob 不會有沒有執行到的狀況發生，重建叢集跟搬服務的過程很辛苦的 😰 希望大家不要發生 QQ，最後我們來看一下重建完後，在 master control plane 更新的時候，還會不會有 CronJob 沒有執行的情況發生。\n新叢集使用 regional cluster 來建立，在 2/1 也有 master control plane 的升級。 (圖六)\n(圖六) 發生 master control plane 的升級\n查看 CronJob 執行的紀錄可以發現並沒有 Job 沒有執行的情況發生。 (圖七)\n(圖七) 叢集位置類型\n(圖八) 新叢集位置類型"},"title":"Google Kubernetes Engine CronJob 會有短暫時間沒有執行 Job"},"/blog/gcp/iam/":{"data":{"":"IAM 的全名是 Identity and Access Management，當我們藉由 IAM，可以授與特定 Google Cloud 資源的 精細 訪問權限，並防止對其他資源的訪問。疑！？為什麼是精細？我們接著看下去，我們可以採用最小權限安全原則，該原則要求任何人都不應擁有超出實際所需的權限。","iam-測試#IAM 測試":"接下來，我們來測試看看 IAM 實際設定以及用途吧！ 我們會使用 GCP 所以提供的 [Qwiklabs] Cloud IAM：Qwik Start 來進行測試，之後的步驟會跟 Cloud IAM：Qwik Start 內容一樣，所以大家可以邊操作邊參考呦！那我們開始囉 🙃\n進入網頁後，請先登入自己的 Google 帳號，接著點選左上角的 Start Lab，會跳出與下面圖片類似的內容：\n測試用的帳號密碼\n這邊會提供兩組的帳號及密碼，分別是 Username1 以及 Username2 (後面會以 Username 1 跟 2 來說明)，密碼會用共用，且會在同一個專案下。\n用第一個 Username1 登入 GCP 點選 Open Google Console 按鈕來開啟 GCP 主控台 登入的帳號就輸入第一個 Username1 ，密碼輸入共用密碼 ，最後登入 登入成功會進入 GCP 主控台，會跳出下方圖片內容，國家選擇台灣，點選同意 Terms of Service，最後按 AGREE AND CONTINUE 登入 Username 1 GCP 主控台\n用第二個 Username2 登入 GCP 步驟與第一個相同，這邊就不在重複，但建議使用無痕，避免 Username1 跟 Username2 搶來搶去，以及登入帳號要選 Username2，應該不會選錯吧 🤣 Username1 IAM 主控台 到 Username1 的 GCP 主控台首頁 點選左側的 menu \u003e IAM 與管理 點擊頁面上方的 +ADD 按鈕，可以從下拉選單去查看各式各項的專案相關角色，可以看到超級多的角色設定，所以我們一開始才會說他是可以設定 ** 精細** 的訪問權限 我們選擇 Base，右側有 4 種角色，分別是瀏覽者 (Browser)、編輯者 (Editor)、所有者 (Owner)、檢視者 (Viewer)，詳細區別請看下面 👇👇 ADD IAM\n此表是取 Google Cloud IAM 文章基本角色中的定義，簡單說明 4 種角色的差別：\n角色名稱 權限 role/viewer (檢視者) 不影響狀態的只讀操作權限，例如：查看 (但不修改) 現有資源或是資料 role/editor (編輯者) 所有查看者權限，以及修改狀態的操作權限，例如：更改現有資源 role/owner (擁有者) 以下操作的所有編輯權限： 1. 管理項目和項目內的所有資源角色及權限 2. 為項目設置帳單 role/browser (瀏覽者) 讀取權限以及瀏覽項目的層次結構，包含資料夾、組織和 IAM 政策。但此角色不包含查看項目中資源的權限 我們的 Username1 為 owner，Username2 為 viewer\nGoogle 建議：Base 角色包含所有 Google Cloud 服務的數千個權限。除非別無選擇，否則不要授與用戶 Base 角色，請設定最有限的自訂義角色或是可以滿足該需求的角色即可 切換到 Username2 IAM 主控台 我們可以在表格裡面收尋 Username1 跟 Username2 的帳號，可以看一下他們授與的角色是否與上面說的一致，大概可以參考以下圖片：\nUsername1 跟 Username2 權限\n在 Username2 因為是 Viewer 權限，所以點擊上面的 + ADD，不會反應，會跳出以下照片內容：\nUsername2 權限不足\n再切回 Username1 Cloud Storage 接下來我們要建立一個 GCS 儲存空間，點選 menu \u003e Cloud Storage \u003e Browser 點選 Create a bucket 給予他一個獨特的名稱，以及在 Choose where to store your data 選擇 Multi-Region 最後點選 CREATE Create a bucket\n上傳範例檔案 進入新建立的 Cloud Storage，點選 Upload fiiles 按鈕 上傳一個 txt 檔案，可以先取名為 sample.txt，或是上傳後使用最後三個小點圖案內的 Rename 來修改名稱 Upload sample.txt fiiles\n驗證專案檢視者存取權限 我們再切換回 Username2 的主控台，選擇 menu \u003e Cloud Storage \u003e Browser 就可以看到跟上面一樣的儲存區 Username2 被授與 “檢視者 Viewer” 角色，這個角色有不影響狀態的只讀權限。這個範例中說明這個功能，他僅能檢視，沒有辦法上傳\n移除專案存取權限 我們再次切回 Username 主控台，選擇 menu \u003e IAM 與管理 ，找到 Username2 旁邊的鉛筆圖案 修改 Username2 權限\n點擊角色名稱的垃圾桶來移除 Username2 的檢視者權限，點擊 Save 移除 Username2 權限\n這個動作要完成生效到所有服務上，所以會需要一點時間，詳細可以參考點我 檢查 Username2 是否有存取權限 切換到 Username2 主控台，選擇 menu \u003e Cloud Storage \u003e Browser 會發現出現以下的錯誤訊息，代表我們移除權限成功 Username2 沒有權限\n新增儲存角色 我們再次切換到 Username1 主控版，選擇 menu \u003e \u003e IAM 與管理，點選上方 + ADD，在 New principals 上貼上 Username2 的帳號，Role 選擇 Storage Object Viewer 新增 Username2 角色\n查看 Username2 權限\n檢查 Username2 是否有存取權限 切換到 Username2 主控台，因為 Username2 沒有專案檢視者的角色，所以看不到專案以及任何的資源，但這個使用者對我們剛剛設定的 Cloud Storage 有特別的存取權 打開右上角的 Activate Cloud Shell 命令列工具 ，如下圖： 開啟 Activate Cloud Shell 命令列工具\n輸入以下指令 gsutil ls gs://\u003c剛剛建的 Cloud Storage 名稱\u003e 如果出現跟下方圖片一樣，就代表我們設定成功囉！\nUsername2 Storage Object Viewer 權限\n最後的最後，如果有跟我們一步一步來的朋友，在 [Qwiklabs] Cloud IAM：Qwik Start 頁面中，應該會看到一個叫 Check my progress 的按鈕，做完每一步驟，都可以點一下，他會自動去判斷你是否有完成這項動作歐！\nCheck my progress\n到這邊就完成了我們在 IAM 的測試囉～我們知道 IAM 可以設定很多的角色，以及測試了查看、新增、修改、刪除角色的功能，希望大家會喜歡今天的文章 🥰","iam-的工作原理#IAM 的工作原理":"首先我們先來了解一下 IAM 的工作原理，藉由 IAM，我們可以定義誰 (哪一個身份) 對哪些資源有哪種的訪問權限 (角色) 來管理訪問權限控制。什麼是資源？例如， Compute Engine 虛擬機 (GCE)、Google Kubernetes Engine (GKE) 集群和 Cloud Storage 存儲分區都是 Google Cloud 資源，我們用於整理資源的組織或資料夾、項目等也都是資源\nGCP IAM Logo\n我們可以把它理解成\n什麼 『 人 』，可以對什麼『 資源 』，做什麼『 事情 』 IAM 不會直接向用戶授與資源的訪問權限，而是將權限分成多個角色，然後將這些角色授與經過身份驗證的主帳號。(以前 IAM 會將主帳號稱為成員，目前部分 API 仍然使用此術語。)\nIAM 中的權限管理\n可以看到這張圖片，訪問權限管理主要包含三個部分：\n主帳號 (Principal)：主帳號可以是 Google 帳號 (針對用戶)、服務帳號 (針對應用和計算工作負載)、Google 群組或 Workspace 帳號或可以訪問資源的 Cloud Identity 網域等等 角色 (Role)：一個角色對應一組權限，權限決定了可以對資源執行的操作。向主帳號授與某個腳色， 代表授與該角色包含的所有權限給主帳號 政策 (Policy)：允許政策 (Allow Policy) 是將一個或多個主體綁定在各個角色，當想要定義誰 (主體) 對資源擁有何種類型的訪問 (角色) 時，可以創建允許政策並將其附加到資源。 ","參考資料#參考資料":"IAM 概覽：https://cloud.google.com/iam/docs/overview\n什麼是 Cloud IAM？GCP 權限管理服務介紹：https://blog.cloud-ace.tw/identity-security/what-is-cloud-iam/"},"title":"Google Cloud Platform (GCP) - IAM 與管理"},"/blog/git/":{"data":{"":"此分類包含 Git 相關的文章。\n如何啟用 GitLab 的 Package Registry 以及將儲存位置從伺服器改到 GCS 上 如何合併多個 commit，且推到遠端呢？ 部署 Laravel 於 Heroku 搭配 GitLab CI/CD "},"title":"Git 相關"},"/blog/git/git-merge-multiple-commit/":{"data":{"":"當我們在使用 Git 時，常常修改完內容後，會推 commit 到 github or gitlab，在一個分支上開發久了， commit 會累積很多，很雜且很亂，所以我們可以試著將 commit 給合併。\n大家可以使用這個檔案來做練習：點我 GoGo 😉\ngit commit\n可以看到上面這張圖，這個與範例檔案的 commit 相似(不同專案，所以 SHA-1 也會不同，為了模擬所以 commit 相同而已)，我們模擬在同一個分支底下，有很多的 commit，那我們試著把他給合併起來。先說明一下目前的 commit 狀況，我們在 master 分支上有 3 個 commit，且已經推到遠端上。所以我們本地修改後，還要讓遠端的也合併，這個步驟要怎麼做呢？大家可以先想想看，後面會告訴大家答案 🥰","參考資料#參考資料":"如何合併多個 commits：https://zerodie.github.io/blog/2012/01/19/git-rebase-i/\n【狀況題】聽說 git push -f 這個指令很可怕，什麼情況可以使用它呢？：https://gitbook.tw/chapters/github/using-force-push","合併本地端-commit#合併本地端 commit":"首先我們目的是想要讓 add 2.txt 與 add 3.txt 的 commit 合併成 add txt，可以先使用以下指令來找到他的 commit 的 SHA-1：\ngit log git log 查看 commit 的 SHA-1\n要怎麼合併呢？我們先使用 rebase 到不會變動的 commit，也就是 add 1.txt 這個 commit：\ngit rebase -i 3b5bab9d5fb65b965ae55236734103b178f9daf2 git rebase\n下完後，會跳出上面圖片內容，可以看到上面是 rebase interactive (-i) 要執行的指令，下面是每個指令的簡單說明，我們本次會使用的只有 pick 以及 squash，分別的意思是：\npick：會執行該 commit。 squash：會把這個版本的 commit 合併到前一個 commit。 所以我們要將它改成以下：\npick f8e5882 add 2.txt squash 3eb0ef4 add 3.txt 也就是將 3eb0ef4 這個版本的 commit 合併到 f8e5882 的 commit，對應我們的例子，將 add 3.txt 合併到 add 2.txt 這個 commit。\n儲存離開後，會跳出以下的畫面，他會告訴你原本兩個的 commit message 分別是 add 2.txt 以及 add 3.txt，這時候我們要輸入新的 commit message，也就是 add txt，建議可以把原本的訊息註解掉。\n輸入新的 commit\n儲存後，我們查看 git log，就可以看到我們將 add 2.txt 跟 add 3.txt 合併成 add txt 😝\n查看目前合併狀態的 git log","合併遠端-commit#合併遠端 commit":"可以看到下方是我們已經將本機端的 commit 給合併，但遠端還是一樣有 3 個 commit，如果我們就這樣直接推上去，只會多一次的 commit，所以我們該怎辦呢 ?\n遠端與本地端的 commit 不同\n我們就是要使用大家都害怕的：\ngit push -f 強制覆蓋掉分支上的內容，但切記切記，這個只適用於自己的分支上歐～不然會直接大爆炸 💣\n使用 git push -f 後的 commit"},"title":"如何合併多個 commit，且推到遠端呢？"},"/blog/git/gitlab-package-registry-to-gcs/":{"data":{"":"今天接到一個案子，RD 部門之後想要使用 GitLab 的 Package Registry 功能來發布套件，且不想把它存在 GitLab 伺服器上，希望可以直接存到 GCP 的 Google Cloud Storage 上，所以才會有了此篇筆記來記錄一下整個過程。\n版本資訊\nGitLab 14.10 (有部分設定會於新版本棄用，請記得確認好自己的版本是否支援) 先說一下，我們的 GitLab 是使用 docker-compose 來建置，所以後續的實作內容都會以 docker-compose 的方式來介紹。","先查看尚未重啟的-gitlab-package#先查看尚未重啟的 GitLab Package":"由於公司 GitLab 預設有先開啟 packages_enabled，所以我就拿同事用 Helm 寫的 CI，來做測試。當更新 value.yaml 後會自動打包 Package 放到 Package Registry 中，我們直接進入到預設 Package Registry 的儲存位置，是在 /var/opt/gitlab/gitlab-rails/shared/packages/，用指令發現打包的 Package 的確存放於此 ，如下：\n檢查是否還有 Package 在預設儲存位置 (尚未遷移)","參考資料#參考資料":"GitLab Package Registry administration：https://docs.gitlab.com/14.10/ee/administration/packages/","啟動-gitlab-的-package-registry#啟動 GitLab 的 Package Registry":"首先，我們當然要先啟動這項 Package Registry 功能，才可以再之後使用它，我們先看一下 GitLab 啟動的 docker-compose.yml 檔案：\nversion: '3' services: gitlab: image: 'gitlab/gitlab-ee:14.10.5-ee.0' restart: always container_name: gitlab hostname: gitlab-pid logging: driver: \"json-file\" options: max-size: \"100m\" max-file: \"50\" environment: GITLAB_OMNIBUS_CONFIG: | external_url '${GITLAB_DOMAIN}' letsencrypt['enable'] = false gitlab_rails['initial_root_password'] = '${GITLAB_ROOT_PASSWORD}' gitlab_rails['gitlab_shell_ssh_port'] = '${GITLAB_HOST_SSH_PORT}' gitlab_rails['backup_keep_time'] = 79200 gitlab_rails['omniauth_allow_single_sign_on'] = ['google_oauth2'] gitlab_rails['omniauth_block_auto_created_users'] = false gitlab_rails['omniauth_sync_profile_from_provider'] = ['google_oauth2'] gitlab_rails['omniauth_sync_profile_attributes'] = ['name', 'email'] gitlab_rails['omniauth_providers'] = [ { 略過．．． } ] ports: - '${GITLAB_HOST_SSH_PORT}:22' - '${GITLAB_HOST_HTTP_PORT}:80' - '${GITLAB_HOST_HTTPS_PORT}:443' volumes: - './config:/etc/gitlab' - './logs:/var/log/gitlab' - './data:/var/opt/gitlab' 有些設定有略過或是省略不寫，大家就依照自己的設定來看就好～","新增-package-registry-設定#新增 Package Registry 設定":"我們在上方的 gitlab_rails['omniauth_providers'] = [ ... 略 ... ] 之後加上新增 Package Registry 設定內容：\ngitlab_rails['packages_enabled'] = true gitlab_rails['packages_object_store_enabled'] = true gitlab_rails['packages_object_store_remote_directory'] = \"GCS 名稱\" gitlab_rails['packages_object_store_direct_upload'] = true gitlab_rails['packages_object_store_background_upload'] = true gitlab_rails['packages_object_store_proxy_download'] = true gitlab_rails['packages_object_store_connection'] = { 'provider' =\u003e 'Google', 'google_project' =\u003e '專案 ID', 'google_json_key_location' =\u003e '/etc/gitlab/google_key.json' } packages_enabled：啟動 packages packages_object_store_enabled：啟動 packages 對象存儲 packages_object_store_remote_directory：設定 packages 對象存儲位置，這邊要輸入 GCS 的名稱 packages_object_store_direct_upload：設定是否可以直接上傳到對象存儲位置 packages_object_store_background_upload：設定是否以後台方式上傳到對象存儲位置 packages_object_store_proxy_download：設定是否可以透過代理伺服器進行套件下載 packages_object_store_connection：設定連接到對象存儲，由於我們要存到 GCS 上面，需要有這三項 provider、google_project、google_json_key_location 才可以將 packages 存到 GCS 上。如果想用其他的儲存位置，例如 Amazon S3、Azure Blob storage 可以參考 Object storage 詳細設定 ( 其中的 google_json_key_location 是要放可以讀寫 GCS 的 SA SECRET 檔案 ) ","重啟設定後再次檢查-gitlab-package#重啟設定後再次檢查 GitLab Package":"當我們重啟設定後，也有建立好可供我們權限 SA 的 GCS 後，會發現原本存在預設 /var/opt/gitlab/gitlab-rails/shared/packages/ 沒有自動跑到 GCS 上，是因為我們還需要手動下指令將他遷移過去，指令是 gitlab-rake \"gitlab:packages:migrate\"，最後等他跑完我們在檢查一下預設儲存位置就發現已經沒有 Package 了\n檢查是否還有 Package 在預設儲存位置 (已遷移)\n開 GCS 網站來看會發現原先在預設儲存位置的 Package 都可以跑到 GCS 上：\n查看已遷移到 Google Cloud Storage 的 Package"},"title":"如何啟用 GitLab 的 Package Registry 以及將儲存位置從伺服器改到 GCS 上"},"/blog/git/laravel-gitlab-cicd-heroku/":{"data":{"":"經過上一篇文章 如何從頭打造專屬的 GitLab CI/CD 的學習，讓我們了解到 GitLab CI/CD 的整個流程，接著我們本次要把 Laravel 給部署到 Heroku 透過 GitLab 的 CI/CD 去達成，不需要透過任何人工去測試，並上架程式到 HeroKu 上，全部都依賴 GitLab CI/CD，讓我們接著看下去吧！\n當然，此文章程式碼也會同步到 Github ，需要的也可以去查看歐！要記得先確定一下自己的版本 Github 程式碼連結 😏","gitlab-cd-建置#GitLab CD 建置":"我們玩完 CI 後，接著要把程式部署到伺服器或是雲端上，這時候我們不需要透過人工手動的方式，只需要有 CD 來幫我們自動化部署就可以拉！如果不太清楚，可以參考這張圖片：\nGitLab CI/CD workflow (圖片來源：GitLab)\n當我們剛剛進行 CI 的整合測試，最後經過 Review and approve 合併到主分支，這時候如果我們有設定 CD，CD 就會幫我們部署到服務上，我把 CD 流程轉成文字步驟說明：\n把新功能分支合併到 master 分支，代表功能已經可以上線 GitLab 觸發 Gitlab-CI 執行 pipeline Gitlab-CI 執行自動化測試 Gitlab-CI 測試成功後，執行部署到正式伺服器 回傳執行結果至 GitLab 那想要達成自動化部署之前，必須能在遠端用指令下達部署更新！簡單來說有兩件事情：\n要先整理再更新專案時需要哪些指令，並將其寫成腳本 需要獲得伺服器的授權，可以對伺服器下達更新專案的腳本 我們以現在 Laravel 專案來說，套用上面講的兩件事情：\n腳本製作：上線新版本大概要執行以下圖片的內容 Laravel 專案上線前會下達的指令\n對遠端伺服器下指令：通常使用 ssh 與 伺服器做溝通，所以先在伺服器產生授權金鑰給要遠端控制的電腦，如果要給 Gitlab-CI 控制的話，也需要把金鑰存在 GitLab 上，通常使用 ssh user@remote.server 'git pull' 來下達更新專案的指令 本篇我們要部署的是 PaaS 的 HeroKu，可以減少時間去架設環境，就可以達到我們想要的效果，那接著會帶大家從 Heroku 設定開始歐！先簡單介紹一下 Heroku：\nHeroku 是一個支援多種程式語言的雲平台即時服務(PaaS)， 是一種雲端運算服務，提供運算平台與解決方案服務，PaaS 提供使用者將雲端基礎設施部署與建立至使用者端，或者藉此獲得使用程式語言、程式庫與服務。使用者不需要管理與控制雲端基礎設施（包含網路、伺服器、作業系統或儲存），但需要控制上層的應用程式部署與應用代管的環境。\n創建 Heroku 專案 那要使用 Heroku 當然需要一組帳號拉，建立帳號我應該不用再多介紹了吧 🤡 我們直接到 Heroku 頁面，右上角 New，點選 Create new app，輸入本次專案名稱，我就取叫 laravel-gitlab-cicd-heroku (這個不能與別人重複，因為他會生成專屬網頁)， 進去後，點選右上角有一個 Open app，就會跳出這個專案專屬的網頁：\nHeroku 專屬網頁\n設定 HeroKu 與 GitLab 連線 先點選右上角個人頭像 → Account setrtings → 在 Account 往下滑 → API Key，點選 Reveal 並將該 API 記住，這是等等透過 GitLab 部署時會用到的 API Token：\n取得部署的 API Token\n回到 GitLab 專案底下，Settings → CI/CD → Variables，他可以將變數設定在這邊，再讓 .gitlab-ci.yml 來抓取變數，設定以下兩個變數：(詳細可以參考官網)\nKey 名稱(HEROKU_PRODUCTION_PROJECT_NAME)，Value 值(設定我們剛剛在 Heroku 部署的專案名稱，我的是 laravel-gitlab-cicd-heroku) Key 名稱(HEROKU_PRODUCTION_API_KEY)，Value 值(這個就是我們上面的 API Key，每個人都要用自己的歐！上面的我已經重設了 😎 ) gitLab 設定 Variables\n這邊要注意先把預設的 Protect variable 給關閉，他預設會只能在受保護的分支或標籤運行，但我們這此以簡單為主，所以這些設定都先關掉。 新增 Heroku 識別檔案 接下來我們要新增一個檔案名為 Procfile，它是 Heroku 部署更新時會啟動的對象，注意他沒有副檔名，我們在裡面輸入以下：(我們使用合併後的 master)\n新增 HeroKu 識別檔案\n它代表我們網頁服務使用 apache2 指令運行並把入口指向專案資料夾中的 laravel 專案的入口資料夾。\n修改 .gitlab-ci.yml 我們修改原本用來 CI 的腳本，來設定自動化部署的任務 Job 及腳本\n.gitlab-ci.ymlimage: lorisleiva/laravel-docker:latest Production_Deploy: stage: Production_Deploy before_script: - apk add ruby ruby-dev ruby-irb ruby-rake ruby-io-console ruby-bigdecimal ruby-json ruby-bundler yarn ruby-rdoc \u003e\u003e /dev/null - apk update - gem install dpl \u003e\u003e /dev/null script: - dpl --provider=heroku --app=$HEROKU_PRODUCTION_PROJECT_NAME --api-key=$HEROKU_PRODUCTION_API_KEY 最後上傳 GitLab 來觸發 Gitlab-CI 執行自動化部署 (上傳指令就不多說囉，想必大家都會了吧！，不會的話可以去看 Git 介紹，裡面有詳細的介紹 😍 )\n觸發 Gitlab-CI 執行自動化部署\n可以看到部署成功，我們也來看看 Runner 運作狀況：\nRunner 運作狀況\n看到他成功將服務給部署到 https://laravel-gitlab-cicd-heroku.herokuapp.com/。\n既然已經部署好了，當然要去看一下我們的網頁啊，但當我們打開部署好的網頁，會發現跳出 500 Error，雖然他與我們 CI/CD 沒有關係，但我們還是試著解決，那這個問題會發生是因為我們沒有給環境變數的 APP_KEY，這個 Key 可以在專案的 .env 取得，拿到後開啟 Heroku → Setting → Config vars 將 APP_KEY 設定上去。\nRunner 運作狀況\n最後重新整理 https://laravel-gitlab-cicd-heroku.herokuapp.com/，就可以看到我們部署上去的網站囉！\n透過 CD 部署到 Heroku 的 Laravel 首頁","gitlab-ci-建置#GitLab CI 建置":"上傳 Laravel 專案 接下來我們要上傳含有 Unit Test 專案到 GitLab 上，步驟如下，如果已經熟悉如何將專案推到 GitLab，可以直接跳到 在 GitLab 上執行單元測試\n在 gitlab.com 上點選建立專案，選擇 Create blank project，也可以直接瀏覽該網址 https://gitlab.com/projects/new#blank_project。 輸入專案名稱可以選擇 Project deployment target 為 Heroko，選擇 Public，最後按下 Create project 在 GitLab 上建立新專案\n於專案資料夾下加入 remote 遠端 GitLab，並 Push 將專案推上去。 將 Laravel 專案推到 GitLab 上\n成功推上去，可以到 GitLab 上，看到我們剛剛的專案！\n成功推到 GitLab 上\n在 GitLab 上執行單元測試 要在 GitLab 上執行 CI/CD 就需要有 Runner，這次我們選擇使用 gitlab.com 的 Shared runners，想要使用 Specific runners，可以查看上一篇 如何從頭打造專屬的 GitLab CI/CD 文章\n本次使用 Share runners\n接下來在專案的根目錄撰寫我們的 .gitlab-ci.yml 檔案，之後再次上傳 GitLab，當我們根目錄有此檔案，GitLab-CI 就會讀取並依照內容啟動 Runner 來執行工作：\n.gitlab-ci.ymlimage: lorisleiva/laravel-docker:latest Unit_test: before_script: - composer install --prefer-dist --no-ansi --no-interaction --no-progress --no-scripts script: - ./vendor/bin/phpunit --testsuit Unit --coverage-text --colors=never 說明一下這個 yml 檔內的設定是在做什麼：\nimage：因為我們執行 CI/CD 過程中，需要有 PHP、Compose、NPM 等工具，有這些套件管理工具就可以延伸去安裝更多套件，如果一開始沒有安裝，就會很麻煩，其中一個辦法就是去 Runner 環境修改並安裝，但因為方便以及我們這次使用 Share runners，所以不能修改別人的 Runner，另一個辦法是可以使用 image 關鍵字，可以讓 Runner 切換到另一個環境去執行工作 (Job)，我們這邊使用 lorisleiva/laravel-docker:latest ，他裡面已經幫我們安裝好上述的工具了！ Unit_test：這邊也是我們的 Job，那裡面主要是先用 composer install 去安裝我們需要的套件，最後在執行 phpunit 來做單元測試。 上傳 .gitlab-ci.yml 接著我們使用以下指令將含有 .gitlab-ci.yml 的專案上傳到 GitLab，並回到 GitLab 選擇 CI/CD，可以查看目前的 Pipelines，會有我們剛剛所新增的 Runner。\n將 .gitlab-ci.yml 推到 GitLab\n查看 Runner 已經進行執行單元測試檢測\n可以看到 Runner 先安裝我們的環境，再執行單元測試的腳本\n設置須通過測試才可以合併 當我們有了測試還不夠，要怎麼確保每隻要上線 (合併到主分支) 的程式都有經過測試才上線呢？\n接下來我們可以在 GitLab 裡面做這些設定，先到專案的 Setting → General → Merge requests → Merge checks 點選 Pipelines must succeed：\n點選 Pipelines must succeed 來確保程式合併前都必須經過測試\n測試是否可以阻擋未成功情況 我們先模擬要開發新功能，所以在 master 最新 commit 下，建立一個新分支 new\ngit checkout -b \"new\" 接著修改單元測試，故意新增錯誤的測試，開啟專案的 tests/Unit/ExampleTest.php，最下面加上紅色框框程式碼：\n新增錯誤測試，還模擬看看是否能成功擋住\nassertEquals 會檢查這兩個值是否相同，不同的話，就會跳出錯誤，所以我們故意輸入 1 和 2。\n並將它上傳到 GitLab，並發出 Merge Request 看看會有什麼結果！\n將新增錯誤的 ExampleTest 加入暫存，推到 GitLab\n並將 new 分支透過 Merge Request 來合併到 master\n可以看到我們合併在 Pipeline 測試時，因為 new 沒有通過測試，所以也沒有辦法進行合併！\n分支 new 沒有通過測試，所以沒有進行 Merge","參考資料#參考資料":"Gitlab-CI 入門實作教學 - 單元測試篇：https://nick-chen.medium.com/gitlab-ci-%E5%85%A5%E9%96%80%E7%AD%86%E8%A8%98-%E5%96%AE%E5%85%83%E6%B8%AC%E8%A9%A6%E7%AF%87-156455e2ad9f\nGitlab-CI 自動化部屬部署：https://medium.com/@nick03008/%E6%95%99%E5%AD%B8-gitlab-ci-%E5%85%A5%E9%96%80%E5%AF%A6%E4%BD%9C-%E8%87%AA%E5%8B%95%E5%8C%96%E9%83%A8%E7%BD%B2%E7%AF%87-ci-cd-%E7%B3%BB%E5%88%97%E5%88%86%E4%BA%AB%E6%96%87-cbb5100a73d4\n部署 Laravel 於 Heroku 搭配 Gitlab CI/CD：https://medium.com/@vip131430g/%E9%83%A8%E7%BD%B2-laravel-%E6%96%BC-heroku-%E6%90%AD%E9%85%8D-gitlab-ci-cd-6d59a66aebdb","建立-laravel-專案#建立 Laravel 專案":"請大家依照 Laravel 官方文件來建立 Laravel 環境，也可以看小弟我的文章拉 👆👆👆，請記得要先安裝好 php 以及 composer，接著按照以下步驟來建立。\n新建一個 Laravel 新專案\n這時候瀏覽 http://127.0.0.1:8000，如果都正確，應該會看到 Laravel 的首頁\nLaravel 首頁","測試本地-unit-test#測試本地 Unit Test":"接著我們剛剛有提到選用 Laravel 的原因是 Laravel 有 PHPUnit 單元測試可以使用，所以我們現在先在本地端來測試 Unit Test，專案預設有放一個單元測試在 tests/Unit/ExampleTest.php。我們先再次確認環境是否有安裝好，再來執行單元測試。\n在本地端執行單元測試\n執行後，應該都會是通過的畫面，如下圖：\n執行單元測試結果","版本資訊#版本資訊":" macOS：11.6 Docker：Docker version 20.10.14, build a224086 Laravel Installer：2.3.0 Laravel Framework：9.14.1 gitlab.com：GitLab Enterprise Edition 15.1.0-pre 首先，我們第一步驟就是先建立一個 Laravel 專案，至於為什麼要選擇用 Laravel 來當作 GitLab CI/CD 的範例呢？因為 Laravel 內建有 PHPUnit 的測試腳本，可以讓我們在 CI 測試時，更好的展現 CI 的功能！，有關於 Laravel 相關內容，這邊一樣推薦兩篇文章給大家閱讀：🤣\nLaravel 介紹 (使用 Laravel 從零到有開發出一個留言板功能並搭配 RESTful API 來實現 CRUD) Laravel 進階 (內建會員系統、驗證 RESTful API 是否登入、使用 Repository 設計模式) 又工商了一波 XD"},"title":"部署 Laravel 於 Heroku 搭配 GitLab CI/CD"},"/blog/kubernetes/":{"data":{"":"此分類包含 Kubernetes 相關的文章。\n在正式環境上踩到 StatefulSet 的雷，拿到 P1 的教訓 部署 Pod 遇到 container veth name provided (eth0) already exists 錯誤 Kubernetes (K8s) HorizontalPodAutoscaler (HPA) 原理與實作 Kubernetes (K8s) 自定義 PHP HorizontalPodAutoscaler (HPA) 指標 用大型社區來介紹 Kubernetes 元件 "},"title":"Kubernetes"},"/blog/kubernetes/gcp-k8s-hpa-php-custom-metrics/":{"data":{"":"此篇要介紹 HorizontalPodAutoscaler 的自定義指標，K8s 內建的指標 (metrics) 只支援 CPU 以及 Memory，如果我們今天想要使用其他的指標來讓 HPA 擴縮呢！？ 不知道什麼是 HorizontalPodAutoscaler 嗎？可以先查看：\nKubernetes (K8s) HorizontalPodAutoscaler (HPA) 原理與實作 這時候我們就必須使用自定義指標，我們一樣來說說他的工作原理吧：HorizontalPodAutoscaler 是怎麼取得自定義指標，以及是跟誰拿到指標的呢？我們從下圖得知：\nKubernetes HPA : ExternalMetrics+Prometheus\nHorizontalPodAutoscaler 會先訪問 K8s 的 API，並向 API 取得指標資料。這邊的 API 就是 custom.k8s.io/v1beta1。\n大致了解後，我們就來進入今天的重點，也就是透過自定義化 PHP 的指標來讓 HorizontalPodAutoscaler 進行擴縮，這次使用的平台是 Google Cloud Platform，前面介紹 GCP 服務的大家可以參考 Google Cloud Platform (GCP) 百科全書 - 介紹與開頭 [ EP.0 ]，我們這邊就直接跳到程式碼與操作部分。\n此文章程式碼也會同步到 Github ，需要的也可以去 clone 使用歐！ Github 程式碼連結 😆","參考資料#參考資料":"Autoscaling Deployments with Cloud Monitoring metric：https://cloud.google.com/kubernetes-engine/docs/tutorials/autoscaling-metrics#custom-prometheus_1\nGoogleCloudPlatform/k8s-stackdriver：https://github.com/GoogleCloudPlatform/k8s-stackdriver/tree/master/custom-metrics-stackdriver-adapter\nhipages/php-fpm_exporter：https://github.com/hipages/php-fpm_exporter\nScaling PHP-FPM with custom metrics on GKE/kubernetes：https://www.ashsmith.io/scaling-phpfpm-with-custom-metrics-gke","實作#實作":"安裝自定義的 Adapter 我們要先 Apply 自定義的 Adapter，這邊我們使用 Google 提供的 Stackdriver Adapter 來使用 (也可以直接使用 Github 程式碼中的 adapter_new_resource_model.yaml 檔案歐)：\nkubectl apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/k8s-stackdriver/master/custom-metrics-stackdriver-adapter/deploy/production/adapter_new_resource_model.yaml Deployment apiVersion: apps/v1 kind: Deployment metadata: name: demo labels: app: demo spec: replicas: 1 selector: matchLabels: app: demo template: metadata: labels: app: demo spec: containers: - name: php-fpm image: php:fpm workingDir: /var/www/service ports: - containerPort: 9000 resources: requests: cpu: 100m memory: 1G limits: cpu: 100m memory: 1G volumeMounts: - name: application mountPath: /var/www/service/ - name: php-fpm-config mountPath: /usr/local/etc/php-fpm.d/www.conf subPath: www.conf - name: nginx image: nginx:alpine workingDir: /var/www/service ports: - containerPort: 80 volumeMounts: - name: application mountPath: /var/www/service/ - name: nginx-config mountPath: /etc/nginx/conf.d/ - name: phpfpm-exporter image: hipages/php-fpm_exporter:latest env: - name: PHP_FPM_SCRAPE_URI value: \"tcp://localhost:9000/status\" - name: PHP_FPM_FIX_PROCESS_COUNT value: \"true\" resources: requests: cpu: 10m limits: cpu: 10m - name: prometheus-to-sd image: gcr.io/google-containers/prometheus-to-sd:v0.9.0 ports: - containerPort: 6060 protocol: TCP command: - /monitor - --stackdriver-prefix=custom.googleapis.com - --monitored-resource-type-prefix=k8s_ - --source=:http://localhost:9253 - --pod-id=$(POD_NAME) - --namespace-id=$(POD_NAMESPACE) resources: requests: cpu: 10m limits: cpu: 10m env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace volumes: - name: application emptyDir: {} - name: php-fpm-config configMap: name: php-fpm-conf - name: nginx-config configMap: name: nginx-conf 我來簡單說明一下，這是一個 Deployment，我們在每一個 Pod 裡面都放 4 個 Container，分別是 php-fpm、nginx、phpfpm-exporter、prometheus-to-sd\nphp-fpm 就是我們要使用的 php，nginx 會提供 9000 Port 讓 phpfpm-exporter 去抓到目前的 Process 數值，最後丟給 prometheus-to-sd，讓他去通知我們剛剛所安裝的 Adapter，就可以透過 API 讓 HPA 知道！聽不太懂嗎？沒關係，幫大家畫了一張圖，請參考下方圖片：\n架構圖\n可以看到 Deployment 裡面，我們使用 ConfigMap 來掛載 php 的 www.conf 以及 nginx 的設定檔，那我們接下來就寫一份 ConfigMap 吧！\nConfigMap apiVersion: v1 kind: ConfigMap metadata: name: php-fpm-conf data: www.conf: | [www] user = 900 group = 900 listen = 9000 listen.owner = 900 listen.group = 900 listen.mode = 0660 pm = dynamic pm.max_children = 150 pm.max_requests = 300 pm.start_servers = 24 pm.min_spare_servers = 24 pm.max_spare_servers = 126 pm.status_path = /status ping.path = /ping ping.response = OK catch_workers_output = yes request_terminate_timeout = 300 clear_env = no --- apiVersion: v1 kind: ConfigMap metadata: name: nginx-conf namespace: ian data: nginx.conf: | server { listen 80; listen [::]:80; server_name _; root /var/www/service/; index index.php; location / { try_files $uri $uri/ /index.php$is_args$args; } location ~ ^/(status|ping)$ { fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; } } 這份 ConfigMap.yaml 檔案裡面分成 php-fpm-conf 來放 www.conf，以及 nginx-conf 來放 nginx.conf 檔案，這邊要注意的是 www.conf 記得要加上 pm.status_path = /status，phpfpm-exporter 是透過這個頁面來獲得 Process 數量。\nHorizontalPodAutoscaler apiVersion: autoscaling/v2beta2 kind: HorizontalPodAutoscaler metadata: name: demo-hpa spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: demo minReplicas: 1 maxReplicas: 10 metrics: - type: Pods pods: metric: name: phpfpm_active_processes target: averageValue: 50 type: AverageValue - type: Pods pods: metric: name: phpfpm_idle_processes target: averageValue: 50 type: AverageValue - type: Pods pods: metric: name: phpfpm_total_processes target: averageValue: 50 type: AverageValue - type: Pods pods: metric: name: phpfpm_accepted_connections target: averageValue: 50 type: AverageValue 這份 HorizontalPodAutoscaler 我們使用的版本是 autoscaling/v2beta2， v2beta1 跟 v2beta2 的設定檔語法有些不同！\n可以看到我們 metrics.pods.metric.name 分別是 phpfpm_active_processes (活動的進程個數)、phpfpm_idle_processes (空閒的進程個數)、phpfpm_total_processes (總共的進程個數)、phpfpm_accepted_connections (當前的連接數量)，如果超過我們所設定的 target 值，HPA 就會作動。\n我們依序把 Deployment \u003e ConfigMap \u003e HorizontalPodAutoscaler 的 yaml 檔案給 Apply，可以觀察一下 Pod 是否都有正常啟動：\nPod 是否正常啟動\n我們到 HorizontalPodAutoscaler 查看我們所設定的 metrics 是否都有抓到目前的值：\nmetrics 是否都有抓到目前的值\n我們也可以用 Google Cloud Platform 的監控來查看：\nGoogle Cloud Platform 的監控\n以上就完成 自定義 HorizontalPodAutoscaler 指標囉～ 😍"},"title":"Kubernetes (K8s) 自定義 PHP HorizontalPodAutoscaler (HPA) 指標"},"/blog/kubernetes/k8s-hpa/":{"data":{"":"此篇是要介紹 HorizontalPodAutoscaler (HPA) 的原理以及實作內容，那我們先來說明一下 HorizontalPodAutoscaler 是什麼吧！","horizontalpodautoscaler-原理#HorizontalPodAutoscaler 原理":"HorizontalPodAutoscaler (HPA) 中文可以叫水平 Pod 自動擴縮，他會自動更新工作負載資源 (Deployment 或 StatefulSet)，其目的是透過自動擴縮工作負載來滿足使用需求。\n簡單來說他會根據你現在的負載去調整你的 Pod 數量，當目前的負載超過配置的設定時，HPA 會指示工作負載資源 (Deployment 或 StatefulSet) 擴增，來避免塞爆同一個負載資源。\n如果負載減少，且 Pod 數量高於配置的最小值，HPA 也會指示工作負載資源 (Deployment 或 StatefulSet) 慢慢縮減。其中水平 Pod 自動擴縮不適用 DaemonSet 工作負載資源。\nHPA 是如何工作呢？ HorizontalPodAutoscaler 工作流程圖\nKubernetes 將水平 Pod 自動擴縮定義為一個間歇運行的控制迴路，他不是連續的，其間隔是由 kube-controller-manager 的 --horizontal-pod-autoscaler-sync-period 參數來配置，預設是 15 秒，controller-manager 會依據每一個 HPA 定義的 scaleTargetRef 來找到是哪一個工作負載資源需要進行水平 Pod 自動擴縮，然後根據目標資源的 .spec.selector 標籤選擇對應的 Pod，並從資源指標 API 或自定義指標獲取 API，目前總共有三種的資源指標，分別是 CPU、Memory、自定義指標。\n有關於其他 Kubernetes 觀念部分，可以先查看：\nKubernetes : Kubernetes (K8s) 介紹 - 基本 kubernetes : Kubernetes (K8s) 介紹 - 進階 (Service、Ingress、StatefulSet、Deployment、ReplicaSet、ConfigMap) 此文章程式碼也會同步到 Github ，需要的也可以去 clone 使用歐！要記得先確定一下自己的版本 Github 程式碼連結 😆","參考資料#參考資料":"Horizontal Pod Autoscaling：https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/\nHorizontalPodAutoscaler Walkthrough：https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/","實作#實作":"這次實作要使用的叢集是 Minikube，所以照以前文章一樣，我們先啟動 Minikube。本次會使用 K8s 的管理工具：k8slens 來做為輔助，大家有興趣也可以先去下載來使用歐 😘\n啟動 Minikube 叢集 啟動 Minikube 叢集 minikube start --vm-driver=docker 啟動 Minikube\n設定 Metrics Server 由於我們 HorizontalPodAutoscaler 會根據現在的負載來判斷是否要新增新的 Pod 來解決負載資源用完的問題，所以第一個條件就是要先獲的目前的負載資源使用量，這時候我們必須先在 K8s 叢集上安裝 Metrics Server，透過他讓我們可以知道目前的負載使用量！\n先到 kubernetes-sigs/metrics-server 下載最新的 components.yaml 檔案下來，以我這次示範的版本為例，大家可以點我下載 👇\n下載完，需要先修改兩個地方，才能 apply 到 Minikube 叢集，第一個是修改 kubelet-preferred-address-types=InternalIP，以及新增 kubelet-insecure-tls=ture 讓 Metrics Server 禁用 TLS 證書驗證，詳細可以參考以下照片：\n修改 components.yaml\n接著 apply 這個 yaml 檔案： apply components.yaml\n可以檢查一下 deployment.apps/metrics-server 是否有成功建立或是 Pod 是否有問題：\n檢查 metrics-server 是否有問題\n開始撰寫實作檔案 接著我們就依照官方的 HorizontalPodAutoscaler Walkthrough 的文章開始囉！首先先寫一個 index.php，這個檔案是用於後續測試 HPA 負載使用量的程式： index.php\n新增 Dockerfile，我們使用 php:5-apache 的 image，並複製剛剛寫的 index.php 到容器內： Dockerfile\n將該 Dockerfile Build 起來，推到 DockerHub 上，這部分就不多說明，有興趣可以參考以前文章，大家可以直接使用 880831ian/php-apache 我推好的 image 來使用。 新增 Deployment.yaml，裡面會使用我們剛剛打包推到 DockerHub 上的 image： apiVersion: apps/v1 kind: Deployment metadata: name: php-apache spec: selector: matchLabels: run: php-apache replicas: 1 template: metadata: labels: run: php-apache spec: containers: - name: php-apache image: 880831ian/php-apache ports: - containerPort: 80 resources: limits: cpu: 500m requests: cpu: 200m --- apiVersion: v1 kind: Service metadata: name: php-apache labels: run: php-apache spec: ports: - port: 80 selector: run: php-apache 這邊大家可以先記得我們在 containers.resources 有設定 cpu 的 requests 為 200m。後續在計算副本數時會再提到 😬\n最後將 Deployment.yaml 給 apply，檢查一下是否有正常啟動： k8slens 檢查是否正常\n使用 kubectl autoscale 來幫助我們創建 HorizontalPodAutoscaler： kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10 我們在 Deployment.yaml 裡面 containers.resources 有設定 cpu 的 requests 是 200m，也就是 200 milli-cores，當我們現在設定 HPA 的平均 CPU 使用率為 50%，所以我們只要超過 200m / 2 = 100m，也就是 requests 超過 100 milli-cores 就會產生新的 Pod。這邊最少 Pod 為 1，最多為 10。後續等測試時，會帶大家計算他是如何產生 Pod 的 😏 查看目前 HPA 使用量，因為我們這個 php-apache 還沒有任何的訪問，所以是 0% / 50% (承上面所說，所以這邊的值是 0 / 100 milli-cores)，後面也可以看到我們所設定最高跟最低的 Pod，以及目前的副本數。 查看目前 HPA 使用量\n當然我們除了用剛剛的指令以外，我們也可以自己寫 HPA 的 yaml 檔案。我這邊先使用 kubectl get hpa php-apache -o yaml \u003e hpa.yaml 來將剛剛用指令 run 起來的 HPA 變成 yaml 檔，我們來看看裡面有哪些內容吧： apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: creationTimestamp: \"2022-07-12T03:16:11Z\" name: php-apache namespace: default resourceVersion: \"19454\" uid: dde68e68-9b6e-46a8-b50f-5525b8ec3bdf spec: maxReplicas: 10 metrics: - resource: name: cpu target: averageUtilization: 50 type: Utilization type: Resource minReplicas: 1 scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: php-apache 後面狀態省略.... apiVersion：要記得使用 autoscaling/v2 kind：HorizontalPodAutoscaler maxReplicas：是我們剛剛用指令的最大 Pod 副本數 metrics：我們指標設定 cpu resource，其設定平均使用率為 50 % (百分比) minReplicas：剛剛用指令的最小 Pod 副本數 scaleTargetRef：設定我們這個 HPA 是依照 php-apache 這個 Deployment。 ","常見問題及解決辦法#常見問題及解決辦法":" ℹ️ Q1 . 出現 x509: cannot validate certificate for 192.168.XXX.XXX because it doesn’t contain any IP SANs 錯誤\nAns 1：是因爲沒有加入 kubelet-insecure-tls=ture 讓 Metrics Server 禁用 TLS 證書驗證，才導致錯誤發生。","測試-hpa#測試 HPA":" 接下來我們都設定好後，我們要來模擬增加負載，看看 HPA 的後續動作，首先我們先使用以下指令來持續觀察 HPA： kubectl get hpa php-apache --watch NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE php-apache Deployment/php-apache 0%/50% 1 10 1 4h40m 以及執行以下指令，該指令是建一個新的 Pod，由新的 Pod 無限循環的去向 php-apahe 服務發出請求 kubectl run -i --tty load-generator --rm --image=busybox:1.28 --restart=Never -- /bin/sh -c \"while sleep 0.01; do wget -q -O- http://php-apache; done\" 模擬增加負載\n我們使用 k8slens 觀察，會發現當負載使用量超過我們前面所設定的 50%，也就是 100 milli-cores 時，他就會自動長新的 Pod 出來： HPA 自動長 Pod\n我們切回去看觀察 HPA 指令 kubectl get hpa php-apache --watch： 觀察 HPA\n我們剛剛有說超過 50%，也就是 100 milli-cores 時，會長新的 Pod。我們以上面圖片的來說明，來計算期望需要幾個副本?\n官方有提供一個公式： 期望副本数 = ceil[目前副本数 * (目前指標 / 期望指標)] ，可以看到我們負載從 0% 從到 250%，也就是我們實際上是從 0 變成 500 milli-cores (250 / 50 _ 100 )。我們將值帶入公式內，目前的副本：1 _ (目前指標：500 / 期望指標是：100)得出來的值是 5，如果有小數，因為前面有ceil，所以會取整數(不可能開半個 Pod 對吧 🙄)，最後以這個例子來說，最終得到的 期望副本数 = 5\n根據我們計算出來的副本數，他就會依照計算結果，幫我們自動生成該數量的 Pod，來減緩同一個 Pod 的負載量，接著我們先中斷測試指令，再繼續觀察 HPA ： 觀察 HPA\n可以發現因為我們將測試指令中斷後，負載使用量會慢慢降低，但負載降低後，副本數不會馬上變回去，因為怕如果又有大量的使用量會導致 Pod 來不及長出來，所以預設是 5 分鐘 (--horizontal-pod-autoscaler-downscale-stabilization) 才會減至原來的 Pod 數量\nPod 自動減少","版本資訊#版本資訊":" macOS：11.6 Minikube：v1.25.2 Kubectl：Client Version：v1.24.1、Server Version：v1.23.3 Metrics Server 3.8.2 "},"title":"Kubernetes (K8s) HorizontalPodAutoscaler (HPA) 原理與實作"},"/blog/kubernetes/k8s-plain/":{"data":{"":"前陣子對於 Kubernetes 部分內容還不是很清楚，在網路上閒逛的時候發現一篇很有趣的文章，標題是 『 給行銷跟業務的 Kubernetes 101 中翻中介紹 』，點進去後才發現，作者 Phil Huang 將 Kubernetes 元件的內容用大型住戶社區來介紹，讓我更清楚每一個元件的意思，以下我會用我理解的以及作者的思考邏輯來去寫這篇筆記，再次感謝作者文章 😍","kubernetes-元件白話文#Kubernetes 元件白話文":"\n大型社區示意圖 (圖片來源：蘋果地產)\n可以看到上面這張圖片，他是一個很典型的大型社區，我們這次的 Kubernetes 元件白話文，會以大型社區來當作現實中的元件，以社區的例子來說明 Kubernetes 。\n當然，我們之前的文章也有介紹過 Kubernetes，有興趣可以先飛回去看看！\nKubernetes (K8s) 介紹 - 基本\nKubernetes (K8s) 介紹 - 進階 (Service、Ingress、StatefulSet、Deployment、ReplicaSet、ConfigMap)\nKubernetes (K8s) 搭配 EFK 實作 (Deployment、StatefulSet、DaemonSet)\n我們先想像一下我們建立 Kubernetes 完整的叢集服務，就好比是建立一個大型的社區，所以以下會將名詞與社區來做連結，那．．．開始囉！\nKubernetes 元件 Kubernetes：建立這個大型社區的藍圖，規劃的社區內的大大小小的設計。 虛擬化平台、公有雲平台、實體機器：就是我們要蓋社區的地皮。(虛擬化平台：vSphere/Proxmox/VMware、公有雲平台：AWS/GCP/Azuer、實體機器：就是實體機器 😂) OS 作業系統：每棟大樓的骨架和地基。 Master Node：住戶管委會所居住大樓 (真好自己有所屬的大樓ＸＤ)，為了保證他們不會吵架，所以建議最少需要三棟。 Etcd Cluster：管委會的人，一樣為了怕一黨獨大(?，所以建議最少需要有三位管委，且互相投票選出一個頭頭。 Worker Node：就是偶們住戶所居住的大樓。 Pod：住戶，所以我們一棟 Worker Node 大樓，可以有很多 Pod 住戶。 Pod IP：每個住戶的門牌，既然是門牌，代表他也不會重複。 Container Registry：包裹集中的存放中心。 Container Images：還沒有被打開的包裹。 Containers：已經被打開且正在使用的包裹，那每個 Pod 住戶裡面，可以有一個或很多個以上的 Containers 包裹。 Service：社區裡面的社團，例如：羽球社、麻將社等等，可以集合大家的地方。 Service Mesh：社團的聯絡名冊，會記住誰是哪一個社團的成員。 Ingress Controller：社區的大門，可以指定讓社區成員都固定走同一個或是多個門的入口管控。 Egress Controller：社區的後門，可以指定讓社區成員都固定走同一個或是多個門的出口管控。 Internal DNS / Service Discovery：大樓住戶的電話簿。 External DNS：指向各大樓的路標。 OCI (Open Container Initiative)：制定大樓鋼筋水泥或是行李箱標準的組織。 CRI (Container Runtime Interface)：大樓鋼筋水泥的廠商。 CNI (Container Network Interface)：大樓水電系統的廠商。 CSI (Container Storage Interface)：大樓空間規劃的廠商。 Bastion：專門維護社區的工程車。 常用套件 Promethus：社區整體的監控中心 (Meterics)，一堆攝影機的管理室 XD Grafana：監控中心裡面大型的 LED 儀表板。 Elaticsearch：社區整體的情資中心 (Logging)，這應該是一群愛八卦的大媽吧 😏 Kibana：情資中心裡面大型的 LED 訊息版。 ","參考資料#參考資料":"給行銷跟業務的 Kubernetes 101 中翻中介紹","常見問題#常見問題":"作者也有整理了一些常見的問題，我把它整理一下挑選出幾個我自己一開始也會有疑問的問題，我們一起來看看吧！一樣我會用我所理解的意思來介紹 ~\nQ1：為什麼 Kubernetes 的最小單位是 Pod 這個要怎麼理解？ Ans ： 試想一下，難道管委會會管你的包裹裡面內容物放什麼嗎 XD\nQ2：Docker 在 Kubernetes 的角色是什麼？ Ans ： 是眾多的 CRI (Container Runtime Interface) 選擇之一，也就是大樓的鋼筋水泥廠商有很多間，有一間叫做 Docker 的廠商特別有名。\nQ3：呈上，那 CRI/CNI/CSI 是不是也可以替換成其他的廠商？ Ans ： 當然可以，現在可以看到越來越多廠商都開始支援 Kubernetes 就是因為這個原因，因為一般情況下，Kubernetes 並不會特別限定 大樓鋼筋水泥廠商、大樓水電系統廠商、大樓空間規劃廠商，只要有符合特定的標準即可，但要留意 Kubernetes 的版號也會受到這三個介面支援發行的版號所影響，要留意相容性的問題！\nQ4：整個住戶社區最重要的角色是什麼？ Ans ： 那三棟管委會大樓，和裡面的的三位委員，三位掛掉一位還可以維持正常運作，掛掉兩位會維持唯獨運作。\nQ5：Kubernetes、VM、Container 的差異性 Ans ： 我們可以建立好一個大樓(VM)，隨意放置一個或是多個包裹(Containers)，必須手動管理這些包裹，如果資源不足或是這棟大樓倒了，就沒有辦法自動轉移這些包裹的內容了。\n但如果是 Kubernetes，我們就可以建立多個大樓(VM)，透過 Kubernetes 所規定的放置計畫 (例如：Deployment、DaemonSet)，我們將可以統一調度這些 Pod，當某棟大樓資源不夠或是這棟大樓倒時，可以根據 Kubernetes 規則來進行搬遷或是擴充大樓。\nQ6：為什麼有人會把 Container 跟 Pod 混在一起講 Ans ： 因為大部分的情況下，都是一個住戶(Pod)放一個包裹(Container)，，才會導致這樣的誤會。但實際上，一個住戶(Pod)是可以放一個或多個包裹在裡面的！\nQ7：什麼是 Node Scaling ? Ans ： 可以把它理解成，當住戶太多時，Kubernetes 會自動或手動興建大樓，來把多出來的用戶給塞進去。\nQ8：在 Q5 有提到放置計畫是什麼意思？ Ans ： 只要你有需要將包裹放在社區內，都必須提出部署計畫(Deployment) 給管委會審核，只要通過審核，他們依照你事先聲明的計畫，盡最大可能性來放置包裹。\n所以當我們有任何變更計畫時，都需要重新提交一份新的部署計畫給管委會重新審核和接受。"},"title":"用大型社區來介紹 Kubernetes 元件"},"/blog/kubernetes/k8s-statefulset-podmanagementpolicy/":{"data":{"":"此文章要來記錄一下前陣子在公司的正式環境踩到 StatefulSet 的雷，事情是這樣的，我們有些服務，是使用 StatefulSet 來建置，至於為什麼不用 Deployment，這個說來話長 (也不是因為需要特定的 Pod 名稱、或是網路標記等等)，我們這邊先不討論，這個 StatefulSet 服務是 Nginx + PHP-FPM，為了避免流量進入到 processes 已經被用光的 Pod 中，我們在 StatefulSet 的 PHP Container 上有設定 readiness，readiness 的設定長得像以下：\nreadinessProbe: exec: command: - \"/bin/bash\" - \"-c\" - | CHECK_INFO=$(curl -s -w 'http code:\\t%{http_code}\\n' 127.0.0.1/status) HTTP_CODE=$(echo -e \"${CHECK_INFO}\" | awk '/http code:/ {print $3}') IDLE_PROCESSES=$(echo -e \"${CHECK_INFO}\" | awk '/idle processes:/ {print $3}') [[ $HTTP_CODE -eq 200 \u0026\u0026 $IDLE_PROCESSES -ge 10 ]] || exit 1 我們會用 curl 來打 /status，檢查回傳的 http code 是否為 200，以及 idle processes 是否大於等於 10，如果不符合，就會回傳 1，讓他被標記不健康，讓 Kubernetes 停止流量到不健康的容器，以確保流量被路由到其他健康的副本。","參考資料#參考資料":"Kubernetes — 健康檢查：https://medium.com/learn-or-die/kubernetes-%E5%81%A5%E5%BA%B7%E6%AA%A2%E6%9F%A5-59ee2a798115\nPod Management Policies：https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#pod-management-policies","問題#問題":"當天遇到的情況是，我們上程式後，Pod 都一切正常，當流量開始進來後，發現 10 個 Pod 會開始偶發的噴 Readiness probe failed，查看監控發現 processes 越來越低，最後被反應服務有問題，我們查看 Hpa 的紀錄的確有觸發到 40 個 Pod，只是查看 Pod 數還是依樣卡在 10 個，當下我們有嘗試使用調整 yaml 在 apply，發現 StatefulSet 的 yaml 也已經更新了，但 Pod 還是一樣卡在 10 個，也有使用 kubectl 下 kubectl scale sts [服務名稱] --replicas=0，想要切換 Pod 數也沒有辦法。\n當下我們有先 Call Google 的 Support 一起找原因，Google 是建議我們 readiness 的條件不要設的太嚴格，可以加上 timeoutSeconds: 秒數，但對於 Pod 卡住，還是沒有找到原因，後來我們查了一下 StatefulSet 的文件發現，StatefulSet 有一個設定 podManagementPolicy，預設是 OrderedReady，他必須等待前面的 Pod 是 Ready 狀態，才會再繼續建立新的，也就是說我們的 StatefulSet 已經卡住，導致就算 Hpa 觸發要長到 40 個 Pod 也沒有用。","測試結果#測試結果":"最後我們就使用兩種模式來測試看看，已下是測試結果(透過 P1 才知道的設定ＱＱ)：\n有將測試的 StatefulSet 放在 Github，可以點我查看 (可以調整 readinessProbe 的 httpGet.Path 故意把他用壞)\n使用 OrderedReady 模式 StatefulSet 在 podManagementPolicy 預設 OrderedReady 的模式，故意讓 readiness 卡住時 (Pod 卡住時)：\n當下的 StatefulSet 設定： StatefulSet 設定\nPod 狀態： Pod 狀態\n使用指令調整 Pod 數量 我們這時候下指令調整 Pod 數量，看看會發生什麼事：\nkubectl scale sts my-statefulset --replicas=5 我們先看 StatefulSet 的 yaml 可以看到 Pod replicas 已經改變，也可以看 generation 有更新，代表 StatefulSet 本身有接收到調整設定的請求。\n下指令調整後的 StatefulSet 設定\n看了一下 Pod 數量，也是一樣卡住，且 Pod 數量也沒有變化。\n下指令調整後的 Pod 狀態\n使用 yaml 調整 Pod 數量 我們直接調整 StatefulSet yaml 的 Pod 數量，看看會發生什麼事：\n一樣我們先看 StatefulSet 的 yaml 可以看到 Pod replicas 已經改變(這裡應該切別的 Pod 數量，切回 3 個好像沒有意義 xD)，也可以看 generation 有更新。\n使用 yaml 調整後的 StatefulSet 設定\n看了一下 Pod 數量，也是一樣卡住，且 Pod 數量也沒有變化。\n使用 yaml 調整後的 Pod 狀態\n所以代表在 OrderedReady 的模式下，Pod 卡住時，無法對 Pod 進行任何操作，必須要手動刪除卡住的 Pod 才吃得到最新的設定。\n使用 Parallel 模式 StatefulSet 在 podManagementPolicy Parallel 的模式，故意讓 readiness 卡住時 (Pod 卡住時)：\n當下的 StatefulSet 設定： StatefulSet 設定\nPod 狀態： Pod 狀態\n使用指令調整 Pod 數量 我們這時候下指令調整 Pod 數量，看看會發生什麼事：\nkubectl scale sts my-statefulset --replicas=5 我們先看 StatefulSet 的 yaml 可以看到 Pod replicas 已經改變，也可以看 generation 有更新，代表 StatefulSet 本身有接收到調整設定的請求。\n下指令調整後的 StatefulSet 設定\n看了一下 Pod 數量，就算 my-statefulset-2 卡住，還是可以擴到 5 個 Pod。\n下指令調整後的 Pod 狀態\n使用 yaml 調整 Pod 數量 我們直接調整 StatefulSet yaml 的 Pod 數量，看看會發生什麼事：\n一樣我們先看 StatefulSet 的 yaml 可以看到 Pod replicas 已經改變，也可以看 generation 有更新。\n使用 yaml 調整後的 StatefulSet 設定\n看了一下 Pod 數量，也不會管其他 Pod 是否 Ready，一樣可以縮小成 2 個 Pod。\n使用 yaml 調整後的 Pod 狀態","結論#結論":"後來我們重新檢查了一下為什麼 processes 會用完，結果發現是 RD 的程式邏輯，導致每筆 Request 必須等待前一筆 Request 做完，才會開始動作，讓 processes 一直被占用，沒辦法即時消化，導致 processes 用完，又加上服務是使用 StatefulSet，預設模式的 OrderedReady，必須等待前一個 Pod 是 Ready 才可以自動擴縮，所以當我們 Hpa 想要擴縮，來增加可用的 processes 數量，也因為沒辦法擴縮，最後導致這一連串的問題 😕。\n另外，如果想要從 OrderedReady 模式切成 Parallel 模式 (反正過來也是)，必須先將原本的 StatefulSet 給刪除，才可以調整：\nOrderedReady 模式切成 Parallel 模式","解決辦法#解決辦法":"當下想趕快解決 readiness 這個問題，調整 timeoutSeconds 後，單純 apply 是沒有用的，要記得刪掉卡住的 Pod，讓他重新建立，才會套用新的設定 (但我們當下太在意為甚麼 Pod 會卡住，沒有想到要先把 readiness 問題修掉 xD，我們當下的解法是先將流量導到地端正常的服務上)。\n另外 Google 也說，假如我們還是必須使用 StatefulSet 來建立服務，建議我們把 podManagementPolicy 改成 Parallel，它會有點像是 Deployment 的感覺，不會等待其他 Pod 變成 Ready 狀態，所以可以讓我們就算在 readiness 卡住的情況下，也可以自動擴縮服務。\nℹ️ StatefulSet podManagementPolicy 參數說明\nOrderedReady (預設) Pods 會按照順序一個接一個地被創建。即，n+1 號 Pod 不會在 n 號 Pod 成功創建且 Ready 之前開始創建。 在縮小 StatefulSet 的大小時，Pods 會按照反向順序一個接一個地被終止。即，n 號 Pod 不會在 n+1 號 Pod 完全終止之前開始終止。 這確保了 Pods 的啟動和終止的順序性。\nParallel 所有 Pods 會同時地被創建或終止。 當 StatefulSet 擴展時，新的 Pods 會立即開始創建，不用等待其他 Pods 成為 Ready 狀態。 當縮小 StatefulSet 的大小時，要終止的 Pods 會立即開始終止，不用等待其他 Pods 先終止。 這種策略提供了快速的擴展和縮小操作，但缺乏順序性保證。"},"title":"正式環境上踩到 StatefulSet 的雷，拿到 P1 的教訓"},"/blog/kubernetes/pod-veth-name-provided-eth0-already-exists/":{"data":{"":"此文章要來記錄一下公司同事在正式服務上遇到的問題，會詳細說明遇到事情的經過，以及開單詢問 google support 最後討論出的暫時解決的辦法：\n簡單列出正式站當下服務環境：\ngke master version：1.25.10-gke.2700 gke node version：1.25.8-gke.1000 該問題發生的 node pool 有設定 taint 發生問題的 Pod 是用 Statefulset 建立的服務 ","事情發生的經過#事情發生的經過":" RD 同仁反應，發現使用 Statefulset 建立的排程服務有問題，下 kubectl delete 指令想要刪除 Pod，讓 Pod 重新長，卻卡在 Terminating，等待一段時間後，決定下 kubectl delete --force --grace-period=0 來強制刪除 Pod，這時候狀態會卡在 ContainerCreating，使用 Describe 查看，會出現以下錯誤： Warning (combined from similar events): Failed to create pod sandbox: rpo error: code = Unknown desc = failed to setup network for sandbox \"14fe0cd3d688aed4ffed4c36ffab1a145230449881bcbe4cac6478a63412b0c*: plugin type=*gke\" failed (add): container veth name provided (etho) already exists 我們 SRE 協助查看後，也有嘗試去下 kubectl delete --force --grace-period=0 來刪除 Pod，但還是一樣卡在 ContainerCreating，最後是先開一個新的 Node 並讓該 Pod 建立到新的 Node 上，才解決問題。為了方便 google support 協助檢查出問題的 Node，先將 Node 設定成 cordon，避免其他 Pod 被調度到該問題 node 上。 Node 設定成 cordon\nNode 可以設定 cordon、drain 和 delete 三個指定都會使 Node 停止被調度，只是每個的操作暴力程度不同：\ncordon：影響最小，只會將 Node 標示為 SchedulingDisabled 不可調度狀態，但不會影響到已經在該 Node 上的 Pod，使用 kubectl cordon [node name] 來停止調度，使用 kubectl uncordon [node name] 來恢復調度。\ndrain：會先驅逐在 Node 上的 Pod，再將 Node 標示為 SchedulingDisabled 不可調度狀態，使用 kubectl drain [node name] --ignore-daemonsets --delete-local-data 來停止調度，使用 kubectl uncordon [node name] 來恢復調度。\ndelete：會先驅逐 Node 上的 Pod，再刪除 Node 節點，它是一種暴力刪除 Node 的作法，在驅逐 Pod 時，會強制 Kill 容器進程，沒辦法優雅的終止 Pod。\n我們隨後開單詢問 goolge support。 ","參考資料#參考資料":"Node 節點禁止調度（平滑維護）方式- cordon，drain，delete：https://www.cnblogs.com/kevingrace/p/14412254.html","與-google-support-討論內容#與 Google Support 討論內容":"Google Support 經過查詢後，回覆說：這個問題是因為 Pod 被強制刪除導致，強制刪除是一種危險的操作，不建議這樣處理，下面有詳細討論。\n一開始卡在 Terminating 狀態，我們也有請 RD 說明一下當下遇到的問題以及處理動作：RD 當時想要刪除 Pod 是因為該程式當下有 Bug，將 redis 與 db 連線給關閉，程式找不到就會一直 retry，導致相關進程無法結束，再加上 terminationGracePeriodSeconds 我們設定 14400，也就是 4 小時，才會卡在 Terminating 狀態。 (terminationGracePeriodSeconds 設定這麼久是希望如果有被 on call，工程師上來時，可以查看該 Pod 的錯誤原因)\n因為卡在 Terminating 太久，RD 有執行 kubectl delete --force，就是因為下了 --force 才造成相關資源問題 (例如 container proccess, sandbox, 以及網路資源)沒有刪乾淨。所以引起了此次的報錯 “container veth name provided (eth0) already exists”。 (因為我們服務使用 Statefulset，Pod 名稱相同，導致 eth0 這個網路資源名稱重複，所以造成錯誤，可以用 deployment 來改善這個問題，只是資源如果沒有清理乾淨會佔用 IP，所以單純調整成 deployment 也不是最佳解)\nGoogle 產品團隊建議，如果 Pod 處於 Running 狀態時，想要快速刪除 Pod 時，一開始就先使用 kubectl delete pod --grace-period=number[秒數] 來刪除，如果已經是 Terminating 狀態則無效。(SRE 同仁已測試過，與 Google Support 說明相同)\n那如果已經處於 Terminating 狀態，要怎麽讓 Pod 被順利刪除，這部分 Google Support 後續會在測試並給出建議，目前測試是：進去卡住的 Pod Container，手動刪除主進程 (pkill) 就可以了。\nGoogle Support 回覆"},"title":"部署 Pod 遇到 container veth name provided (eth0) already exists 錯誤"},"/blog/nginx/":{"data":{"":"此分類包含 Nginx 相關的文章。\n想使用 Nginx Upstream Proxy 到外部服務，並帶入對應的 header 該怎麼做？ Soketi WebSocket Server LOG 不定時出現 502 error 以及 connect() failed (111: Connection refused) "},"title":"Nginx"},"/blog/nginx/nginx-upstream-set-host-header/":{"data":{"":"此文章要來記錄一下最近在公司服務入口遇到的一些小問題，以及解決的方法。簡單說明一下，我們的服務入口是用 Nginx 來當作 proxy server，將不同路徑或是 servername 導到對應的後端程式，或是外部的服務上(例如 AWS cloudfront.net)，本篇要測試的是如果使用要同時使用 upstream 到外部服務，且需要帶 host header 該怎麼做。\nNginx 的 upstream 是什麼？\n通常我們 proxy_pass 的寫法會是這樣：\nlocation /aaa { proxy_pass http://aaa.example.com; } 當 Nginx 收到的 request 是 /aaa 時，就會將 request 轉發到 http://aaa.example.com。\n但假如後端有多台機器或是服務，可以處理同一種 request，這時候就可以使用 upstream 來處理：\nupstream backend_hosts { server aaa.example.com; server bbb.example.com; server ccc.example.com; } location /aaa { proxy_pass http://backend_hosts; } 這樣子的好處是可以有多個機器或是後端服務可以分散請求，做到負載平衡的效果。","參考資料#參考資料":"Make nginx to pass hostname of the upstream when reverseproxying：https://serverfault.com/questions/598202/make-nginx-to-pass-hostname-of-the-upstream-when-reverseproxying","問題#問題":"那如果我們使用 Nginx upstream 時，還想要同時帶 host 的 header 到後端該怎麼做呢？我們先來看一下目前的寫法：\n( 測試範例是使用 docker 來模擬，可以參考程式碼 \u003e 點我前往 github，會有三個 nginx，其中一個是負責 proxy 的 nginx 名為 proxy，另外兩台是 upstream 後的服務，名為 upstream_server1、upstream_server2 )\nnginx-old.conf upstream upstream_server { server upstream_server1; server upstream_server2; } server { listen 80; server_name localhost; location /upstream_server/ { proxy_pass http://upstream_server; proxy_set_header Host \"upstream_server1\"; proxy_set_header Host \"upstream_server2\"; access_log /var/log/nginx/access.log upstream_log; } } } 可以看到我們希望 Nginx 收到 request 是 /upstream_server 時，將 request 轉發到 http://upstream_server，而 upstream_server 後面有兩個 server，並且在 proxy 時，帶入兩個不同的 host header。但如果真的這樣寫，可以達到我們想要得效果嗎？我們實際跑看看程式 (範例可以使用 nginx-old.conf)：\nnginx 原本寫法\n從上面的 LOG 可以發現，我們 call /upstream_server 時，後端的 upstream_server1、upstream_server2 收到的 host 只會收到第一個設定的 Host，且服務會出現 400 Bad Request，查了一下網路文章，發現出現 400 Bad Request，可能跟 header 送太多資訊過去，詳細可以參考 解決網站出現 400 Bad Request 狀態的方法。\n這邊推測應該是後端如果也是用 nginx 直接接收才會遇到 400 的問題，還好目前公司服務還是正常的 xDD，檢查一下後發現，其實後端根本沒有要求對應 header 才能接收(應該是對方忘記加上此限制)。","解決#解決":"好，不管是否需要對應 header，我們還是找看看有沒有辦法同時使用 upstream，並帶入對應 host 的方法呢？\n最後參考網路上的文章，似乎只能使用兩層的 proxy，才能完成這兩個需求，我們來看看要怎麼寫吧 (範例可以使用 nginx.conf)：\nnginx.conf server { listen 777; server_name localhost; location / { proxy_pass http://upstream_server1; proxy_set_header Host \"upstream_server1\"; access_log /var/log/nginx/access.log upstream_log; } } server { listen 888; server_name localhost; location / { proxy_pass http://upstream_server2; proxy_set_header Host \"upstream_server2\"; access_log /var/log/nginx/access.log upstream_log; } } upstream upstream_server { server 127.0.0.1:777; server 127.0.0.1:888; } server { listen 80; server_name localhost; location /upstream_server/ { proxy_pass http://upstream_server; access_log /var/log/nginx/access.log upstream_log; } } 可以看到上面的程式碼，我們透過兩層的 proxy，來達到我們想要的效果，這樣子就可以同時使用 upstream，並且帶入對應的 host header。\n首先在 28 ~ 36 行，我們一樣如果 Nginx 收到 request 是 /upstream_server 時，會 proxy 到 upstream_server 這個 upstream 中，而 upstream_server 有兩個 server，分別是 127.0.0.1:777、127.0.0.1:888，但實際上沒有這兩個 port，所以我們需要再寫一層一般的 proxy 設定，分別是 1 ~ 10 行、12 ~ 21 行，這樣子就可以達到我們想要的效果。\n但這個方法比較適用於 upstream 後端沒有太多個服務或是機器的情況，如果有很多個服務或是機器，就需要寫很多的 proxy，這樣子會變得很麻煩，所以如果有更好的方法，也歡迎留言跟我分享 🤣。\n最後我們來看一下實際執行的結果：\n使用多層的 nginx proxy 處理"},"title":"想使用 Nginx Upstream Proxy 到外部服務，並帶入對應的 header 該怎麼做？"},"/blog/nginx/soketi-log-502-error/":{"data":{"":"此文章要來記錄一下 RD 同仁前陣子有反應使用 Soketi 這個 WebSocket Server 會不定時在 LOG 出現 502 error 錯誤訊息以及 connect() failed (111: Connection refused) while connecting to upstream，雖然說服務使用上不會影響很大，但還是希望我們可以協助找出 502 的原因。\n出錯的 LOG\n在開始找問題前，先簡單介紹一下 Soketi 是什麼東西好了，根據官網的說明，他是簡單、快速且有彈性的開源 WebSockets server，想要了解更多的可以到它官網去查看。\n另外會把程式碼相關放到 GitHub » 點我前往","參考資料#參考資料":"[Nginx] 解決 connect() failed (111: Connection refused) while connecting to upstream：https://wshs0713.github.io/posts/8c1276a7/\nWebSocket proxying：http://nginx.org/en/docs/http/websocket.html\nday 10 Pod(3)-生命週期, 容器探測：https://ithelp.ithome.com.tw/articles/10236314","壓測#壓測":"最後調整完，我們來測試看看是否在 Pod 自動重啟 or 更新 Deployment 的時候(並且有大量連線時)還會噴 502 error 或是 connect() failed (111: Connection refused)，我們這邊使用 k6 來做 websocket 服務的壓測，有簡單寫一個壓測程式如下：\nk6 壓測\nk6 是一個開源的壓測工具，可以用來測試 API、WebSocket、gRPC 等服務，可以到它的官網查看更多資訊。\nMacOS 安裝方式：brew install k6\nwebsocket.jsimport ws from \"k6/ws\"; import { check } from \"k6\"; export const options = { vus: 1000, duration: \"30s\", }; export default function () { const url = \"wss://socket.XXX.com/app/hex-ws?protocol=7\u0026client=js\u0026version=7.4.1\u0026flash=false\"; const res = ws.connect(url, function (socket) { socket.on(\"open\", () =\u003e console.log(\"connected\")); socket.on(\"message\", (data) =\u003e console.log(\"Message received: \", data)); socket.on(\"close\", () =\u003e console.log(\"disconnected\")); }); check(res, { \"status is 101\": (r) =\u003e r \u0026\u0026 r.status === 101 }); } 簡單說明一下上面程式在寫什麼，我們在 const 設定 vus 代表有 1000 個虛擬使用者，會在 duration 30s 內完成測試，下面的 default 就是測試連線 ws 以及 message 跟 close 等動作，最後需要回傳 101 (ws 交握)\n執行 k6 run websocket.js 後，就會開始壓測，可以看到會開始執行剛剛在上面提到 default 的動作：\nk6 壓測過程\n等到跑完，就會告訴你 1000 筆裡面有多少的 http 101，這邊顯示 status is 101，就代表都是 101，代表都有連線成功，沒有出現 502 error 或是 connect() failed (111: Connection refused) 的錯誤，這樣就代表我們的問題解決了。\nk6 壓測結果","解決過程#解決過程":"我們可以看到上方錯誤 LOG 中，發現有出現 502 error 以及 connect() failed (111: Connection refused) while connecting to upstream，這兩個錯誤都是由 Nginx 所產生的，那我們先來理解一下，Nginx 與 Soketi 之間的關係。\n在使用上，RD 的程式會打 Soketi 專用的 Subdomain 來使用這個 WebSocket 服務，而在我們的架構上，這個 Subdomain 會經過用 nginx proxy server，來轉發到 Soketi WebSocket Server (走 k8s svc)，設定檔如下圖：\n入口 nginx 設定\n然後會出現 connect() failed (111: Connection refused) while connecting to upstream 的錯誤訊息，代表我們的 Nginx 設定少了一個重要的一行設定，就是 proxy_http_version 1.1;，這個設定要讓 Nginx 作為 proxy 可以和 upstream 的後端服務也是用 keepalive，必須使用 http 1.1，但如果沒有設定預設是 1.0，也要記得設定 proxy_set_header Upgrade、proxy_set_header Connection。調整過後就變成：\nws.confserver { server_name socket.XXX.com; listen 80 ; listen [::]:80 ; listen 443 ssl; listen [::]:443 ssl; ssl_certificate /etc/nginx/ingress.gcp.cert; ssl_certificate_key /etc/nginx/ingress.gcp.key; access_log /var/log/nginx/access.log main; location / { proxy_pass http://soketi-ws-ci:6001; proxy_connect_timeout 10s; proxy_read_timeout 1800s; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \"Upgrade\"; proxy_set_header X-Real-IP $remote_addr; } } 解決完 connect() failed (111: Connection refused) 這個問題後，接下來就是要解決 502 error 這個問題，會導致 502 代表 Nginx 這個 proxy server 連不上後端的 Soketi WebSocket Server，再觀察 LOG 以及測試後發現，當 Pod 自動重啟，或是手動重啟 Deployment 的時候，就會有 502 的錯誤，代表 Nginx 在 proxy 到後面的 Soketi svc 再到 Pod 的時候，有一段時間是連不上的，所以就會出現 502 的錯誤，可以推測是流量進到正在關閉的 Pod 或是進到還沒有啟動好的 Pod 才導致的。\n那我們先來看一下 Soketi WebSocket Server 的服務 yaml 檔案：\ndeployment.yaml deployment.yaml spec: terminationGracePeriodSeconds: 30 securityContext: {} containers: - name: soketi securityContext: {} image: \"quay.io/soketi/soketi::1.6.0-16-alpine\" ... 省略 (可以到 github 看 code)... livenessProbe: failureThreshold: 3 httpGet: httpHeaders: - name: X-Kube-Healthcheck value: \"Yes\" path: / port: 6001 initialDelaySeconds: 5 periodSeconds: 2 successThreshold: 1 可以看到原來的設定只有 livenessProbe 而已，因此我們為了要避免流量進到正在關閉的 Pod 或是進到還沒有啟動好的 Pod，所以我們需要加上 readinessProbe 以及 preStop，讓 Pod 確定啟動完畢，或是等待 Service 的 endpoint list 中移除 Pod，才開始接收流量，這樣就可以避免出現 502 的錯誤。\ndeployment.yaml spec: terminationGracePeriodSeconds: 30 securityContext: {} containers: - name: soketi securityContext: {} image: \"quay.io/soketi/soketi::1.6.0-16-alpine\" ... 省略 (可以到 github 看 code)... livenessProbe: failureThreshold: 3 httpGet: httpHeaders: - name: X-Kube-Healthcheck value: \"Yes\" path: / port: 6001 initialDelaySeconds: 5 periodSeconds: 2 successThreshold: 1 readinessProbe: failureThreshold: 3 httpGet: httpHeaders: - name: X-Kube-Healthcheck value: \"Yes\" path: /ready port: 6001 initialDelaySeconds: 5 periodSeconds: 2 successThreshold: 1 lifecycle: preStop: exec: command: [\"/bin/sh\", \"-c\", \"sleep 20\"] Pod 終止的過程"},"title":"Soketi WebSocket Server LOG 不定時出現 502 error 以及 connect() failed (111: Connection refused)"},"/blog/opentelemetry/":{"data":{"":"此分類包含 Opentelemetry 相關的文章。\n什麼是 Opentelemetry？可觀測性 (Observability) 又是什麼？ 如何透過 OpenTelemetry 來收集 Ingress Nginx Controller 的 Metrics 與 Traces 並送到 Datadog 上 "},"title":"Opentelemetry"},"/blog/opentelemetry/opentelemetry-ingress-nginx-controller/":{"data":{"":"由於最近公司想要導入 Datadog，在測試過程中順便導入 OpenTelemetry 來收集 Metrics 與 Traces 並送到 Datadog 上 ～\n🔥 這個範例比較特別，因為 Datadog 有提供 Ingress Nginx Controller 的 integrations，可以透過 Datadog Agent 來收集 Metrics，不需要透過 OpenTelemetry Collector 來收集。 ( Datadog Agent 請參考：https://docs.datadoghq.com/containers/kubernetes/ )\n程式部分也同步上傳到 github 上，可以點我前往","參考資料#參考資料":"Configure Nginx Ingress Controller to use JSON log format：https://dev.to/bzon/send-gke-nginx-ingress-controller-logs-to-stackdriver-2ih4\n淺談 OpenTelemetry - Collector Compoents：https://ithelp.ithome.com.tw/articles/10290703","執行步驟#執行步驟":" 先 clone 這個 repo (廢話 xD)\n先建立 OpenTelemetry Collector，執行以下指令：\nhelm upgrade collector \\ opentelemetry-collector \\ --repo https://open-telemetry.github.io/opentelemetry-helm-charts \\ --install \\ --create-namespace \\ --namespace opentelemetry \\ -f \"otel-collector.yaml\" 再建立 Ingress Nginx Controller，執行以下指令：\nhelm upgrade ingress-nginx \\ ingress-nginx \\ --repo https://kubernetes.github.io/ingress-nginx \\ --install \\ --create-namespace \\ --namespace ingress-nginx \\ -f \"ingress-nginx-values.yaml\" 接著建立測試用 Nginx 服務，執行以下指令：\nkubectl apply -f nginx.yaml ","檔案說明#檔案說明":" otel-collector.yaml： OpenTelemetry Collector 的設定檔，主要是設定要收集哪些 metrics、traces，並且要送到哪個 exporter，要注意的是 exporters 的 datadog 需要設定 site、api_key，以及 image 要記得用 otel/opentelemetry-collector-contrib，才會有 datadog 的 exporter。\ningress-nginx-values.yaml： Ingress Nginx Controller 的設定檔，這邊的 podAnnotations 是為了讓 Ingress Nginx Controller 的 Pod 能夠透過 Datadog agent 收集 metrics 到 Datadog 才加上的。\nconfig 裡面的設定有很多，主要都是 openTelemetry 的設定，要注意的是 enable-opentelemetry 要設為 true，另外 otlp-collector-host 以及 otlp-collector-port 要送到哪個 collector 等等也要記得設定。 另外如果想要將 LOG 與 Trace 串再一起，記得要把 log-format 設為 json，並且帶入，trace_id 與 span_id ( 這邊有多帶 dd.trace_id 是為了讓 datadog 可以自動串接 LOG \u0026 Trace )。\nnginx.yaml： 一個簡單的 Nginx 整套服務 (Deployment、Service、Ingress)，要注意的是 Ingress 需要設定 annotations kubernetes.io/ingress.class: nginx (這個是 Ingress Nginx Controller 的預設 class name)，才會被 Ingress Nginx Controller 接管 (才會有 Load Balancer 的 IP)","測試#測試":"當你執行完上面的步驟後，你會發現有產生兩個 namespace，一個是 ingress-nginx，另一個是 opentelemetry，並且會有 OpenTelemetry Collector、Ingress Nginx Controller、Nginx 等服務，如下：\n啟動服務\n我們試著打 http://nginx.example.com/ (測試網址，需要先在 /etc/hosts 綁定 Ingress Nginx Controller 咬住的 Load Balancer IP)，查看一下 Datadog 的 LOG，看看是否有收到 Nginx 的 LOG (此收集 LOG 的方式是透過在 cluster 上安裝 Datadog 的 agent)，如下：\nDatadog LOG\n接著查看 Datadog APM 的 trace，如下：\nDatadog APM\n由於我們在後面目前沒有串其他服務，所以只有一個 span，之後還有另外兩篇文章是介紹如何串其他服務 (會增加服務以及部分設定)，可以參考看看：opentelemetry-roadrunner、opentelemetry-nodejs\n順便看一下透過 Datadog Agent 收集的 Ingress Nginx Controller 的 Metrics，如下：\nDatadog Ingress Nginx Controller 的 Metrics\n可以用這些 Metrics 來做 Dashboard，如下：\nDatadog Dashboard","結論#結論":"透過 OpenTelemetry Collector 來收集 Ingress Nginx Controller 的 Metrics 與 Traces 並送到 Datadog 上，這樣就可以透過 Ingress Nginx Controller 的 Metrics 來做監控了，對於 RD 再開發上，有 Traces 也更方便 RD 他們找到程式的瓶頸 (有可能是服務導致的)。"},"title":"如何透過 OpenTelemetry 來收集 Ingress Nginx Controller 的 Metrics 與 Traces 並送到 Datadog 上"},"/blog/opentelemetry/opentelemetry-observability/":{"data":{"":"在介紹 Opentelemetry 之前，我們要先了解一下目前軟體架構以及基礎設施的演進：\n軟體架構以及基礎設施的演進\n第一階段在軟體架構設計上較為簡單，不會有什麼特別需要拆分出來的程式，所以都是一整包的程式，再測試以及除錯也比較不會有什麼問題。基礎設施都是使用 VM 或是使用放在 IDC 的機房來當 Server。\n第二階段隨著雲端技術的推出，會開始將服務搬上雲供應商提供的 IaaS 服務，或是使用私有雲給企業放置較機密的內容，其他則放置公有雲上，達成混合雲的模式。\n第三階段隨著雲端技術越來越成熟，有更多的雲端 IaC 以及功能推出，會開始考慮使用分散式的系統架構，將 DB 等服務也改用 Cloud SQL 的方式。在基礎設施上也隨著容器化的技術成熟而進入新的時代。\n第四階段已經使用 docker 來管理好一陣子，但發現虛擬容器技術在管理上十分不方便，因此 K8s 逐漸盛行，將架構從分散式改成微服務的方式進行，在讓開發團隊使用上可以更靈活且容易。\n雖然使用 K8s 可以讓我們的服務更靈活方便，但也會將服務切的越來越細，這時會讓開發變的十分複雜，我們在架構上從一開始的單體式架構，變成分散式架構，再到最後的微服務，讓開發人員需要處理的事情會越來越多。服務要如何連線？Log 要如何記錄？以及當一個請求會經過多個服務時，相對的延遲也會增加，這時要怎麼去處理等。在監控上，因為服務切分得很細，當線上有一個服務有問題時，要如何快速的找到問題點也是一大挑戰。\n當我們使用分散式系統或是微服務時發生故障時，會很難快速的恢復服務，因為每個服務都互相依賴，在以往都是透過經驗以及對系統的了解來得以解決。那有什麼其他的方式，能夠讓我們更快掌握每個服務呢？我們先來了解一個名詞：可觀測性(Observability)","opentelemetry#Opentelemetry":"那我們這次要介紹的可觀測性(Observability)工具就是 Opentelemetry，縮寫 OTel，它是由 CNCF (Cloud Native Computing Foundation) 組織孵化的開源專案，在 2021 年 5 月由 OpenTracing 與 OpenCensus 兩個框架合併，結合兩項分散式追蹤框架最重要的特性成為下一代收集遙測數據的新標準。\ntelemetry 又叫做遙測，是指能夠跨越不同系統來收集資料 (包含 LOG、Metric、trace) 的能力。\n我們可以看一下官網的說明：\nOpentelemetry 是雲原生的可觀測性(Observability)框架，提供標準化的 API、SDK 與協議自動檢測、蒐集、導出遙測數據資料 (Metrics、Log、Trace)，並支援 W3C 定義的 Http trace-context 規範，降低開發者在搜集遙測數據上的困難度，以及方便進行後續分析以及性能的優化。\nOpentelemetry\n在 OpenTelemetry 核心元件如下：\nAPI：開發人員可以透過 OpenTelemetry API 自動生成、蒐集應用程式(Application)的遙測數據資料(Metrics, Log, Trace)，每個程式語言都需實作 OpenTelemetry 規範所定義的 API 方法簽章。\nSDK：是 OpenTelemetry API 的實現。\nOTLP：規範定義了遙測數據的編碼與客戶端及服務器之間如何交換的協議 (gRPC、HTTP)。\nCollector：OpenTelemetry 中儲存庫，用於接收、處理、導出遙測數據到各種後端平台。","參考資料#參考資料":"Observability：https://linkedin.github.io/school-of-sre/level101/metrics_and_monitoring/observability/\n[OpenTelemetry] 現代化監控使用 OpenTelemetry 實現 : 可觀測性(Observability)：https://marcus116.blogspot.com/2022/01/modern-monitoring-using-openTelemetry-with-Observability.html\n[OpenTelemetry] 現代化監控使用 OpenTelemetry 實現 : OpenTelemetry 開放遙測：https://marcus116.blogspot.com/2022/01/opentelemetry-opentelemetry.html\n淺談 Observability(下)：https://ithelp.ithome.com.tw/m/articles/10287598\nManage services, spans, and traces in Splunk APM：https://docs.splunk.com/Observability/apm/apm-spans-traces/traces-spans.html","可觀測性observability#可觀測性(Observability)":"可觀測性有三個重要的特性，分別是：\nMetrics 負責監控系統有什麼狀況，當要發生服務故障前可以透過設定閥值搭配告警提早得知。\nLogs 當問題發生時，可以用來查看故障時正在執行哪些服務，以及產生的錯誤資訊。\nTraces （後面詳細介紹）\n可觀測性三大支柱\n我們對於 Metrics 跟 Logs 有基本的了解，所以我這邊會注重在 Traces 的部分：\n當有多個微服務的複雜分散式系統，用戶的請求會由系統中的多個微服務進行處理。Metrics 跟 Logs 可以提供我們有關系統如何去處理這些請求的一些資訊，但沒有辦法提供微服務的詳細訊息以及他是如何影響客戶端的請求。這時候就需要透過 Trace 來協助我們追蹤。\nTrace 可以在連續的時間維度上，透過 Trace 以及 Span 關聯，把空間給排列展示出來，並且有 Trace-Context 規範，能夠直觀的看到請求在分散式系統中經過所有服務的紀錄。\n什麼是 Trace、Span 、Trace-Context 呢？\n我們先說 Span，Span 又可以叫跨度，是系統中最小的單位，可以看下方圖片，SpanA 的資料是來源 SpanB，SpanB 來源是 SpanC 等等，每一個 Span 可以把它想成一個請求後面所有經過服務的工作流程，例如：nginx_module、db、redis 等等。\n請求的整個過程叫做 Trace，那他要怎麼知道 SpanA ~ SpanE 是同一個請求呢？\n就需要透過 TraceID 以及 SpanID 來記錄：\nTraceID：是唯一的 ID，用於識別整個分散式追蹤的一條請求路徑。在下方圖片中，當請求進入時，就會被賦予一個 TraceID，所有有經過的 Span 都會記錄此 TraceID，這樣才可以把不同服務依據 TraceID 關聯成同一個請求。\nSpanID：是一條請求路徑中單個操作唯一的 ID。追蹤路徑是由多個 Span 組成，每個 Span 都代表一個操作或特定的時間段。當請求進入時，每個服務就會產生一個 Span 來代表它處理請求的的時間。這些 Span 使用 TraceID 來連接再一起，形成完整的請求追蹤。\nTrace 示意圖\n那要怎麼查看每個 Span 的紀錄內容呢，就需要 Trace-Context：\n會放置一些用於追蹤和識別請求的上下文信息，例如 Trace ID、Span ID 和其他相關的數據。這些上下文信息可以是一些關鍵的數據，可以幫助我們在整個分佈式系統中追蹤請求的路徑，並將相關請求和操作關聯起來。\nECK Trace 示意圖\n上面的圖片中，可以看到 call /product/XXXX 後，會經過需多的 Span，隨便點擊一個 Span 可以看到它記錄的 Trace-Context，以及都會包含 TraceID 及 SpanID\nECK Trace 示意圖\nTrace 優點可以看到跨維度看到中間的資訊，對於找到問題以及瓶頸十分方便，但缺點就是因為需要在 Span 中產生 ID 以及內容，需要在程式裡面加入一定的套件以及調整程式碼。\n所以我們在可觀測性(Observability)最終的目的是希望可以透過可觀測性工具讓我們知道：\n請求通過哪些服務 每個服務在處理請求時做了些什麼 如果請求很慢，瓶頸在哪邊 如果請求失敗，錯誤點在哪 請求的路徑是什麼 為什麼花這麼長的時間 "},"title":"什麼是 Opentelemetry？可觀測性 (Observability) 又是什麼？"},"/blog/other/":{"data":{"":"先放置還沒有想到的分類 Blog 文章。\nLinux 常用指令 清除 Linux 機器上的 Swap (Buff、Cache、Swap 比較) 找出程式碼、開源套件、容器的安全漏洞工具 - Snyk Bookstack 開源知識庫筆記平台安裝 (K8s + docker) "},"title":"其他(還想不到分類)"},"/blog/other/bookstack/":{"data":{"":"最近剛好有公司同事離職，想要把交接的資料給整理整理，雖然部門之前有架設專用的 wiki 給 RD 使用，但覺得介面沒有到很好用，於是就在網路上尋找，可以多人編輯的筆記系統，一開始有想過用 CodiMD (HackMD)，但考量到需要多層的架構來區分文件，最後選擇 Bookstack 這個開源知識庫筆平台來作為組內的筆記系統，以下會簡單說一下 Bookstack 的特色以及使用 K8s 跟 docker 的安裝教學。","bookstack-介紹#Bookstack 介紹":"介紹部分主要參考 Bookstack 簡介，以下列出會選擇它的三個特色：\n簡潔的書本列表模式 書架分類\n書本分類\n頁面章節分類\n最主要是因為有以上幾個不同的分層架構，在資料整理上會更方便、更好彙整，可以自訂書架以及書本、頁面或是章節的封面以及內容。\n強大的搜尋功能 搜尋功能\n當我們的筆記內容越來越多，雖然有上面提到的分類模式，想要找到內容還是需要花一段時間，但 Bookstack 有強大的搜尋功能，可以針對書架、書本、章節或是書面個別搜尋，可以利用時間、標籤、標題或是內容來快速找到想要的內容。\n畫圖功能 在討論系統或是程式的架構，最好的方式就是用畫圖的來表示。在以往都是使用 Draw.io 來畫圖，當畫完圖後需要匯出畫好的圖以外，如果怕畫圖的原檔消失，還需要再另外下載原檔來保留，很不方便，又怕忘記下載，而 Bookstack 內建可直接編輯的功能，當畫完圖後有問題，可以直接點擊圖片來編輯，而這些功能還會搭配內建的版控，若有問題還可以還原到正確的圖片版本。\n畫圖功能","參考資料#參考資料":"BookStack 簡介：https://docs.ossii.com.tw/books/bookstack/page/bookstack\nBookStack Installation：https://www.bookstackapp.com/docs/admin/installation/","安裝說明#安裝說明":"那簡單說明為什麼會選擇 Bookstack 我們就來安裝它，這邊有使用 K8s + docker 來測試安裝，那我們就一起來看程式碼吧，程式碼放在這 👈：\nK8s namespace.yamlapiVersion: v1 kind: Namespace metadata: name: bookstack 我習慣會將不同的服務切 namespace 來部署，大家可以依照習慣來使用，下方的 yaml 都是建在此 namespace 上。\ndeployment.yamlapiVersion: apps/v1 kind: Deployment metadata: name: bookstack namespace: bookstack labels: app: bookstack spec: replicas: 1 selector: matchLabels: app: bookstack template: metadata: labels: app: bookstack spec: containers: - name: bookstack image: linuxserver/bookstack ports: - name: http containerPort: 80 env: - name: DB_DATABASE value: bookstack - name: DB_HOST value: \u003c\u003c更換此處\u003e\u003e - name: DB_PORT value: \"3306\" - name: DB_PASSWORD value: \u003c\u003c更換此處\u003e\u003e - name: DB_USERNAME value: \u003c\u003c更換此處\u003e\u003e - name: MAIL_USERNAME value: example@test.com - name: MAIL_PASSWORD value: mailpass - name: MAIL_HOST value: smtp.server.com - name: MAIL_PORT value: \"465\" - name: MAIL_ENCRYPTION value: SSL - name: MAIL_DRIVER value: smtp - name: MAIL_FROM value: no-reply@test.com - name: APP_URL value: https://\u003c\u003c更換此處\u003e\u003e - name: APP_LANG value: zh_TW - name: APP_TIMEZONE value: Asia/Taipei resources: limits: cpu: \"0.5\" memory: \"512Mi\" 必要更換的參數有標示 «更換此處»，其餘可以依照各組織來自行配置，Bookstack 會將內容存在 db，圖片等存在 pod 中，需要永久保存請使用 pvc + pv 或是另外掛 nas。\nsvc.yamlapiVersion: v1 kind: Service metadata: name: bookstack namespace: bookstack spec: type: NodePort selector: app: bookstack ports: - name: http protocol: TCP port: 80 targetPort: 80 ingress.yamlapiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: bookstack-ingress namespace: bookstack annotations: kubernetes.io/ingress.class: nginx service.beta.kubernetes.io/do-loadbalancer-enable-proxy-protocol: \"true\" spec: rules: - host: \u003c\u003c更換此處\u003e\u003e http: paths: - path: / pathType: Prefix backend: service: name: bookstack port: number: 80 最後透過 ingress 的 domain 去訪問 svc \u003e pod 上，就完成部署拉～第一次登入要使用預設帳號及密碼 admin@admin.com/password\ndocker docker-compose.yamlversion: \"3.8\" services: bookstack: image: lscr.io/linuxserver/bookstack container_name: bookstack environment: - PUID=\u003c\u003c更換此處\u003e\u003e - PGID=\u003c\u003c更換此處\u003e\u003e - DB_HOST=bookstack_db - DB_PORT=3306 - DB_USER=bookstack - DB_PASS=bookstack - DB_DATABASE=bookstackapp - APP_LANG=zh_TW - APP_TIMEZONE=Asia/Taipei - APP_URL=\u003c\u003c更換此處\u003e\u003e - GOOGLE_APP_ID=\u003c\u003c更換此處\u003e\u003e - GOOGLE_APP_SECRET=\u003c\u003c更換此處\u003e\u003e volumes: - /bookstack/config:/config ports: - 80:80 restart: unless-stopped depends_on: - bookstack_db bookstack_db: image: lscr.io/linuxserver/mariadb container_name: bookstack_db environment: - PUID=\u003c\u003c更換此處\u003e\u003e - PGID=\u003c\u003c更換此處\u003e\u003e - MYSQL_ROOT_PASSWORD=bookstack - TZ=Asia/Taipei - MYSQL_DATABASE=bookstackapp - MYSQL_USER=bookstack - MYSQL_PASSWORD=bookstack volumes: - /bookstack/config:/config restart: unless-stopped docker 的部分就更簡單了，一樣是把 «更換此處» 換成對應的內容即可，env 的部分可以參考官方文件，這邊比較特別的是我們有多使用 Google 來 Oauth 登入，Bookstack 支援多種的 Oauth 登入方式，可以參考 Third Party Authentication。\n架設好 Bookstack 就可以開始寫部門或是組織內的筆記拉～ 如果有開放外網連線，要記得修改預設的管理員帳號，以及用管理員帳號登入，到功能與安全的公開存取給取消，這樣就必須要登入才可以瀏覽筆記了！那就交給大家自己去玩這個好用的筆記工具囉～～ 😍\n功能與安全設定"},"title":"Bookstack 開源知識庫筆記平台安裝 (K8s + docker)"},"/blog/other/linux-clear-swap/":{"data":{"":"今天在工作時，遇到機器的 Swap 超過預警值，需要手動去清除 Swap，那剛好就由這次機會來介紹要如何清除 Linux 機器上的 Swap，以及查詢 Linux 記憶體的使用狀況！","buff-跟-cache-以及-swap-的比較#buff 跟 cache 以及 Swap 的比較":" 比較 buff cache Swap 功用 記憶體寫完資料會先暫存起來，等之後再定期將資料存到硬碟上 記憶體讀完資料後暫存起來，可以在下此查詢時快速的顯示 硬碟的交換分區，當 buff/cache 記憶體已經用完後，又有新的讀寫請求時，就會將部份內存的資料存入硬碟，也就是把內存的部分空間當成虛擬的記憶體來做使用 ","free-與-available-的比較#free 與 available 的比較":" free：是真正尚未被使用的實體記憶體數量 available：是應用程式認為可用的記憶體數量，可以理解成 available = free + buff/cache 接下來我們就要進入主題，如何清除 Swap ：","參考資料#參考資料":"Linux 內存、Swap、Cache、Buffer 詳細解析：https://os.51cto.com/article/636622.html\nlinux free 命令下 free/available 區別：https://www.796t.com/content/1545715382.html\n釋放 linux 的 swap 記憶體：https://www.796t.com/article.php?id=207781","如何查詢-linux-記憶體#如何查詢 Linux 記憶體":"在講 Swap 之前，我們先來說一下怎麼查詢 Linux 記憶體，可以使用以下指令來顯示：\nfree 下完後，格式會長這樣：\ntotal used free shared buff/cache available Mem: 32765564 8499252 1825132 1857720 22441180 19693100 Swap: 16776188 0 16776188 但這樣子不是很好觀察，所以我們可以加上 -h 來顯示大小的單位，讓我更清楚的知道每一個的大小：\ntotal used free shared buff/cache available Mem: 31G 8.1G 1.6G 1.8G 21G 18G Swap: 15G 0B 15G 那我們接著先來說說使用 free 查詢後，所有欄位的意思吧！\nfree 第一列 Mem：記憶體的使用資訊 Swap：交換空間的使用資訊 free 第一行 total：系統總共的可用實體記憶體大小 used：已被使用的實體記憶體大小 free：還剩下多少可用的實體記憶體 shared：被共享使用的實體記憶體大小 buff/cache：被 buffer 和 cache 使用的實體記憶體大小 available：可被 應用程式 使用的實體記憶體大小 ","清除-swap#清除 Swap":"首先第一步我們先使用 free 來查看目前的 Swap 使用狀態：\nSwap 使用 (尚未清除)\n可以看到我們的 Swap used 是 797M，我們設定它不能超過 5%，超過就會通知，所以我們要把它手動清除。\n先檢查記憶體 available 為什麼要先檢查 available，是因為一開始會使用到 Swap 的原因就是因為應用程式的可用記憶體空間不足，所以現在要清除 Swap 條件就是：Mem 的 available 必須要大於 Swap 的 used 才可以，否則會導致記憶體爆炸 💥\n將記憶體資料暫存到硬碟 接下來因為我們要清除 Swap ，所以不能讓資料在寫入記憶體中，所以我們先使用下方指令，讓記憶體的資料暫存到硬碟。\nsync 這個指令就是將存於暫存的資料強制寫入到硬碟中，來確保清除時導致資料遺失。\n關閉 Swap，再打開 Swap 沒錯，Swap 的清除就是把他先關掉，再重新打開，他就會自己清除 Swap 的資料了！使用的指令如下：(清除過程需要稍等，讓他進行刪除動作)\nswapoff -a \u0026\u0026 swapon -a 確認都沒問題後，我們就使用 free 來重新查看記憶體狀態：\nSwap 使用 (已清除)\n可以看到我們清除完 Swap 後，Swap 的 used 已經從 797M 變成 0B。\n如果碰到執行 swapoff -a \u0026\u0026 swapon -a 出現 swapoff: Not superuser.，只需要在指令前面加上 sudo 就可以了！ "},"title":"清除 Linux 機器上的 Swap (Buff、Cache、Swap 比較)"},"/blog/other/linux-command/":{"data":{"":"因為最近在管理機器時，常常會使用各式各樣的指令來協助管理，所以把常用的指令依照不同類別整理在底下呦 😘","參考資料#參考資料":"3 種檢查遠端埠號是否開啟的方法：https://www.ltsplus.com/linux/3-ways-check-remote-server-open-port","系統類#系統類":"顯示當前進程狀態 ps ps [參數] -A 列出所有的進程 -w 可以加寬顯示較多的訊息 -au 顯示更詳細的訊息 -aux 顯示所有包含其他用戶的進程 au(x) 輸出的格式：\nUSER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND USER：行程的擁有者 PID：pid %CPU：佔用的 CPU 使用率 %MEM：佔用的記憶體使用率 VSZ：佔用的虛擬記憶體大小 RSS：佔用的記憶體大小 TTY：終端的次要裝置號碼 STAT：該行程的狀態： D：無法中斷的休眠狀態 (通常都是 IO 進程) R：正在執行中 S：靜止狀態 T：暫停執行 Z：不存在但暫時無法消除 W：沒有足夠的記憶體可以分配 \u003c：高優先序的進程 N：低優先序的進程 START：進程開始時間 TIME：執行的時間 COMMAND：所執行的指令 刪除執行中的進程 kill kill [-s \u003c訊息名稱或編號\u003e] [程序] or [-l \u003c訊息編號\u003e] -l \u003c訊息編號\u003e：若不加 \u003c訊息編號\u003e選項，-l 會列出全部的訊息名稱 -s \u003c訊息名稱或編號\u003e：指定要送出的訊息 最常用的訊息是\n1 (HUP)：重新加載進程 9 (KILL)：刪除一個進程 15 (TERM)：正常停止一個進程 顯示系統開機紀錄 who who [參數] -b：查看最後一次(上次)系統啟動時間 -r：查看最後一次(上次)系統啟動時間及運行級別 top up 後面代表系統到目前運行的時間，所以反推就可以知道啟動時間\nuptime uptime 17:44 up 2 days, 8:48, 3 users, load averages: 3.48 3.37 3.56 一樣會顯示到目前已經啟動多久，跟是幾點啟動等資訊","網路類#網路類":"查詢遠端 Port 是否開放 nc (netcat) nc (netcat)，可以讀取經過 TCP 及 UDP 的網路連線資料，是一套很實用的網路除錯工具。\n安裝指令：\nyum install nc 檢查 Port 是否有開放，可以用以下指令來查詢：\nnc -zvw3 \u003cIP:Port\u003e -z 只進行掃描，不進行任何的資料傳輸\n-v 顯示掃描訊息\n-w3 等待 3 秒\n如果 Port 有開放，會回傳以下內容：\nNcat: Version 7.50 ( https://nmap.org/ncat ) Ncat: Connected to \u003cIP:Port\u003e. Ncat: 0 bytes sent, 0 bytes received in 0.01 seconds. 如果沒有開放，會回傳以下內容：\nNcat: No route to host. nmap nmap (Network Mapper) 是另一個可以檢查 Port 的工具，安裝語法是這樣：\nyum install nmap 檢查 Port 是否有開放，可以用以下指令來查詢：\nnmap \u003cIP\u003e -p \u003cPort\u003e 如果 Port 有開放，會回傳以下內容：\nStarting Nmap 7.92 ( https://nmap.org ) at 2022-06-17 16:06 CST Nmap scan report for\u003cDomain\u003e (\u003cIP\u003e) Host is up (0.0039s latency). PORT STATE SERVICE 80/tcp open http Nmap done: 1 IP address (1 host up) scanned in 1.09 seconds 如果沒有開放，會回傳以下內容：\nStarting Nmap 7.92 ( https://nmap.org ) at 2022-06-17 16:06 CST Nmap scan report for\u003cDomain\u003e (\u003cIP\u003e) Host is up (0.0039s latency). PORT STATE SERVICE 80/tcp filtered http Nmap done: 1 IP address (1 host up) scanned in 1.09 seconds Telnet Telnet 也是一個可以檢查 Port 的工具，安裝語法是這樣：\nyum install telnet 檢查 Port 是否有開放，可以用以下指令來查詢：\ntelnet \u003cIP\u003e \u003cPort\u003e 如果 Port 有開放，會回傳以下內容：\nTrying \u003cIP\u003e… Connected to \u003cIP\u003e. Escape character is ‘^]’. ^CConnection closed by foreign host. 如果沒有開放，會回傳以下內容：\nTrying \u003cIP\u003e… telnet: Unable to connect to remote host: Connection refused "},"title":"Linux 常用指令"},"/blog/other/snyk/":{"data":{"":"前幾天有機會可以去參加 DevOpsDays Taipei 2022 活動，聽到了一門有關於如何將 Security 加入 DevOps - GitLab CI/CD with Snyk 的講座，覺得蠻有趣的，想說順便紀錄一下這幾天去聽的內容，文章底下有附上本次大會的共筆以及該講者於自己部落格所發的文章，歡迎大家先行查看歐 😆\nDevOps 圖片 (醫院 DevOps 如何落地(1) ─ DevOps 與醫院)\n我們平常熟知的 DevOps 文化應該是像上面這張圖一樣，可以透過自動化「軟體交付」和「架構變更」的流程，來使得構建、測試、發布軟體能夠更加地快捷、頻繁和可靠。\n通常在 Development 跟 Operations 之外還有一個專門負責檢查程式有無漏洞或是安全性問題的單位，那我們試想一下，如果在 Dev 或是 Ops 中間先加上了檢查安全的步驟，是不是可以讓整體流程效率更加提升呢！？ 可以免去程式寫完才發現有安全性問題的漏洞，需要重新修改，而是在部署前，或是程式開發中就先被檢查出來。\n對於資訊安全方面，開發人員進行開發時，必須關注目前的寫法是不是安全的，除了開發人員的程式碼外，還要注意使用的開源程式或是套件是不是已經有被掃出漏洞，加上現在越來越多人使用容器化，對於 Container Images 裡面所包含的套件安全也是一大問題。\n所以單純的自動化整合與部署已經不能滿足需求，就像下圖一樣，開始有了 DevSecOps 這個詞產生，在 DevOps 的 CI/CD 流程中加入資安解決方案，藉由資安解決方案來減少開發人員去檢查程式漏洞的時間，也讓 DevOps 快速迭代更加的完整。\nDevSecOps 圖片 (Apps Built Better: Why DevSecOps is Your Security Team’s Silver Bullet)","snyk#Snyk":"那我們這次要使用的工具是 - Snyk，我先簡單介紹一下這個公司以及工具，它是位於波士頓的網絡安全公司，專門從事雲計算，提供增輕鬆集成，可以將安全掃描結合到 IDE、Repository、CI/CD 等，連續掃描以及一鍵修復等功能。他本身是一個 SaaS 的服務，所以需要到官網註冊一個帳號才能使用，也支援不同的 SSO 登入，詳細可以參考註冊畫面。\n這個工具主要提供了 4 種掃描的功能， 分別是：\nCode Open Source Container IaC 那我們會依照這４種功能依序介紹，詳細的可以參考 Snyk User Documentation # Explore Snyk products。\nCode 能夠直接掃描目前開發 Project 的程式碼有沒有安全性的問題，可以大幅減少開發人員去檢查程式漏洞的時間，目前支援了以下三種掃描方式：\nIDE Plugins Web CLI IDE Plugins IDE 我們這邊以 VS Code 為例來說明，如果有在用 VS Code 的大家應該知道 VS Code 有一個 Marketplace，可以安裝很多不同的套件 Plugins，之後有時間也會寫一篇好用的 Plugins 文章，大家可以在持續關注我歐 😍\n首先先開啟左側的 Marketplace，搜尋 Snyk，選擇下載量最高的準沒錯，選擇 Snyk Security - Code and Open Source Dependencies。 VS Code Marketplace 安裝 Snyk\n安裝完後，在左側欄位應該就會看到 Snyk 的狗狗 Logo，如果沒有登入過，會先要求你登入 Snyk 並授權給 Plugins 使用。最後當你開啟專案時，會開始掃描程式碼，並且會依照開源套件安全性、程式碼安全性來分類，會顯示套件遇到什麼漏洞需要升級以及程式碼哪一段會有安全性的問題，能夠讓開發人員在第一時間就修復安全問題。 IDE Plugins 掃描\nWeb 登入 Snyk 網站，直接加入對應的 Project，會直接於網頁上掃描漏洞，並提示錯誤內容。\n選擇加入 project\n掃描漏洞並顯示錯誤內容\nCLI 要使用 CLI 需要先安裝 Snyk CLI，我們一樣使用 Homebrew 來安裝：\nbrew tap snyk/tap brew install snyk 當然除了 Homebrew 以外還有其他的安裝方式，例如：curl 下載執行檔、npm、yarn、docker 執行等等，詳細可以參考以下安裝連結。\n可以直接使用以下指令才進行測試 sync test，還有其他 CLI 指令，詳細可以參考 Snyk CLI：\nCLI 掃描 (官網)\nOpen Source 能夠掃描目前開發的 Project 中有沒有使用到有漏洞的 library，目前支援以下圖片語言，詳細可以參考 Open Source - Supported languages and package managers：\nOpen Source library 掃描 (官網)\nContainer 在 Container image 中會存放著不同的 base image ，在 base image 中又會使用不同的 library。所以這些 container 的安全性也是需要被關注的。\n可以直接使用 web 來選擇相對的 Container 倉庫去做掃描，如果有安裝上面說的 Snyk CLI，也可以使用該指令執行 snyk container test \u003ccontainer image\u003e\nContainer 掃描\nIac Snyk 的基礎設施即代碼 (IaC) 可幫助開發人員編寫安全的基礎設施配置，預防錯誤的設定產生，並且支援多種格式：\nK8s YAML HashiCorp Terraform AWS CloudFormation Azure Resource Manager (ARM) ","參考資料#參考資料":"DevOpsDays Taipei 2022 共同筆記 - 在你的 DevOps 中加入一點 Security — GitLab CI/CD with Snyk - 鄭荃樺 (Barry.Cheng) ：https://hackmd.io/@DevOpsDay/2022/%2F%40DevOpsDay%2FHkm1iY6xi\n身為 DevOps 工程師，使用 Snyk 掃描漏洞也是很正常的：https://barry-cheng.medium.com/%E8%BA%AB%E7%82%BAdevops%E5%B7%A5%E7%A8%8B%E5%B8%AB-%E4%BD%BF%E7%94%A8snyk%E6%8E%83%E6%8F%8F%E6%BC%8F%E6%B4%9E%E4%B9%9F%E6%98%AF%E5%BE%88%E6%AD%A3%E5%B8%B8%E7%9A%84-d7d8f2ad2304\n在你的 DevOps 中加入一點 Security — GitLab CI/CD with Snyk (講師簡報)：https://s.itho.me/ccms_slides/2022/9/26/3a2ac1ef-4143-4328-9fef-066782ac7dc6.pdf"},"title":"找出程式碼、開源套件、容器的安全漏洞工具 - Snyk"},"/blog/rd/":{"data":{"":"此分類包含 RD 使用的相關工具文章。\nFluentd-Server 出現 Fluent::Plugin::Elasticsearch Error 400 - Rejected by Elasticsearch 錯誤解決 Kibana 新增 index 索引時一直轉圈圈以及顯示 HTTP 403 Forbidden "},"title":"RD 使用的相關工具"},"/blog/rd/fluentd-server-show-fluent-plugin-elasticsearch-error400/":{"data":{"":"先說結論 … EFK 真的好多雷 😆 我們今天又再度踩雷拉，這次又是炸的分身碎骨，花了 4 個多小時才找到原因，也有可能是小弟我跟 EFK 不是很熟 XD。就如標題所說，我在抽 Log 的時候發現有部分的 Log 送不到 Kibana，檢查 Fluentd-Server 的 Log 發現裡面有 Fluent::Plugin::Elasticsearch Error 400 - Rejected by Elasticsearch 的錯誤訊息，到底這個 Error 400 是什麼咧，就跟著我一起重溫找問題的苦難吧 XD","efk-結構#EFK 結構":"在開始之前我先簡單說一下我現在的 EFK 結構：\nEFK 結構\n現在的 EFK 結構如上圖，我在要抽 Log 的 Pod 中多塞一個 fluentd-bit container，將 fluentd-bit 與服務(例如 nginx) 的 log 掛相同路徑，讓 fluentd-bit 可以抽到服務 log，接著就透過 fluentd-server-svc 打到 fluentd-server pod，後面就是常見的 EFK 結構，就不多做說明。","參考資料#參考資料":"Elasticsearch 400 error #467：https://github.com/uken/fluent-plugin-elasticsearch/issues/467","問題的出現#問題的出現":"我們就像平常一樣寫好整個 EFK 的 yaml，run 的時候也正常，可以收到 log，但很奇怪的是我們先送 log_false 的 log 可以正常收到，但再接著送 log_true 就收不到，我們也嘗試反過來送，先送 log_true，再送 log_false，發現換收不到 log_false 的 log\nlog_false { \"result\": false, \"message\": \"我是馬賽克\", \"code\": 11223344, \"data\": { \"end_at\": \"2022-12-19 04:09:00\" }, \"response_code\": \"326dgf241geh1596489\", \"job_name\": \"叮叮噹噹聖誕節\", \"method\": \"GET\", \"url\": \"url.com\" } log_true { \"result\": true, \"data\": true, \"response_code\": \"wkCJl1dfr45671607965\" } 接著在 Fluentd-Server Log 中發現 Fluent::Plugin::Elasticsearch Error 400 - Rejected by Elasticsearch 錯誤\nfluentd-server 跳出錯誤 LOG\n我到網路上搜尋，看看有沒有人有遇到跟我一樣的問題，發現在 uken/fluent-plugin-elasticsearch 也有其他人有遇到同樣的問題\nuken/fluent-plugin-elasticsearch issus 詢問\n有人說可能是 Elasticsearch 的 disk 滿了，也有人說是裝在 Fluentd-Server 的套件 fluent-plugin-elasticsearch 版本有問題，或是將 Elasticsearch 跟 Kibana index 刪除就可以，我有砍掉 index 再重新倒入 Log，還是會有問題。\n一開始以為是 log_false 跟 log_true 的欄位不同，所以才會有這個問題發生（你看就知道我跟 EFK 不熟吧ＸＤ），所以有另外倒一些其他服務(欄位也不同)的 log 進去測試，發現是可以正常抽到，且 kibana 那邊會依照 es 提供的欄位自動新增，所以也不是欄位的問題，那最後的問題是什麼呢？","如何解決問題#如何解決問題":"最後在我們仔細檢查後發現，log_false 跟 log_true 的欄位其中一樣存的資料型態不太一樣，發現其中的 data 欄位在 log_false 的資料型態是 array，在 log_true 存的時候是字串，所以導致當其中一個 log 寫入後，另一個 log 就會因為同欄位的資料型態有所不同，而無法寫入 log，且在 Fluentd-Server 會出現標題的 Error 400 原因拉～\n最後重新調整欄位的資料型態，就順利解決此次問題 ✌️✌️✌️"},"title":"Fluentd-Server 出現 Fluent::Plugin::Elasticsearch Error 400 - Rejected by Elasticsearch 錯誤解決"},"/blog/rd/kibana-create-index-forbidden/":{"data":{"":"前幾天在工作使用 Kibana 時，想要新增一個新的索引，發現選擇索引並按下新增的按鈕，會一直轉圈圈，等了一陣子，使用開發工具 F12 查看，跳出了 HTTP 403 Forbidden，到底是什麼原因導致的呢！？我們一起看下去吧，會從問題的出現到問題原因再到如何解決問題，來仔細介紹，希望大家不要像我一樣踩到雷 🤣","參考資料#參考資料":"Kibana 创建索引 POST 403 (forbidden) on create index：https://www.cnblogs.com/caoweixiong/p/10972120.html","問題原因#問題原因":"文章的說明是索引變成只允許讀取的狀態，其原因是因為出現這個 HTTP 403 Forbidden 前，ElasticSearch 的空間滿了，導致 Kibana 會自動的將索引改成只允許讀取的狀態，我們來看一下剛剛的 Index 狀態是不是像他說的一樣變成只允許讀取的狀態呢\n可以到 kibana 的 Dev Tools 下指令來查詢歐，輸入 GET _settings，就會顯示以下圖片的內容囉 索引只允許讀取的設定變成\n發現正如文章所說，是因為 read_only_allow_delete 的狀態變成了 true，所以才沒辦法新增索引～","問題的出現#問題的出現":"那天是個變冷的 12 月，要幫 RD 同仁新增 Kibana 的索引時，照往常一樣輸入 Index pattern，按下一步，選擇 Configure settings ：\n輸入 Index pattern (圖片為範例，正常都是用 -* )\n按下新增索引，他就開始無限的轉圈圈，有去查看 ElasticSearch 的 log，發現也沒有特別的錯誤訊息\n新增索引後，一直轉圈圈\n接著想說打開開發者工具 F12 來看看，是卡在哪一個點，卻發現有幾個紅字寫著 HTTP 403 Forbidden\n開發者工具網頁內容顯示 HTTP 403 Forbidden\n想說為什麼會有 HTTP 403 Forbidden，之前也沒有看過類似的錯誤訊息，於是就開始在網路上亂晃，最後在同事的幫助下找到了一個跟我們情況很相似的文章 Kibana 创建索引 POST 403 (forbidden) on create index","如何解決問題#如何解決問題":"我們查看文章的解決辦法，有兩種辦法，一個是擴大 ElasticSearch 的空間，以及使用 kibana 的 Dev Tools 下指令修改，那我們兩種都有做，這邊就直接介紹下指令需要輸入什麼～\n需要再 Dev Tools 輸入以下指令，來修改 Index 的狀態：\nPUT _settings { \"index\": { \"blocks\": { \"read_only_allow_delete\": \"false\" } } } Dev Tools 修改 Index 的狀態\n最後我們用 GET _settings 檢查一下索引狀態是不是已經變回原本的了：\n索引只允許讀取的設定變成 false\n最後就可以順利新增索引拉 👍👍👍\n順利新增索引"},"title":"Kibana 新增 index 索引時一直轉圈圈以及顯示 HTTP 403 Forbidden"},"/blog/terraform/":{"data":{"":"此分類包含 Terraform 相關的文章。\n什麼是 IaC ? Terraform 又是什麼？ 使用 Terraform 建立 Google Compute Engine 使用 Terraform 建立 Google Kubernetes Engine Terraform 如何多人共同開發 (將 tfstate 存在後端) 如何將 Terraform 改寫成 module ? 如何導入 Terragrunt，Terragrunt 好處是什麼？ "},"title":"Terraform"},"/blog/terraform/terraform-gce/":{"data":{"":"嗨嗨大家好，距離上一篇筆記又隔了 3 個月，最近公司有專案在忙，沒時間把上次提到的 Terraform 應用筆記寫完，現在他來拉～～～ 😂 我們這次的主題是使用 Terraform 來建立 Google Compute Engine 的機器，想知道要怎麼用一段程式碼就可以建立、修改、刪除 Google Compute Engine 的機器一定要來看這一篇～我們開始囉 🧑‍💻","修改-google-compute-engine#修改 Google Compute Engine":"當我們發現我們建立的 Google Compute Engine 參數有錯，想要修改時，我們只需要修改程式碼部分，並重新下一次 terraform apply 來修改 Google Compute Engine，就會看到以下畫面 (有些設定檔是不能修改的，若修改他會重新創建一個新的機器，像是 name 之類的，使用時要小心一點 😉)\n我們拿剛剛提到的 nat_ip，我們先把它註解掉，再下 terraform apply 看看機器有什麼變化～\nterraform changed\n可以看到他會提示說，他會改變 network_interface，也移除 access_config 的設定，執行後的 Resources 也會從剛剛的 added 變成 changed，我們看一下 GCP 有沒有改變：\nterraform changed\n可以看到原本的外部 IP 位置被改掉了～ 最後提醒：如果有使用 terraform 來修改資源設定，不能動到特定的項目，不然他的流程是先把原本的給刪掉，再重新建立一個新的，原本的機器沒有備份，東西就會不見歐～","刪除-google-compute-engine#刪除 Google Compute Engine":"最後假如我們要刪除 Google Compute Engine，也可以使用 terraform 的指令來刪除，我們順便來測試一下上面有設定的 deletion_protection 刪除保護機制是不是正常～\n目前 deletion_protection 還是 true，我們直接下 terraform destroy，看看是否可以刪除 Google Compute Engine\nterraform destroy\n可以看到他會提醒你說 Deletion Protection is enabled，必須先把他改成 false terraform apply 後才可以刪除～\n改成 false 在下 terraform apply\n現在 deletion_protection 是 false，我們就可以下 terraform destroy 來刪除 Google Compute Engine 🔪\nterraform destroy\nGoogle Compute Engine 刪除中…\n以上就是簡單的使用 Terraform 建立 Google Compute Engine 介紹囉～歡迎大家留言指教，明天的文章是介紹如何使用 Terraform 建立 GKE 💕","參考資料#參考資料":"registry.terraform.io/providers (google_compute_instance)：https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/compute_instance","建立-google-compute-engine#建立 Google Compute Engine":"當我們寫好 Google Compute Engine tf 檔案後，我們接著把他建立，建立前要先使用 terraform init 來做初始化\nterraform init\n接著可以先使用 terraform plan 來查看我們的設定是否是我們想要的，或是直接用 terraform apply 來建立 Google Compute Engine\nterraform apply\n可以看到成功建立我們的 test Google Compute Engine（也可以看到因為我們有開 nat_ip 所以有外部 IP）\nGCP Google Compute Engine","撰寫-google-compute-engine-tf-檔案#撰寫 Google Compute Engine tf 檔案":"相信大家有先看完上一篇 什麼是 IaC ? Terraform 又是什麼？ 才來看這一篇的對吧 😎，對於 Terraform 的程式架構及指令，我們這邊就不多做介紹，我們直接來看程式要怎麼寫～(程式碼主要是參考官方文件，加上一些其他的設定來做介紹，程式碼也會同步到 Github ，需要的也可以去 Clone 來使用歐！ Github 程式碼連結 )\n小提醒：由於程式碼較長，我將他拆開來說明 💖\n選擇供應者以及對應的專案 provider \"google\" { project = \"gcp-20210526-001\" } 由於我們要建立的 Google Compute Engine 是由 Google 所提供的 api 來建立的，所以一開始要先設定好提供者的名稱 google 以及我們要在哪一個 GCP 的專案 ID\nresource 設定 接下來的設定都會放在以下的 google_compute_instance resource 內，為了方便介紹，就不會標明 google_compute_instance，詳細完整程式碼請參考 GitLab Github 程式碼連結\nresource \"google_compute_instance\" \"default\" { } 基本設定 name = \"test\" description = \"我是 test 機器\" machine_type = \"n2-standard-8\" zone = \"asia-east1-b\" tags = [\"test\"] labels = { env = \"test\" } deletion_protection = \"true\" name：GCE 要求資源的唯一名稱。如果有更改此項會直接強制創建的新資源 (必填) description：對此資源的簡單說明 (選填) machine_type：要創建的機器類型 (必填) zone：創建機器的所在區域，若沒有填寫，則會使用提供者的區域 (選填) tags：附加到實體的網路標籤列表 (選填) labels：一組分配給 disk 的 key/value 標籤 (選填) deletion_protection：刪除保護，預設是 false，當我們使用 terraform destroy 刪除 GCE 時，必須先改成 false，才可以刪除，否則會無法刪除且 Terraform 運行也會失敗，算是一個保護機制，後面再刪除 Google Compute Engine 時會測試畫面 (選填) 啟動 disk 設定 boot_disk { initialize_params { image = \"debian-cloud/debian-10-buster-v20210512\" type = \"pd-balanced\" size = \"50\" } } image：初始化此 disk 的 image (選填) type：GCE disk 類型 (選填) size：image 大小，已 GB 為單位，如果未指定，將會繼承其初始化 disk 的 image 大小 (選填) 網路設定 network_interface { network = \"projects/rd-gateway/global/networks/rd-common\" subnetwork = \"projects/rd-gateway/regions/asia-east1/subnetworks/rd-common-asia-east1-pid-cicd\" access_config { nat_ip = \"\" } } network：設定附加到的網路名稱或是 self_link (選填) subnetwork：設定附加到的子網路名稱或是 self_link (選填) nat_ip：如果想要有外網的 ip，必須加上此參數，才會產生一組外網 ip (選填) 權限設定 service_account { email = \"676962704505-compute@developer.gserviceaccount.com\" scopes = [\"storage-rw\", \"logging-write\", \"monitoring-write\", \"service-control\", \"service-management\", \"trace\"] } email：服務帳戶電子郵件地址。如果未提供，則使用預設的 Google Compute Engine 服務帳戶 (選填) scopes：服務範圍列表，可以點我查看範圍的完整列表 (必填) 以上只是我在建立 Google Compute Engine 最簡單的設定，當然還有很多其他的設定，可以參考 registry.terraform.io/providers (google_compute_instance) 裡面有更多的 resource 設定，有需要的就自己來看看吧 🧐"},"title":"使用 Terraform 建立 Google Compute Engine"},"/blog/terraform/terraform-gke/":{"data":{"":"我們接續昨天的建立 Google Kubernetes Engine 文章，今天要來介紹的是如何用 Terraform 建立 Google Kubernetes Engine，由於使用 terraform 去建立、修改、刪除的指令大家應該都清楚了，那我今天的文章就不在多說，直接來介紹一下要怎麼撰寫 Google Kubernetes Engine tf 檔案 😏","參考資料#參考資料":"registry.terraform.io/providers (google_container_cluster)：https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/container_cluster\nregistry.terraform.io/providers (google_container_node_pool)：https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/container_node_pool","撰寫-google-kubernetes-engine-tf-檔案#撰寫 Google Kubernetes Engine tf 檔案":"程式碼會同步到 Github ，需要的也可以去 Clone 來使用歐！ Github 程式碼連結，小提醒：由於程式碼較長，我將他拆開來說明 💖\n由於等等程式碼較長，所以我在前面這邊先做說明，GKE 的結構是 叢集(cluster) \u003e 節點池(node_pool) \u003e 節點(node)，本次的介紹範例，會有一個叢集裡面有一個節點池，節點池裡面有 6 個節點數量，範例裡面會加上我比較常用到的一些設定，以及一些文件裡面的用法，大家可以依照自己的需求來使用參數：\n限制使用的版本 在上一篇 使用 Terraform 建立 Google Compute Engine，我們知道 Terraform 其實就是對應的提供商，提供對應的 api 來讓我們可以用 terraform 去建置很多 IaC，但供應商提供的 api 會隨著版本而有所更動，可能換了一個版本，原本可以使用的 resource 參數就會有所不同，所以我們可以在一開始，先設定好這隻 tf 要使用的供應商及對應的版本，可以參考以下程式碼：\nterraform { required_providers { google = { source = \"hashicorp/google\" version = \"~\u003e 4.38.0\" } } } 可以看到我們把 google 這個供應商裡面設定好他的 source 以及 version，這樣就算之後 goolge 有更新 terraform 的 api，我們也不需要去更換參數就可以使用了～\n選擇供應者以及對應的專案 provider \"google\" { project = \"project\" } 除了可以使用專案 ID 以外，當然也可以使用專案的名稱拉 🥳\nresource 設定 google_container_cluster resource \"google_container_cluster\" \"cluster\" { name = \"test\" location = \"asia-southeast1-b\" min_master_version = \"1.22.12-gke.300\" network = \"projects/gcp-202011216-001/global/networks/XXXX\" subnetwork = \"projects/gcp-202011216-001/regions/asia-southeast1/subnetworks/XXXX\" default_max_pods_per_node = 64 remove_default_node_pool = true initial_node_count = 1 enable_intranode_visibility = false ip_allocation_policy { cluster_secondary_range_name = \"gke-pods\" services_secondary_range_name = \"gke-service\" } resource_labels = { \"env\" = \"test\" } addons_config { http_load_balancing { disabled = false } horizontal_pod_autoscaling { disabled = false } network_policy_config { disabled = false } } master_auth { client_certificate_config { issue_client_certificate = false } } private_cluster_config { enable_private_endpoint = false enable_private_nodes = true master_ipv4_cidr_block = \"172.16.0.0/28\" } logging_config { enable_components = [\"SYSTEM_COMPONENTS\", \"WORKLOADS\"] } monitoring_config { enable_components = [\"SYSTEM_COMPONENTS\"] } node_config { machine_type = \"e2-medium\" disk_size_gb = 100 disk_type = \"pd-standard\" image_type = \"COS_CONTAINERD\" oauth_scopes = [ \"https://www.googleapis.com/auth/devstorage.read_only\", \"https://www.googleapis.com/auth/logging.write\", \"https://www.googleapis.com/auth/monitoring\", \"https://www.googleapis.com/auth/servicecontrol\", \"https://www.googleapis.com/auth/service.management.readonly\", \"https://www.googleapis.com/auth/trace.append\" ] metadata = { disable-legacy-endpoints = \"true\" } } } name：叢集的名稱，在這個專案及區域唯一名稱 (必填) location：要將此叢集建立在哪一個區域 (選填) min_master_version：master 的最低版本 (選填) network：叢集連接到的 Google Compute Engine 網絡的名稱或 self_link (選填) subnetwork：啟動叢集的 Google Compute Engine 子網的名稱或 self_link (選填) default_max_pods_per_node：此叢集中每個節點的預設最大 pod 數 (選填) remove_default_node_pool：如果設定為 true，則在創建叢集時會幫我們刪除預設的節點池。會使用到這個的原因是因為 terraform 沒辦法修改預設節點池的名稱，所以我的做法是，會新增要的節點池，在使用這個參數把預設的給刪掉(選填) initial_node_count：要在此叢集的預設節點池中創建的節點數 (選填) enable_intranode_visibility：是否為此叢集啟用了節點內可見性 (選填) ip_allocation_policy：為 VPC 原生叢集分配叢集 IP (選填) resource_labels：應用於叢集的 GCE 資源標籤 key/value (選填) addons_config http_load_balancing：是否要啟用 HTTP (L7) 的負載平衡 (選填) horizontal_pod_autoscaling：是否要啟用 HPA 水平 Pod 自動擴展 (選填) network_policy_config：是否要啟用網路策略 (選填) master_auth client_certificate_config：是否要啟用該叢集客戶端證書授權 (選填) private_cluster_config enable_private_endpoint：是否要啟用叢集專用的私有端點，禁止公共端點的訪問 (選填) enable_private_nodes：是否要啟用私有叢集功能，在叢集創建私有端點 (選填) master_ipv4_cidr_block：私有端點 IP 範圍 (選填) logging_config：叢集的日誌記錄配置 enable_components (公開日誌的 GKE 組件) 設定，包含：SYSTEM_COMPONENTS、APISERVER、CONTROLLER_MANAGER、SCHEDULER、WORKLOADS (必填) monitoring_config：叢集的監控配置 enable_components (GKE 組件公開指標) 設定，包含：SYSTEM_COMPONENTS、APISERVER、CONTROLLER_MANAGER、SCHEDULER (選填) node_config：創建預設節點池參數 machine_type：Google Compute Engine 機器類型，預設為 e2-medium (選填) disk_size_gb：每個節點的 disk 大小，以 GB 為單位。允許最小為 10 GB，預設為 100 GB (選填) disk_type：連接到每個節點的 disk 類型，有 pd-standard、pd-balanced 或 pd-ssd，預設為 pd-standard (選填) image_type：創建新節點池後 NAP 使用的預設 image 類型。該值必須是 [COS_CONTAINERD、COS、UBUNTU_CONTAINERD、UBUNTU] 之一。COS 和 UBUNTU 已於 GKE 1.24 棄用 (選填) oauth_scopes：在預設服務帳戶下的所有節點虛擬機上可用的一組 Google API 範圍。 (選填) metadata：分配給叢集中實例的 key/value (選填) google_container_node_pool resource \"google_container_node_pool\" \"aaa\" { name = \"aaa\" project = \"project\" location = google_container_cluster.cluster.location cluster = google_container_cluster.cluster.name node_count = 6 node_locations = [ google_container_cluster.cluster.location ] node_config { # 省略 ... 與上面的 google_container_cluster 相同 } management { auto_repair = true auto_upgrade = false } upgrade_settings { max_surge = 1 max_unavailable = 0 } } name：節點池名稱 (選填) project：創建節點池的項目 ID (選填) location：叢集所在位置，可以使用資源名稱 (google_container_node_pool) +命名 (cluster) + 參數 (location) 來代表 (選填) cluster：叢集名稱，可以使用資源名稱 (google_container_node_pool) +命名 (cluster) + 參數 (name) 來代表 (選填) node_count：節點數量 (選填) node_locations：節點區域 (選填) management：節點管理配置 auto_repair：是否要自動修復 (選填) auto_upgrade：是否要自動升級 (選填) upgrade_settings：指定節點升級設定及方式 max_surge：升級期間可以添加到節點池的額外節點數 (選填) max_unavailable：升級期間可以同時不可用的節點數 (選填) 可以看到有很多設定都是選填的，所以不需要像我範例一樣，把所有的都打出來，可以參考官方文件，將自己想要的設定寫出來，並注意其他參數的預設值是多少，就可以打造屬於自己的 Terraform 建立 Google Kubernetes Engine IaC 程式囉～"},"title":"使用 Terraform 建立 Google Kubernetes Engine"},"/blog/terraform/terraform-module/":{"data":{"":"當我們要管理的資源越來越多後，會產生很多的 tf 檔案，假設我們現在有三個 gce 服務，會在以下三個不同環境上面運作，每個環境都會有我們之前學會的基本 tf 檔案(包含 provider.tf 、main.tf、backend.tf)，且其中的 main.tf 檔案內有些設定會不太一樣，如下：\ndev backend.tf main.tf provider.tf prod backend.tf main.tf provider.tf qa backend.tf main.tf provider.tf dev/main.tfresource \"google_compute_instance\" \"instance\" { project = \"馬賽克\" name = \"test-dev\" machine_type = \"e2-small\" zone = \"asia-east1-b\" boot_disk { initialize_params { image = \"debian-cloud/debian-10\" size = 50 } } .... 其他省略不寫 .... } qa/main.tf (多了 tags)resource \"google_compute_instance\" \"instance\" { project = \"馬賽克\" name = \"test-qa\" machine_type = \"e2-small\" zone = \"asia-east1-b\" tags = [\"for-qa\"] boot_disk { initialize_params { image = \"debian-cloud/debian-10\" size = 50 } } .... 其他省略不寫 .... } prod/main.tf (多了 labels)resource \"google_compute_instance\" \"instance\" { project = \"馬賽克\" name = \"test-prod\" machine_type = \"e2-small\" zone = \"asia-east1-b\" labels = { aaa = \"test1\" bbb = \"test2\" ccc = \"test3\" } boot_disk { initialize_params { image = \"debian-cloud/debian-10\" size = 50 } } .... 其他省略不寫 .... } 可以看到三個 main.tf 檔案除了 name 以外，在 qa 還多了 tags、prod 多了 labels 等設定，等於我們會依照每個不同環境不同服務去客製化他的 tf 資源設定，雖然非常直覺，但往後的維護以及調整卻非常不方便 ( 假設我們現在要全部都加上 labels，就必須一個一個檢查並調整 )。\n為了方便我們維護以及重複使用，因此有了 module，可以先將全部會使用到的設定寫成模板，透過參數的方式帶入即可，module 有以下幾個優點：\n重複使用性： module 讓程式碼更易於重複使用。當我們需要在多個項目中使用相同的基礎架構或配置時，可以先將其封裝為一個 module。這樣，我們只需要在不同的項目中引用並調整模組的參數，而不需要重新寫整個 tf 檔。\n抽象化：將 Terraform 代碼轉換為 module 可以將詳細的實現細節抽象化，僅寫必要的參數。這樣做可以提高程式碼的可讀性和可維護性，並降低使用者學習和使用的門檻。\n參數化配置：module 可以使用輸入參數來接收不同的配置值。這意味著您可以根據需要動態更改模組的行為，而不需要直接修改模組的內部程式。這使得配置更靈活並支持不同環境的部署。\nmodule 版本控制：將 Terraform 程式封裝為 module 後，可以使用 git 對其進行版本控制。可以更輕鬆地協作和共享 module (可以將 module 與 Terraform 分別存放，並使用對應 tag or 分支來做開發 )。","參考資料#參考資料":"Types and Values：https://developer.hashicorp.com/terraform/language/expressions/types","實作#實作":"當我們完成上面的架構後，我們進入 projects/prod/main.tf 路徑下，開始用 module 的方式建立資源，建立資源的流程與原本的相同，一樣是 init \u003e plan \u003e apply 這三個步驟，那我們一個一個來看，與原本的建立方式有哪些不同之處吧～\ninit 我們使用 terraform init 來看看原本 init 與使用模組 init 後差在哪裡：\n原先 terraform init 結果\n使用 module init 結果\n可以看到有使用 module 在初始化的時候，會連同 module 也一併初始化，接著我們進到 .terraform 資料夾內，可以看到有 moduels 資料夾。\n.terraform 檔案差異\n在進去看會看有一個 modules.json 檔案，會紀錄 module 使用的路徑，因此當我們使用的 module 有改變時，要記得重新 init 才可以確保使用的 module 是正確的。\n使用 module 會多一個 modules.json 檔案\nplan 我們一樣下 terraform plan 指令，來看看兩者顯示的差異：\n原先 terraform plan 結果\n使用 module plan 結果\n可以看到使用 module 在 plan 時，預覽創建的資源格式不同，也就代表他存在 tfstate 檔案的格式也會不同 (這個後面會在提到，與 import 也有關係)\napply 使用 terraform apply 來看建立資源後的結果有什麼不同：\n原先 terraform apply 結果\n使用 module apply 結果\napply 看到的與 plan 顯示的一樣，使用 module 建立的資料格式會不太一樣，所以我們來看看兩者 tfstate 檔案的差異：\n原先 terraform 建立的 tfstate 檔案\n使用 module 建立的 tfstate 檔案\nimport import 的功用是可以從雲上服務轉成 tf，在之前原本的 terraform 是要先建立一個空的 resource：\nresource \"google_compute_instance\" \"instance\" { } 再使用 terraform import google_compute_instance.instance 專案ID/機器地區/機器名稱 來匯入雲上服務的狀態到後端存到 tfstate 的位子。\n原先 terraform import 線上服務\n那我們現在改成 module，會比較麻煩一點，因為我們有在 variables.tf 設定我們的變數，若是沒有設定預設值，就必須一定要輸入，所以我們在建立時，要先把變數的空值也補上，如下：\nmodule \"ian-test\" { source = \"../../module/google_compute_instance\" project_id = \"\" instance_name = \"\" machine_type = \"\" instance_zone = \"\" instance_tags = [] instance_labels = {} boot_disk_image_name = \"\" boot_disk_size = 50 attached_disk_enabled = false network_name = \"\" subnetwork_name = \"\" nat_ip_enabled = false metadata = {} resource_policies = [] service_account_email = \"\" service_account_scopes = [] } \" \" 是 string 格式的空值，[ ] 是 list 格式的空值，{ } 是 map 格式的空值，其他的 bool 我預設會給他 false，number 我會隨便給他一個數字 xD。這邊帶入的內容不是很重要，主要是讓他可以去抓到他的架構，我們也可以在 variables.tf 設定時都補上預設值。\n再使用 terraform import module.ian-test.google_compute_instance.instance 專案ID/機器地區/機器名稱 來匯入狀態檔案。(這邊要記得依照你 module 設定的名稱帶入)\n使用 module import 線上服務","檔案說明#檔案說明":"\n首先我們要先定義我們的 module，我們先建立以下資料夾結構以及對應檔案：(同步到 GitHub 需要程式碼的可以前往查看)\n(再次提醒，會區分檔案名稱是因為方便調整跟維護，也可以把它全部寫在同一個 tf 檔案內歐)\nmodule google_compute_instance main.tf outputs.tf variables.tf projects dev backend.tf main.tf provider.tf prod backend.tf main.tf provider.tf qa backend.tf main.tf provider.tf module 資料夾：放我們 module 設定 (這邊範例是放 gce)\nprojects 資料夾：放我們不同服務、不同環境設定 (這邊為了簡化，範例只以不同環境為例)\nmodule/google_compute_instance/main.tfprovider \"google\" { project = var.project_id zone = var.instance_zone } resource \"google_compute_instance\" \"instance\" { name = var.instance_name machine_type = var.machine_type zone = var.instance_zone tags = var.instance_tags labels = var.instance_labels boot_disk { auto_delete = var.boot_disk_auto_delete initialize_params { image = var.boot_disk_image_name size = var.boot_disk_size } } dynamic \"attached_disk\" { for_each = var.attached_disk_enabled ? [1] : [] content { device_name = var.attached_disk_name mode = var.attached_disk_mode source = var.attached_disk_source } } network_interface { network = var.network_name subnetwork = var.subnetwork_name dynamic \"access_config\" { for_each = var.nat_ip_enabled ? [1] : [] content { } } } metadata = var.metadata enable_display = var.enable_display resource_policies = var.resource_policies service_account { email = var.service_account_email scopes = var.service_account_scopes } timeouts {} deletion_protection = var.deletion_protection allow_stopping_for_update = var.allow_stopping_for_update } 我們需要把所有設定的值都挖洞，使用 var 的方式來帶入參數，這邊要注意的是等號前面的值或是 block 名稱都是不能修改的，他是 google 定義的 api 變數，但 var 後的參數名稱我們可以自訂 (後面 variable.tf 會在詳細說明)，那這邊比較特別的用法是 dynamic，以下說明：\ndynamic \"attached_disk\" { for_each = var.attached_disk_enabled ? [1] : [] content { device_name = var.attached_disk_name mode = var.attached_disk_mode source = var.attached_disk_source } } 我們有些 block 只有在特定服務時才需使用，例如上面的 attached_disk 他是 gce 另外掛載其他磁碟的設定，如果有需要我們才會多設定這個 block，沒有則不需要加，因此須使用 dynamic 來動態產生 block，這邊的設定是我們要在參數要帶入 attached_disk_enabled 用 for_each 來判斷是否需要這個 block，如果是 true，就會產生 attached_disk block，且需要輸入 attached_disk_name、attached_disk_mode、attached_disk_source。\nmodule/google_compute_instance/variables.tfvariable \"project_id\" { type = string description = \"GCP 專案 ID\" } variable \"instance_name\" { type = string description = \"GCE 名稱\" } variable \"machine_type\" { type = string description = \"GCE 類型\" } variable \"instance_zone\" { type = string description = \"GCE 所在區域\" } variable \"instance_tags\" { type = list(string) description = \"GCE 網路標記\" } variable \"instance_labels\" { type = map(string) description = \"GCE 標籤\" } variable \"boot_disk_auto_delete\" { type = bool description = \"是否刪除 instance 時，自動刪除開機磁碟\" default = true } variable \"boot_disk_image_name\" { type = string description = \"GCE 映像檔名稱\" } variable \"boot_disk_size\" { type = number description = \"GCE 開機磁碟大小 (單位: GB)\" } variable \"attached_disk_enabled\" { type = bool description = \"是否啟用附加磁碟\" default = false } variable \"attached_disk_name\" { type = string description = \"GCE 附加磁碟名稱\" default = \"\" } variable \"attached_disk_mode\" { type = string description = \"GCE 附加磁碟模式\" default = \"READ_ONLY\" validation { condition = contains([\"READ_WRITE\", \"READ_ONLY\"], var.attached_disk_mode) error_message = \"不符合附加磁碟模式的值，請輸入 READ_WRITE 或 READ_ONLY\" } } variable \"attached_disk_source\" { type = string description = \"GCE 附加磁碟來源\" default = \"\" } variable \"network_name\" { type = string description = \"GCE 網路名稱\" } variable \"subnetwork_name\" { type = string description = \"GCE 子網路名稱\" } variable \"nat_ip_enabled\" { type = bool description = \"是否啟用 NAT IP\" default = false } variable \"metadata\" { type = map(string) description = \"GCE 中繼資料\" } variable \"enable_display\" { type = bool description = \"是否啟用虛擬顯示\" default = false } variable \"resource_policies\" { type = list(string) description = \"GCE 資源原則\" } variable \"service_account_email\" { type = string description = \"GCE 服務帳戶電子郵件\" } variable \"service_account_scopes\" { type = list(string) description = \"GCE 服務帳戶範圍\" } variable \"deletion_protection\" { type = bool description = \"是否啟用刪除保護\" default = false } variable \"allow_stopping_for_update\" { type = bool description = \"是否允許自動停止後更新\" default = false } 這個檔案會定義每個變數的名稱以及資料型態，也可以寫說明以及預設的值，這邊比較特別的是 validation ，他可以驗證帶入的參數是否符合 condition 內容，也可以自定義錯誤的訊息，如下：\nvariable \"attached_disk_mode\" { type = string description = \"GCE 附加磁碟模式\" default = \"READ_ONLY\" validation { condition = contains([\"READ_WRITE\", \"READ_ONLY\"], var.attached_disk_mode) error_message = \"不符合附加磁碟模式的值，請輸入 READ_WRITE 或 READ_ONLY\" } } 這邊限制 attached_disk_mode 輸入必須符合 READ_WRITE or READ_ONLY 的值，如果輸入其他不符合的會顯示 error_message 內容。\n另外 variable 這邊有幾個資料型態可以選擇，如下：\nstring：字串，不知道要選什麼就選他沒錯 xD\nbool：布林值，只有 true、false 兩種選項，適用於判斷的內容，例如剛剛上面說的 attached_disk_enabled 就是使用 bool\nnumber：數字，只能輸入數字\nlist (tuple)：清單，內容可以放置類似 [\"us-west-1a\", \"us-west-1c\"] 的資料\nmap (object)： key value 存放模式，例如：\n{ \"aaa\": \"test1\", \"bbb\": \"test2\", \"ccc\": \"test3\" } module/google_compute_instance/outputs.tfoutput \"instance_id\" { value = google_compute_instance.instance.instance_id } 這邊主要放置要輸出的內容，像我們這邊就會把 instance_id 給顯示出來。\nprojects 我這邊只示範 prod 的部分\nprojects/prod/main.tfmodule \"ian-test\" { source = \"../../module/google_compute_instance\" project_id = \"馬賽克\" instance_name = \"test-prod\" machine_type = \"e2-small\" instance_zone = \"asia-east1-b\" instance_tags = [] instance_labels = { \"aaa\" = \"test1\" \"bbb\" = \"test2\" \"ccc\" = \"test3\" } boot_disk_image_name = \"debian-cloud/debian-10\" boot_disk_size = \"50\" attached_disk_enabled = false network_name = \"馬賽克\" subnetwork_name = \"馬賽克\" nat_ip_enabled = false metadata = {} resource_policies = [] service_account_email = \"馬賽克\" service_account_scopes = [\"storage-ro\", \"logging-write\", \"monitoring-write\", \"service-control\", \"service-management\", \"trace\"] } 這邊我們可以定義要使用 module 的叫什麼，這邊我就取名 google_compute_instance，然後他會去 source \"../../module/ian-test\"，也就是我們剛剛在上面先挖洞的模板，底下就開始帶入我們在 variables.tf 有設定的參數。這邊比較要注意的是，在 main.tf、variables.tf 有使用的變數設定，都必須要寫在個別資源 tf 的檔案裡面，沒有的就帶入對應資料型態的空值，例如 instance_tags、metadata、resource_policies 等等。"},"title":"如何將 Terraform 改寫成 module ?"},"/blog/terraform/terraform-tfstate/":{"data":{"":"此篇是接續上一篇 什麼是 IaC ? Terraform 又是什麼？的 Terraform 文章，我們在上一篇有提到 terraform apply 完後，會多一個檔案 *.tfstate，這個檔案是用來存放服務狀態的檔案，它包含基礎架構的狀態和資源的詳細信息。假設大家都在自己的本地去 apply 同一個服務，會導致每個人的 tfstate 檔案內容不同，有可能去覆蓋掉其他人已經調整的內容，因此我們必須將此 tfstate 檔案存放在一個地方，讓大家都去使用同一份檔案來調整資源。\n我們常用的儲存方式會將 tfstate 存在 gitlab 或 gcs (gcp 架構為例)，以下會簡單說明要如何把 tfstate 存到後端以及各頁面的功能：","gcs#gcs":"gcs 儲存比較簡單一點，因為他就是一個 bucket，所以頁面就跟一般的 gcs 一樣，會顯示檔案名稱、大小、類型等，如果需要查看 tfstate，可以點擊最後的下載按鈕來查看\n那我們接著使用上面 gitlab 的範例檔案，只是要將 backend.tf 內容改為以下：\nbackend.tfterraform { backend \"gcs\" { bucket = \"pid-terraform-state\" prefix = \"/aaa\" } } 上面的設定是指，我們將 backend 後端設定改成 gcs，並且選擇名為 pid-terraform-state 的 bucket，此 bucket 需要先手動建立(因為 bucket 名稱是全域不重複，所以不需要特別設定其 project_id，只要有權限正確都可以跨專案使用)，以及我們要將此 tfstate 存在 aaa 資料夾內。\n接著我們重新刪除剛剛 gitlab 已產生的 .terraform/ 跟 .terraform.lock.hcl 檔案，重新下 init，就可以到 gcs 對應資料夾下，新增了 defaulte.tfstate 檔案。\n產生 defaulte.tfstate\nLock 我們一樣來看一下 gcs 的 lock 會長什麼樣子，gcs lock 會產生一個 default.tflock 檔案，由他去判斷現在是否是 Lock 狀態\ngcs Lock 會出現 .tflock 檔案\n當有其他人也執行 plan or apply 後，就會顯示以下：\ngcs Lock 其他人不能操作","gitlab#gitlab":"那我們要怎麼把 tfstate 存到 gitlab 呢？首先跟之前一樣，先新增 provider.tf 來放供應商的來源以及版本，以及 main.tf 來放 gce 相關設定，最後還要多一個 backend.tf 來放我們要儲存 tfstate 的位置設定，如下：(同步到 GitHub 需要程式碼的可以前往查看)\n(這次範例會使用 gce，此項會需要 gitlab 先啟用 Infrastructure 功能以及建立自己的 gitlab token)\nprovider.tfterraform { required_providers { google = { source = \"hashicorp/google\" version = \"~\u003e 4.48.0\" } } } main.tf provider \"google\" { project = \"XXXXX\" zone = \"asia-east1-b\" } resource \"google_compute_instance\" \"instance\" { name = \"test\" machine_type = \"e2-small\" zone = \"asia-east1-b\" labels = { env = \"11\" } boot_disk { initialize_params { image = \"debian-cloud/debian-10\" } } network_interface { network = \"projects/XXXX/global/networks/test\" subnetwork = \"projects/XXXX/regions/asia-east1/subnetworks/testtest\" } } backend.tf « 新的 專案 ID 要寫我們想要放 terraform state 的 GitLab Project ID，服務名稱是指顯示在 GitLab terraform state 的名稱\ngitlab 個人 token 是指個人存取權杖，大家再依照自己的來做設定\nbackend.tfterraform { backend \"http\" { address = \"[Gitlab 網址]/api/v4/projects/[專案ID]/terraform/state/[服務名稱]\" lock_address = \"[Gitlab 網址]/api/v4/projects/[專案ID]/terraform/state/[服務名稱]/lock\" unlock_address = \"[Gitlab 網址]/api/v4/projects/[專案ID]/terraform/state/[服務名稱]/lock\" username = \"[Gitlab 帳號]\" password = \"[Gitlab 個人 token]\" lock_method = \"POST\" unlock_method = \"DELETE\" retry_wait_min = 5 } } 當我們新增好後，就跟之前步驟一樣，先 init \u003e plan \u003e apply 來做測試，在 init 時會發現，與之前不太一樣的是，在 Initializing the backend 的下方有多了綠色的成功設定後端字樣，代表他也會將後端的相關資訊存進 .terraform 資料夾中，所以有變更後端儲存位置，要記得重新 init 歐\ninit 初始化後，會將後端資訊也存到 .terraform 資料夾\n當我們 plan \u003e apply 完成後，可以觀察一下，發現原本會產生的 terraform.tfstate 檔案沒有出現在該目錄下：\napply 完，沒有在本地產生 .tfstate 檔案\n這時候，我們可以到剛剛在上面設定的專案 ID 內的有一個 Infrastructure / Terraform，裡面就會存放 Terraform state 檔案，如下：\ngitlab/Infrastructure/Terraform\n會顯示狀態名稱、更新資訊、以及 Actions 等欄位：\ngitlab terraform tfstate 網頁\n功能部分可以看後面的 Actions 欄位底下有三個點點，可以下載對應的 tfstate 檔案、Lock 讓其他人不能對此進行 apply，或是刪除此 tfstate 檔案等\ngitlab terraform tfstate 功能說明\n這樣我們就可以透過同一份的 tfstate 檔案來做管理，但有個前提是，之後對該資源的變更都只能使用 tf，如果還有用 WEB UI 去調整，就會遇到線上服務與 tfstate 儲存狀態不同的問題。\nLock 那當我們已經有了共同儲存的地方，也溝通好，不會使用 WEB UI 去調整，但如果有兩個人同時去下 apply 的話，第一個人的 apply 還在執行，後面那個人的 apply 是不是就會蓋掉前一個人的設定呢？\n所以 Terraform 在 0.14 版本推出了 Lock 功能，當有人在 plan or apply 的時候，我們去查看 gitlab Terraform state，會看到我們的 tfstate 檔案會被 Lock 起來\nGitLab Lock 鎖住\n此時除了第一個操作者，其他人再去 plan or apply 就會出現錯誤，可以看到是誰正在使用，以及操作的動作是 plan or apply\n在 Lock 下，其他人沒辦法去 plan or apply\n在 CI 時，需要在 plan 時就將它給 lock，避免第一個人 plan 完，沒有及時的去執行 apply，後來有其他人比第一個人先調整了資源，第一個人再來執行 apply，就會導致第一個人 apply 的內容與自己原先看 plan 的內容會不同，也有可能會將上一個人調整的設定給覆蓋，當第一個操作者結束動作後，該 Lock 才會被解鎖。\n其他 gitlab terraform state 詳細內容可以參考：https://docs.gitlab.com/ee/user/infrastructure/iac/terraform_state.html"},"title":"Terraform 如何多人共同開發 (將 tfstate 存在後端)"},"/blog/terraform/terraform/":{"data":{"":"跟上一篇 Snyk 一樣，本系列也是去參加 DevOpsDay Taipei 2022 活動聽到各位產業大佬目前在使用的名詞以及技術，想說回家也充實一下自己，了解一下在 DevOpsDay 最常出現的 IaC 是什麼？以及聽說很方便的 Terraform 又是什麼，將學習的過程打成此篇筆記，歡迎大家多交流，那我們就開始囉。\n目前打算寫本篇介紹 IaC 以及 Terraform 以外，之後還想寫另外兩篇說明 Terraform 如何共同維護開發(將 .tfstate 檔案存在 gitlab or gcs 上)、怎麼把 Terraform 轉成 module，最後導入 Terragrunt 達到 DRY 等等，也會有用 Terraform 建立 GCE 以及 GKE。大家可以持續關注此篇文章，最後會在文章後附上連結 😍","terraform-又是什麼#Terraform 又是什麼？":"IaC 的工具有很多種，接著我們就來介紹其中一個工具 - Terraform，Terraform 是什麼呢？根據官網的說明可以知道，Terraform 是 HashiCorp 所開發的基礎設施即代碼工具。它可以使用人類方便讀的配置文件來定義資源和基礎設施，以下有使用 Terraform 幾個優點：\nTerraform 可以管理多個雲平台上的基礎架構 使用人類可讀的配置語言來幫助我們快速編寫基礎設施代碼 可以將配置提交給版本控制，安全地在基礎架構上進行協作 管理任何基礎設施 Terraform 提供插件讓 Terraform 可以通過其 API 與雲平台和其他服務進行交互。HashiCorp 和 Terraform 社區編寫了 3193 多個提供商來管理像 AWS、Azure、GCP、Kubernetes、Helm、GitHub 等資源，可以到 Terraform Registry 查看更多平台或服務的提供者，當然如果沒有找到想要的提供者，也可以自己編寫自己的套件。\n標準化部署工作流程 提供商會將基礎設施的每個單元 (例如建立 VM 或是 VPC) 定義為資源。你可以將來自不同提供者的資源組合，變成模組，讓我們可以用一致的語言和工作流程去管理他們。\nTerraform 什麼是 Terraform 的基礎設施即代碼？","什麼是-iac-#什麼是 IaC ?":"IaC 全名是 Infrastructure as Code (基礎設施即代碼)，從字面意思就可以略知一二，也就是把基礎設施變成程式碼，在還沒有這些 IaC 工具之前，大家都是開啟 WEB UI 畫面來進行建置或設定，雖然使用 UI 點一點就建好了，但這些步驟都沒有被紀錄下來 (git)，也沒有辦法透過其他人一起 Review 的方式來避免人為操作錯誤。因此有了 IaC 這些工具，可以將實際的操作流程，轉換成程式碼或是其他像是 JSON、Yaml 的方式給紀錄下來，以下是導入 IaC 帶來的好處：\n建置 CI/CD 自動化 (不需要再仰賴 UI 進行操作) 版本控制 (大家可以透過 MR 規定 code review，避免出現人為錯誤) 重複使用 (可以將常用的變成參數代入，減少建置時間) 環境一致性 (以上 IaC 說明來自 小惡魔 - 初探 Infrastructure as Code 工具 Terraform vs Pulumi 文章，寫得真的很好，推推)\nInfrastructure as Code 初心企服行研07：认识「基础设施即代码」(Infrastructure as Code) — 初心内参","參考資料#參考資料":"初探 Infrastructure as Code 工具 Terraform vs Pulumi：https://blog.wu-boy.com/2021/02/introduction-to-infrastructure-as-code-terraform-vs-pulumi/\n今晚我想認識 Terraform：https://ithelp.ithome.com.tw/articles/10233759","安裝-terraform#安裝 Terraform":"安裝 Terraform 的方式有很多種，我就以我在使用的 Mac OS 為例，其他可以參考 Install Terraform：\n安裝步驟 先安裝 HashiCorp tap，這是 HashiCorp 在 Homebrew 的儲存庫： brew tap hashicorp/tap 使用 hashicorp/tap/terraform brew install hashicorp/tap/terraform 如何驗證是否安裝成功 打開一個新的 Terminal，使用 terraform -help 檢查是否有安裝成功，也可以在 -help 後面加入參數來查看該參數的功能與更多訊息\n驗證 Terraform 安裝成功 Install Terraform\n自動補全指令 可以啟動終端機上的 Tab 自動補全功能，執行以下指令，再重開終端機，就會出現了：\nterraform -install-autocomplete 想要解除自動補全 (雖然應該不會拉)，執行以下指令：\nterraform -uninstall-autocomplete 放上成果圖片\n快速入門 當我們安裝好，想要最快的了解 Terraform ，當然是自己動手做一次，我們依照官網的教學，可以在一分鐘內使用 Docker 配置好 NGINX 伺服器，那我們開始囉！\n首先，我們必須要先安裝好 Docker，下載 Mac 版 Docker 桌面 建立一個資料夾，並進入該資料夾內 將以下 Terraform 配置文件貼到檔案中，並取名 main.tf：(同步到 GitHub 需要程式碼的可以前往查看) provider \"docker\" {} resource \"docker_image\" \"nginx\" { name = \"nginx:1.23\" keep_locally = false } resource \"docker_container\" \"nginx\" { image = docker_image.nginx.name name = \"nginx\" ports { internal = 80 external = 8000 } } 再開一個檔案取名為 provider.tf，將以下配置文件貼到檔案中： terraform { required_providers { docker = { source = \"kreuzwerker/docker\" version = \"~\u003e 2.13.0\" } } } (以上程式碼來自官網 安裝 Terraform#快速入門 加上小修改)\n先來簡單說明一下 Terraform 程式碼格式，Terraform 的檔案副檔名是 *.tf，採用名為 HCL (HashiCorp Configuration Language) 的組態語言來描述基礎架構。\n(補充說明：只要是同一個目錄下有 .tf 檔案結尾的，Terraform 都會去執行，所以檔案名稱可以自己取名，但為了方便管理都會使用 main、provider、backend、output 的檔案來放置對應的內容)\nHCL 是一種宣告式的語言，讓你直接寫下期望的基礎架構，而不是寫下過程的每一個步驟。\n檔案介紹 我們先看 provider.tf 檔案，檔案內會先寫好需要的供應商來源以及版本 (版本有點像是對應供應商提供的 api 版本)\nterraform { required_providers { 供應商名稱 = { source = \"供應商來源\" version = \"~\u003e 所使用的版本\" } } } main.tf 檔案內的 provider 區塊會寫供應商的相關設定，假設我們使用 google 就會在裡面先設定好 project id 等。\nresource 區塊會需要寫雲端資源名稱以及自定義的名稱，雲端資源名稱這項是不可以更改的，假設我們要使用 docker 的 container 服務，這邊就需要填寫 docker_container。自定義的名稱可以是你想要為使用這個雲端資源去定義的名稱。\nprovider \"供應商名稱\" {} resource \"雲端資源名稱\" \"自定義的名稱\" { 屬性 = 值 } 所以我們已上面的 Docker 配置好 NGINX 伺服器為例，provider 我們這次使用的是 docker， resource 我們可以拆開來寫，\n像是第一個 resource docker_image 我們幫他取叫 nginx，裡面就是放有關 image 的設定，詳細的 image 設定可以參考 Resource (docker_image)，\n第二個 resource docker_container 一樣叫 nginx，裡面用的 image 是拿前面的 docker_image resource name 來使用，一樣詳細可以參考 Resource (docker_container)。\n小提醒，如果不知道要怎麼寫 provider 供應商設定，可以打開 terraform 官網找到該供應商，點選右邊的 USE PROVIDER\n官方教學\n可以看到官方教學要怎麼使用這個供應商。\n指令說明 接著有幾個指令要帶大家認識：\nterraform init：初始化項目，下載 tf 檔案中所需要的外掛套件 terraform plan：會產生一份執行計劃。上面會寫著它將會做哪些事，你可以驗證是否符合你預期的設計 terraform apply：實際運作，把基礎架構建置完成。在完成之後，會把目前的狀態儲存到一份檔案中 (*.tfstate) terraform destroy：會銷毀用 Terraform 起的服務 terraform fmt：幫你整理好 tf 文件 terraform validate：靜態檢查 tf 文件 附上懶人指令\nalias ti='terraform init' alias ta='terraform apply' alias tp='terraform plan' alias td='terraform destroy' 由於在 apply 的時候會跳出詢問視窗，如果是要寫成腳本，可以把指令改成 terraform apply -auto-approve 就不需要輸入 yes 了！\n實際操作 有上面的指令後，我們來實際操作看看：\n首先到該 main.tf 檔案目錄下，先使用 terraform apply 來測試看看： 無法直接執行 apply\n會發現沒有辦法直接用 terraform apply 指令來建置服務，我們看一下他提示的說明，他說他找不到 lock file，需要先進行初始化才可以執行，所以我們的建置流程是先 init –\u003e apply\nterraform init 我們先執行 terraform init，可以看到他會下載 tf 檔案中所需要的外掛套件 (docker) terraform init\n當我們初始化後，資料夾會多一個檔案 (.terraform.lock.hcl) 以及資料夾 (.terraform)\ninit 前後檔案差異\n.terraform.lock.hcl：是 Terraform 中用於鎖定和管理外部提供者（providers）版本的檔案。它的主要功能是確保在不同的環境中使用相同的外部提供者版本，以避免在團隊合作或不同環境中引入不一致性和問題。\n.terraform/：資料夾主要用於存儲初始化和管理基礎架構相關的臨時文件。\nterraform plan 接著我們使用 terraform plan 來查看我們的計劃，可以看到他會列出我們所寫的 tf 裡面有用到的 resource，除了我們有設定的屬性，其他的屬性也會顯示出來，可以更方便地讓我們知道這個 resource 有哪些屬性可以設定 terraform plan\nterraform apply 最後我們檢查都沒有問題，就可以使用 terraform apply 來建置，apply 其實跟 plan 一樣都會先讓我們看一下計劃，但會跳出詢問是否要執行，除非你輸入 yes，否則就跟 plan 單純顯示計劃內容，最後我們就可以看到他成功在 docker 上面建立 nginx 服務 terraform apply\n查看 docker nginx 以及檢查其服務\n當我們 apply 完，服務也建立後，查看一下資料夾後會發現，又多了一個檔案 terraform.tfstate：\n多了一個檔案 terraform.tfstate\nterraform.tfstate： 是 Terraform 的狀態檔案，它包含了基礎架構的狀態和資源的詳細信息。預設情況下，這個檔案是本地的並且只存在於 Terraform 初始化和操作的目錄中。(但要如何實現共同維護同一個 IaC 呢，敬待後續分享 🤣)\nterraform destory 另外，當你想要移除服務時，可以使用 terraform destroy 來將服務給移除 terraform destroy\nterraform import 最後還有一個蠻重要的，就是我們已經有很多服務都是使用 WEB UI 方式建立的服務，那我們要怎麼把它變成 tf 檔案呢？ 跟我們剛剛說的 terraform.tfstate 檔案有關，他會儲存我們 IaC 的狀態，所以我們才可以透過他知道現在是對資源做新增、修改、刪除哪個操作\n那當這個檔案不見時，如果再重新下 terraform apply，他會認為你是新增狀態，但實際上 docker 服務還是啟動的狀態，所以就會錯誤，會跟你說他已經存在。\n測試沒有 terraform.tfstate 直接下 terraform apply\n所以這時候我們要把線上服務的資源轉成 tf ，第一步要先把 resource 的框架給寫出來，其他可以先留空白，如下 main.tf：\nprovider \"docker\" {} resource \"docker_image\" \"nginx\" { name = \"nginx:1.23\" keep_locally = false } resource \"docker_container\" \"nginx\" { } 接著使用 terraform import 來將線上服務的資源套用到我們 main.tf 裡面的 resource，所以會長得像：\nterraform import docker_container.nginx 7f363ea3f6a64b5432ae3627f490b3e297abf80f196bce9c028ec2eb82706f12 import 後面會加上 main.tf resource 名稱 docker_container，以及我們取名的 nginx，最後帶 container id (docker ps 查詢)，就可以匯出 terraform.tfstate 檔案囉，詳細的 import 可以參考每個供應商資源的網頁(這邊以 docker 為例)\nterraform import 匯出 terraform.tfstate\nterraform show 當我們匯出後，可以看一下 terraform.tfstate，它會是一個 json 格式，如果要轉成 tf 格式，還需要使用 terraform show 來將 terraform.tfstate 檔案轉成 tf 格式，如下：\nterraform show 將 terraform.tfstate 轉成 tf\n如果透過上述的方式來轉成 tf，會發現在重新 apply 時，會出現錯誤，這邊以 gce 的當範例，轉完的 tf 設定，有些是 tf 不支援的參數，只會顯示在 tfstate，所以還需要手動刪除。\n轉換後還需將不用的設定給移除"},"title":"什麼是 IaC ? Terraform 又是什麼？"},"/blog/terraform/terragrunt/":{"data":{"":"我們接續上一篇的 如何將 Terraform 改寫成 module ? ，我們已經將 Terraform 改成 module 的方式來進行管理，但當我們要管理的資源越來越多，且有分不同的專案時，整個服務架構會長的像以下：\nmodules google_compute_instance main.tf outputs.tf variables.tf projects gcp-1234 aaa backend.tf main.tf provider.tf bbb backend.tf main.tf provider.tf ccc backend.tf main.tf provider.tf gcp-2345 aaa backend.tf main.tf provider.tf bbb backend.tf main.tf provider.tf ccc backend.tf main.tf provider.tf gcp-3456 aaa backend.tf main.tf provider.tf bbb backend.tf main.tf provider.tf ccc backend.tf main.tf provider.tf 這邊的範例是以不同專案來分，再分區不同的服務，每個服務裡面都會有 backend.tf、main.tf、provider.tf 檔案所組成，我們可以來比較一下 gcp-1234 的 aaa 服務以及 gcp-2345 的 aaa 服務差異：\n檔案差異\n可以看到在 backend.tf 除了 prefix 路徑以外，其他設定也都一樣，但因為 Terraform 本身沒辦法透過帶入參數的方式來設定 backend.tf 後端部分，所以必須要先寫好每個服務所存放的後端位置，十分的不方便。\nbackend.tfterraform { backend \"gcs\" { bucket = \"terragrunt-tfstate\" prefix = \"/gcp-1234-aaa\" } } 為了減少上述這些需要一直重複寫差不多檔案的工作內容，因此有了 Terragrunt 這個工具，Terragrunt 是 Terraform 的包裝器，可以彌補 Terraform 上的一些缺陷，並且讓我們的 IaC 更貼近 DRY 原則。\n這邊說明一下 DRY 原則\nDRY 全名是 Don’t repeat yourself，也就是不要做重複的事情，能夠一次做完的就不要重複的去做。","terragrunt-好處#Terragrunt 好處":"\n接著介紹一下 Terragrunt 的好處：\n方便管理後端狀態設定\n將後端存儲桶納入管理\n使用 generate 自動生成檔案\n使用 include 檔案來達到 DRY 原則\n管理 Module 之間的依賴性\n產生依賴關聯圖\n方便管理後端狀態設定 首先第一個方便管理後端狀態設定，也就是我們上面提到的 backend.tf 設定。在 Terraform 原生為了區別不同專案不同服務的狀態檔，就必須先寫好每個儲存的路徑，但使用 Terragrunt，可以先在該目錄下，也就是 gcp-3456 目錄下先寫一個設定檔案 (我們以 gcp-3456 專案為例)，讓底下的 aaa、bbb、ccc 服務可以去 include 它，我們就不需要每個服務都寫幾乎差不多的設定檔，接著我們在 gcp-3456 資料夾下新增 terragrunt.hcl 檔案來說明：\nterragrunt.hclremote_state { backend = \"gcs\" generate = { path = \"backend.tf\" if_exists = \"overwrite\" } config = { bucket = \"terragrunt-tfstate\" prefix = \"${path_relative_to_include()}\" } } 這邊的設定其實跟之前的 backend.tf 差不多，只是後端現在儲存的 block 改叫做 remote_state，可以看到 backend 設定我們一樣是存在 gcs 上，generate 這個 block 它會判斷 backend.tf 檔案是否存在，如果沒有它就會幫我們建立，其中設定檔案內容是將狀態檔案存在 terragrunt-tfstate 這個 bucket，並透過${path_relative_to_include()} 這個變數來自動帶入有 include 這份檔案的路徑，並在相對路徑產生 backend.tf 檔案。\n有點抽象，所以我畫一個比較簡單的架構圖來做說明一下，假設現在有三個服務，如下：\naaa terragrunt.hcl bbb terragrunt.hcl ccc terragrunt.hcl terragrunt.hcl 我們剛剛的後端設定是寫在此根目錄的 terragrunt.hcl 檔案(第 8 行)，然後 ccc 這個服務去 include 根目錄的 terragrunt.hcl 檔案， Terragrunt 就會自動幫你產生以下的 backend.tf 檔案：\nbackend.tfterraform { backend \"gcs\" { bucket = \"terragrunt-tfstate\" prefix = \"/ccc\" } } 這樣就可以省下我們要重複寫 backend.tf 的時間，在維護上也會更加的方便。\n將後端存儲桶納入管理 接著，大家有沒有想過，我們都已經使用 Terraform 來管理 IaC ，並把狀態檔案放到 gcs 上面來保存，但一開始還沒有用 Terraform 管理 gcs 的資源，我們還需要在設定 backend.tf 前，先手動去新增一個 gcs ，才能來存放 tfstate 狀態檔案呢？\n因此在 Terragrunt remote_state 的 config 時，可以多設定 gcs 的 project id 以及 location，Terragrunt 在初始化後端時，會檢查是否有該 gcs bucket，如果沒有就會自動建立，設定檔如下：\nterragrunt.hclremote_state { backend = \"gcs\" generate = { path = \"backend.tf\" if_exists = \"overwrite\" } config = { project = \"gcp-xxxxxx\" location = \"asia\" bucket = \"terragrunt-tfstate\" prefix = \"${path_relative_to_include()}\" } } 使用 generate 自動生成檔案 在剛剛我們可以使用 generate 來自動 backend.tf 檔案，那代表我們也可以把每個 provider.tf 的內容，也透過 generate 來生成，如下：\nterragrunt.hclgenerate \"provider\" { path = \"provider.tf\" if_exists = \"overwrite\" contents = \u003c\u003cEOF terraform { required_providers { google = { source = \"hashicorp/google\" version = \"~\u003e 4.48.0\" } } } EOF } 這樣子每個 include 這份設定檔的服務除了 backend.tf 檔案以外，還會自動產生 provider.tf 檔案。\n使用 include 檔案來達到 DRY 原則 我們搞定了 backend.tf 跟 provider.tf 後，還剩下 main.tf，所以我們也將它改成 Terragrunt 的格式如下：\nterragrunt.hclterraform { source = \"${get_path_to_repo_root()}/modules/google_compute_instance\" } include \"root\" { path = find_in_parent_folders() } inputs = { instance_name = \"gcp-3456-ccc\" machine_type = \"e2-small\" instance_zone = \"asia-east1-b\" instance_tags = [] instance_labels = {} boot_disk_auto_delete = true boot_disk_image_name = \"debian-cloud/debian-10\" ... 設定部分省略 ... } 這邊可以看到 terraform source 它就是我們使用 module 的路徑，也可以用 ${get_path_to_repo_root()} 這個變數他會自動抓該專案的根目錄，我們就不需要去特別設定。\n此外也可以將 module 獨立成一個專案，或是使用其他人寫好的 module，在 source 的時候可以使用 git::https://[gitlab-網址]/sre/terraform/module.git//google_compute_address 的方式來取得 module，可以設定要使用哪個分支或是 tag，只需要在網址後面加上，?ref=[分之 or tag 名稱] 即可，這樣可以讓開發中的 module 不會影響到線上其他正在使用中的 module 設定。\n(module.git 後面的 // 是 Terraform module source 的規則，如果不加會跳警告訊息)\n接著我們可以看到 include \"root\" {} 這段，裡面有使用 find_in_parent_folders 這邊變數，他就是上面的提到會自動去抓放在父資料夾的 remote_state 跟 generate terragrunt.hcl 檔案。\n後面的 input 就跟使用 module 時一樣，將 module 的參數帶入即可。\n補充：所以我們也可以把一些通用的設定寫在根目錄的 terragrunt.hcl 檔案，例如專案的 id，可以寫以下內容來讓 include 它的檔案吃到同一個參數設定：\nterragrunt.hclinputs = { project_id = \"gcp-3456\" } 管理 Module 之間的依賴性 由於 Terragrunt 是把每個服務拆分成最小化，沒辦法把使用不同 module 的資源放在一起(單純使用 module 的話，可以一次 source 多了 module，並把他放在同一個 tf 檔案中)，那像是我們建立 k8s 會使用到 google_container_cluster、google_container_node_pool 兩種不同的 module 該怎麼辦呢？\n首先我們先在 modules 資料夾放上 google_container_cluster、google_container_node_pool 兩個 module 的設定檔案，詳細程式可以點我前往\nmodules google_container_cluster main.tf outputs.tf variables.tf google_container_node_pool main.tf variables.tf 在 projects 底下新增 gke 資料夾，新增 terragrunt.hcl 來放 remote_state provider 的設定，並區分兩個資料夾，分別是 cluster 資料夾來存放 cluster 資訊，以及 test 資料夾來存放 test node-pool 資訊：\nprojects gke cluster terragrunt.hcl terragrunt.hcl test terragrunt.hcl cluster 的 terragrunt.hcl 檔案如下：\nterragrunt.hclterraform { source = \"${get_path_to_repo_root()}/modules/google_container_cluster\" } include { path = find_in_parent_folders() } inputs = { cluster_name = \"tf-test\" cluster_location = \"asia-east1-b\" node_locations = [] cluster_version = \"1.24.12-gke.500\" network_name = \"projects/gcp-202011216-001/global/networks/bbin-testdev\" subnetwork_name = \"projects/gcp-202011216-001/regions/asia-east1/subnetworks/bbin-testdev-dev-platform\" node_max_pods = 64 remove_default_node_pool = true initial_node_count = 1 enable_shielded_nodes = false resource_labels = { \"dept\" : \"pid\", \"env\" : \"dev\", \"product\" : \"bbin\" } dns_enabled = false cluster_dns = \"PROVIDER_UNSPECIFIED\" cluster_dns_scope = \"DNS_SCOPE_UNSPECIFIED\" private_cluster_ipv4_cidr = \"172.16.0.176/28\" binary_authorization_enabled = true binary_authorization = \"DISABLED\" } test node-pool 檔案如下：\nterragrunt.hclterraform { source = \"${get_path_to_repo_root()}/modules/google_container_node_pool\" } include { path = find_in_parent_folders() } dependency \"cluster\" { config_path = \"../cluster\" } inputs = { cluster_name = dependency.cluster.outputs.cluster_name cluster_location = dependency.cluster.outputs.cluster_location cluster_version = dependency.cluster.outputs.cluster_version node_pool_name = \"test\" node_count = 1 node_machine_type = \"e2-small\" node_disk_size = 100 node_disk_type = \"pd-standard\" node_image_type = \"COS_CONTAINERD\" node_oauth_scopes = [ \"https://www.googleapis.com/auth/devstorage.read_only\", \"https://www.googleapis.com/auth/logging.write\", \"https://www.googleapis.com/auth/monitoring\", \"https://www.googleapis.com/auth/service.management.readonly\", \"https://www.googleapis.com/auth/servicecontrol\", \"https://www.googleapis.com/auth/trace.append\" ] node_tags = [] node_taint_enabled = false node_taint_key = \"\" node_taint_value = \"\" node_taint_effect = \"\" auto_repair = true auto_upgrade = true upgrade_max_surge = 1 upgrade_max_unavailable = 0 upgrade_strategy = \"SURGE\" autoscaling_enabled = true autoscaling_max_node_count = 2 autoscaling_min_node_count = 1 autoscaling_total_max_node_count = 0 autoscaling_total_min_node_count = 0 } 上面兩個檔案分別是 cluster 的設定，以及 test node-pool 的設定，裡面的設定，上面基本都有提過，這邊要提的是 dependency，dependency 他是 Terragrunt 提供讓我們可以方便地去管理 IaC 之間的相依性，像是我們這邊，需要先建立好 cluster 才能建立 node_pool，此時就可以依靠 dependency block 來完成需求。\n( 靠 dependency 來取得 cluster 的資訊，並帶入 node_pool 中 )\n此時的執行指令是在 gke 目錄下，使用 terragrunt run-all [參數] 來跑整個相依的 module，我們這邊就建立一個名為 tf-test 的 cluster，並且有一個名為 test 的 node_pool ，其他設定請參考上面程式：\n測試 terragrunt run-all\n(黃色的 WARN 是因為 gcs 上還沒有存過該狀態檔案，所以會跳出提示)\n等到 cluster 建立完成後，會將 cluster 的資訊帶入 node_pool，才開始建立 node_pool 的資源：\n測試 terragrunt run-all\n產生依賴關聯圖 當我們服務使用到很多依賴關係，想要釐清是誰依賴誰，如果單純看程式會比較麻煩，在 Terragrunt 還有一個好用的指令，可以使用以下指令，產生對應的依賴關係圖，在檢視時可以更清楚知道關係：\nterragrunt graph-dependencies | dot -Tpng \u003e graph.png ( 這個 dot 指令是另外的套件，會將關係圖程式碼轉成圖檔，請先安裝 brew install graphviz )\n關聯圖","terragrunt-安裝方式#Terragrunt 安裝方式":"那要怎麼使用 Terragrunt 呢！？\n第一步當然是安裝它囉，我們系統是 macOS，所以我們安裝方式是 Homebrew 來進行安裝：\nbrew install terragrunt 接著我們的指令會從 terraform XXX 變成以下：\nterragrunt plan terragrunt apply terragrunt output terragrunt destroy Terragrunt 會將所有命令、參數和選項直接轉發到 Terraform。(所以我們也需要下載 Terraform)\nTerragrunt 的預設檔案名稱是 terragrunt.hcl ，Terragrunt 的設定檔案基本上與 Module 差不多，只是有更多更方便的變數可以使用。","參考資料#參考資料":"事半功倍 — 使用 Terragrunt 搭配 Terraform 管理基礎設置：https://medium.com/act-as-a-software-engineer/%E4%BA%8B%E5%8D%8A%E5%8A%9F%E5%80%8D-%E4%BD%BF%E7%94%A8-terragrunt-%E6%90%AD%E9%85%8D-terraform-%E7%AE%A1%E7%90%86%E5%9F%BA%E7%A4%8E%E8%A8%AD%E6%96%BD-f70c30166639"},"title":"如何導入 Terragrunt，Terragrunt 好處是什麼？"},"/projects/":{"data":{"":" 活動申請系統 (Web)幫學校開發的大型校務系統，方便系統社團使用電子填單方式申請活動，也讓之後的學弟妹，可以更快速的查看到歷屆辦理的活動。 巔峰極速 兌換虛寶網站 (Web)該遊戲有大量虛寶可以兌換，但官方提供的網頁，需要重複輸入 ID 以及驗證碼還有序號，因此寫了一個小網頁，可以只輸入一次 ID 以及驗證碼，就兌換完所有序號。 京緯工程有限公司官網 (Web)協助京緯工程有限公司建立公司官方網頁。 "},"title":"專案成就"}}