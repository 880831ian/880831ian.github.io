{"/about/":{"data":{"":" 哈囉大家好，我叫莊品毅，也可以叫我 Ian，目前是一位 Site Reliability Engineering (SRE) 工程師，主要負責維護公司的 Google Cloud Platform (GCP) 雲端相關服務，包含 GKE、GCE、GCS、GLB 等等，除此之外也熟悉使用 Terraform + Terragrunt 來管理雲端大量的 IaC 資源，配合 Prometheus、Datadog、EFK 等監控工具來確保服務的穩定性，協助 RD 建立 CICD 部署流程。\n在下班空閒時間，我喜歡閱讀技術相關的文件、部落格，也會參加一些線下技術社群的活動，例如 DevOpsDay，希望能夠透過這些活動來學習更多的知識，歡迎大家使用下方 giscus 留言系統留言交流。","工作經驗#工作經驗":" 凡谷興業有限公司 - SRE 工程師 (2022/02 - 現在)"},"title":"關於我"},"/blog/":{"data":{"":"","介紹#介紹":"👋 你好、妳好、大家好，歡迎來到我的秘密花園，這邊主要會記錄我研究一個新的技術或工具、以及處理一些 SRE 遇到的靈異事件問題的小天地。\n會開始寫 Blog 的初衷主要是我的小腦袋瓜，如果不寫下來，過陣子很容易就忘記 (´_ゝ`)，當然也希望可以幫助到有相同問題的人 (可以使用搜尋功能來快速找到相關文檔喔)，如果有任何問題或建議，歡迎在下方留言。","聲明#聲明":"由於在學習新的技術或工具時，會參考網路上許多的文件和照片，雖然會加上自己的見解與實作內容改寫而成，且會於文章後附上相關資料來源，如有侵犯到您的權益，請於下方告知，我會立即刪除相關內容，謝謝 (๑•́ ₃ •̀๑)。"},"title":"Blog"},"/blog/gcp/":{"data":{"":"此分類包含 Google Cloud Platform 相關的文章。\n如何過濾 GCP LOG，減少 Cloud Logging API 的花費 "},"title":"Google Cloud Platform"},"/blog/gcp/gcp-log-reduce-cloud-logging-api/":{"data":{"":"當我們使用 GCP 的 Cloud Logging 服務來查看 Log 時，有時候會有一些我們不需要顯示出來的，或是從來都不會去查詢的 Log，再者是 GCP 本身的錯誤導致大量噴錯的 Log ，這些 Log 都會導致 Cloud Logging 的費用增加。","介紹-cloud-logging#介紹 Cloud Logging":"先來簡單說一下 Cloud Logging 這項服務的基本架構，請看圖：\nCloud Logging 基本架構\n可以看到 Logs Data 會透過 API 再經過 _Default log sink (router) 存到相應命名的 log bucket (預設配置)，圖中 _Required 以及 _Default 的 log sink 都是 GCP 自動創建的接收器，下面簡述一下它們的區別：\n_Required 日誌儲存桶 Cloud Logging 會將以下類型的 Log 存到 _Required 儲存桶\n管理員活動審核 Log\n系統事件審核 Log\nAccess Transparency Log\nCloud Logging 會將 _Required 儲存桶 Log 保留 400 天，無法調整該期限，且無法修改或刪除 _Required 儲存桶，也沒辦法停用 _Required log sink 接受器路由到 _Required 儲存桶的設定。\n_Default 日誌儲存桶 只要不是存在 _Required 日誌儲存桶的 Log 就會透過 _Default log sink 接受器路由到 _Default 儲存桶。\n除非有另外配置自定義設定， 否則 _Default 日誌儲存桶 Log 只會保留 30 天，也一樣無法刪除 _Default 日誌儲存桶。此外 Cloud Logging 的費用是以存在 _Default 日誌儲存桶來計算。\n功能 價格 每月免費額度 Logging 提取 提取的 Log $0.5/GiB 每個項目前 50 GiB Logging 儲存 保留超過 30 天的 Log，每月每 GiB $0.01 在默認保留期限的 Log 不會有額外儲存費用 查看該專案使用的 Cloud Logging API 費用：https://console.cloud.google.com/apis/api/logging.googleapis.com/cost","參考資料#參考資料":"Routing and storage overview https://cloud.google.com/logging/docs/routing/overview","過濾-log#過濾 Log":"那現在知道 Cloud Logging 的架構，那當我們遇到需要過濾 Log 時，我們可以使用以下步驟來過濾以節省 Cloud Logging API 費用：\n範例說明 這次範例是 google 在 2023/07/06 所發佈的 Service Health，當 GKE 版本大於 1.24 以上，就會噴\nFailed to get record: decoder: failed to decode payload: EOF\ncannot parse ‘0609 05:31:59.491654’ after %L\ninvalid time format %m%d %H:%M:%S.%L%z for ‘0609 05:31:59.490647’\n這三種類型的錯誤 Log，在等待官方修復前，官方的建議是先將他給過濾掉，避免一直刷噴錢 ┐(´д`)┌\n附上當時的 Service Health 連結：https://status.cloud.google.com/incidents/y5XvpyBXFhsphSt4DfiE\n我們在上面架構圖有說到，Log 會透過 log sink 路由到 bucket，所以我們要將過濾條件加在 log router 上：\n選擇 Log Router 選擇 Log Router\n選擇 Log Router Sinks 選擇 _Default 的 Log Router Sinks，點選右邊按鈕的 Edit Sink\n選擇 _Default 的 Log Router Sinks\n設定 Sink details 第一個是 details，可以輸入說明，這邊輸入：google 有 bug 會噴大量的意外 LOG，怕費用飆高，先用官方建議的來過濾 LOG，詳細可以看： https://status.cloud.google.com/incidents/y5XvpyBXFhsphSt4DfiE\n輸入 Sink details 說明\n選擇 Sink Service 跟 Bucket 接著 sink 服務選擇 Logging bucket，以及對應儲存的 log bucket (這邊基本上都是預設)\n選擇 Logging bucket\n設定 include Log 選擇那些可以被 include 到 sink 接收器的 LOG 格式 (這邊基本上都是預設)\n預設的 LOG 格式\n設定 filter Log 這邊就是我們要輸入過濾的地方，先點擊 ADD EXCLUSION，輸入過濾的名稱，以及過濾的內容格式，我們輸入 google 在 Service Health 所提供的格式，最後按下 UPDATE SINK\n新增要過濾的 LOG 格式\n設定完成 等待更新完成，就可以看到我們已經在接收器上設定好過濾條件囉～\n查看詳細接收器設定\n檢查 Log 是否過濾成功 最後再檢查一下 Log 是不是沒有收到該錯誤的 Log 內容\n檢查 LOG 是否不會再出現"},"title":"如何過濾 GCP LOG，減少 Cloud Logging API 的花費"},"/blog/kubernetes/":{"data":{"":"此分類包含 Kubernetes 相關的文章。\n在正式環境上踩到 StatefulSet 的雷，拿到 P1 的教訓 部署 Pod 遇到 container veth name provided (eth0) already exists 錯誤 "},"title":"Kubernetes"},"/blog/kubernetes/k8s-statefulset-podmanagementpolicy/":{"data":{"":"此文章要來記錄一下前陣子在公司的正式環境踩到 StatefulSet 的雷，事情是這樣的，我們有些服務，是使用 StatefulSet 來建置，至於為什麼不用 Deployment，這個說來話長 (也不是因為需要特定的 Pod 名稱、或是網路標記等等)，我們這邊先不討論，這個 StatefulSet 服務是 Nginx + PHP-FPM，為了避免流量進入到 processes 已經被用光的 Pod 中，我們在 StatefulSet 的 PHP Container 上有設定 readiness，readiness 的設定長得像以下：\nreadinessProbe: exec: command: - \"/bin/bash\" - \"-c\" - | CHECK_INFO=$(curl -s -w 'http code:\\t%{http_code}\\n' 127.0.0.1/status) HTTP_CODE=$(echo -e \"${CHECK_INFO}\" | awk '/http code:/ {print $3}') IDLE_PROCESSES=$(echo -e \"${CHECK_INFO}\" | awk '/idle processes:/ {print $3}') [[ $HTTP_CODE -eq 200 \u0026\u0026 $IDLE_PROCESSES -ge 10 ]] || exit 1 我們會用 curl 來打 /status，檢查回傳的 http code 是否為 200，以及 idle processes 是否大於等於 10，如果不符合，就會回傳 1，讓他被標記不健康，讓 Kubernetes 停止流量到不健康的容器，以確保流量被路由到其他健康的副本。","參考資料#參考資料":"Kubernetes — 健康檢查：https://medium.com/learn-or-die/kubernetes-%E5%81%A5%E5%BA%B7%E6%AA%A2%E6%9F%A5-59ee2a798115\nPod Management Policies：https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#pod-management-policies","問題#問題":"當天遇到的情況是，我們上程式後，Pod 都一切正常，當流量開始進來後，發現 10 個 Pod 會開始偶發的噴 Readiness probe failed，查看監控發現 processes 越來越低，最後被反應服務有問題，我們查看 Hpa 的紀錄的確有觸發到 40 個 Pod，只是查看 Pod 數還是依樣卡在 10 個，當下我們有嘗試使用調整 yaml 在 apply，發現 StatefulSet 的 yaml 也已經更新了，但 Pod 還是一樣卡在 10 個，也有使用 kubectl 下 kubectl scale sts [服務名稱] --replicas=0，想要切換 Pod 數也沒有辦法。\n當下我們有先 Call Google 的 Support 一起找原因，Google 是建議我們 readiness 的條件不要設的太嚴格，可以加上 timeoutSeconds: 秒數，但對於 Pod 卡住，還是沒有找到原因，後來我們查了一下 StatefulSet 的文件發現，StatefulSet 有一個設定 podManagementPolicy，預設是 OrderedReady，他必須等待前面的 Pod 是 Ready 狀態，才會再繼續建立新的，也就是說我們的 StatefulSet 已經卡住，導致就算 Hpa 觸發要長到 40 個 Pod 也沒有用。","測試結果#測試結果":"最後我們就使用兩種模式來測試看看，已下是測試結果(透過 P1 才知道的設定ＱＱ)：\n有將測試的 StatefulSet 放在 Github，可以點我查看 (可以調整 readinessProbe 的 httpGet.Path 故意把他用壞)\n使用 OrderedReady 模式 StatefulSet 在 podManagementPolicy 預設 OrderedReady 的模式，故意讓 readiness 卡住時 (Pod 卡住時)：\n當下的 StatefulSet 設定： StatefulSet 設定\nPod 狀態： Pod 狀態\n使用指令調整 Pod 數量 我們這時候下指令調整 Pod 數量，看看會發生什麼事：\nkubectl scale sts my-statefulset --replicas=5 我們先看 StatefulSet 的 yaml 可以看到 Pod replicas 已經改變，也可以看 generation 有更新，代表 StatefulSet 本身有接收到調整設定的請求。\n下指令調整後的 StatefulSet 設定\n看了一下 Pod 數量，也是一樣卡住，且 Pod 數量也沒有變化。\n下指令調整後的 Pod 狀態\n使用 yaml 調整 Pod 數量 我們直接調整 StatefulSet yaml 的 Pod 數量，看看會發生什麼事：\n一樣我們先看 StatefulSet 的 yaml 可以看到 Pod replicas 已經改變(這裡應該切別的 Pod 數量，切回 3 個好像沒有意義 xD)，也可以看 generation 有更新。\n使用 yaml 調整後的 StatefulSet 設定\n看了一下 Pod 數量，也是一樣卡住，且 Pod 數量也沒有變化。\n使用 yaml 調整後的 Pod 狀態\n所以代表在 OrderedReady 的模式下，Pod 卡住時，無法對 Pod 進行任何操作，必須要手動刪除卡住的 Pod 才吃得到最新的設定。\n使用 Parallel 模式 StatefulSet 在 podManagementPolicy Parallel 的模式，故意讓 readiness 卡住時 (Pod 卡住時)：\n當下的 StatefulSet 設定： StatefulSet 設定\nPod 狀態： Pod 狀態\n使用指令調整 Pod 數量 我們這時候下指令調整 Pod 數量，看看會發生什麼事：\nkubectl scale sts my-statefulset --replicas=5 我們先看 StatefulSet 的 yaml 可以看到 Pod replicas 已經改變，也可以看 generation 有更新，代表 StatefulSet 本身有接收到調整設定的請求。\n下指令調整後的 StatefulSet 設定\n看了一下 Pod 數量，就算 my-statefulset-2 卡住，還是可以擴到 5 個 Pod。\n下指令調整後的 Pod 狀態\n使用 yaml 調整 Pod 數量 我們直接調整 StatefulSet yaml 的 Pod 數量，看看會發生什麼事：\n一樣我們先看 StatefulSet 的 yaml 可以看到 Pod replicas 已經改變，也可以看 generation 有更新。\n使用 yaml 調整後的 StatefulSet 設定\n看了一下 Pod 數量，也不會管其他 Pod 是否 Ready，一樣可以縮小成 2 個 Pod。\n使用 yaml 調整後的 Pod 狀態","結論#結論":"後來我們重新檢查了一下為什麼 processes 會用完，結果發現是 RD 的程式邏輯，導致每筆 Request 必須等待前一筆 Request 做完，才會開始動作，讓 processes 一直被占用，沒辦法即時消化，導致 processes 用完，又加上服務是使用 StatefulSet，預設模式的 OrderedReady，必須等待前一個 Pod 是 Ready 才可以自動擴縮，所以當我們 Hpa 想要擴縮，來增加可用的 processes 數量，也因為沒辦法擴縮，最後導致這一連串的問題 😕。\n另外，如果想要從 OrderedReady 模式切成 Parallel 模式 (反正過來也是)，必須先將原本的 StatefulSet 給刪除，才可以調整：\nOrderedReady 模式切成 Parallel 模式","解決辦法#解決辦法":"當下想趕快解決 readiness 這個問題，調整 timeoutSeconds 後，單純 apply 是沒有用的，要記得刪掉卡住的 Pod，讓他重新建立，才會套用新的設定 (但我們當下太在意為甚麼 Pod 會卡住，沒有想到要先把 readiness 問題修掉 xD，我們當下的解法是先將流量導到地端正常的服務上)。\n另外 Google 也說，假如我們還是必須使用 StatefulSet 來建立服務，建議我們把 podManagementPolicy 改成 Parallel，它會有點像是 Deployment 的感覺，不會等待其他 Pod 變成 Ready 狀態，所以可以讓我們就算在 readiness 卡住的情況下，也可以自動擴縮服務。\nℹ️ StatefulSet podManagementPolicy 參數說明\nOrderedReady (預設) Pods 會按照順序一個接一個地被創建。即，n+1 號 Pod 不會在 n 號 Pod 成功創建且 Ready 之前開始創建。 在縮小 StatefulSet 的大小時，Pods 會按照反向順序一個接一個地被終止。即，n 號 Pod 不會在 n+1 號 Pod 完全終止之前開始終止。 這確保了 Pods 的啟動和終止的順序性。\nParallel 所有 Pods 會同時地被創建或終止。 當 StatefulSet 擴展時，新的 Pods 會立即開始創建，不用等待其他 Pods 成為 Ready 狀態。 當縮小 StatefulSet 的大小時，要終止的 Pods 會立即開始終止，不用等待其他 Pods 先終止。 這種策略提供了快速的擴展和縮小操作，但缺乏順序性保證。"},"title":"正式環境上踩到 StatefulSet 的雷，拿到 P1 的教訓"},"/blog/kubernetes/pod-veth-name-provided-eth0-already-exists/":{"data":{"":"此文章要來記錄一下公司同事在正式服務上遇到的問題，會詳細說明遇到事情的經過，以及開單詢問 google support 最後討論出的暫時解決的辦法：\n簡單列出正式站當下服務環境：\ngke master version：1.25.10-gke.2700 gke node version：1.25.8-gke.1000 該問題發生的 node pool 有設定 taint 發生問題的 Pod 是用 Statefulset 建立的服務 ","事情發生的經過#事情發生的經過":" RD 同仁反應，發現使用 Statefulset 建立的排程服務有問題，下 kubectl delete 指令想要刪除 Pod，讓 Pod 重新長，卻卡在 Terminating，等待一段時間後，決定下 kubectl delete --force --grace-period=0 來強制刪除 Pod，這時候狀態會卡在 ContainerCreating，使用 Describe 查看，會出現以下錯誤： Warning (combined from similar events): Failed to create pod sandbox: rpo error: code = Unknown desc = failed to setup network for sandbox \"14fe0cd3d688aed4ffed4c36ffab1a145230449881bcbe4cac6478a63412b0c*: plugin type=*gke\" failed (add): container veth name provided (etho) already exists 我們 SRE 協助查看後，也有嘗試去下 kubectl delete --force --grace-period=0 來刪除 Pod，但還是一樣卡在 ContainerCreating，最後是先開一個新的 Node 並讓該 Pod 建立到新的 Node 上，才解決問題。為了方便 google support 協助檢查出問題的 Node，先將 Node 設定成 cordon，避免其他 Pod 被調度到該問題 node 上。 Node 設定成 cordon\nNode 可以設定 cordon、drain 和 delete 三個指定都會使 Node 停止被調度，只是每個的操作暴力程度不同：\ncordon：影響最小，只會將 Node 標示為 SchedulingDisabled 不可調度狀態，但不會影響到已經在該 Node 上的 Pod，使用 kubectl cordon [node name] 來停止調度，使用 kubectl uncordon [node name] 來恢復調度。\ndrain：會先驅逐在 Node 上的 Pod，再將 Node 標示為 SchedulingDisabled 不可調度狀態，使用 kubectl drain [node name] --ignore-daemonsets --delete-local-data 來停止調度，使用 kubectl uncordon [node name] 來恢復調度。\ndelete：會先驅逐 Node 上的 Pod，再刪除 Node 節點，它是一種暴力刪除 Node 的作法，在驅逐 Pod 時，會強制 Kill 容器進程，沒辦法優雅的終止 Pod。\n我們隨後開單詢問 goolge support。 ","參考#參考":"Node 節點禁止調度（平滑維護）方式- cordon，drain，delete：https://www.cnblogs.com/kevingrace/p/14412254.html","與-google-support-討論內容#與 Google Support 討論內容":"Google Support 經過查詢後，回覆說：這個問題是因為 Pod 被強制刪除導致，強制刪除是一種危險的操作，不建議這樣處理，下面有詳細討論。\n一開始卡在 Terminating 狀態，我們也有請 RD 說明一下當下遇到的問題以及處理動作：RD 當時想要刪除 Pod 是因為該程式當下有 Bug，將 redis 與 db 連線給關閉，程式找不到就會一直 retry，導致相關進程無法結束，再加上 terminationGracePeriodSeconds 我們設定 14400，也就是 4 小時，才會卡在 Terminating 狀態。 (terminationGracePeriodSeconds 設定這麼久是希望如果有被 on call，工程師上來時，可以查看該 Pod 的錯誤原因)\n因為卡在 Terminating 太久，RD 有執行 kubectl delete --force，就是因為下了 --force 才造成相關資源問題 (例如 container proccess, sandbox, 以及網路資源)沒有刪乾淨。所以引起了此次的報錯 “container veth name provided (eth0) already exists”。 (因為我們服務使用 Statefulset，Pod 名稱相同，導致 eth0 這個網路資源名稱重複，所以造成錯誤，可以用 deployment 來改善這個問題，只是資源如果沒有清理乾淨會佔用 IP，所以單純調整成 deployment 也不是最佳解)\nGoogle 產品團隊建議，如果 Pod 處於 Running 狀態時，想要快速刪除 Pod 時，一開始就先使用 kubectl delete pod --grace-period=number[秒數] 來刪除，如果已經是 Terminating 狀態則無效。(SRE 同仁已測試過，與 Google Support 說明相同)\n那如果已經處於 Terminating 狀態，要怎麽讓 Pod 被順利刪除，這部分 Google Support 後續會在測試並給出建議，目前測試是：進去卡住的 Pod Container，手動刪除主進程 (pkill) 就可以了。\nGoogle Support 回覆"},"title":"部署 Pod 遇到 container veth name provided (eth0) already exists 錯誤"},"/blog/nginx/":{"data":{"":"此分類包含 Nginx 相關的文章。\n想使用 Nginx Upstream Proxy 到外部服務，並帶入對應的 header 該怎麼做？ Soketi WebSocket Server LOG 不定時出現 502 error 以及 connect() failed (111: Connection refused) "},"title":"Nginx"},"/blog/nginx/nginx-upstream-set-host-header/":{"data":{"":"此文章要來記錄一下最近在公司服務入口遇到的一些小問題，以及解決的方法。簡單說明一下，我們的服務入口是用 Nginx 來當作 proxy server，將不同路徑或是 servername 導到對應的後端程式，或是外部的服務上(例如 AWS cloudfront.net)，本篇要測試的是如果使用要同時使用 upstream 到外部服務，且需要帶 host header 該怎麼做。\nNginx 的 upstream 是什麼？\n通常我們 proxy_pass 的寫法會是這樣：\nlocation /aaa { proxy_pass http://aaa.example.com; } 當 Nginx 收到的 request 是 /aaa 時，就會將 request 轉發到 http://aaa.example.com。\n但假如後端有多台機器或是服務，可以處理同一種 request，這時候就可以使用 upstream 來處理：\nupstream backend_hosts { server aaa.example.com; server bbb.example.com; server ccc.example.com; } location /aaa { proxy_pass http://backend_hosts; } 這樣子的好處是可以有多個機器或是後端服務可以分散請求，做到負載平衡的效果。","參考#參考":"Make nginx to pass hostname of the upstream when reverseproxying：https://serverfault.com/questions/598202/make-nginx-to-pass-hostname-of-the-upstream-when-reverseproxying","問題#問題":"那如果我們使用 Nginx upstream 時，還想要同時帶 host 的 header 到後端該怎麼做呢？我們先來看一下目前的寫法：\n( 測試範例是使用 docker 來模擬，可以參考程式碼 \u003e 點我前往 github，會有三個 nginx，其中一個是負責 proxy 的 nginx 名為 proxy，另外兩台是 upstream 後的服務，名為 upstream_server1、upstream_server2 )\nnginx-old.conf upstream upstream_server { server upstream_server1; server upstream_server2; } server { listen 80; server_name localhost; location /upstream_server/ { proxy_pass http://upstream_server; proxy_set_header Host \"upstream_server1\"; proxy_set_header Host \"upstream_server2\"; access_log /var/log/nginx/access.log upstream_log; } } } 可以看到我們希望 Nginx 收到 request 是 /upstream_server 時，將 request 轉發到 http://upstream_server，而 upstream_server 後面有兩個 server，並且在 proxy 時，帶入兩個不同的 host header。但如果真的這樣寫，可以達到我們想要得效果嗎？我們實際跑看看程式 (範例可以使用 nginx-old.conf)：\nnginx 原本寫法\n從上面的 LOG 可以發現，我們 call /upstream_server 時，後端的 upstream_server1、upstream_server2 收到的 host 只會收到第一個設定的 Host，且服務會出現 400 Bad Request，查了一下網路文章，發現出現 400 Bad Request，可能跟 header 送太多資訊過去，詳細可以參考 解決網站出現 400 Bad Request 狀態的方法。\n這邊推測應該是後端如果也是用 nginx 直接接收才會遇到 400 的問題，還好目前公司服務還是正常的 xDD，檢查一下後發現，其實後端根本沒有要求對應 header 才能接收(應該是對方忘記加上此限制)。","解決#解決":"好，不管是否需要對應 header，我們還是找看看有沒有辦法同時使用 upstream，並帶入對應 host 的方法呢？\n最後參考網路上的文章，似乎只能使用兩層的 proxy，才能完成這兩個需求，我們來看看要怎麼寫吧 (範例可以使用 nginx.conf)：\nnginx.conf server { listen 777; server_name localhost; location / { proxy_pass http://upstream_server1; proxy_set_header Host \"upstream_server1\"; access_log /var/log/nginx/access.log upstream_log; } } server { listen 888; server_name localhost; location / { proxy_pass http://upstream_server2; proxy_set_header Host \"upstream_server2\"; access_log /var/log/nginx/access.log upstream_log; } } upstream upstream_server { server 127.0.0.1:777; server 127.0.0.1:888; } server { listen 80; server_name localhost; location /upstream_server/ { proxy_pass http://upstream_server; access_log /var/log/nginx/access.log upstream_log; } } 可以看到上面的程式碼，我們透過兩層的 proxy，來達到我們想要的效果，這樣子就可以同時使用 upstream，並且帶入對應的 host header。\n首先在 28 ~ 36 行，我們一樣如果 Nginx 收到 request 是 /upstream_server 時，會 proxy 到 upstream_server 這個 upstream 中，而 upstream_server 有兩個 server，分別是 127.0.0.1:777、127.0.0.1:888，但實際上沒有這兩個 port，所以我們需要再寫一層一般的 proxy 設定，分別是 1 ~ 10 行、12 ~ 21 行，這樣子就可以達到我們想要的效果。\n但這個方法比較適用於 upstream 後端沒有太多個服務或是機器的情況，如果有很多個服務或是機器，就需要寫很多的 proxy，這樣子會變得很麻煩，所以如果有更好的方法，也歡迎留言跟我分享 🤣。\n最後我們來看一下實際執行的結果：\n使用多層的 nginx proxy 處理"},"title":"想使用 Nginx Upstream Proxy 到外部服務，並帶入對應的 header 該怎麼做？"},"/blog/nginx/soketi-log-502-error/":{"data":{"":"此文章要來記錄一下 RD 同仁前陣子有反應使用 Soketi 這個 WebSocket Server 會不定時在 LOG 出現 502 error 錯誤訊息以及 connect() failed (111: Connection refused) while connecting to upstream，雖然說服務使用上不會影響很大，但還是希望我們可以協助找出 502 的原因。\n出錯的 LOG\n在開始找問題前，先簡單介紹一下 Soketi 是什麼東西好了，根據官網的說明，他是簡單、快速且有彈性的開源 WebSockets server，想要了解更多的可以到它官網去查看。\n另外會把程式碼相關放到 GitHub » 點我前往","參考#參考":"[Nginx] 解決 connect() failed (111: Connection refused) while connecting to upstream：https://wshs0713.github.io/posts/8c1276a7/\nWebSocket proxying：http://nginx.org/en/docs/http/websocket.html\nday 10 Pod(3)-生命週期, 容器探測：https://ithelp.ithome.com.tw/articles/10236314","壓測#壓測":"最後調整完，我們來測試看看是否在 Pod 自動重啟 or 更新 Deployment 的時候(並且有大量連線時)還會噴 502 error 或是 connect() failed (111: Connection refused)，我們這邊使用 k6 來做 websocket 服務的壓測，有簡單寫一個壓測程式如下：\nk6 壓測\nk6 是一個開源的壓測工具，可以用來測試 API、WebSocket、gRPC 等服務，可以到它的官網查看更多資訊。\nMacOS 安裝方式：brew install k6\nwebsocket.jsimport ws from \"k6/ws\"; import { check } from \"k6\"; export const options = { vus: 1000, duration: \"30s\", }; export default function () { const url = \"wss://socket.XXX.com/app/hex-ws?protocol=7\u0026client=js\u0026version=7.4.1\u0026flash=false\"; const res = ws.connect(url, function (socket) { socket.on(\"open\", () =\u003e console.log(\"connected\")); socket.on(\"message\", (data) =\u003e console.log(\"Message received: \", data)); socket.on(\"close\", () =\u003e console.log(\"disconnected\")); }); check(res, { \"status is 101\": (r) =\u003e r \u0026\u0026 r.status === 101 }); } 簡單說明一下上面程式在寫什麼，我們在 const 設定 vus 代表有 1000 個虛擬使用者，會在 duration 30s 內完成測試，下面的 default 就是測試連線 ws 以及 message 跟 close 等動作，最後需要回傳 101 (ws 交握)\n執行 k6 run websocket.js 後，就會開始壓測，可以看到會開始執行剛剛在上面提到 default 的動作：\nk6 壓測過程\n等到跑完，就會告訴你 1000 筆裡面有多少的 http 101，這邊顯示 status is 101，就代表都是 101，代表都有連線成功，沒有出現 502 error 或是 connect() failed (111: Connection refused) 的錯誤，這樣就代表我們的問題解決了。\nk6 壓測結果","解決過程#解決過程":"我們可以看到上方錯誤 LOG 中，發現有出現 502 error 以及 connect() failed (111: Connection refused) while connecting to upstream，這兩個錯誤都是由 Nginx 所產生的，那我們先來理解一下，Nginx 與 Soketi 之間的關係。\n在使用上，RD 的程式會打 Soketi 專用的 Subdomain 來使用這個 WebSocket 服務，而在我們的架構上，這個 Subdomain 會經過用 nginx proxy server，來轉發到 Soketi WebSocket Server (走 k8s svc)，設定檔如下圖：\n入口 nginx 設定\n然後會出現 connect() failed (111: Connection refused) while connecting to upstream 的錯誤訊息，代表我們的 Nginx 設定少了一個重要的一行設定，就是 proxy_http_version 1.1;，這個設定要讓 Nginx 作為 proxy 可以和 upstream 的後端服務也是用 keepalive，必須使用 http 1.1，但如果沒有設定預設是 1.0，也要記得設定 proxy_set_header Upgrade、proxy_set_header Connection。調整過後就變成：\nws.confserver { server_name socket.XXX.com; listen 80 ; listen [::]:80 ; listen 443 ssl; listen [::]:443 ssl; ssl_certificate /etc/nginx/ingress.gcp.cert; ssl_certificate_key /etc/nginx/ingress.gcp.key; access_log /var/log/nginx/access.log main; location / { proxy_pass http://soketi-ws-ci:6001; proxy_connect_timeout 10s; proxy_read_timeout 1800s; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \"Upgrade\"; proxy_set_header X-Real-IP $remote_addr; } } 解決完 connect() failed (111: Connection refused) 這個問題後，接下來就是要解決 502 error 這個問題，會導致 502 代表 Nginx 這個 proxy server 連不上後端的 Soketi WebSocket Server，再觀察 LOG 以及測試後發現，當 Pod 自動重啟，或是手動重啟 Deployment 的時候，就會有 502 的錯誤，代表 Nginx 在 proxy 到後面的 Soketi svc 再到 Pod 的時候，有一段時間是連不上的，所以就會出現 502 的錯誤，可以推測是流量進到正在關閉的 Pod 或是進到還沒有啟動好的 Pod 才導致的。\n那我們先來看一下 Soketi WebSocket Server 的服務 yaml 檔案：\ndeployment.yaml deployment.yaml spec: terminationGracePeriodSeconds: 30 securityContext: {} containers: - name: soketi securityContext: {} image: \"quay.io/soketi/soketi::1.6.0-16-alpine\" ... 省略 (可以到 github 看 code)... livenessProbe: failureThreshold: 3 httpGet: httpHeaders: - name: X-Kube-Healthcheck value: \"Yes\" path: / port: 6001 initialDelaySeconds: 5 periodSeconds: 2 successThreshold: 1 可以看到原來的設定只有 livenessProbe 而已，因此我們為了要避免流量進到正在關閉的 Pod 或是進到還沒有啟動好的 Pod，所以我們需要加上 readinessProbe 以及 preStop，讓 Pod 確定啟動完畢，或是等待 Service 的 endpoint list 中移除 Pod，才開始接收流量，這樣就可以避免出現 502 的錯誤。\ndeployment.yaml spec: terminationGracePeriodSeconds: 30 securityContext: {} containers: - name: soketi securityContext: {} image: \"quay.io/soketi/soketi::1.6.0-16-alpine\" ... 省略 (可以到 github 看 code)... livenessProbe: failureThreshold: 3 httpGet: httpHeaders: - name: X-Kube-Healthcheck value: \"Yes\" path: / port: 6001 initialDelaySeconds: 5 periodSeconds: 2 successThreshold: 1 readinessProbe: failureThreshold: 3 httpGet: httpHeaders: - name: X-Kube-Healthcheck value: \"Yes\" path: /ready port: 6001 initialDelaySeconds: 5 periodSeconds: 2 successThreshold: 1 lifecycle: preStop: exec: command: [\"/bin/sh\", \"-c\", \"sleep 20\"] Pod 終止的過程"},"title":"Soketi WebSocket Server LOG 不定時出現 502 error 以及 connect() failed (111: Connection refused)"},"/blog/opentelemetry/":{"data":{"":"此分類包含 Opentelemetry 相關的文章。\n什麼是 Opentelemetry？可觀測性 (Observability) 又是什麼？ 如何透過 OpenTelemetry 來收集 Ingress Nginx Controller 的 Metrics 與 Traces 並送到 Datadog 上 "},"title":"Opentelemetry"},"/blog/opentelemetry/opentelemetry-ingress-nginx-controller/":{"data":{"":"由於最近公司想要導入 Datadog，在測試過程中順便導入 OpenTelemetry 來收集 Metrics 與 Traces 並送到 Datadog 上 ～\n🔥 這個範例比較特別，因為 Datadog 有提供 Ingress Nginx Controller 的 integrations，可以透過 Datadog Agent 來收集 Metrics，不需要透過 OpenTelemetry Collector 來收集。 ( Datadog Agent 請參考：https://docs.datadoghq.com/containers/kubernetes/ )\n程式部分也同步上傳到 github 上，可以點我前往","參考#參考":"Configure Nginx Ingress Controller to use JSON log format：https://dev.to/bzon/send-gke-nginx-ingress-controller-logs-to-stackdriver-2ih4\n淺談 OpenTelemetry - Collector Compoents：https://ithelp.ithome.com.tw/articles/10290703","執行步驟#執行步驟":" 先 clone 這個 repo (廢話 xD)\n先建立 OpenTelemetry Collector，執行以下指令：\nhelm upgrade collector \\ opentelemetry-collector \\ --repo https://open-telemetry.github.io/opentelemetry-helm-charts \\ --install \\ --create-namespace \\ --namespace opentelemetry \\ -f \"otel-collector.yaml\" 再建立 Ingress Nginx Controller，執行以下指令：\nhelm upgrade ingress-nginx \\ ingress-nginx \\ --repo https://kubernetes.github.io/ingress-nginx \\ --install \\ --create-namespace \\ --namespace ingress-nginx \\ -f \"ingress-nginx-values.yaml\" 接著建立測試用 Nginx 服務，執行以下指令：\nkubectl apply -f nginx.yaml ","檔案說明#檔案說明":" otel-collector.yaml： OpenTelemetry Collector 的設定檔，主要是設定要收集哪些 metrics、traces，並且要送到哪個 exporter，要注意的是 exporters 的 datadog 需要設定 site、api_key，以及 image 要記得用 otel/opentelemetry-collector-contrib，才會有 datadog 的 exporter。\ningress-nginx-values.yaml： Ingress Nginx Controller 的設定檔，這邊的 podAnnotations 是為了讓 Ingress Nginx Controller 的 Pod 能夠透過 Datadog agent 收集 metrics 到 Datadog 才加上的。\nconfig 裡面的設定有很多，主要都是 openTelemetry 的設定，要注意的是 enable-opentelemetry 要設為 true，另外 otlp-collector-host 以及 otlp-collector-port 要送到哪個 collector 等等也要記得設定。 另外如果想要將 LOG 與 Trace 串再一起，記得要把 log-format 設為 json，並且帶入，trace_id 與 span_id ( 這邊有多帶 dd.trace_id 是為了讓 datadog 可以自動串接 LOG \u0026 Trace )。\nnginx.yaml： 一個簡單的 Nginx 整套服務 (Deployment、Service、Ingress)，要注意的是 Ingress 需要設定 annotations kubernetes.io/ingress.class: nginx (這個是 Ingress Nginx Controller 的預設 class name)，才會被 Ingress Nginx Controller 接管 (才會有 Load Balancer 的 IP)","測試#測試":"當你執行完上面的步驟後，你會發現有產生兩個 namespace，一個是 ingress-nginx，另一個是 opentelemetry，並且會有 OpenTelemetry Collector、Ingress Nginx Controller、Nginx 等服務，如下：\n啟動服務\n我們試著打 http://nginx.example.com/ (測試網址，需要先在 /etc/hosts 綁定 Ingress Nginx Controller 咬住的 Load Balancer IP)，查看一下 Datadog 的 LOG，看看是否有收到 Nginx 的 LOG (此收集 LOG 的方式是透過在 cluster 上安裝 Datadog 的 agent)，如下：\nDatadog LOG\n接著查看 Datadog APM 的 trace，如下：\nDatadog APM\n由於我們在後面目前沒有串其他服務，所以只有一個 span，之後還有另外兩篇文章是介紹如何串其他服務 (會增加服務以及部分設定)，可以參考看看：opentelemetry-roadrunner、opentelemetry-nodejs\n順便看一下透過 Datadog Agent 收集的 Ingress Nginx Controller 的 Metrics，如下：\nDatadog Ingress Nginx Controller 的 Metrics\n可以用這些 Metrics 來做 Dashboard，如下：\nDatadog Dashboard","結論#結論":"透過 OpenTelemetry Collector 來收集 Ingress Nginx Controller 的 Metrics 與 Traces 並送到 Datadog 上，這樣就可以透過 Ingress Nginx Controller 的 Metrics 來做監控了，對於 RD 再開發上，有 Traces 也更方便 RD 他們找到程式的瓶頸 (有可能是服務導致的)。"},"title":"如何透過 OpenTelemetry 來收集 Ingress Nginx Controller 的 Metrics 與 Traces 並送到 Datadog 上"},"/blog/opentelemetry/opentelemetry-observability/":{"data":{"":"在介紹 Opentelemetry 之前，我們要先了解一下目前軟體架構以及基礎設施的演進：\n軟體架構以及基礎設施的演進\n第一階段在軟體架構設計上較為簡單，不會有什麼特別需要拆分出來的程式，所以都是一整包的程式，再測試以及除錯也比較不會有什麼問題。基礎設施都是使用 VM 或是使用放在 IDC 的機房來當 Server。\n第二階段隨著雲端技術的推出，會開始將服務搬上雲供應商提供的 IaaS 服務，或是使用私有雲給企業放置較機密的內容，其他則放置公有雲上，達成混合雲的模式。\n第三階段隨著雲端技術越來越成熟，有更多的雲端 IaC 以及功能推出，會開始考慮使用分散式的系統架構，將 DB 等服務也改用 Cloud SQL 的方式。在基礎設施上也隨著容器化的技術成熟而進入新的時代。\n第四階段已經使用 docker 來管理好一陣子，但發現虛擬容器技術在管理上十分不方便，因此 K8s 逐漸盛行，將架構從分散式改成微服務的方式進行，在讓開發團隊使用上可以更靈活且容易。\n雖然使用 K8s 可以讓我們的服務更靈活方便，但也會將服務切的越來越細，這時會讓開發變的十分複雜，我們在架構上從一開始的單體式架構，變成分散式架構，再到最後的微服務，讓開發人員需要處理的事情會越來越多。服務要如何連線？Log 要如何記錄？以及當一個請求會經過多個服務時，相對的延遲也會增加，這時要怎麼去處理等。在監控上，因為服務切分得很細，當線上有一個服務有問題時，要如何快速的找到問題點也是一大挑戰。\n當我們使用分散式系統或是微服務時發生故障時，會很難快速的恢復服務，因為每個服務都互相依賴，在以往都是透過經驗以及對系統的了解來得以解決。那有什麼其他的方式，能夠讓我們更快掌握每個服務呢？我們先來了解一個名詞：可觀測性(Observability)","opentelemetry#Opentelemetry":"那我們這次要介紹的可觀測性(Observability)工具就是 Opentelemetry，縮寫 OTel，它是由 CNCF (Cloud Native Computing Foundation) 組織孵化的開源專案，在 2021 年 5 月由 OpenTracing 與 OpenCensus 兩個框架合併，結合兩項分散式追蹤框架最重要的特性成為下一代收集遙測數據的新標準。\ntelemetry 又叫做遙測，是指能夠跨越不同系統來收集資料 (包含 LOG、Metric、trace) 的能力。\n我們可以看一下官網的說明：\nOpentelemetry 是雲原生的可觀測性(Observability)框架，提供標準化的 API、SDK 與協議自動檢測、蒐集、導出遙測數據資料 (Metrics、Log、Trace)，並支援 W3C 定義的 Http trace-context 規範，降低開發者在搜集遙測數據上的困難度，以及方便進行後續分析以及性能的優化。\nOpentelemetry\n在 OpenTelemetry 核心元件如下：\nAPI：開發人員可以透過 OpenTelemetry API 自動生成、蒐集應用程式(Application)的遙測數據資料(Metrics, Log, Trace)，每個程式語言都需實作 OpenTelemetry 規範所定義的 API 方法簽章。\nSDK：是 OpenTelemetry API 的實現。\nOTLP：規範定義了遙測數據的編碼與客戶端及服務器之間如何交換的協議 (gRPC、HTTP)。\nCollector：OpenTelemetry 中儲存庫，用於接收、處理、導出遙測數據到各種後端平台。","參考資料#參考資料":"Observability：https://linkedin.github.io/school-of-sre/level101/metrics_and_monitoring/observability/\n[OpenTelemetry] 現代化監控使用 OpenTelemetry 實現 : 可觀測性(Observability)：https://marcus116.blogspot.com/2022/01/modern-monitoring-using-openTelemetry-with-Observability.html\n[OpenTelemetry] 現代化監控使用 OpenTelemetry 實現 : OpenTelemetry 開放遙測：https://marcus116.blogspot.com/2022/01/opentelemetry-opentelemetry.html\n淺談 Observability(下)：https://ithelp.ithome.com.tw/m/articles/10287598\nManage services, spans, and traces in Splunk APM：https://docs.splunk.com/Observability/apm/apm-spans-traces/traces-spans.html","可觀測性observability#可觀測性(Observability)":"可觀測性有三個重要的特性，分別是：\nMetrics 負責監控系統有什麼狀況，當要發生服務故障前可以透過設定閥值搭配告警提早得知。\nLogs 當問題發生時，可以用來查看故障時正在執行哪些服務，以及產生的錯誤資訊。\nTraces （後面詳細介紹）\n可觀測性三大支柱\n我們對於 Metrics 跟 Logs 有基本的了解，所以我這邊會注重在 Traces 的部分：\n當有多個微服務的複雜分散式系統，用戶的請求會由系統中的多個微服務進行處理。Metrics 跟 Logs 可以提供我們有關系統如何去處理這些請求的一些資訊，但沒有辦法提供微服務的詳細訊息以及他是如何影響客戶端的請求。這時候就需要透過 Trace 來協助我們追蹤。\nTrace 可以在連續的時間維度上，透過 Trace 以及 Span 關聯，把空間給排列展示出來，並且有 Trace-Context 規範，能夠直觀的看到請求在分散式系統中經過所有服務的紀錄。\n什麼是 Trace、Span 、Trace-Context 呢？\n我們先說 Span，Span 又可以叫跨度，是系統中最小的單位，可以看下方圖片，SpanA 的資料是來源 SpanB，SpanB 來源是 SpanC 等等，每一個 Span 可以把它想成一個請求後面所有經過服務的工作流程，例如：nginx_module、db、redis 等等。\n請求的整個過程叫做 Trace，那他要怎麼知道 SpanA ~ SpanE 是同一個請求呢？\n就需要透過 TraceID 以及 SpanID 來記錄：\nTraceID：是唯一的 ID，用於識別整個分散式追蹤的一條請求路徑。在下方圖片中，當請求進入時，就會被賦予一個 TraceID，所有有經過的 Span 都會記錄此 TraceID，這樣才可以把不同服務依據 TraceID 關聯成同一個請求。\nSpanID：是一條請求路徑中單個操作唯一的 ID。追蹤路徑是由多個 Span 組成，每個 Span 都代表一個操作或特定的時間段。當請求進入時，每個服務就會產生一個 Span 來代表它處理請求的的時間。這些 Span 使用 TraceID 來連接再一起，形成完整的請求追蹤。\nTrace 示意圖\n那要怎麼查看每個 Span 的紀錄內容呢，就需要 Trace-Context：\n會放置一些用於追蹤和識別請求的上下文信息，例如 Trace ID、Span ID 和其他相關的數據。這些上下文信息可以是一些關鍵的數據，可以幫助我們在整個分佈式系統中追蹤請求的路徑，並將相關請求和操作關聯起來。\nECK Trace 示意圖\n上面的圖片中，可以看到 call /product/XXXX 後，會經過需多的 Span，隨便點擊一個 Span 可以看到它記錄的 Trace-Context，以及都會包含 TraceID 及 SpanID\nECK Trace 示意圖\nTrace 優點可以看到跨維度看到中間的資訊，對於找到問題以及瓶頸十分方便，但缺點就是因為需要在 Span 中產生 ID 以及內容，需要在程式裡面加入一定的套件以及調整程式碼。\n所以我們在可觀測性(Observability)最終的目的是希望可以透過可觀測性工具讓我們知道：\n請求通過哪些服務 每個服務在處理請求時做了些什麼 如果請求很慢，瓶頸在哪邊 如果請求失敗，錯誤點在哪 請求的路徑是什麼 為什麼花這麼長的時間 "},"title":"什麼是 Opentelemetry？可觀測性 (Observability) 又是什麼？"},"/blog/terraform/":{"data":{"":"此分類包含 Terraform 相關的文章。\n如何導入 Terragrunt，Terragrunt 好處是什麼？ "},"title":"Terraform"},"/blog/terraform/terragrunt/":{"data":{"":"我們接續上一篇的 如何將 Terraform 改寫成 module ? ，我們已經將 Terraform 改成 module 的方式來進行管理，但當我們要管理的資源越來越多，且有分不同的專案時，整個服務架構會長的像以下：\nmodules google_compute_instance main.tf outputs.tf variables.tf projects gcp-1234 aaa backend.tf main.tf provider.tf bbb backend.tf main.tf provider.tf ccc backend.tf main.tf provider.tf gcp-2345 aaa backend.tf main.tf provider.tf bbb backend.tf main.tf provider.tf ccc backend.tf main.tf provider.tf gcp-3456 aaa backend.tf main.tf provider.tf bbb backend.tf main.tf provider.tf ccc backend.tf main.tf provider.tf 這邊的範例是以不同專案來分，再分區不同的服務，每個服務裡面都會有 backend.tf、main.tf、provider.tf 檔案所組成，我們可以來比較一下 gcp-1234 的 aaa 服務以及 gcp-2345 的 aaa 服務差異：\n檔案差異\n可以看到在 backend.tf 除了 prefix 路徑以外，其他設定也都一樣，但因為 Terraform 本身沒辦法透過帶入參數的方式來設定 backend.tf 後端部分，所以必須要先寫好每個服務所存放的後端位置，十分的不方便。\nbackend.tfterraform { backend \"gcs\" { bucket = \"terragrunt-tfstate\" prefix = \"/gcp-1234-aaa\" } } 為了減少上述這些需要一直重複寫差不多檔案的工作內容，因此有了 Terragrunt 這個工具，Terragrunt 是 Terraform 的包裝器，可以彌補 Terraform 上的一些缺陷，並且讓我們的 IaC 更貼近 DRY 原則。\n這邊說明一下 DRY 原則\nDRY 全名是 Don’t repeat yourself，也就是不要做重複的事情，能夠一次做完的就不要重複的去做。","terragrunt-好處#Terragrunt 好處":"\n接著介紹一下 Terragrunt 的好處：\n方便管理後端狀態設定\n將後端存儲桶納入管理\n使用 generate 自動生成檔案\n使用 include 檔案來達到 DRY 原則\n管理 Module 之間的依賴性\n產生依賴關聯圖\n方便管理後端狀態設定 首先第一個方便管理後端狀態設定，也就是我們上面提到的 backend.tf 設定。在 Terraform 原生為了區別不同專案不同服務的狀態檔，就必須先寫好每個儲存的路徑，但使用 Terragrunt，可以先在該目錄下，也就是 gcp-3456 目錄下先寫一個設定檔案 (我們以 gcp-3456 專案為例)，讓底下的 aaa、bbb、ccc 服務可以去 include 它，我們就不需要每個服務都寫幾乎差不多的設定檔，接著我們在 gcp-3456 資料夾下新增 terragrunt.hcl 檔案來說明：\nterragrunt.hclremote_state { backend = \"gcs\" generate = { path = \"backend.tf\" if_exists = \"overwrite\" } config = { bucket = \"terragrunt-tfstate\" prefix = \"${path_relative_to_include()}\" } } 這邊的設定其實跟之前的 backend.tf 差不多，只是後端現在儲存的 block 改叫做 remote_state，可以看到 backend 設定我們一樣是存在 gcs 上，generate 這個 block 它會判斷 backend.tf 檔案是否存在，如果沒有它就會幫我們建立，其中設定檔案內容是將狀態檔案存在 terragrunt-tfstate 這個 bucket，並透過${path_relative_to_include()} 這個變數來自動帶入有 include 這份檔案的路徑，並在相對路徑產生 backend.tf 檔案。\n有點抽象，所以我畫一個比較簡單的架構圖來做說明一下，假設現在有三個服務，如下：\naaa terragrunt.hcl bbb terragrunt.hcl ccc terragrunt.hcl terragrunt.hcl 我們剛剛的後端設定是寫在此根目錄的 terragrunt.hcl 檔案(第 8 行)，然後 ccc 這個服務去 include 根目錄的 terragrunt.hcl 檔案， Terragrunt 就會自動幫你產生以下的 backend.tf 檔案：\nbackend.tfterraform { backend \"gcs\" { bucket = \"terragrunt-tfstate\" prefix = \"/ccc\" } } 這樣就可以省下我們要重複寫 backend.tf 的時間，在維護上也會更加的方便。\n將後端存儲桶納入管理 接著，大家有沒有想過，我們都已經使用 Terraform 來管理 IaC ，並把狀態檔案放到 gcs 上面來保存，但一開始還沒有用 Terraform 管理 gcs 的資源，我們還需要在設定 backend.tf 前，先手動去新增一個 gcs ，才能來存放 tfstate 狀態檔案呢？\n因此在 Terragrunt remote_state 的 config 時，可以多設定 gcs 的 project id 以及 location，Terragrunt 在初始化後端時，會檢查是否有該 gcs bucket，如果沒有就會自動建立，設定檔如下：\nterragrunt.hclremote_state { backend = \"gcs\" generate = { path = \"backend.tf\" if_exists = \"overwrite\" } config = { project = \"gcp-xxxxxx\" location = \"asia\" bucket = \"terragrunt-tfstate\" prefix = \"${path_relative_to_include()}\" } } 使用 generate 自動生成檔案 在剛剛我們可以使用 generate 來自動 backend.tf 檔案，那代表我們也可以把每個 provider.tf 的內容，也透過 generate 來生成，如下：\nterragrunt.hclgenerate \"provider\" { path = \"provider.tf\" if_exists = \"overwrite\" contents = \u003c\u003cEOF terraform { required_providers { google = { source = \"hashicorp/google\" version = \"~\u003e 4.48.0\" } } } EOF } 這樣子每個 include 這份設定檔的服務除了 backend.tf 檔案以外，還會自動產生 provider.tf 檔案。\n使用 include 檔案來達到 DRY 原則 我們搞定了 backend.tf 跟 provider.tf 後，還剩下 main.tf，所以我們也將它改成 Terragrunt 的格式如下：\nterragrunt.hclterraform { source = \"${get_path_to_repo_root()}/modules/google_compute_instance\" } include \"root\" { path = find_in_parent_folders() } inputs = { instance_name = \"gcp-3456-ccc\" machine_type = \"e2-small\" instance_zone = \"asia-east1-b\" instance_tags = [] instance_labels = {} boot_disk_auto_delete = true boot_disk_image_name = \"debian-cloud/debian-10\" ... 設定部分省略 ... } 這邊可以看到 terraform source 它就是我們使用 module 的路徑，也可以用 ${get_path_to_repo_root()} 這個變數他會自動抓該專案的根目錄，我們就不需要去特別設定。\n此外也可以將 module 獨立成一個專案，或是使用其他人寫好的 module，在 source 的時候可以使用 git::https://[gitlab-網址]/sre/terraform/module.git//google_compute_address 的方式來取得 module，可以設定要使用哪個分支或是 tag，只需要在網址後面加上，?ref=[分之 or tag 名稱] 即可，這樣可以讓開發中的 module 不會影響到線上其他正在使用中的 module 設定。\n(module.git 後面的 // 是 Terraform module source 的規則，如果不加會跳警告訊息)\n接著我們可以看到 include \"root\" {} 這段，裡面有使用 find_in_parent_folders 這邊變數，他就是上面的提到會自動去抓放在父資料夾的 remote_state 跟 generate terragrunt.hcl 檔案。\n後面的 input 就跟使用 module 時一樣，將 module 的參數帶入即可。\n補充：所以我們也可以把一些通用的設定寫在根目錄的 terragrunt.hcl 檔案，例如專案的 id，可以寫以下內容來讓 include 它的檔案吃到同一個參數設定：\nterragrunt.hclinputs = { project_id = \"gcp-3456\" } 管理 Module 之間的依賴性 由於 Terragrunt 是把每個服務拆分成最小化，沒辦法把使用不同 module 的資源放在一起(單純使用 module 的話，可以一次 source 多了 module，並把他放在同一個 tf 檔案中)，那像是我們建立 k8s 會使用到 google_container_cluster、google_container_node_pool 兩種不同的 module 該怎麼辦呢？\n首先我們先在 modules 資料夾放上 google_container_cluster、google_container_node_pool 兩個 module 的設定檔案，詳細程式可以點我前往\nmodules google_container_cluster main.tf outputs.tf variables.tf google_container_node_pool main.tf variables.tf 在 projects 底下新增 gke 資料夾，新增 terragrunt.hcl 來放 remote_state provider 的設定，並區分兩個資料夾，分別是 cluster 資料夾來存放 cluster 資訊，以及 test 資料夾來存放 test node-pool 資訊：\nprojects gke cluster terragrunt.hcl terragrunt.hcl test terragrunt.hcl cluster 的 terragrunt.hcl 檔案如下：\nterragrunt.hclterraform { source = \"${get_path_to_repo_root()}/modules/google_container_cluster\" } include { path = find_in_parent_folders() } inputs = { cluster_name = \"tf-test\" cluster_location = \"asia-east1-b\" node_locations = [] cluster_version = \"1.24.12-gke.500\" network_name = \"projects/gcp-202011216-001/global/networks/bbin-testdev\" subnetwork_name = \"projects/gcp-202011216-001/regions/asia-east1/subnetworks/bbin-testdev-dev-platform\" node_max_pods = 64 remove_default_node_pool = true initial_node_count = 1 enable_shielded_nodes = false resource_labels = { \"dept\" : \"pid\", \"env\" : \"dev\", \"product\" : \"bbin\" } dns_enabled = false cluster_dns = \"PROVIDER_UNSPECIFIED\" cluster_dns_scope = \"DNS_SCOPE_UNSPECIFIED\" private_cluster_ipv4_cidr = \"172.16.0.176/28\" binary_authorization_enabled = true binary_authorization = \"DISABLED\" } test node-pool 檔案如下：\nterragrunt.hclterraform { source = \"${get_path_to_repo_root()}/modules/google_container_node_pool\" } include { path = find_in_parent_folders() } dependency \"cluster\" { config_path = \"../cluster\" } inputs = { cluster_name = dependency.cluster.outputs.cluster_name cluster_location = dependency.cluster.outputs.cluster_location cluster_version = dependency.cluster.outputs.cluster_version node_pool_name = \"test\" node_count = 1 node_machine_type = \"e2-small\" node_disk_size = 100 node_disk_type = \"pd-standard\" node_image_type = \"COS_CONTAINERD\" node_oauth_scopes = [ \"https://www.googleapis.com/auth/devstorage.read_only\", \"https://www.googleapis.com/auth/logging.write\", \"https://www.googleapis.com/auth/monitoring\", \"https://www.googleapis.com/auth/service.management.readonly\", \"https://www.googleapis.com/auth/servicecontrol\", \"https://www.googleapis.com/auth/trace.append\" ] node_tags = [] node_taint_enabled = false node_taint_key = \"\" node_taint_value = \"\" node_taint_effect = \"\" auto_repair = true auto_upgrade = true upgrade_max_surge = 1 upgrade_max_unavailable = 0 upgrade_strategy = \"SURGE\" autoscaling_enabled = true autoscaling_max_node_count = 2 autoscaling_min_node_count = 1 autoscaling_total_max_node_count = 0 autoscaling_total_min_node_count = 0 } 上面兩個檔案分別是 cluster 的設定，以及 test node-pool 的設定，裡面的設定，上面基本都有提過，這邊要提的是 dependency，dependency 他是 Terragrunt 提供讓我們可以方便地去管理 IaC 之間的相依性，像是我們這邊，需要先建立好 cluster 才能建立 node_pool，此時就可以依靠 dependency block 來完成需求。\n( 靠 dependency 來取得 cluster 的資訊，並帶入 node_pool 中 )\n此時的執行指令是在 gke 目錄下，使用 terragrunt run-all [參數] 來跑整個相依的 module，我們這邊就建立一個名為 tf-test 的 cluster，並且有一個名為 test 的 node_pool ，其他設定請參考上面程式：\n測試 terragrunt run-all\n(黃色的 WARN 是因為 gcs 上還沒有存過該狀態檔案，所以會跳出提示)\n等到 cluster 建立完成後，會將 cluster 的資訊帶入 node_pool，才開始建立 node_pool 的資源：\n測試 terragrunt run-all\n產生依賴關聯圖 當我們服務使用到很多依賴關係，想要釐清是誰依賴誰，如果單純看程式會比較麻煩，在 Terragrunt 還有一個好用的指令，可以使用以下指令，產生對應的依賴關係圖，在檢視時可以更清楚知道關係：\nterragrunt graph-dependencies | dot -Tpng \u003e graph.png ( 這個 dot 指令是另外的套件，會將關係圖程式碼轉成圖檔，請先安裝 brew install graphviz )\n關聯圖","terragrunt-安裝方式#Terragrunt 安裝方式":"那要怎麼使用 Terragrunt 呢！？\n第一步當然是安裝它囉，我們系統是 macOS，所以我們安裝方式是 Homebrew 來進行安裝：\nbrew install terragrunt 接著我們的指令會從 terraform XXX 變成以下：\nterragrunt plan terragrunt apply terragrunt output terragrunt destroy Terragrunt 會將所有命令、參數和選項直接轉發到 Terraform。(所以我們也需要下載 Terraform)\nTerragrunt 的預設檔案名稱是 terragrunt.hcl ，Terragrunt 的設定檔案基本上與 Module 差不多，只是有更多更方便的變數可以使用。","參考資料#參考資料":"事半功倍 — 使用 Terragrunt 搭配 Terraform 管理基礎設置：https://medium.com/act-as-a-software-engineer/%E4%BA%8B%E5%8D%8A%E5%8A%9F%E5%80%8D-%E4%BD%BF%E7%94%A8-terragrunt-%E6%90%AD%E9%85%8D-terraform-%E7%AE%A1%E7%90%86%E5%9F%BA%E7%A4%8E%E8%A8%AD%E6%96%BD-f70c30166639"},"title":"如何導入 Terragrunt，Terragrunt 好處是什麼？"},"/projects/":{"data":{"":" 活動申請系統 (Web)幫學校開發的大型校務系統，方便系統社團使用電子填單方式申請活動，也讓之後的學弟妹，可以更快速的查看到歷屆辦理的活動。 巔峰極速 兌換虛寶網站 (Web)該遊戲有大量虛寶可以兌換，但官方提供的網頁，需要重複輸入 ID 以及驗證碼還有序號，因此寫了一個小網頁，可以只輸入一次 ID 以及驗證碼，就兌換完所有序號。 京緯工程有限公司官網 (Web)協助京緯工程有限公司建立公司官方網頁。 "},"title":"專案成就"}}