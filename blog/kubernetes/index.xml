<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>PIN-YI – Kubernetes</title><link>https://pin-yi.me/blog/kubernetes/</link><description>Recent content in Kubernetes on PIN-YI</description><generator>Hugo -- gohugo.io</generator><language>zh-tw</language><atom:link href="https://pin-yi.me/blog/kubernetes/index.xml" rel="self" type="application/rss+xml"/><item><title>正式環境上踩到 StatefulSet 的雷，拿到 P1 的教訓</title><link>https://pin-yi.me/blog/kubernetes/k8s-statefulset-podmanagementpolicy/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pin-yi.me/blog/kubernetes/k8s-statefulset-podmanagementpolicy/</guid><description>
&lt;p>此文章要來記錄一下前陣子在公司的正式環境踩到 StatefulSet 的雷，事情是這樣的，我們有些服務，是使用 StatefulSet 來建置，至於為什麼不用 Deployment，這個說來話長 (也不是因為需要特定的 Pod 名稱、或是網路標記等等)，我們這邊先不討論，這個 StatefulSet 服務是 Nginx + PHP-FPM，為了避免流量進入到 processes 已經被用光的 Pod 中，我們在 StatefulSet 的 PHP Container 上有設定 readiness，readiness 的設定長得像以下：&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;div class="code-block relative mt-6 first:mt-0 group/code">&lt;div>&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">readinessProbe&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">exec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">command&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#e6db74">&amp;#34;/bin/bash&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#e6db74">&amp;#34;-c&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - |&lt;span style="color:#e6db74">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> CHECK_INFO=$(curl -s -w &amp;#39;http code:\t%{http_code}\n&amp;#39; 127.0.0.1/status)
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> HTTP_CODE=$(echo -e &amp;#34;${CHECK_INFO}&amp;#34; | awk &amp;#39;/http code:/ {print $3}&amp;#39;)
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> IDLE_PROCESSES=$(echo -e &amp;#34;${CHECK_INFO}&amp;#34; | awk &amp;#39;/idle processes:/ {print $3}&amp;#39;)
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> [[ $HTTP_CODE -eq 200 &amp;amp;&amp;amp; $IDLE_PROCESSES -ge 10 ]] || exit 1&lt;/span> &lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/div>&lt;div class="opacity-0 transition group-hover/code:opacity-100 flex gap-1 absolute m-[11px] right-0 top-0">
&lt;button
class="code-copy-btn group/copybtn transition-all active:opacity-50 bg-primary-700/5 border border-black/5 text-gray-600 hover:text-gray-900 rounded-md p-1.5 dark:bg-primary-300/10 dark:border-white/10 dark:text-gray-400 dark:hover:text-gray-50"
title="Copy code"
>
&lt;div class="group-[.copied]/copybtn:hidden copy-icon pointer-events-none h-4 w-4">&lt;/div>
&lt;div class="hidden group-[.copied]/copybtn:block success-icon pointer-events-none h-4 w-4">&lt;/div>
&lt;/button>
&lt;/div>
&lt;/div>
&lt;!-- raw HTML omitted -->
&lt;p>我們會用 curl 來打 &lt;code>/status&lt;/code>，檢查回傳的 http code 是否為 200，以及 idle processes 是否大於等於 10，如果不符合，就會回傳 1，讓他被標記不健康，讓 Kubernetes 停止流量到不健康的容器，以確保流量被路由到其他健康的副本。&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;h2>問題&lt;span class="absolute -mt-20" id="問題">&lt;/span>
&lt;a href="#%e5%95%8f%e9%a1%8c" class="subheading-anchor" aria-label="Permalink for this section">&lt;/a>&lt;/h2>&lt;p>當天遇到的情況是，我們上程式後，Pod 都一切正常，當流量開始進來後，發現 10 個 Pod 會開始偶發的噴 &lt;code>Readiness probe failed&lt;/code>，查看監控發現 processes 越來越低，最後被反應服務有問題，我們查看 Hpa 的紀錄的確有觸發到 40 個 Pod，只是查看 Pod 數還是依樣卡在 10 個，當下我們有嘗試使用調整 yaml 在 apply，發現 StatefulSet 的 yaml 也已經更新了，但 Pod 還是一樣卡在 10 個，也有使用 kubectl 下 &lt;code>kubectl scale sts [服務名稱] --replicas=0&lt;/code>，想要切換 Pod 數也沒有辦法。&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;p>當下我們有先 Call Google 的 Support 一起找原因，Google 是建議我們 readiness 的條件不要設的太嚴格，可以加上 &lt;code>timeoutSeconds: 秒數&lt;/code>，但對於 Pod 卡住，還是沒有找到原因，後來我們查了一下 StatefulSet 的文件發現，StatefulSet 有一個設定 &lt;code>podManagementPolicy&lt;/code>，預設是 &lt;code>OrderedReady&lt;/code>，他必須等待前面的 Pod 是 Ready 狀態，才會再繼續建立新的，也就是說我們的 StatefulSet 已經卡住，導致就算 Hpa 觸發要長到 40 個 Pod 也沒有用。&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;h2>解決辦法&lt;span class="absolute -mt-20" id="解決辦法">&lt;/span>
&lt;a href="#%e8%a7%a3%e6%b1%ba%e8%be%a6%e6%b3%95" class="subheading-anchor" aria-label="Permalink for this section">&lt;/a>&lt;/h2>&lt;p>當下想趕快解決 readiness 這個問題，調整 &lt;code>timeoutSeconds&lt;/code> 後，單純 apply 是沒有用的，要記得刪掉卡住的 Pod，讓他重新建立，才會套用新的設定 (但我們當下太在意為甚麼 Pod 會卡住，沒有想到要先把 readiness 問題修掉 xD，我們當下的解法是先將流量導到地端正常的服務上)。&lt;/p>
&lt;p>另外 Google 也說，假如我們還是必須使用 StatefulSet 來建立服務，建議我們把 podManagementPolicy 改成 &lt;code>Parallel&lt;/code>，它會有點像是 Deployment 的感覺，不會等待其他 Pod 變成 Ready 狀態，所以可以讓我們就算在 readiness 卡住的情況下，也可以自動擴縮服務。&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;div class="overflow-x-auto mt-6 flex rounded-lg border py-2 ltr:pr-4 rtl:pl-4 contrast-more:border-current contrast-more:dark:border-current border-orange-100 bg-orange-50 text-orange-800 dark:border-orange-400/30 dark:bg-orange-400/20 dark:text-orange-300">
&lt;div class="ltr:pl-3 ltr:pr-2 rtl:pr-3 rtl:pl-2">&lt;/div>
&lt;div class="w-full min-w-0 leading-7">
&lt;div class="mt-6 leading-7 first:mt-0">
&lt;p>StatefulSet podManagementPolicy 參數說明&lt;/p>
&lt;ul>
&lt;li>OrderedReady (預設)&lt;/li>
&lt;/ul>
&lt;p>Pods 會按照順序一個接一個地被創建。即，n+1 號 Pod 不會在 n 號 Pod 成功創建且 Ready 之前開始創建。
在縮小 StatefulSet 的大小時，Pods 會按照反向順序一個接一個地被終止。即，n 號 Pod 不會在 n+1 號 Pod 完全終止之前開始終止。
這確保了 Pods 的啟動和終止的順序性。&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;ul>
&lt;li>Parallel&lt;/li>
&lt;/ul>
&lt;p>所有 Pods 會同時地被創建或終止。
當 StatefulSet 擴展時，新的 Pods 會立即開始創建，不用等待其他 Pods 成為 Ready 狀態。
當縮小 StatefulSet 的大小時，要終止的 Pods 會立即開始終止，不用等待其他 Pods 先終止。
這種策略提供了快速的擴展和縮小操作，但缺乏順序性保證。&lt;/p>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;!-- raw HTML omitted -->
&lt;h2>測試結果&lt;span class="absolute -mt-20" id="測試結果">&lt;/span>
&lt;a href="#%e6%b8%ac%e8%a9%a6%e7%b5%90%e6%9e%9c" class="subheading-anchor" aria-label="Permalink for this section">&lt;/a>&lt;/h2>&lt;p>最後我們就使用兩種模式來測試看看，已下是測試結果(透過 P1 才知道的設定ＱＱ)：&lt;/p>
&lt;p>有將測試的 StatefulSet 放在 Github，&lt;a href="https://github.com/880831ian/k8s-statefulset-podmanagementpolicy" target="_blank" rel="noopener">可以點我查看&lt;/a> (可以調整 readinessProbe 的 httpGet.Path 故意把他用壞)&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;h3>使用 OrderedReady 模式&lt;span class="absolute -mt-20" id="使用-orderedready-模式">&lt;/span>
&lt;a href="#%e4%bd%bf%e7%94%a8-orderedready-%e6%a8%a1%e5%bc%8f" class="subheading-anchor" aria-label="Permalink for this section">&lt;/a>&lt;/h3>&lt;p>StatefulSet 在 podManagementPolicy 預設 OrderedReady 的模式，故意讓 readiness 卡住時 (Pod 卡住時)：&lt;/p>
&lt;ul>
&lt;li>當下的 StatefulSet 設定：&lt;/li>
&lt;/ul>
&lt;!-- raw HTML omitted -->
&lt;p>&lt;figure>
&lt;img src="https://pin-yi.me/kubernetes/k8s-statefulset-podmanagementpolicy/1.png" title="StatefulSet 設定" alt="" loading="lazy" />
&lt;figcaption>StatefulSet 設定&lt;/figcaption>
&lt;/figure>&lt;/p>
&lt;ul>
&lt;li>Pod 狀態：&lt;/li>
&lt;/ul>
&lt;!-- raw HTML omitted -->
&lt;p>&lt;figure>
&lt;img src="https://pin-yi.me/kubernetes/k8s-statefulset-podmanagementpolicy/2.png" title="Pod 狀態" alt="" loading="lazy" />
&lt;figcaption>Pod 狀態&lt;/figcaption>
&lt;/figure>&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;h4>使用指令調整 Pod 數量&lt;span class="absolute -mt-20" id="使用指令調整-pod-數量">&lt;/span>
&lt;a href="#%e4%bd%bf%e7%94%a8%e6%8c%87%e4%bb%a4%e8%aa%bf%e6%95%b4-pod-%e6%95%b8%e9%87%8f" class="subheading-anchor" aria-label="Permalink for this section">&lt;/a>&lt;/h4>&lt;p>我們這時候下指令調整 Pod 數量，看看會發生什麼事：&lt;/p>
&lt;div class="code-block relative mt-6 first:mt-0 group/code">&lt;pre>&lt;code>kubectl scale sts my-statefulset --replicas=5&lt;/code>&lt;/pre>&lt;div class="opacity-0 transition group-hover/code:opacity-100 flex gap-1 absolute m-[11px] right-0 top-0">
&lt;button
class="code-copy-btn group/copybtn transition-all active:opacity-50 bg-primary-700/5 border border-black/5 text-gray-600 hover:text-gray-900 rounded-md p-1.5 dark:bg-primary-300/10 dark:border-white/10 dark:text-gray-400 dark:hover:text-gray-50"
title="Copy code"
>
&lt;div class="group-[.copied]/copybtn:hidden copy-icon pointer-events-none h-4 w-4">&lt;/div>
&lt;div class="hidden group-[.copied]/copybtn:block success-icon pointer-events-none h-4 w-4">&lt;/div>
&lt;/button>
&lt;/div>
&lt;/div>
&lt;!-- raw HTML omitted -->
&lt;p>我們先看 StatefulSet 的 yaml 可以看到 Pod replicas 已經改變，也可以看 generation 有更新，代表 StatefulSet 本身有接收到調整設定的請求。&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;p>&lt;figure>
&lt;img src="https://pin-yi.me/kubernetes/k8s-statefulset-podmanagementpolicy/3.png" title="下指令調整後的 StatefulSet 設定" alt="" loading="lazy" />
&lt;figcaption>下指令調整後的 StatefulSet 設定&lt;/figcaption>
&lt;/figure>&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;p>看了一下 Pod 數量，也是一樣卡住，且 Pod 數量也沒有變化。&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;p>&lt;figure>
&lt;img src="https://pin-yi.me/kubernetes/k8s-statefulset-podmanagementpolicy/4.png" title="下指令調整後的 Pod 狀態" alt="" loading="lazy" />
&lt;figcaption>下指令調整後的 Pod 狀態&lt;/figcaption>
&lt;/figure>&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;h4>使用 yaml 調整 Pod 數量&lt;span class="absolute -mt-20" id="使用-yaml-調整-pod-數量">&lt;/span>
&lt;a href="#%e4%bd%bf%e7%94%a8-yaml-%e8%aa%bf%e6%95%b4-pod-%e6%95%b8%e9%87%8f" class="subheading-anchor" aria-label="Permalink for this section">&lt;/a>&lt;/h4>&lt;p>我們直接調整 StatefulSet yaml 的 Pod 數量，看看會發生什麼事：&lt;/p>
&lt;p>一樣我們先看 StatefulSet 的 yaml 可以看到 Pod replicas 已經改變(這裡應該切別的 Pod 數量，切回 3 個好像沒有意義 xD)，也可以看 generation 有更新。&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;p>&lt;figure>
&lt;img src="https://pin-yi.me/kubernetes/k8s-statefulset-podmanagementpolicy/5.png" title="使用 yaml 調整後的 StatefulSet 設定" alt="" loading="lazy" />
&lt;figcaption>使用 yaml 調整後的 StatefulSet 設定&lt;/figcaption>
&lt;/figure>&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;p>看了一下 Pod 數量，也是一樣卡住，且 Pod 數量也沒有變化。&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;p>&lt;figure>
&lt;img src="https://pin-yi.me/kubernetes/k8s-statefulset-podmanagementpolicy/6.png" title="使用 yaml 調整後的 Pod 狀態" alt="" loading="lazy" />
&lt;figcaption>使用 yaml 調整後的 Pod 狀態&lt;/figcaption>
&lt;/figure>&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;p>所以代表在 OrderedReady 的模式下，Pod 卡住時，無法對 Pod 進行任何操作，必須要手動刪除卡住的 Pod 才吃得到最新的設定。&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;h3>使用 Parallel 模式&lt;span class="absolute -mt-20" id="使用-parallel-模式">&lt;/span>
&lt;a href="#%e4%bd%bf%e7%94%a8-parallel-%e6%a8%a1%e5%bc%8f" class="subheading-anchor" aria-label="Permalink for this section">&lt;/a>&lt;/h3>&lt;p>StatefulSet 在 podManagementPolicy Parallel 的模式，故意讓 readiness 卡住時 (Pod 卡住時)：&lt;/p>
&lt;ul>
&lt;li>當下的 StatefulSet 設定：&lt;/li>
&lt;/ul>
&lt;!-- raw HTML omitted -->
&lt;p>&lt;figure>
&lt;img src="https://pin-yi.me/kubernetes/k8s-statefulset-podmanagementpolicy/7.png" title="StatefulSet 設定" alt="" loading="lazy" />
&lt;figcaption>StatefulSet 設定&lt;/figcaption>
&lt;/figure>&lt;/p>
&lt;ul>
&lt;li>Pod 狀態：&lt;/li>
&lt;/ul>
&lt;!-- raw HTML omitted -->
&lt;p>&lt;figure>
&lt;img src="https://pin-yi.me/kubernetes/k8s-statefulset-podmanagementpolicy/8.png" title="Pod 狀態" alt="" loading="lazy" />
&lt;figcaption>Pod 狀態&lt;/figcaption>
&lt;/figure>&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;h4>使用指令調整 Pod 數量&lt;span class="absolute -mt-20" id="使用指令調整-pod-數量-1">&lt;/span>
&lt;a href="#%e4%bd%bf%e7%94%a8%e6%8c%87%e4%bb%a4%e8%aa%bf%e6%95%b4-pod-%e6%95%b8%e9%87%8f-1" class="subheading-anchor" aria-label="Permalink for this section">&lt;/a>&lt;/h4>&lt;p>我們這時候下指令調整 Pod 數量，看看會發生什麼事：&lt;/p>
&lt;div class="code-block relative mt-6 first:mt-0 group/code">&lt;pre>&lt;code>kubectl scale sts my-statefulset --replicas=5&lt;/code>&lt;/pre>&lt;div class="opacity-0 transition group-hover/code:opacity-100 flex gap-1 absolute m-[11px] right-0 top-0">
&lt;button
class="code-copy-btn group/copybtn transition-all active:opacity-50 bg-primary-700/5 border border-black/5 text-gray-600 hover:text-gray-900 rounded-md p-1.5 dark:bg-primary-300/10 dark:border-white/10 dark:text-gray-400 dark:hover:text-gray-50"
title="Copy code"
>
&lt;div class="group-[.copied]/copybtn:hidden copy-icon pointer-events-none h-4 w-4">&lt;/div>
&lt;div class="hidden group-[.copied]/copybtn:block success-icon pointer-events-none h-4 w-4">&lt;/div>
&lt;/button>
&lt;/div>
&lt;/div>
&lt;!-- raw HTML omitted -->
&lt;p>我們先看 StatefulSet 的 yaml 可以看到 Pod replicas 已經改變，也可以看 generation 有更新，代表 StatefulSet 本身有接收到調整設定的請求。&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;p>&lt;figure>
&lt;img src="https://pin-yi.me/kubernetes/k8s-statefulset-podmanagementpolicy/9.png" title="下指令調整後的 StatefulSet 設定" alt="" loading="lazy" />
&lt;figcaption>下指令調整後的 StatefulSet 設定&lt;/figcaption>
&lt;/figure>&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;p>看了一下 Pod 數量，就算 my-statefulset-2 卡住，還是可以擴到 5 個 Pod。&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;p>&lt;figure>
&lt;img src="https://pin-yi.me/kubernetes/k8s-statefulset-podmanagementpolicy/10.png" title="下指令調整後的 Pod 狀態" alt="" loading="lazy" />
&lt;figcaption>下指令調整後的 Pod 狀態&lt;/figcaption>
&lt;/figure>&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;h4>使用 yaml 調整 Pod 數量&lt;span class="absolute -mt-20" id="使用-yaml-調整-pod-數量-1">&lt;/span>
&lt;a href="#%e4%bd%bf%e7%94%a8-yaml-%e8%aa%bf%e6%95%b4-pod-%e6%95%b8%e9%87%8f-1" class="subheading-anchor" aria-label="Permalink for this section">&lt;/a>&lt;/h4>&lt;p>我們直接調整 StatefulSet yaml 的 Pod 數量，看看會發生什麼事：&lt;/p>
&lt;p>一樣我們先看 StatefulSet 的 yaml 可以看到 Pod replicas 已經改變，也可以看 generation 有更新。&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;p>&lt;figure>
&lt;img src="https://pin-yi.me/kubernetes/k8s-statefulset-podmanagementpolicy/11.png" title="使用 yaml 調整後的 StatefulSet 設定" alt="" loading="lazy" />
&lt;figcaption>使用 yaml 調整後的 StatefulSet 設定&lt;/figcaption>
&lt;/figure>&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;p>看了一下 Pod 數量，也不會管其他 Pod 是否 Ready，一樣可以縮小成 2 個 Pod。&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;p>&lt;figure>
&lt;img src="https://pin-yi.me/kubernetes/k8s-statefulset-podmanagementpolicy/12.png" title="使用 yaml 調整後的 Pod 狀態" alt="" loading="lazy" />
&lt;figcaption>使用 yaml 調整後的 Pod 狀態&lt;/figcaption>
&lt;/figure>&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;h2>結論&lt;span class="absolute -mt-20" id="結論">&lt;/span>
&lt;a href="#%e7%b5%90%e8%ab%96" class="subheading-anchor" aria-label="Permalink for this section">&lt;/a>&lt;/h2>&lt;p>後來我們重新檢查了一下為什麼 processes 會用完，結果發現是 RD 的程式邏輯，導致每筆 Request 必須等待前一筆 Request 做完，才會開始動作，讓 processes 一直被占用，沒辦法即時消化，導致 processes 用完，又加上服務是使用 StatefulSet，預設模式的 OrderedReady，必須等待前一個 Pod 是 Ready 才可以自動擴縮，所以當我們 Hpa 想要擴縮，來增加可用的 processes 數量，也因為沒辦法擴縮，最後導致這一連串的問題 😕。&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;p>另外，如果想要從 OrderedReady 模式切成 Parallel 模式 (反正過來也是)，必須先將原本的 StatefulSet 給刪除，才可以調整：&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;p>&lt;figure>
&lt;img src="https://pin-yi.me/kubernetes/k8s-statefulset-podmanagementpolicy/13.png" title="OrderedReady 模式切成 Parallel 模式" alt="" loading="lazy" />
&lt;figcaption>OrderedReady 模式切成 Parallel 模式&lt;/figcaption>
&lt;/figure>&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;h2>參考資料&lt;span class="absolute -mt-20" id="參考資料">&lt;/span>
&lt;a href="#%e5%8f%83%e8%80%83%e8%b3%87%e6%96%99" class="subheading-anchor" aria-label="Permalink for this section">&lt;/a>&lt;/h2>&lt;p>Kubernetes — 健康檢查：&lt;a href="https://medium.com/learn-or-die/kubernetes-%E5%81%A5%E5%BA%B7%E6%AA%A2%E6%9F%A5-59ee2a798115" target="_blank" rel="noopener">https://medium.com/learn-or-die/kubernetes-%E5%81%A5%E5%BA%B7%E6%AA%A2%E6%9F%A5-59ee2a798115&lt;/a>&lt;/p>
&lt;p>Pod Management Policies：&lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#pod-management-policies" target="_blank" rel="noopener">https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#pod-management-policies&lt;/a>&lt;/p></description></item><item><title>部署 Pod 遇到 container veth name provided (eth0) already exists 錯誤</title><link>https://pin-yi.me/blog/kubernetes/pod-veth-name-provided-eth0-already-exists/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pin-yi.me/blog/kubernetes/pod-veth-name-provided-eth0-already-exists/</guid><description>
&lt;p>此文章要來記錄一下公司同事在正式服務上遇到的問題，會詳細說明遇到事情的經過，以及開單詢問 google support 最後討論出的暫時解決的辦法：&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;p>簡單列出正式站當下服務環境：&lt;/p>
&lt;ul>
&lt;li>gke master version：1.25.10-gke.2700&lt;/li>
&lt;li>gke node version：1.25.8-gke.1000&lt;/li>
&lt;li>該問題發生的 node pool 有設定 taint&lt;/li>
&lt;li>發生問題的 Pod 是用 Statefulset 建立的服務&lt;/li>
&lt;/ul>
&lt;!-- raw HTML omitted -->
&lt;h2>事情發生的經過&lt;span class="absolute -mt-20" id="事情發生的經過">&lt;/span>
&lt;a href="#%e4%ba%8b%e6%83%85%e7%99%bc%e7%94%9f%e7%9a%84%e7%b6%93%e9%81%8e" class="subheading-anchor" aria-label="Permalink for this section">&lt;/a>&lt;/h2>&lt;ol>
&lt;li>RD 同仁反應，發現使用 Statefulset 建立的排程服務有問題，下 &lt;code>kubectl delete&lt;/code> 指令想要刪除 Pod，讓 Pod 重新長，卻卡在 Terminating，等待一段時間後，決定下 &lt;code>kubectl delete --force --grace-period=0&lt;/code> 來強制刪除 Pod，這時候狀態會卡在 ContainerCreating，使用 Describe 查看，會出現以下錯誤：&lt;/li>
&lt;/ol>
&lt;!-- raw HTML omitted -->
&lt;div class="code-block relative mt-6 first:mt-0 group/code">&lt;pre>&lt;code>Warning
(combined from similar events): Failed to create pod sandbox: rpo error: code = Unknown desc = failed to setup network for sandbox
&amp;#34;14fe0cd3d688aed4ffed4c36ffab1a145230449881bcbe4cac6478a63412b0c*: plugin type=*gke&amp;#34; failed (add): container veth name provided (etho) already exists&lt;/code>&lt;/pre>&lt;div class="opacity-0 transition group-hover/code:opacity-100 flex gap-1 absolute m-[11px] right-0 top-0">
&lt;button
class="code-copy-btn group/copybtn transition-all active:opacity-50 bg-primary-700/5 border border-black/5 text-gray-600 hover:text-gray-900 rounded-md p-1.5 dark:bg-primary-300/10 dark:border-white/10 dark:text-gray-400 dark:hover:text-gray-50"
title="Copy code"
>
&lt;div class="group-[.copied]/copybtn:hidden copy-icon pointer-events-none h-4 w-4">&lt;/div>
&lt;div class="hidden group-[.copied]/copybtn:block success-icon pointer-events-none h-4 w-4">&lt;/div>
&lt;/button>
&lt;/div>
&lt;/div>
&lt;!-- raw HTML omitted -->
&lt;ol start="2">
&lt;li>我們 SRE 協助查看後，也有嘗試去下 &lt;code>kubectl delete --force --grace-period=0&lt;/code> 來刪除 Pod，但還是一樣卡在 ContainerCreating，最後是先開一個新的 Node 並讓該 Pod 建立到新的 Node 上，才解決問題。為了方便 google support 協助檢查出問題的 Node，先將 Node 設定成 cordon，避免其他 Pod 被調度到該問題 node 上。&lt;/li>
&lt;/ol>
&lt;!-- raw HTML omitted -->
&lt;div class="overflow-x-auto mt-6 flex rounded-lg border py-2 ltr:pr-4 rtl:pl-4 contrast-more:border-current contrast-more:dark:border-current border-orange-100 bg-orange-50 text-orange-800 dark:border-orange-400/30 dark:bg-orange-400/20 dark:text-orange-300">
&lt;div class="ltr:pl-3 ltr:pr-2 rtl:pr-3 rtl:pl-2">&lt;/div>
&lt;div class="w-full min-w-0 leading-7">
&lt;div class="mt-6 leading-7 first:mt-0">
&lt;p>Node 設定成 cordon&lt;/p>
&lt;p>Node 可以設定 cordon、drain 和 delete 三個指定都會使 Node 停止被調度，只是每個的操作暴力程度不同：&lt;/p>
&lt;p>cordon：影響最小，只會將 Node 標示為 SchedulingDisabled 不可調度狀態，但不會影響到已經在該 Node 上的 Pod，使用 &lt;code>kubectl cordon [node name]&lt;/code> 來停止調度，使用 &lt;code>kubectl uncordon [node name]&lt;/code> 來恢復調度。&lt;/p>
&lt;p>drain：會先驅逐在 Node 上的 Pod，再將 Node 標示為 SchedulingDisabled 不可調度狀態，使用 &lt;code>kubectl drain [node name] --ignore-daemonsets --delete-local-data&lt;/code> 來停止調度，使用 &lt;code>kubectl uncordon [node name]&lt;/code> 來恢復調度。&lt;/p>
&lt;p>delete：會先驅逐 Node 上的 Pod，再刪除 Node 節點，它是一種暴力刪除 Node 的作法，在驅逐 Pod 時，會強制 Kill 容器進程，沒辦法優雅的終止 Pod。&lt;/p>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;!-- raw HTML omitted -->
&lt;ol start="3">
&lt;li>我們隨後開單詢問 goolge support。&lt;/li>
&lt;/ol>
&lt;!-- raw HTML omitted -->
&lt;h2>與 Google Support 討論內容&lt;span class="absolute -mt-20" id="與-google-support-討論內容">&lt;/span>
&lt;a href="#%e8%88%87-google-support-%e8%a8%8e%e8%ab%96%e5%85%a7%e5%ae%b9" class="subheading-anchor" aria-label="Permalink for this section">&lt;/a>&lt;/h2>&lt;p>Google Support 經過查詢後，回覆說：這個問題是因為 Pod 被強制刪除導致，強制刪除是一種危險的操作，不建議這樣處理，下面有詳細討論。&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;ol>
&lt;li>
&lt;p>一開始卡在 Terminating 狀態，我們也有請 RD 說明一下當下遇到的問題以及處理動作：RD 當時想要刪除 Pod 是因為該程式當下有 Bug，將 redis 與 db 連線給關閉，程式找不到就會一直 retry，導致相關進程無法結束，再加上 terminationGracePeriodSeconds 我們設定 14400，也就是 4 小時，才會卡在 Terminating 狀態。
(terminationGracePeriodSeconds 設定這麼久是希望如果有被 on call，工程師上來時，可以查看該 Pod 的錯誤原因)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>因為卡在 Terminating 太久，RD 有執行 &lt;code>kubectl delete --force&lt;/code>，就是因為下了 &lt;code>--force&lt;/code> 才造成相關資源問題 (例如 container proccess, sandbox, 以及網路資源)沒有刪乾淨。所以引起了此次的報錯 &amp;ldquo;container veth name provided (eth0) already exists&amp;rdquo;。
(因為我們服務使用 Statefulset，Pod 名稱相同，導致 eth0 這個網路資源名稱重複，所以造成錯誤，可以用 deployment 來改善這個問題，只是資源如果沒有清理乾淨會佔用 IP，所以單純調整成 deployment 也不是最佳解)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Google 產品團隊建議，如果 Pod 處於 Running 狀態時，想要快速刪除 Pod 時，一開始就先使用 &lt;code>kubectl delete pod --grace-period=number[秒數]&lt;/code> 來刪除，如果已經是 Terminating 狀態則無效。(SRE 同仁已測試過，與 Google Support 說明相同)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>那如果已經處於 Terminating 狀態，要怎麽讓 Pod 被順利刪除，這部分 Google Support 後續會在測試並給出建議，目前測試是：進去卡住的 Pod Container，手動刪除主進程 (pkill) 就可以了。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;!-- raw HTML omitted -->
&lt;p>&lt;figure>
&lt;img src="https://pin-yi.me/kubernetes/pod-veth-name-provided-eth0-already-exists/1.png" title="Google Support 回覆" alt="" loading="lazy" />
&lt;figcaption>Google Support 回覆&lt;/figcaption>
&lt;/figure>&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted -->
&lt;h2>參考&lt;span class="absolute -mt-20" id="參考">&lt;/span>
&lt;a href="#%e5%8f%83%e8%80%83" class="subheading-anchor" aria-label="Permalink for this section">&lt;/a>&lt;/h2>&lt;p>Node 節點禁止調度（平滑維護）方式- cordon，drain，delete：&lt;a href="https://www.cnblogs.com/kevingrace/p/14412254.html" target="_blank" rel="noopener">https://www.cnblogs.com/kevingrace/p/14412254.html&lt;/a>&lt;/p></description></item></channel></rss>